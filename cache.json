{"2025-02-13T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.09623v1","updated":"2025-02-13T18:59:50Z","published":"2025-02-13T18:59:50Z","title":"Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF\n  Architectures","summary":"  Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for\nrepresenting 3D objects and scenes by encoding shape and appearance information\ninto the weights of a neural network. Recent works have shown how such weights\ncan be used as input to frameworks processing them to solve deep learning\ntasks. Yet, these frameworks can only process NeRFs with a specific, predefined\narchitecture. In this paper, we present the first framework that can ingest\nNeRFs with multiple architectures and perform inference on architectures unseen\nat training time. We achieve this goal by training a Graph Meta-Network in a\nrepresentation learning framework. Moreover, we show how a contrastive\nobjective is conducive to obtaining an architecture-agnostic latent space. In\nexperiments on both MLP-based and tri-planar NeRFs, our approach demonstrates\nrobust performance in classification and retrieval tasks that either matches or\nexceeds that of existing frameworks constrained to single architectures, thus\nproviding the first architecture-agnostic method to perform tasks on NeRFs by\nprocessing their weights.\n","authors":["Francesco Ballerini","Pierluigi Zama Ramirez","Samuele Salti","Luigi Di Stefano"],"pdf_url":"https://arxiv.org/pdf/2502.09623v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.09621v1","updated":"2025-02-13T18:59:46Z","published":"2025-02-13T18:59:46Z","title":"MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for\n  Reasoning Quality, Robustness, and Efficiency","summary":"  Answering questions with Chain-of-Thought (CoT) has significantly enhanced\nthe reasoning capabilities of Large Language Models (LLMs), yet its impact on\nLarge Multimodal Models (LMMs) still lacks a systematic assessment and in-depth\ninvestigation. In this paper, we introduce MME-CoT, a specialized benchmark\nevaluating the CoT reasoning performance of LMMs, spanning six domains: math,\nscience, OCR, logic, space-time, and general scenes. As the first comprehensive\nstudy in this area, we propose a thorough evaluation suite incorporating three\nnovel metrics that assess the reasoning quality, robustness, and efficiency at\na fine-grained level. Leveraging curated high-quality data and a unique\nevaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,\nuncovering several key insights: 1) Models with reflection mechanism\ndemonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and\ndemonstrating the highest quality results; 2) CoT prompting often degrades LMM\nperformance on perception-heavy tasks, suggesting a potentially harmful\noverthinking behavior; and 3) Although the CoT quality is high, LMMs with\nreflection exhibit significant inefficiency in both normal response and\nself-correction phases. We hope MME-CoT serves as a foundation for advancing\nmultimodal reasoning in LMMs. Project Page: https://mmecot.github.io/\n","authors":["Dongzhi Jiang","Renrui Zhang","Ziyu Guo","Yanwei Li","Yu Qi","Xinyan Chen","Liuhui Wang","Jianhan Jin","Claire Guo","Shen Yan","Bo Zhang","Chaoyou Fu","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2502.09621v1.pdf","comment":"Project Page: https://mmecot.github.io/"},{"id":"http://arxiv.org/abs/2502.09620v1","updated":"2025-02-13T18:59:45Z","published":"2025-02-13T18:59:45Z","title":"Exploring the Potential of Encoder-free Architectures in 3D LMMs","summary":"  Encoder-free architectures have been preliminarily explored in the 2D visual\ndomain, yet it remains an open question whether they can be effectively applied\nto 3D understanding scenarios. In this paper, we present the first\ncomprehensive investigation into the potential of encoder-free architectures to\novercome the challenges of encoder-based 3D Large Multimodal Models (LMMs).\nThese challenges include the failure to adapt to varying point cloud\nresolutions and the point features from the encoder not meeting the semantic\nneeds of Large Language Models (LLMs). We identify key aspects for 3D LMMs to\nremove the encoder and enable the LLM to assume the role of the 3D encoder: 1)\nWe propose the LLM-embedded Semantic Encoding strategy in the pre-training\nstage, exploring the effects of various point cloud self-supervised losses. And\nwe present the Hybrid Semantic Loss to extract high-level semantics. 2) We\nintroduce the Hierarchical Geometry Aggregation strategy in the instruction\ntuning stage. This incorporates inductive bias into the LLM early layers to\nfocus on the local details of the point clouds. To the end, we present the\nfirst Encoder-free 3D LMM, ENEL. Our 7B model rivals the current\nstate-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the\nclassification, captioning, and VQA tasks, respectively. Our results\ndemonstrate that the encoder-free architecture is highly promising for\nreplacing encoder-based architectures in the field of 3D understanding. The\ncode is released at https://github.com/Ivan-Tang-3D/ENEL\n","authors":["Yiwen Tang","Zoey Guo","Zhuhao Wang","Ray Zhang","Qizhi Chen","Junli Liu","Delin Qu","Zhigang Wang","Dong Wang","Xuelong Li","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.09620v1.pdf","comment":"The code is released at https://github.com/Ivan-Tang-3D/ENEL"},{"id":"http://arxiv.org/abs/2502.09619v1","updated":"2025-02-13T18:59:44Z","published":"2025-02-13T18:59:44Z","title":"Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights","summary":"  With the increasing numbers of publicly available models, there are probably\npretrained, online models for most tasks users require. However, current model\nsearch methods are rudimentary, essentially a text-based search in the\ndocumentation, thus users cannot find the relevant models. This paper presents\nProbeLog, a method for retrieving classification models that can recognize a\ntarget concept, such as \"Dog\", without access to model metadata or training\ndata. Differently from previous probing methods, ProbeLog computes a descriptor\nfor each output dimension (logit) of each model, by observing its responses on\na fixed set of inputs (probes). Our method supports both logit-based retrieval\n(\"find more logits like this\") and zero-shot, text-based retrieval (\"find all\nlogits corresponding to dogs\"). As probing-based representations require\nmultiple costly feedforward passes through the model, we develop a method,\nbased on collaborative filtering, that reduces the cost of encoding\nrepositories by 3x. We demonstrate that ProbeLog achieves high retrieval\naccuracy, both in real-world and fine-grained search tasks and is scalable to\nfull-size repositories.\n","authors":["Jonathan Kahana","Or Nathan","Eliahu Horwitz","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2502.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09617v1","updated":"2025-02-13T18:59:19Z","published":"2025-02-13T18:59:19Z","title":"LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback\n  Over Multi-Resolution Gaussians-on-Mesh","summary":"  Generalizable rendering of an animatable human avatar from sparse inputs\nrelies on data priors and inductive biases extracted from training on large\ndata to avoid scene-specific optimization and to enable fast reconstruction.\nThis raises two main challenges: First, unlike iterative gradient-based\nadjustment in scene-specific optimization, generalizable methods must\nreconstruct the human shape representation in a single pass at inference time.\nSecond, rendering is preferably computationally efficient yet of high\nresolution. To address both challenges we augment the recently proposed dual\nshape representation, which combines the benefits of a mesh and Gaussian\npoints, in two ways. To improve reconstruction, we propose an iterative\nfeedback update framework, which successively improves the canonical human\nshape representation during reconstruction. To achieve computationally\nefficient yet high-resolution rendering, we study a coupled-multi-resolution\nGaussians-on-Mesh representation. We evaluate the proposed approach on the\nchallenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an\nanimatable representation from sparse inputs in less than 1s, renders views\nwith 95.1FPS at $1024 \\times 1024$, and achieves PSNR/LPIPS*/FID of\n24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in\nrendering quality.\n","authors":["Jing Wen","Alexander G. Schwing","Shenlong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09617v1.pdf","comment":"ICLR 2025; Project page: https://wenj.github.io/LIFe-GoM/"},{"id":"http://arxiv.org/abs/2502.09616v1","updated":"2025-02-13T18:59:15Z","published":"2025-02-13T18:59:15Z","title":"Variational Rectified Flow Matching","summary":"  We study Variational Rectified Flow Matching, a framework that enhances\nclassic rectified flow matching by modeling multi-modal velocity vector-fields.\nAt inference time, classic rectified flow matching 'moves' samples from a\nsource distribution to the target distribution by solving an ordinary\ndifferential equation via integration along a velocity vector-field. At\ntraining time, the velocity vector-field is learnt by linearly interpolating\nbetween coupled samples one drawn from the source and one drawn from the target\ndistribution randomly. This leads to ''ground-truth'' velocity vector-fields\nthat point in different directions at the same location, i.e., the velocity\nvector-fields are multi-modal/ambiguous. However, since training uses a\nstandard mean-squared-error loss, the learnt velocity vector-field averages\n''ground-truth'' directions and isn't multi-modal. In contrast, variational\nrectified flow matching learns and samples from multi-modal flow directions. We\nshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variational\nrectified flow matching leads to compelling results.\n","authors":["Pengsheng Guo","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2502.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09614v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"DexTrack: Towards Generalizable Neural Tracking Control for Dexterous\n  Manipulation from Human References","summary":"  We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.\n","authors":["Xueyi Liu","Jianibieke Adalibieke","Qianwei Han","Yuzhe Qin","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2502.09614v1.pdf","comment":"Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/\n  Code: https://github.com/Meowuu7/DexTrack/ Video:\n  https://youtu.be/zru1Z-DaiWE"},{"id":"http://arxiv.org/abs/2502.09615v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets","summary":"  We present RigAnything, a novel autoregressive transformer-based model, which\nmakes 3D assets rig-ready by probabilistically generating joints, skeleton\ntopologies, and assigning skinning weights in a template-free manner. Unlike\nmost existing auto-rigging methods, which rely on predefined skeleton template\nand are limited to specific categories like humanoid, RigAnything approaches\nthe rigging problem in an autoregressive manner, iteratively predicting the\nnext joint based on the global input shape and the previous prediction. While\nautoregressive models are typically used to generate sequential data,\nRigAnything extends their application to effectively learn and represent\nskeletons, which are inherently tree structures. To achieve this, we organize\nthe joints in a breadth-first search (BFS) order, enabling the skeleton to be\ndefined as a sequence of 3D locations and the parent index. Furthermore, our\nmodel improves the accuracy of position prediction by leveraging diffusion\nmodeling, ensuring precise and consistent placement of joints within the\nhierarchy. This formulation allows the autoregressive model to efficiently\ncapture both spatial and hierarchical relationships within the skeleton.\nTrained end-to-end on both RigNet and Objaverse datasets, RigAnything\ndemonstrates state-of-the-art performance across diverse object types,\nincluding humanoids, quadrupeds, marine creatures, insects, and many more,\nsurpassing prior methods in quality, robustness, generalizability, and\nefficiency. Please check our website for more details:\nhttps://www.liuisabella.com/RigAnything.\n","authors":["Isabella Liu","Zhan Xu","Wang Yifan","Hao Tan","Zexiang Xu","Xiaolong Wang","Hao Su","Zifan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09615v1.pdf","comment":"Project page: https://www.liuisabella.com/RigAnything"},{"id":"http://arxiv.org/abs/2402.17767v2","updated":"2025-02-13T18:59:11Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Objects in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated objects as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-objects/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v2.pdf","comment":"Project webpage:\n  https://arjung128.github.io/opening-articulated-objects/"},{"id":"http://arxiv.org/abs/2502.09613v1","updated":"2025-02-13T18:59:09Z","published":"2025-02-13T18:59:09Z","title":"Latent Radiance Fields with 3D-aware 2D Representations","summary":"  Latent 3D reconstruction has shown great promise in empowering 3D semantic\nunderstanding and 3D generation by distilling 2D features into the 3D space.\nHowever, existing approaches struggle with the domain gap between 2D feature\nspace and 3D representations, resulting in degraded rendering performance. To\naddress this challenge, we propose a novel framework that integrates 3D\nawareness into the 2D latent space. The framework consists of three stages: (1)\na correspondence-aware autoencoding method that enhances the 3D consistency of\n2D latent representations, (2) a latent radiance field (LRF) that lifts these\n3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field\n(VAE-RF) alignment strategy that improves image decoding from the rendered 2D\nrepresentations. Extensive experiments demonstrate that our method outperforms\nthe state-of-the-art latent 3D reconstruction approaches in terms of synthesis\nperformance and cross-dataset generalizability across diverse indoor and\noutdoor scenes. To our knowledge, this is the first work showing the radiance\nfield representations constructed from 2D latent representations can yield\nphotorealistic 3D reconstruction performance.\n","authors":["Chaoyi Zhou","Xi Liu","Feng Luo","Siyu Huang"],"pdf_url":"https://arxiv.org/pdf/2502.09613v1.pdf","comment":"Accepted to ICLR 2025; Project page:\n  https://latent-radiance-field.github.io/LRF"},{"id":"http://arxiv.org/abs/2502.09611v1","updated":"2025-02-13T18:58:15Z","published":"2025-02-13T18:58:15Z","title":"Designing a Conditional Prior Distribution for Flow-Based Generative\n  Models","summary":"  Flow-based generative models have recently shown impressive performance for\nconditional generation tasks, such as text-to-image generation. However,\ncurrent methods transform a general unimodal noise distribution to a specific\nmode of the target data distribution. As such, every point in the initial\nsource distribution can be mapped to every point in the target distribution,\nresulting in long average paths. To this end, in this work, we tap into a\nnon-utilized property of conditional flow-based models: the ability to design a\nnon-trivial prior distribution. Given an input condition, such as a text\nprompt, we first map it to a point lying in data space, representing an\n``average\" data point with the minimal average distance to all data points of\nthe same conditional mode (e.g., class). We then utilize the flow matching\nformulation to map samples from a parametric distribution centered around this\npoint to the conditional target distribution. Experimentally, our method\nsignificantly improves training times and generation efficiency (FID, KID and\nCLIP alignment scores) compared to baselines, producing high quality samples\nusing fewer sampling steps.\n","authors":["Noam Issachar","Mohammad Salama","Raanan Fattal","Sagie Benaim"],"pdf_url":"https://arxiv.org/pdf/2502.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09608v1","updated":"2025-02-13T18:56:05Z","published":"2025-02-13T18:56:05Z","title":"Instance Segmentation of Scene Sketches Using Natural Image Priors","summary":"  Sketch segmentation involves grouping pixels within a sketch that belong to\nthe same object or instance. It serves as a valuable tool for sketch editing\ntasks, such as moving, scaling, or removing specific components. While image\nsegmentation models have demonstrated remarkable capabilities in recent years,\nsketches present unique challenges for these models due to their sparse nature\nand wide variation in styles. We introduce SketchSeg, a method for instance\nsegmentation of raster scene sketches. Our approach adapts state-of-the-art\nimage segmentation and object detection models to the sketch domain by\nemploying class-agnostic fine-tuning and refining segmentation masks using\ndepth cues. Furthermore, our method organizes sketches into sorted layers,\nwhere occluded instances are inpainted, enabling advanced sketch editing\napplications. As existing datasets in this domain lack variation in sketch\nstyles, we construct a synthetic scene sketch segmentation dataset featuring\nsketches with diverse brush strokes and varying levels of detail. We use this\ndataset to demonstrate the robustness of our approach and will release it to\npromote further research in the field.\n  Project webpage: https://sketchseg.github.io/sketch-seg/\n","authors":["Mia Tang","Yael Vinker","Chuan Yan","Lvmin Zhang","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2502.09608v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09598v1","updated":"2025-02-13T18:52:14Z","published":"2025-02-13T18:52:14Z","title":"GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for\n  Remote Sensing Image Analysis","summary":"  The continuous operation of Earth-orbiting satellites generates vast and\never-growing archives of Remote Sensing (RS) images. Natural language presents\nan intuitive interface for accessing, querying, and interpreting the data from\nsuch archives. However, existing Vision-Language Models (VLMs) are\npredominantly trained on web-scraped, noisy image-text data, exhibiting limited\nexposure to the specialized domain of RS. This deficiency results in poor\nperformance on RS-specific tasks, as commonly used datasets often lack\ndetailed, scientifically accurate textual descriptions and instead emphasize\nsolely on attributes like date and location. To bridge this critical gap, we\nintroduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and\nmulti-modal RS image analysis. GAIA comprises of 205,150 meticulously curated\nRS image-text pairs, representing a diverse range of RS modalities associated\nto different spatial resolutions. Unlike existing vision-language datasets in\nRS, GAIA specifically focuses on capturing a diverse range of RS applications,\nproviding unique information about environmental changes, natural disasters,\nand various other dynamic phenomena. The dataset provides a spatially and\ntemporally balanced distribution, spanning across the globe, covering the last\n25 years with a balanced temporal distribution of observations. GAIA's\nconstruction involved a two-stage process: (1) targeted web-scraping of images\nand accompanying text from reputable RS-related sources, and (2) generation of\nfive high-quality, scientifically grounded synthetic captions for each image\nusing carefully crafted prompts that leverage the advanced vision-language\ncapabilities of GPT-4o. Our extensive experiments, including fine-tuning of\nCLIP and BLIP2 models, demonstrate that GAIA significantly improves performance\non RS image classification, cross-modal retrieval and image captioning tasks.\n","authors":["Angelos Zavras","Dimitrios Michail","Xiao Xiang Zhu","Beg√ºm Demir","Ioannis Papoutsis"],"pdf_url":"https://arxiv.org/pdf/2502.09598v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.09573v1","updated":"2025-02-13T18:31:17Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09101v2","updated":"2025-02-13T18:20:14Z","published":"2024-11-14T00:18:04Z","title":"Heuristical Comparison of Vision Transformers Against Convolutional\n  Neural Networks for Semantic Segmentation on Remote Sensing Imagery","summary":"  Vision Transformers (ViT) have recently brought a new wave of research in the\nfield of computer vision. These models have performed particularly well in\nimage classification and segmentation. Research on semantic and instance\nsegmentation has accelerated with the introduction of the new architecture,\nwith over 80% of the top 20 benchmarks for the iSAID dataset based on either\nthe ViT architecture or the attention mechanism behind its success. This paper\nfocuses on the heuristic comparison of three key factors of using (or not\nusing) ViT for semantic segmentation of remote sensing aerial images on the\niSAID dataset. The experimental results observed during this research were\nanalyzed based on three objectives. First, we studied the use of a weighted\nfused loss function to maximize the mean Intersection over Union (mIoU) score\nand Dice score while minimizing entropy or class representation loss. Second,\nwe compared transfer learning on Meta's MaskFormer, a ViT-based semantic\nsegmentation model, against a generic UNet Convolutional Neural Network (CNN)\nbased on mIoU, Dice scores, training efficiency, and inference time. Third, we\nexamined the trade-offs between the two models in comparison to current\nstate-of-the-art segmentation models. We show that the novel combined weighted\nloss function significantly boosts the CNN model's performance compared to\ntransfer learning with ViT. The code for this implementation can be found at:\nhttps://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.\n","authors":["Ashim Dahal","Saydul Akbar Murad","Nick Rahimi"],"pdf_url":"https://arxiv.org/pdf/2411.09101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09564v1","updated":"2025-02-13T18:17:03Z","published":"2025-02-13T18:17:03Z","title":"Diffusing DeBias: a Recipe for Turning a Bug into a Feature","summary":"  Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data which, whenever containing strong\nspurious correlations between specific attributes and target labels, can result\nin unrecoverable biases in model predictions. Tackling these biases is crucial\nin improving model generalization and trust, especially in real-world\nscenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting\nas a plug-in for common methods in model debiasing while exploiting the\ninherent bias-learning tendency of diffusion models. Our approach leverages\nconditional diffusion models to generate synthetic bias-aligned images, used to\ntrain a bias amplifier model, to be further employed as an auxiliary method in\ndifferent unsupervised debiasing approaches. Our proposed method, which also\ntackles the common issue of training set memorization typical of this type of\ntech- niques, beats current state-of-the-art in multiple benchmark datasets by\nsignificant margins, demonstrating its potential as a versatile and effective\ntool for tackling dataset bias in deep learning applications.\n","authors":["Massimiliano Ciranni","Vito Paolo Pastore","Roberto Di Via","Enzo Tartaglione","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2502.09564v1.pdf","comment":"29 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2502.09563v1","updated":"2025-02-13T18:15:10Z","published":"2025-02-13T18:15:10Z","title":"Self-Calibrating Gaussian Splatting for Large Field of View\n  Reconstruction","summary":"  In this paper, we present a self-calibrating framework that jointly optimizes\ncamera parameters, lens distortion and 3D Gaussian representations, enabling\naccurate and efficient scene reconstruction. In particular, our technique\nenables high-quality scene reconstruction from Large field-of-view (FOV)\nimagery taken with wide-angle lenses, allowing the scene to be modeled from a\nsmaller number of images. Our approach introduces a novel method for modeling\ncomplex lens distortions using a hybrid network that combines invertible\nresidual networks with explicit grids. This design effectively regularizes the\noptimization process, achieving greater accuracy than conventional camera\nmodels. Additionally, we propose a cubemap-based resampling strategy to support\nlarge FOV images without sacrificing resolution or introducing distortion\nartifacts. Our method is compatible with the fast rasterization of Gaussian\nSplatting, adaptable to a wide variety of camera lens distortion, and\ndemonstrates state-of-the-art performance on both synthetic and real-world\ndatasets.\n","authors":["Youming Deng","Wenqi Xian","Guandao Yang","Leonidas Guibas","Gordon Wetzstein","Steve Marschner","Paul Debevec"],"pdf_url":"https://arxiv.org/pdf/2502.09563v1.pdf","comment":"Project Page: https://denghilbert.github.io/self-cali/"},{"id":"http://arxiv.org/abs/2501.04001v2","updated":"2025-02-13T18:14:33Z","published":"2025-01-07T18:58:54Z","title":"Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of\n  Images and Videos","summary":"  This work presents Sa2VA, the first unified model for dense grounded\nunderstanding of both images and videos. Unlike existing multi-modal large\nlanguage models, which are often limited to specific modalities and tasks,\nSa2VA supports a wide range of image and video tasks, including referring\nsegmentation and conversation, with minimal one-shot instruction tuning. Sa2VA\ncombines SAM-2, a foundation video segmentation model, with LLaVA, an advanced\nvision-language model, and unifies text, image, and video into a shared LLM\ntoken space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2\nin producing precise masks, enabling a grounded, multi-modal understanding of\nboth static and dynamic visual content. Additionally, we introduce Ref-SAV, an\nauto-labeled dataset containing over 72k object expressions in complex video\nscenes, designed to boost model performance. We also manually validate 2k video\nobjects in the Ref-SAV datasets to benchmark referring video object\nsegmentation in complex environments. Experiments show that Sa2VA achieves\nstate-of-the-art across multiple tasks, particularly in referring video object\nsegmentation, highlighting its potential for complex real-world applications.\n","authors":["Haobo Yuan","Xiangtai Li","Tao Zhang","Zilong Huang","Shilin Xu","Shunping Ji","Yunhai Tong","Lu Qi","Jiashi Feng","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04001v2.pdf","comment":"Project page: https://lxtgh.github.io/project/sa2va"},{"id":"http://arxiv.org/abs/2502.09560v1","updated":"2025-02-13T18:11:34Z","published":"2025-02-13T18:11:34Z","title":"EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language\n  Models for Vision-Driven Embodied Agents","summary":"  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied\nagents offers a promising avenue for tackling real-world tasks. While\nlanguage-centric embodied agents have garnered substantial attention,\nMLLM-based embodied agents remain underexplored due to the lack of\ncomprehensive evaluation frameworks. To bridge this gap, we introduce\nEmbodiedBench, an extensive benchmark designed to evaluate vision-driven\nembodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing\ntasks across four environments, ranging from high-level semantic tasks (e.g.,\nhousehold) to low-level tasks involving atomic actions (e.g., navigation and\nmanipulation); and (2) six meticulously curated subsets evaluating essential\nagent capabilities like commonsense reasoning, complex instruction\nunderstanding, spatial awareness, visual perception, and long-term planning.\nThrough extensive experiments, we evaluated 13 leading proprietary and\nopen-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel\nat high-level tasks but struggle with low-level manipulation, with the best\nmodel, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a\nmultifaceted standardized evaluation platform that not only highlights existing\nchallenges but also offers valuable insights to advance MLLM-based embodied\nagents. Our code is available at https://embodiedbench.github.io.\n","authors":["Rui Yang","Hanyang Chen","Junyu Zhang","Mark Zhao","Cheng Qian","Kangrui Wang","Qineng Wang","Teja Venkat Koripella","Marziyeh Movahedi","Manling Li","Heng Ji","Huan Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09560v1.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2408.09110v2","updated":"2025-02-13T18:01:16Z","published":"2024-08-17T06:24:43Z","title":"Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for\n  Remote Sensing Community","summary":"  Object detection, particularly open-vocabulary object detection, plays a\ncrucial role in Earth sciences, such as environmental monitoring, natural\ndisaster assessment, and land-use planning. However, existing open-vocabulary\ndetectors, primarily trained on natural-world images, struggle to generalize to\nremote sensing images due to a significant data domain gap. Thus, this paper\naims to advance the development of open-vocabulary object detection in remote\nsensing community. To achieve this, we first reformulate the task as Locate\nAnything on Earth (LAE) with the goal of detecting any novel concepts on Earth.\nWe then developed the LAE-Label Engine which collects, auto-annotates, and\nunifies up to 10 remote sensing datasets creating the LAE-1M - the first\nlarge-scale remote sensing object detection dataset with broad category\ncoverage. Using the LAE-1M, we further propose and train the novel LAE-DINO\nModel, the first open-vocabulary foundation object detector for the LAE task,\nfeaturing Dynamic Vocabulary Construction (DVC) and Visual-Guided Text Prompt\nLearning (VisGT) modules. DVC dynamically constructs vocabulary for each\ntraining batch, while VisGT maps visual features to semantic space, enhancing\ntext features. We comprehensively conduct experiments on established remote\nsensing benchmark DIOR, DOTAv2.0, as well as our newly introduced 80-class\nLAE-80C benchmark. Results demonstrate the advantages of the LAE-1M dataset and\nthe effectiveness of the LAE-DINO method.\n","authors":["Jiancheng Pan","Yanxing Liu","Yuqian Fu","Muyuan Ma","Jiahao Li","Danda Pani Paudel","Luc Van Gool","Xiaomeng Huang"],"pdf_url":"https://arxiv.org/pdf/2408.09110v2.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.09533v1","updated":"2025-02-13T17:50:23Z","published":"2025-02-13T17:50:23Z","title":"Long-Term TalkingFace Generation via Motion-Prior Conditional Diffusion\n  Model","summary":"  Recent advances in conditional diffusion models have shown promise for\ngenerating realistic TalkingFace videos, yet challenges persist in achieving\nconsistent head movement, synchronized facial expressions, and accurate lip\nsynchronization over extended generations. To address these, we introduce the\n\\textbf{M}otion-priors \\textbf{C}onditional \\textbf{D}iffusion \\textbf{M}odel\n(\\textbf{MCDM}), which utilizes both archived and current clip motion priors to\nenhance motion prediction and ensure temporal consistency. The model consists\nof three key elements: (1) an archived-clip motion-prior that incorporates\nhistorical frames and a reference frame to preserve identity and context; (2) a\npresent-clip motion-prior diffusion model that captures multimodal causality\nfor accurate predictions of head movements, lip sync, and expressions; and (3)\na memory-efficient temporal attention mechanism that mitigates error\naccumulation by dynamically storing and updating motion features. We also\nrelease the \\textbf{TalkingFace-Wild} dataset, a multilingual collection of\nover 200 hours of footage across 10 languages. Experimental results demonstrate\nthe effectiveness of MCDM in maintaining identity and motion continuity for\nlong-term TalkingFace generation. Code, models, and datasets will be publicly\navailable.\n","authors":["Fei Shen","Cong Wang","Junyao Gao","Qin Guo","Jisheng Dang","Jinhui Tang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.09533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09528v1","updated":"2025-02-13T17:39:28Z","published":"2025-02-13T17:39:28Z","title":"SteROI-D: System Design and Mapping for Stereo Depth Inference on\n  Regions of Interest","summary":"  Machine learning algorithms have enabled high quality stereo depth estimation\nto run on Augmented and Virtual Reality (AR/VR) devices. However, high energy\nconsumption across the full image processing stack prevents stereo depth\nalgorithms from running effectively on battery-limited devices. This paper\nintroduces SteROI-D, a full stereo depth system paired with a mapping\nmethodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity\nat the system level to save energy. SteROI-D's flexible and heterogeneous\ncompute fabric supports diverse ROIs. Importantly, we introduce a systematic\nmapping methodology to effectively handle dynamic ROIs, thereby maximizing\nenergy savings. Using these techniques, our 28nm prototype SteROI-D design\nachieves up to 4.35x reduction in total system energy compared to a baseline\nASIC.\n","authors":["Jack Erhardt","Ziang Li","Reid Pinkham","Andrew Berkovich","Zhengya Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.09528v1.pdf","comment":"Accepted as a full paper by the 2025 EDGE AI FOUNDATION Austin"},{"id":"http://arxiv.org/abs/2502.09520v1","updated":"2025-02-13T17:35:57Z","published":"2025-02-13T17:35:57Z","title":"SQ-GAN: Semantic Image Communications Using Masked Vector Quantization","summary":"  This work introduces Semantically Masked VQ-GAN (SQ-GAN), a novel approach\nintegrating generative models to optimize image compression for\nsemantic/task-oriented communications. SQ-GAN employs off-the-shelf semantic\nsemantic segmentation and a new specifically developed semantic-conditioned\nadaptive mask module (SAMM) to selectively encode semantically significant\nfeatures of the images. SQ-GAN outperforms state-of-the-art image compression\nschemes such as JPEG2000 and BPG across multiple metrics, including perceptual\nquality and semantic segmentation accuracy on the post-decoding reconstructed\nimage, at extreme low compression rates expressed in bits per pixel.\n","authors":["Francesco Pezone","Sergio Barbarossa","Giuseppe Caire"],"pdf_url":"https://arxiv.org/pdf/2502.09520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09507v1","updated":"2025-02-13T17:21:37Z","published":"2025-02-13T17:21:37Z","title":"When and How Does CLIP Enable Domain and Compositional Generalization?","summary":"  The remarkable generalization performance of contrastive vision-language\nmodels like CLIP is often attributed to the diversity of their training\ndistributions. However, key questions remain unanswered: Can CLIP generalize to\nan entirely unseen domain when trained on a diverse mixture of domains (domain\ngeneralization)? Can it generalize to unseen classes within partially seen\ndomains (compositional generalization)? What factors affect such\ngeneralization? To answer these questions, we trained CLIP models on\nsystematically constructed training distributions with controlled domain\ndiversity and object class exposure. Our experiments show that domain diversity\nis essential for both domain and compositional generalization, yet\ncompositional generalization can be surprisingly weaker than domain\ngeneralization when the training distribution contains a suboptimal subset of\nthe test domain. Through data-centric and mechanistic analyses, we find that\nsuccessful generalization requires learning of shared representations already\nin intermediate layers and shared circuitry.\n","authors":["Elias Kempf","Simon Schrodi","Max Argus","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2502.09507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09501v1","updated":"2025-02-13T17:13:46Z","published":"2025-02-13T17:13:46Z","title":"Prior-Constrained Association Learning for Fine-Grained Generalized\n  Category Discovery","summary":"  This paper addresses generalized category discovery (GCD), the task of\nclustering unlabeled data from potentially known or unknown categories with the\nhelp of labeled instances from each known category. Compared to traditional\nsemi-supervised learning, GCD is more challenging because unlabeled data could\nbe from novel categories not appearing in labeled data. Current\nstate-of-the-art methods typically learn a parametric classifier assisted by\nself-distillation. While being effective, these methods do not make use of\ncross-instance similarity to discover class-specific semantics which are\nessential for representation learning and category discovery. In this paper, we\nrevisit the association-based paradigm and propose a Prior-constrained\nAssociation Learning method to capture and learn the semantic relations within\ndata. In particular, the labeled data from known categories provides a unique\nprior for the association of unlabeled data. Unlike previous methods that only\nadopts the prior as a pre or post-clustering refinement, we fully incorporate\nthe prior into the association process, and let it constrain the association\ntowards a reliable grouping outcome. The estimated semantic groups are utilized\nthrough non-parametric prototypical contrast to enhance the representation\nlearning. A further combination of both parametric and non-parametric\nclassification complements each other and leads to a model that outperforms\nexisting methods by a significant margin. On multiple GCD benchmarks, we\nperform extensive experiments and validate the effectiveness of our proposed\nmethod.\n","authors":["Menglin Wang","Zhun Zhong","Xiaojin Gong"],"pdf_url":"https://arxiv.org/pdf/2502.09501v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09482v1","updated":"2025-02-13T16:45:39Z","published":"2025-02-13T16:45:39Z","title":"Standardisation of Convex Ultrasound Data Through Geometric Analysis and\n  Augmentation","summary":"  The application of ultrasound in healthcare has seen increased diversity and\nimportance. Unlike other medical imaging modalities, ultrasound research and\ndevelopment has historically lagged, particularly in the case of applications\nwith data-driven algorithms. A significant issue with ultrasound is the extreme\nvariability of the images, due to the number of different machines available\nand the possible combination of parameter settings. One outcome of this is the\nlack of standardised and benchmarking ultrasound datasets. The method proposed\nin this article is an approach to alleviating this issue of disorganisation.\nFor this purpose, the issue of ultrasound data sparsity is examined and a novel\nperspective, approach, and solution is proposed; involving the extraction of\nthe underlying ultrasound plane within the image and representing it using\nannulus sector geometry. An application of this methodology is proposed, which\nis the extraction of scan lines and the linearisation of convex planes.\nValidation of the robustness of the proposed method is performed on both\nprivate and public data. The impact of deformation and the invertibility of\naugmentation using the estimated annulus sector parameters is also studied.\nKeywords: Ultrasound, Annulus Sector, Augmentation, Linearisation.\n","authors":["Alistair Weld","Giovanni Faoro","Luke Dixon","Sophie Camp","Arianna Menciassi","Stamatia Giannarou"],"pdf_url":"https://arxiv.org/pdf/2502.09482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09477v1","updated":"2025-02-13T16:41:44Z","published":"2025-02-13T16:41:44Z","title":"DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation\n  Networks for Quantitative Nanomaterial Analysis through Differentiable\n  Rendering and Generative Modelling","summary":"  Nanomaterials exhibit distinctive properties governed by parameters such as\nsize, shape, and surface characteristics, which critically influence their\napplications and interactions across technological, biological, and\nenvironmental contexts. Accurate quantification and understanding of these\nmaterials are essential for advancing research and innovation. In this regard,\ndeep learning segmentation networks have emerged as powerful tools that enable\nautomated insights and replace subjective methods with precise quantitative\nanalysis. However, their efficacy depends on representative annotated datasets,\nwhich are challenging to obtain due to the costly imaging of nanoparticles and\nthe labor-intensive nature of manual annotations. To overcome these\nlimitations, we introduce DiffRenderGAN, a novel generative model designed to\nproduce annotated synthetic data. By integrating a differentiable renderer into\na Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes\ntextural rendering parameters to generate realistic, annotated nanoparticle\nimages from non-annotated real microscopy images. This approach reduces the\nneed for manual intervention and enhances segmentation performance compared to\nexisting synthetic data methods by generating diverse and realistic data.\nTested on multiple ion and electron microscopy cases, including titanium\ndioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),\nDiffRenderGAN bridges the gap between synthetic and real data, advancing the\nquantification and understanding of complex nanomaterial systems.\n","authors":["Dennis Possart","Leonid Mill","Florian Vollnhals","Tor Hildebrand","Peter Suter","Mathis Hoffmann","Jonas Utz","Daniel Augsburger","Mareike Thies","Mingxuan Wu","Fabian Wagner","George Sarau","Silke Christiansen","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2502.09477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09471v1","updated":"2025-02-13T16:34:59Z","published":"2025-02-13T16:34:59Z","title":"Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for\n  Weakly-supervised Oriented Object Detection","summary":"  Accurately estimating the orientation of visual objects with compact rotated\nbounding boxes (RBoxes) has become a prominent demand, which challenges\nexisting object detection paradigms that only use horizontal bounding boxes\n(HBoxes). To equip the detectors with orientation awareness, supervised\nregression/classification modules have been introduced at the high cost of\nrotation annotation. Meanwhile, some existing datasets with oriented objects\nare already annotated with horizontal boxes or even single points. It becomes\nattractive yet remains open for effectively utilizing weaker single point and\nhorizontal annotations to train an oriented object detector (OOD). We develop\nWholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging\nvarious labeling forms (Points, HBoxes, RBoxes, and their combination) in a\nunified fashion. By only using HBox for training, our Wholly-WOOD achieves\nperformance very close to that of the RBox-trained counterpart on remote\nsensing and other areas, significantly reducing the tedious efforts on\nlabor-intensive annotation for oriented objects. The source codes are available\nat https://github.com/VisionXLab/whollywood (PyTorch-based) and\nhttps://github.com/VisionXLab/whollywood-jittor (Jittor-based).\n","authors":["Yi Yu","Xue Yang","Yansheng Li","Zhenjun Han","Feipeng Da","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2502.09471v1.pdf","comment":"18 pages, 9 figures, 9 tables, accepted by TPAMI"},{"id":"http://arxiv.org/abs/2502.07431v2","updated":"2025-02-13T16:32:33Z","published":"2025-02-11T10:19:50Z","title":"ArthroPhase: A Novel Dataset and Method for Phase Recognition in\n  Arthroscopic Video","summary":"  This study aims to advance surgical phase recognition in arthroscopic\nprocedures, specifically Anterior Cruciate Ligament (ACL) reconstruction, by\nintroducing the first arthroscopy dataset and developing a novel\ntransformer-based model. We aim to establish a benchmark for arthroscopic\nsurgical phase recognition by leveraging spatio-temporal features to address\nthe specific challenges of arthroscopic videos including limited field of view,\nocclusions, and visual distortions. We developed the ACL27 dataset, comprising\n27 videos of ACL surgeries, each labeled with surgical phases. Our model\nemploys a transformer-based architecture, utilizing temporal-aware frame-wise\nfeature extraction through a ResNet-50 and transformer layers. This approach\nintegrates spatio-temporal features and introduces a Surgical Progress Index\n(SPI) to quantify surgery progression. The model's performance was evaluated\nusing accuracy, precision, recall, and Jaccard Index on the ACL27 and Cholec80\ndatasets. The proposed model achieved an overall accuracy of 72.91% on the\nACL27 dataset. On the Cholec80 dataset, the model achieved a comparable\nperformance with the state-of-the-art methods with an accuracy of 92.4%. The\nSPI demonstrated an output error of 10.6% and 9.86% on ACL27 and Cholec80\ndatasets respectively, indicating reliable surgery progression estimation. This\nstudy introduces a significant advancement in surgical phase recognition for\narthroscopy, providing a comprehensive dataset and a robust transformer-based\nmodel. The results validate the model's effectiveness and generalizability,\nhighlighting its potential to improve surgical training, real-time assistance,\nand operational efficiency in orthopedic surgery. The publicly available\ndataset and code will facilitate future research and development in this\ncritical field.\n","authors":["Ali Bahari Malayeri","Matthias Seibold","Nicola Cavalcanti","Jonas Hein","Sascha Jecklin","Lazaros Vlachopoulos","Sandro Fucentese","Sandro Hodel","Philipp Furnstahl"],"pdf_url":"https://arxiv.org/pdf/2502.07431v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14679v3","updated":"2025-02-13T16:29:16Z","published":"2025-01-24T17:57:06Z","title":"Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation","summary":"  Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.\n","authors":["Rongzhao He","Weihao Zheng","Leilei Zhao","Ying Wang","Dalin Zhu","Dan Wu","Bin Hu"],"pdf_url":"https://arxiv.org/pdf/2501.14679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09460v1","updated":"2025-02-13T16:27:23Z","published":"2025-02-13T16:27:23Z","title":"Metamorphic Testing for Pose Estimation Systems","summary":"  Pose estimation systems are used in a variety of fields, from sports\nanalytics to livestock care. Given their potential impact, it is paramount to\nsystematically test their behaviour and potential for failure. This is a\ncomplex task due to the oracle problem and the high cost of manual labelling\nnecessary to build ground truth keypoints. This problem is exacerbated by the\nfact that different applications require systems to focus on different subjects\n(e.g., human versus animal) or landmarks (e.g., only extremities versus whole\nbody and face), which makes labelled test data rarely reusable. To combat these\nproblems we propose MET-POSE, a metamorphic testing framework for pose\nestimation systems that bypasses the need for manual annotation while assessing\nthe performance of these systems under different circumstances. MET-POSE thus\nallows users of pose estimation systems to assess the systems in conditions\nthat more closely relate to their application without having to label an ad-hoc\ntest dataset or rely only on available datasets, which may not be adapted to\ntheir application domain. While we define MET-POSE in general terms, we also\npresent a non-exhaustive list of metamorphic rules that represent common\nchallenges in computer vision applications, as well as a specific way to\nevaluate these rules. We then experimentally show the effectiveness of MET-POSE\nby applying it to Mediapipe Holistic, a state of the art human pose estimation\nsystem, with the FLIC and PHOENIX datasets. With these experiments, we outline\nnumerous ways in which the outputs of MET-POSE can uncover faults in pose\nestimation systems at a similar or higher rate than classic testing using hand\nlabelled data, and show that users can tailor the rule set they use to the\nfaults and level of accuracy relevant to their application.\n","authors":["Matias Duran","Thomas Laurent","Ellen Rushe","Anthony Ventresque"],"pdf_url":"https://arxiv.org/pdf/2502.09460v1.pdf","comment":"Accepted for publication at 2025 IEEE Conference on Software Testing,\n  Verification and Validation (ICST)"},{"id":"http://arxiv.org/abs/2410.10790v2","updated":"2025-02-13T16:20:05Z","published":"2024-10-14T17:56:19Z","title":"Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D\n  Scenes","summary":"  Recent advancements in human motion synthesis have focused on specific types\nof motions, such as human-scene interaction, locomotion or human-human\ninteraction, however, there is a lack of a unified system capable of generating\na diverse combination of motion types. In response, we introduce\nSitcom-Crafter, a comprehensive and extendable system for human motion\ngeneration in 3D space, which can be guided by extensive plot contexts to\nenhance workflow efficiency for anime and game designers. The system is\ncomprised of eight modules, three of which are dedicated to motion generation,\nwhile the remaining five are augmentation modules that ensure consistent fusion\nof motion sequences and system functionality. Central to the generation modules\nis our novel 3D scene-aware human-human interaction module, which addresses\ncollision issues by synthesizing implicit 3D Signed Distance Function (SDF)\npoints around motion spaces, thereby minimizing human-scene collisions without\nadditional data collection costs. Complementing this, our locomotion and\nhuman-scene interaction modules leverage existing methods to enrich the\nsystem's motion generation capabilities. Augmentation modules encompass plot\ncomprehension for command generation, motion synchronization for seamless\nintegration of different motion types, hand pose retrieval to enhance motion\nrealism, motion collision revision to prevent human collisions, and 3D\nretargeting to ensure visual fidelity. Experimental evaluations validate the\nsystem's ability to generate high-quality, diverse, and physically realistic\nmotions, underscoring its potential for advancing creative workflows. Project\npage: https://windvchen.github.io/Sitcom-Crafter.\n","authors":["Jianqi Chen","Panwen Hu","Xiaojun Chang","Zhenwei Shi","Michael Kampffmeyer","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2410.10790v2.pdf","comment":"Accepted by ICLR 2025. Project Page:\n  https://windvchen.github.io/Sitcom-Crafter"},{"id":"http://arxiv.org/abs/2410.10719v3","updated":"2025-02-13T16:18:59Z","published":"2024-10-14T17:00:53Z","title":"4-LEGS: 4D Language Embedded Gaussian Splatting","summary":"  The emergence of neural representations has revolutionized our means for\ndigitally viewing a wide range of 3D scenes, enabling the synthesis of\nphotorealistic images rendered from novel views. Recently, several techniques\nhave been proposed for connecting these low-level representations with the\nhigh-level semantics understanding embodied within the scene. These methods\nelevate the rich semantic understanding from 2D imagery to 3D representations,\ndistilling high-dimensional spatial features onto 3D space. In our work, we are\ninterested in connecting language with a dynamic modeling of the world. We show\nhow to lift spatio-temporal features to a 4D representation based on 3D\nGaussian Splatting. This enables an interactive interface where the user can\nspatiotemporally localize events in the video from text prompts. We demonstrate\nour system on public 3D video datasets of people and animals performing various\nactions.\n","authors":["Gal Fiebelman","Tamir Cohen","Ayellet Morgenstern","Peter Hedman","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2410.10719v3.pdf","comment":"Eurographics 2025. Project webpage:\n  https://tau-vailab.github.io/4-LEGS/"},{"id":"http://arxiv.org/abs/2502.09447v1","updated":"2025-02-13T16:16:54Z","published":"2025-02-13T16:16:54Z","title":"Pixel-Level Reasoning Segmentation via Multi-turn Conversations","summary":"  Existing visual perception systems focus on region-level segmentation in\nsingle-turn dialogues, relying on complex and explicit query instructions. Such\nsystems cannot reason at the pixel level and comprehend dynamic user intent\nthat changes over interaction. Our work tackles this issue by introducing a\nnovel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on\nmulti-turn conversations, tracking evolving user intent via multi-turn\ninteractions for fine-grained segmentation. To establish a benchmark for this\nnovel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on\nMulti-Turn Conversations (PRIST), comprising 24k utterances from 8.3k\nmulti-turn conversational scenarios with segmentation targets. Building on\nPRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning\nSegmentation framework, integrates pixel-level segmentation with robust\nmulti-turn conversation understanding, generating pixel-grounded explanations\naligned with user intent. The PRIST dataset and MIRSA framework fill the gap in\npixel-level reasoning segmentation. Experimental results on the PRIST dataset\ndemonstrate that our method outperforms current segmentation-specific baselines\nin terms of segmentation and LLM-based reasoning metrics. The code and data are\navailable at: https://github.com/ccccai239/PixelRIST.\n","authors":["Dexian Cai","Xiaocui Yang","Yongkang Liu","Daling Wang","Shi Feng","Yifei Zhang","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2502.09447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17438v2","updated":"2025-02-13T16:11:35Z","published":"2023-05-27T10:26:23Z","title":"On the Importance of Backbone to the Adversarial Robustness of Object\n  Detectors","summary":"  Object detection is a critical component of various security-sensitive\napplications, such as autonomous driving and video surveillance. However,\nexisting object detectors are vulnerable to adversarial attacks, which poses a\nsignificant challenge to their reliability and security. Through experiments,\nfirst, we found that existing works on improving the adversarial robustness of\nobject detectors give a false sense of security. Second, we found that\nadversarially pre-trained backbone networks were essential for enhancing the\nadversarial robustness of object detectors. We then proposed a simple yet\neffective recipe for fast adversarial fine-tuning on object detectors with\nadversarially pre-trained backbones. Without any modifications to the structure\nof object detectors, our recipe achieved significantly better adversarial\nrobustness than previous works. Finally, we explored the potential of different\nmodern object detector designs for improving adversarial robustness with our\nrecipe and demonstrated interesting findings, which inspired us to design\nstate-of-the-art (SOTA) robust detectors. Our empirical results set a new\nmilestone for adversarially robust object detection. Code and trained\ncheckpoints are available at https://github.com/thu-ml/oddefense.\n","authors":["Xiao Li","Hang Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2305.17438v2.pdf","comment":"Accepted by IEEE TIFS"},{"id":"http://arxiv.org/abs/2410.01404v2","updated":"2025-02-13T16:06:54Z","published":"2024-10-02T10:31:10Z","title":"Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection","summary":"  Skins wrapping around our bodies, leathers covering over the sofa, sheet\nmetal coating the car - it suggests that objects are enclosed by a series of\ncontinuous surfaces, which provides us with informative geometry prior for\nobjectness deduction. In this paper, we propose Gaussian-Det which leverages\nGaussian Splatting as surface representation for multi-view based 3D object\ndetection. Unlike existing monocular or NeRF-based methods which depict the\nobjects via discrete positional data, Gaussian-Det models the objects in a\ncontinuous manner by formulating the input Gaussians as feature descriptors on\na mass of partial surfaces. Furthermore, to address the numerous outliers\ninherently introduced by Gaussian splatting, we accordingly devise a Closure\nInferring Module (CIM) for the comprehensive surface-based objectness\ndeduction. CIM firstly estimates the probabilistic feature residuals for\npartial surfaces given the underdetermined nature of Gaussian Splatting, which\nare then coalesced into a holistic representation on the overall surface\nclosure of the object proposal. In this way, the surface information\nGaussian-Det exploits serves as the prior on the quality and reliability of\nobjectness and the information basis of proposal refinement. Experiments on\nboth synthetic and real-world datasets demonstrate that Gaussian-Det\noutperforms various existing approaches, in terms of both average precision and\nrecall.\n","authors":["Hongru Yan","Yu Zheng","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2410.01404v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09434v1","updated":"2025-02-13T15:56:44Z","published":"2025-02-13T15:56:44Z","title":"Redistribute Ensemble Training for Mitigating Memorization in Diffusion\n  Models","summary":"  Diffusion models, known for their tremendous ability to generate high-quality\nsamples, have recently raised concerns due to their data memorization behavior,\nwhich poses privacy risks. Recent methods for memory mitigation have primarily\naddressed the issue within the context of the text modality in cross-modal\ngeneration tasks, restricting their applicability to specific conditions. In\nthis paper, we propose a novel method for diffusion models from the perspective\nof visual modality, which is more generic and fundamental for mitigating\nmemorization. Directly exposing visual data to the model increases memorization\nrisk, so we design a framework where models learn through proxy model\nparameters instead. Specially, the training dataset is divided into multiple\nshards, with each shard training a proxy model, then aggregated to form the\nfinal model. Additionally, practical analysis of training losses illustrates\nthat the losses for easily memorable images tend to be obviously lower. Thus,\nwe skip the samples with abnormally low loss values from the current mini-batch\nto avoid memorizing. However, balancing the need to skip memorization-prone\nsamples while maintaining sufficient training data for high-quality image\ngeneration presents a key challenge. Thus, we propose IET-AGC+, which\nredistributes highly memorizable samples between shards, to mitigate these\nsamples from over-skipping. Furthermore, we dynamically augment samples based\non their loss values to further reduce memorization. Extensive experiments and\nanalysis on four datasets show that our method successfully reduces memory\ncapacity while maintaining performance. Moreover, we fine-tune the pre-trained\ndiffusion models, e.g., Stable Diffusion, and decrease the memorization score\nby 46.7\\%, demonstrating the effectiveness of our method. Code is available in:\nhttps://github.com/liuxiao-guan/IET_AGC.\n","authors":["Xiaoliu Guan","Yu Wu","Huayang Huang","Xiao Liu","Jiaxu Miao","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.09434v1.pdf","comment":"12 pages,9 figures. arXiv admin note: substantial text overlap with\n  arXiv:2407.15328"},{"id":"http://arxiv.org/abs/2502.09425v1","updated":"2025-02-13T15:47:45Z","published":"2025-02-13T15:47:45Z","title":"A 3D Facial Reconstruction Evaluation Methodology: Comparing Smartphone\n  Scans with Deep Learning Based Methods Using Geometry and Morphometry\n  Criteria","summary":"  Three-dimensional (3D) facial shape analysis has gained interest due to its\npotential clinical applications. However, the high cost of advanced 3D facial\nacquisition systems limits their widespread use, driving the development of\nlow-cost acquisition and reconstruction methods. This study introduces a novel\nevaluation methodology that goes beyond traditional geometry-based benchmarks\nby integrating morphometric shape analysis techniques, providing a statistical\nframework for assessing facial morphology preservation. As a case study, we\ncompare smartphone-based 3D scans with state-of-the-art deep learning\nreconstruction methods from 2D images, using high-end stereophotogrammetry\nmodels as ground truth. This methodology enables a quantitative assessment of\nglobal and local shape differences, offering a biologically meaningful\nvalidation approach for low-cost 3D facial acquisition and reconstruction\ntechniques.\n","authors":["√Ålvaro Heredia-Lid√≥n","Alejandro Mo√±ux-Bernal","Alejandro Gonz√°lez","Luis M. Echeverry-Quiceno","Max Rubert","Aroa Casado","Mar√≠a Esther Esteban","Mireia Andreu-Montoriol","Susanna Gallardo","Cristina Ruffo","Neus Mart√≠nez-Abad√≠as","Xavier Sevillano"],"pdf_url":"https://arxiv.org/pdf/2502.09425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00315v3","updated":"2025-02-13T15:46:35Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.15959v3","updated":"2025-02-13T15:38:06Z","published":"2024-10-21T12:43:54Z","title":"Diffusion Transformer Policy: Scaling Diffusion Transformer for\n  Generalist Vision-Language-Action Learning","summary":"  Recent large vision-language action models pretrained on diverse robot\ndatasets have demonstrated the potential for generalizing to new environments\nwith a few in-domain data. However, those approaches usually predict individual\ndiscretized or continuous action by a small action head, which limits the\nability in handling diverse action spaces. In contrast, we model the continuous\naction sequence with a large multi-modal diffusion transformer, dubbed as\nDiffusion Transformer Policy, in which we directly denoise action chunks by a\nlarge transformer model rather than a small action head for action embedding.\nBy leveraging the scaling capability of transformers, the proposed approach can\neffectively model continuous end-effector actions across large diverse robot\ndatasets, and achieve better generalization performance. Extensive experiments\ndemonstrate the effectiveness and generalization of Diffusion Transformer\nPolicy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world\nFranka arm, achieving consistent better performance on Real-to-Sim benchmark\nSimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo.\nSpecifically, without bells and whistles, the proposed approach achieves\nstate-of-the-art performance with only a single third-view camera stream in the\nCalvin task ABC->D, improving the average number of tasks completed in a row of\n5 to 3.6, and the pretraining stage significantly facilitates the success\nsequence length on the Calvin by over 1.2. Project Page:\nhttps://zhihou7.github.io/dit_policy_vla/\n","authors":["Zhi Hou","Tianyi Zhang","Yuwen Xiong","Hengjun Pu","Chengyang Zhao","Ronglei Tong","Yu Qiao","Jifeng Dai","Yuntao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.15959v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09411v1","updated":"2025-02-13T15:36:12Z","published":"2025-02-13T15:36:12Z","title":"ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation","summary":"  Diffusion models enable high-quality and diverse visual content synthesis.\nHowever, they struggle to generate rare or unseen concepts. To address this\nchallenge, we explore the usage of Retrieval-Augmented Generation (RAG) with\nimage generation models. We propose ImageRAG, a method that dynamically\nretrieves relevant images based on a given text prompt, and uses them as\ncontext to guide the generation process. Prior approaches that used retrieved\nimages to improve generation, trained models specifically for retrieval-based\ngeneration. In contrast, ImageRAG leverages the capabilities of existing image\nconditioning models, and does not require RAG-specific training. Our approach\nis highly adaptable and can be applied across different model types, showing\nsignificant improvement in generating rare and fine-grained concepts using\ndifferent base models.\n  Our project page is available at: https://rotem-shalev.github.io/ImageRAG\n","authors":["Rotem Shalev-Arkushin","Rinon Gal","Amit H. Bermano","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2502.09411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07508v2","updated":"2025-02-13T15:28:13Z","published":"2025-02-11T12:22:35Z","title":"Enhance-A-Video: Better Generated Video for Free","summary":"  DiT-based video generation has achieved remarkable results, but research into\nenhancing existing models remains relatively unexplored. In this work, we\nintroduce a training-free approach to enhance the coherence and quality of\nDiT-based generated videos, named Enhance-A-Video. The core idea is enhancing\nthe cross-frame correlations based on non-diagonal temporal attention\ndistributions. Thanks to its simple design, our approach can be easily applied\nto most DiT-based video generation frameworks without any retraining or\nfine-tuning. Across various DiT-based video generation models, our approach\ndemonstrates promising improvements in both temporal consistency and visual\nquality. We hope this research can inspire future explorations in video\ngeneration enhancement.\n","authors":["Yang Luo","Xuanlei Zhao","Mengzhao Chen","Kaipeng Zhang","Wenqi Shao","Kai Wang","Zhangyang Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2502.07508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08544v2","updated":"2025-02-13T15:04:42Z","published":"2025-02-12T16:28:21Z","title":"Moment of Untruth: Dealing with Negative Queries in Video Moment\n  Retrieval","summary":"  Video Moment Retrieval is a common task to evaluate the performance of\nvisual-language models - it involves localising start and end times of moments\nin videos from query sentences. The current task formulation assumes that the\nqueried moment is present in the video, resulting in false positive moment\npredictions when irrelevant query sentences are provided. In this paper we\npropose the task of Negative-Aware Video Moment Retrieval (NA-VMR), which\nconsiders both moment retrieval accuracy and negative query rejection accuracy.\nWe make the distinction between In-Domain and Out-of-Domain negative queries\nand provide new evaluation benchmarks for two popular video moment retrieval\ndatasets: QVHighlights and Charades-STA. We analyse the ability of current SOTA\nvideo moment retrieval approaches to adapt to Negative-Aware Video Moment\nRetrieval and propose UniVTG-NA, an adaptation of UniVTG designed to tackle\nNA-VMR. UniVTG-NA achieves high negative rejection accuracy (avg. $98.4\\%$)\nscores while retaining moment retrieval scores to within $3.87\\%$ Recall@1.\nDataset splits and code are available at\nhttps://github.com/keflanagan/MomentofUntruth\n","authors":["Kevin Flanagan","Dima Damen","Michael Wray"],"pdf_url":"https://arxiv.org/pdf/2502.08544v2.pdf","comment":"16 pages, 9 figures. Accepted at WACV 2025. Paper webpage:\n  https://keflanagan.github.io/Moment-of-Untruth"},{"id":"http://arxiv.org/abs/2502.06607v2","updated":"2025-02-13T14:57:44Z","published":"2025-02-10T16:04:54Z","title":"Illegal Waste Detection in Remote Sensing Images: A Case Study","summary":"  Environmental crime currently represents the third largest criminal activity\nworldwide while threatening ecosystems as well as human health. Among the\ncrimes related to this activity, improper waste management can nowadays be\ncountered more easily thanks to the increasing availability and decreasing cost\nof Very-High-Resolution Remote Sensing images, which enable semi-automatic\nterritory scanning in search of illegal landfills. This paper proposes a\npipeline, developed in collaboration with professionals from a local\nenvironmental agency, for detecting candidate illegal dumping sites leveraging\na classifier of Remote Sensing images. To identify the best configuration for\nsuch classifier, an extensive set of experiments was conducted and the impact\nof diverse image characteristics and training settings was thoroughly analyzed.\nThe local environmental agency was then involved in an experimental exercise\nwhere outputs from the developed classifier were integrated in the experts'\neveryday work, resulting in time savings with respect to manual\nphoto-interpretation. The classifier was eventually run with valuable results\non a location outside of the training area, highlighting potential for\ncross-border applicability of the proposed pipeline.\n","authors":["Federico Gibellini","Piero Fraternali","Giacomo Boracchi","Luca Morandini","Andrea Diecidue","Simona Malegori"],"pdf_url":"https://arxiv.org/pdf/2502.06607v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09356v1","updated":"2025-02-13T14:21:03Z","published":"2025-02-13T14:21:03Z","title":"Galileo: Learning Global and Local Features in Pretrained Remote Sensing\n  Models","summary":"  From crop mapping to flood detection, machine learning in remote sensing has\na wide range of societally beneficial applications. The commonalities between\nremote sensing data in these applications present an opportunity for pretrained\nmachine learning models tailored to remote sensing to reduce the labeled data\nand effort required to solve individual tasks. However, such models must be:\n(i) flexible enough to ingest input data of varying sensor modalities and\nshapes (i.e., of varying spatial and temporal dimensions), and (ii) able to\nmodel Earth surface phenomena of varying scales and types. To solve this gap,\nwe present Galileo, a family of pretrained remote sensing models designed to\nflexibly process multimodal remote sensing data. We also introduce a novel and\nhighly effective self-supervised learning approach to learn both large- and\nsmall-scale features, a challenge not addressed by previous models. Our Galileo\nmodels obtain state-of-the-art results across diverse remote sensing tasks.\n","authors":["Gabriel Tseng","Anthony Fuller","Marlena Reil","Henry Herzog","Patrick Beukema","Favyen Bastani","James R. Green","Evan Shelhamer","Hannah Kerner","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2502.09356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06181v2","updated":"2025-02-13T14:18:50Z","published":"2025-02-10T06:21:16Z","title":"CANeRV: Content Adaptive Neural Representation for Video Compression","summary":"  Recent advances in video compression introduce implicit neural representation\n(INR) based methods, which effectively capture global dependencies and\ncharacteristics of entire video sequences. Unlike traditional and deep learning\nbased approaches, INR-based methods optimize network parameters from a global\nperspective, resulting in superior compression potential. However, most current\nINR methods utilize a fixed and uniform network architecture across all frames,\nlimiting their adaptability to dynamic variations within and between video\nsequences. This often leads to suboptimal compression outcomes as these methods\nstruggle to capture the distinct nuances and transitions in video content. To\novercome these challenges, we propose Content Adaptive Neural Representation\nfor Video Compression (CANeRV), an innovative INR-based video compression\nnetwork that adaptively conducts structure optimisation based on the specific\ncontent of each video sequence. To better capture dynamic information across\nvideo sequences, we propose a dynamic sequence-level adjustment (DSA).\nFurthermore, to enhance the capture of dynamics between frames within a\nsequence, we implement a dynamic frame-level adjustment (DFA). {Finally, to\neffectively capture spatial structural information within video frames, thereby\nenhancing the detail restoration capabilities of CANeRV, we devise a structure\nlevel hierarchical structural adaptation (HSA).} Experimental results\ndemonstrate that CANeRV can outperform both H.266/VVC and state-of-the-art\nINR-based video compression techniques across diverse video datasets.\n","authors":["Lv Tang","Jun Zhu","Xinfeng Zhang","Li Zhang","Siwei Ma","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2502.06181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09352v1","updated":"2025-02-13T14:18:41Z","published":"2025-02-13T14:18:41Z","title":"Wasserstein distributional adversarial training for deep neural networks","summary":"  Design of adversarial attacks for deep neural networks, as well as methods of\nadversarial training against them, are subject of intense research. In this\npaper, we propose methods to train against distributional attack threats,\nextending the TRADES method used for pointwise attacks. Our approach leverages\nrecent contributions and relies on sensitivity analysis for Wasserstein\ndistributionally robust optimization problems. We introduce an efficient\nfine-tuning method which can be deployed on a previously trained model. We test\nour methods on a range of pre-trained models on RobustBench. These experimental\nresults demonstrate the additional training enhances Wasserstein distributional\nrobustness, while maintaining original levels of pointwise robustness, even for\nalready very successful networks. The improvements are less marked for models\npre-trained using huge synthetic datasets of 20-100M images. However,\nremarkably, sometimes our methods are still able to improve their performance\neven when trained using only the original training dataset (50k images).\n","authors":["Xingjian Bai","Guangyi He","Yifan Jiang","Jan Obloj"],"pdf_url":"https://arxiv.org/pdf/2502.09352v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.11934v3","updated":"2025-02-13T14:16:44Z","published":"2024-03-18T16:33:29Z","title":"Image and Point-cloud Classification for Jet Analysis in High-Energy\n  Physics: A survey","summary":"  Nowadays, there has been a growing trend in the field of high-energy physics\n(HEP), in both its experimental and phenomenological studies, to incorporate\nmachine learning (ML) and its specialized branch, deep learning (DL). This\nreview paper provides a thorough illustration of these applications using\ndifferent ML and DL approaches. The first part of the paper examines the basics\nof various particle physics types and establishes guidelines for assessing\nparticle physics alongside the available learning models. Next, a detailed\nclassification is provided for representing Jets that are reconstructed in\nhigh-energy collisions, mainly in proton-proton collisions at well-defined beam\nenergies. This section covers various datasets, preprocessing techniques, and\nfeature extraction and selection methods. The presented techniques can be\napplied to future hadron-hadron colliders (HHC), such as the high-luminosity\nLHC (HL-LHC) and the future circular collider - hadron-hadron (FCChh). The\nauthors then explore several AI techniques analyses designed specifically for\nboth image and point-cloud (PC) data in HEP. Additionally, a closer look is\ntaken at the classification associated with Jet tagging in hadron collisions.\nIn this review, various state-of-the-art (SOTA) techniques in ML and DL are\nexamined, with a focus on their implications for HEP demands. More precisely,\nthis discussion addresses various applications in extensive detail, such as Jet\ntagging, Jet tracking, particle classification, and more. The review concludes\nwith an analysis of the current state of HEP using DL methodologies. It\nhighlights the challenges and potential areas for future research, which are\nillustrated for each application.\n","authors":["Hamza Kheddar","Yassine Himeur","Abbes Amira","Rachik Soualah"],"pdf_url":"https://arxiv.org/pdf/2403.11934v3.pdf","comment":"Accepted paper in Frontier of Physics"},{"id":"http://arxiv.org/abs/2502.09325v1","updated":"2025-02-13T13:38:17Z","published":"2025-02-13T13:38:17Z","title":"A Benchmark for Crime Surveillance Video Analysis with Large Models","summary":"  Anomaly analysis in surveillance videos is a crucial topic in computer\nvision. In recent years, multimodal large language models (MLLMs) have\noutperformed task-specific models in various domains. Although MLLMs are\nparticularly versatile, their abilities to understand anomalous concepts and\ndetails are insufficiently studied because of the outdated benchmarks of this\nfield not providing MLLM-style QAs and efficient algorithms to assess the\nmodel's open-ended text responses. To fill this gap, we propose a benchmark for\ncrime surveillance video analysis with large models denoted as UCVL, including\n1,829 videos and reorganized annotations from the UCF-Crime and UCF-Crime\nAnnotation datasets. We design six types of questions and generate diverse QA\npairs. Then we develop detailed instructions and use OpenAI's GPT-4o for\naccurate assessment. We benchmark eight prevailing MLLMs ranging from 0.5B to\n40B parameters, and the results demonstrate the reliability of this bench.\nMoreover, we finetune LLaVA-OneVision on UCVL's training set. The improvement\nvalidates our data's high quality for video anomaly analysis.\n","authors":["Haoran Chen","Dong Yi","Moyan Cao","Chensen Huang","Guibo Zhu","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09311v1","updated":"2025-02-13T13:25:13Z","published":"2025-02-13T13:25:13Z","title":"Mitigating the Impact of Prominent Position Shift in Drone-based RGBT\n  Object Detection","summary":"  Drone-based RGBT object detection plays a crucial role in many\naround-the-clock applications. However, real-world drone-viewed RGBT data\nsuffers from the prominent position shift problem, i.e., the position of a tiny\nobject differs greatly in different modalities. For instance, a slight\ndeviation of a tiny object in the thermal modality will induce it to drift from\nthe main body of itself in the RGB modality. Considering RGBT data are usually\nlabeled on one modality (reference), this will cause the unlabeled modality\n(sensed) to lack accurate supervision signals and prevent the detector from\nlearning a good representation. Moreover, the mismatch of the corresponding\nfeature point between the modalities will make the fused features confusing for\nthe detection head. In this paper, we propose to cast the cross-modality box\nshift issue as the label noise problem and address it on the fly via a novel\nMean Teacher-based Cross-modality Box Correction head ensemble (CBC). In this\nway, the network can learn more informative representations for both\nmodalities. Furthermore, to alleviate the feature map mismatch problem in RGBT\nfusion, we devise a Shifted Window-Based Cascaded Alignment (SWCA) module. SWCA\nmines long-range dependencies between the spatially unaligned features inside\nshifted windows and cascaded aligns the sensed features with the reference\nones. Extensive experiments on two drone-based RGBT object detection datasets\ndemonstrate that the correction results are both visually and quantitatively\nfavorable, thereby improving the detection performance. In particular, our CBC\nmodule boosts the precision of the sensed modality ground truth by 25.52 aSim\npoints. Overall, the proposed detector achieves an mAP_50 of 43.55 points on\nRGBTDronePerson and surpasses a state-of-the-art method by 8.6 mAP50 on a shift\nsubset of DroneVehicle dataset. The code and data will be made publicly\navailable.\n","authors":["Yan Zhang","Wen Yang","Chang Xu","Qian Hu","Fang Xu","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2502.09311v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2502.01401v2","updated":"2025-02-13T13:15:27Z","published":"2025-02-03T14:32:36Z","title":"Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection","summary":"  3D visual grounding (3DVG) is challenging because of the requirement of\nunderstanding on visual information, language and spatial relationships. While\nsupervised approaches have achieved superior performance, they are constrained\nby the scarcity and high cost of 3D vision-language datasets. On the other\nhand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for\ntraining data. However, these methods incur prohibitive time and token costs\nduring inference. To address the challenges, we introduce a novel training-free\nsymbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual\nGrounder, that offers significantly reduced inference costs compared to\nprevious agent-based methods while maintaining comparable performance. EaSe\nuses LLM generated codes to compute on spatial relationships. EaSe also\nimplements an automatic pipeline to evaluate and optimize the quality of these\ncodes and integrate VLMs to assist in the grounding process. Experimental\nresults demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2%\nAcc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover,\nit substantially reduces the inference time and cost, offering a balanced\ntrade-off between performance and efficiency. Codes are available at\nhttps://github.com/OpenRobotLab/EaSe.\n","authors":["Boyu Mi","Hanqing Wang","Tai Wang","Yilun Chen","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2502.01401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09296v1","updated":"2025-02-13T13:09:55Z","published":"2025-02-13T13:09:55Z","title":"A Physics-Informed Deep Learning Model for MRI Brain Motion Correction","summary":"  Background: MRI is crucial for brain imaging but is highly susceptible to\nmotion artifacts due to long acquisition times. This study introduces\nPI-MoCoNet, a physics-informed motion correction network that integrates\nspatial and k-space information to remove motion artifacts without explicit\nmotion parameter estimation, enhancing image fidelity and diagnostic\nreliability. Materials and Methods: PI-MoCoNet consists of a motion detection\nnetwork (U-net with spatial averaging) to identify corrupted k-space lines and\na motion correction network (U-net with Swin Transformer blocks) to reconstruct\nmotion-free images. The correction is guided by three loss functions:\nreconstruction (L1), perceptual (LPIPS), and data consistency (Ldc). Motion\nartifacts were simulated via rigid phase encoding perturbations and evaluated\non IXI and MR-ART datasets against Pix2Pix, CycleGAN, and U-net using PSNR,\nSSIM, and NMSE. Results: PI-MoCoNet significantly improved image quality. On\nIXI, for minor artifacts, PSNR increased from 34.15 dB to 45.95 dB, SSIM from\n0.87 to 1.00, and NMSE reduced from 0.55% to 0.04%. For moderate artifacts,\nPSNR improved from 30.23 dB to 42.16 dB, SSIM from 0.80 to 0.99, and NMSE from\n1.32% to 0.09%. For heavy artifacts, PSNR rose from 27.99 dB to 36.01 dB, SSIM\nfrom 0.75 to 0.97, and NMSE decreased from 2.21% to 0.36%. On MR-ART,\nPI-MoCoNet achieved PSNR gains of ~10 dB and SSIM improvements of up to 0.20,\nwith NMSE reductions of ~6%. Ablation studies confirmed the importance of data\nconsistency and perceptual losses, yielding a 1 dB PSNR gain and 0.17% NMSE\nreduction. Conclusions: PI-MoCoNet effectively mitigates motion artifacts in\nbrain MRI, outperforming existing methods. Its ability to integrate spatial and\nk-space information makes it a promising tool for clinical use in motion-prone\nsettings. Code: https://github.com/mosaf/PI-MoCoNet.git.\n","authors":["Mojtaba Safari","Shansong Wang","Zach Eidex","Richard Qiu","Chih-Wei Chang","David S. Yu","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2502.09296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03633v3","updated":"2025-02-13T13:02:25Z","published":"2024-12-04T18:55:45Z","title":"NBM: an Open Dataset for the Acoustic Monitoring of Nocturnal Migratory\n  Birds in Europe","summary":"  The persisting threats on migratory bird populations highlight the urgent\nneed for effective monitoring techniques that could assist in their\nconservation. Among these, passive acoustic monitoring is an essential tool,\nparticularly for nocturnal migratory species that are difficult to track\notherwise. This work presents the Nocturnal Bird Migration (NBM) dataset, a\ncollection of 13,359 annotated vocalizations from 117 species of the Western\nPalearctic. The dataset includes precise time and frequency annotations,\ngathered by dozens of bird enthusiasts across France, enabling novel downstream\nacoustic analysis. In particular, we prove the utility of this database by\ntraining an original two-stage deep object detection model tailored for the\nprocessing of audio data. While allowing the precise localization of bird calls\nin spectrograms, this model shows competitive accuracy on the 45 main species\nof the dataset with state-of-the-art systems trained on much larger audio\ncollections. These results highlight the interest of fostering similar\nopen-science initiatives to acquire costly but valuable fine-grained\nannotations of audio files. All data and code are made openly available.\n","authors":["Louis Airale","Adrien Pajot","Juliette Linossier"],"pdf_url":"https://arxiv.org/pdf/2412.03633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09285v1","updated":"2025-02-13T13:00:33Z","published":"2025-02-13T13:00:33Z","title":"EmoAssist: Emotional Assistant for Visual Impairment Community","summary":"  The rapid advancement of large multi-modality models (LMMs) has significantly\npropelled the integration of artificial intelligence into practical\napplications. Visual Question Answering (VQA) systems, which can process\nmulti-modal data including vision, text, and audio, hold great potential for\nassisting the Visual Impairment (VI) community in navigating complex and\ndynamic real-world environments. However, existing VI assistive LMMs overlook\nthe emotional needs of VI individuals, and current benchmarks lack emotional\nevaluation of these LMMs. To address these gaps, this paper introduces the\nEmoAssist Benchmark, a comprehensive benchmark designed to evaluate the\nassistive performance of LMMs for the VI community. To the best of our\nknowledge, this is the first benchmark that incorporates emotional intelligence\nas a key consideration. Furthermore, we propose the EmoAssist Model, an\nEmotion-Assistive LMM specifically designed for the VI community. The EmoAssist\nModel utilizes Direct Preference Optimization (DPO) to align outputs with human\nemotional preferences. Experiment results demonstrate that the EmoAssist Model\nsignificantly enhances the recognition of implicit emotions and intentions of\nVI users, delivers empathetic responses, and provides actionable guidance.\nSpecifically, it shows respective improvements of 147.8% and 89.7% in the\nEmpathy and Suggestion metrics on the EmoAssist Benchmark, compared to the\npre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o.\n","authors":["Xingyu Qi","He Li","Linjie Li","Zhenyu Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18581v2","updated":"2025-02-13T12:59:30Z","published":"2024-06-05T16:27:34Z","title":"Dream-in-Style: Text-to-3D Generation Using Stylized Score Distillation","summary":"  We present a method to generate 3D objects in styles. Our method takes a text\nprompt and a style reference image as input and reconstructs a neural radiance\nfield to synthesize a 3D model with the content aligning with the text prompt\nand the style following the reference image. To simultaneously generate the 3D\nobject and perform style transfer in one go, we propose a stylized score\ndistillation loss to guide a text-to-3D optimization process to output visually\nplausible geometry and appearance. Our stylized score distillation is based on\na combination of an original pretrained text-to-image model and its modified\nsibling with the key and value features of self-attention layers manipulated to\ninject styles from the reference image. Comparisons with state-of-the-art\nmethods demonstrated the strong visual performance of our method, further\nsupported by the quantitative results from our user study.\n","authors":["Hubert Kompanowski","Binh-Son Hua"],"pdf_url":"https://arxiv.org/pdf/2406.18581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09282v1","updated":"2025-02-13T12:54:13Z","published":"2025-02-13T12:54:13Z","title":"FE-LWS: Refined Image-Text Representations via Decoder Stacking and\n  Fused Encodings for Remote Sensing Image Captioning","summary":"  Remote sensing image captioning aims to generate descriptive text from remote\nsensing images, typically employing an encoder-decoder framework. In this\nsetup, a convolutional neural network (CNN) extracts feature representations\nfrom the input image, which then guide the decoder in a sequence-to-sequence\ncaption generation process. Although much research has focused on refining the\ndecoder, the quality of image representations from the encoder remains crucial\nfor accurate captioning. This paper introduces a novel approach that integrates\nfeatures from two distinct CNN based encoders, capturing complementary\ninformation to enhance caption generation. Additionally, we propose a weighted\naveraging technique to combine the outputs of all GRUs in the stacked decoder.\nFurthermore, a comparison-based beam search strategy is incorporated to refine\ncaption selection. The results demonstrate that our fusion-based approach,\nalong with the enhanced stacked decoder, significantly outperforms both the\ntransformer-based state-of-the-art model and other LSTM-based baselines.\n","authors":["Swadhin Das","Raksha Sharma"],"pdf_url":"https://arxiv.org/pdf/2502.09282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09278v1","updated":"2025-02-13T12:49:25Z","published":"2025-02-13T12:49:25Z","title":"ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View\n  Gaussian Optimization","summary":"  Recent advances in diffusion models have significantly improved 3D\ngeneration, enabling the use of assets generated from an image for embodied AI\nsimulations. However, the one-to-many nature of the image-to-3D problem limits\ntheir use due to inconsistent content and quality across views. Previous models\noptimize a 3D model by sampling views from a view-conditioned diffusion prior,\nbut diffusion models cannot guarantee view consistency. Instead, we present\nConsistentDreamer, where we first generate a set of fixed multi-view prior\nimages and sample random views between them with another diffusion model\nthrough a score distillation sampling (SDS) loss. Thereby, we limit the\ndiscrepancies between the views guided by the SDS loss and ensure a consistent\nrough shape. In each iteration, we also use our generated multi-view prior\nimages for fine-detail reconstruction. To balance between the rough shape and\nthe fine-detail optimizations, we introduce dynamic task-dependent weights\nbased on homoscedastic uncertainty, updated automatically in each iteration.\nAdditionally, we employ opacity, depth distortion, and normal alignment losses\nto refine the surface for mesh extraction. Our method ensures better view\nconsistency and visual quality compared to the state-of-the-art.\n","authors":["Onat ≈ûahin","Mohammad Altillawi","George Eskandar","Carlos Carbone","Ziyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09278v1.pdf","comment":"Manuscript accepted by Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2502.09274v1","updated":"2025-02-13T12:39:26Z","published":"2025-02-13T12:39:26Z","title":"FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation","summary":"  3D scene understanding is a critical yet challenging task in autonomous\ndriving, primarily due to the irregularity and sparsity of LiDAR data, as well\nas the computational demands of processing large-scale point clouds. Recent\nmethods leverage the range-view representation to improve processing\nefficiency. To mitigate the performance drop caused by information loss\ninherent to the \"many-to-one\" problem, where multiple nearby 3D points are\nmapped to the same 2D grids and only the closest is retained, prior works tend\nto choose a higher azimuth resolution for range-view projection. However, this\ncan bring the drawback of reducing the proportion of pixels that carry\ninformation and heavier computation within the network. We argue that it is not\nthe optimal solution and show that, in contrast, decreasing the resolution is\nmore advantageous in both efficiency and accuracy. In this work, we present a\ncomprehensive re-design of the workflow for range-view-based LiDAR semantic\nsegmentation. Our approach addresses data representation, augmentation, and\npost-processing methods for improvements. Through extensive experiments on two\npublic datasets, we demonstrate that our pipeline significantly enhances the\nperformance of various network architectures over their baselines, paving the\nway for more effective LiDAR-based perception in autonomous systems.\n","authors":["Bin Yang","Alexandru Paul Condurache"],"pdf_url":"https://arxiv.org/pdf/2502.09274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09269v1","updated":"2025-02-13T12:31:09Z","published":"2025-02-13T12:31:09Z","title":"Memory-based Ensemble Learning in CMR Semantic Segmentation","summary":"  Existing models typically segment either the entire 3D frame or 2D slices\nindependently to derive clinical functional metrics from ventricular\nsegmentation in cardiac cine sequences. While performing well overall, they\nstruggle at the end slices. To address this, we leverage spatial continuity to\nextract global uncertainty from segmentation variance and use it as memory in\nour ensemble learning method, Streaming, for classifier weighting, balancing\noverall and end-slice performance. Additionally, we introduce the End\nCoefficient (EC) to quantify end-slice accuracy. Experiments on ACDC and M\\&Ms\ndatasets show that our framework achieves near-state-of-the-art Dice Similarity\nCoefficient (DSC) and outperforms all models on end-slice performance,\nimproving patient-specific segmentation accuracy.\n","authors":["Yiwei Liu","Ziyi Wu","Liang Zhong","Linyi Wen","Yuankai Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16290v3","updated":"2025-02-13T12:16:47Z","published":"2024-10-05T20:03:57Z","title":"A Unified Model for Compressed Sensing MRI Across Undersampling Patterns","summary":"  Compressed Sensing MRI reconstructs images of the body's internal anatomy\nfrom undersampled measurements, thereby reducing the scan time - the time\nsubjects need to remain still. Recently, deep neural networks have shown great\npotential for reconstructing high-fidelity images from highly undersampled\nmeasurements in the frequency space. However, one needs to train multiple\nmodels for different undersampling patterns and desired output image\nresolutions, since most networks operate on a fixed discretization. Such\napproaches are highly impractical in clinical settings, where undersampling\npatterns and image resolutions are frequently changed to accommodate different\nreal-time imaging and diagnostic requirements.\n  We propose a unified model robust to different measurement undersampling\npatterns and image resolutions in compressed sensing MRI. Our model is based on\nneural operators, a discretization-agnostic architecture. Neural operators are\nemployed in both image and measurement space, which capture local and global\nimage features for MRI reconstruction. Empirically, we achieve consistent\nperformance across different undersampling rates and patterns, with an average\n11 percent SSIM and 4dB PSNR improvement over a state-of-the-art CNN,\nEnd-to-End VarNet. For efficiency, our inference speed is also 1,400x faster\nthan diffusion methods. The resolution-agnostic design also enhances zero-shot\nsuper-resolution and extended field of view in reconstructed images. Our\nunified model offers a versatile solution for MRI, adapting seamlessly to\nvarious measurement undersampling and imaging resolutions, making it highly\neffective for flexible and reliable clinical imaging. Our code is available at\nhttps://armeet.ca/nomri.\n","authors":["Armeet Singh Jatyani","Jiayun Wang","Aditi Chandrashekar","Zihui Wu","Miguel Liu-Schiaffini","Bahareh Tolooshams","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2410.16290v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09256v1","updated":"2025-02-13T12:11:58Z","published":"2025-02-13T12:11:58Z","title":"DynSegNet:Dynamic Architecture Adjustment for Adversarial Learning in\n  Segmenting Hemorrhagic Lesions from Fundus Images","summary":"  The hemorrhagic lesion segmentation plays a critical role in ophthalmic\ndiagnosis, directly influencing early disease detection, treatment planning,\nand therapeutic efficacy evaluation. However, the task faces significant\nchallenges due to lesion morphological variability, indistinct boundaries, and\nlow contrast with background tissues. To improve diagnostic accuracy and\ntreatment outcomes, developing advanced segmentation techniques remains\nimperative. This paper proposes an adversarial learning-based dynamic\narchitecture adjustment approach that integrates hierarchical U-shaped\nencoder-decoder, residual blocks, attention mechanisms, and ASPP modules. By\ndynamically optimizing feature fusion, our method enhances segmentation\nperformance. Experimental results demonstrate a Dice coefficient of 0.6802, IoU\nof 0.5602, Recall of 0.766, Precision of 0.6525, and Accuracy of 0.9955,\neffectively addressing the challenges in fundus image hemorrhage\nsegmentation.[* Corresponding author.]\n","authors":["Zesheng Li","Minwen Liao","Haoran Chen","Yan Su","Chengchang Pan","Honggang Qi"],"pdf_url":"https://arxiv.org/pdf/2502.09256v1.pdf","comment":"12 pages,4 figures"},{"id":"http://arxiv.org/abs/2410.08646v2","updated":"2025-02-13T12:10:11Z","published":"2024-10-11T09:16:30Z","title":"Fully Unsupervised Dynamic MRI Reconstruction via Diffeo-Temporal\n  Equivariance","summary":"  Reconstructing dynamic MRI image sequences from undersampled accelerated\nmeasurements is crucial for faster and higher spatiotemporal resolution\nreal-time imaging of cardiac motion, free breathing motion and many other\napplications. Classical paradigms, such as gated cine MRI, assume periodicity,\ndisallowing imaging of true motion. Supervised deep learning methods are\nfundamentally flawed as, in dynamic imaging, ground truth fully-sampled videos\nare impossible to truly obtain. We propose an unsupervised framework to learn\nto reconstruct dynamic MRI sequences from undersampled measurements alone by\nleveraging natural geometric spatiotemporal equivariances of MRI. Dynamic\nDiffeomorphic Equivariant Imaging (DDEI) significantly outperforms\nstate-of-the-art unsupervised methods such as SSDU on highly accelerated\ndynamic cardiac imaging. Our method is agnostic to the underlying neural\nnetwork architecture and can be used to adapt the latest models and\npost-processing approaches. Our code and video demos are at\nhttps://github.com/Andrewwango/ddei.\n","authors":["Andrew Wang","Mike Davies"],"pdf_url":"https://arxiv.org/pdf/2410.08646v2.pdf","comment":"Conference paper at ISBI 2025"},{"id":"http://arxiv.org/abs/2502.09211v1","updated":"2025-02-13T11:47:59Z","published":"2025-02-13T11:47:59Z","title":"Visual Graph Question Answering with ASP and LLMs for Language Parsing","summary":"  Visual Question Answering (VQA) is a challenging problem that requires to\nprocess multimodal input. Answer-Set Programming (ASP) has shown great\npotential in this regard to add interpretability and explainability to modular\nVQA architectures. In this work, we address the problem of how to integrate ASP\nwith modules for vision and natural language processing to solve a new and\ndemanding VQA variant that is concerned with images of graphs (not graphs in\nsymbolic form). Images containing graph-based structures are an ubiquitous and\npopular form of visualisation. Here, we deal with the particular problem of\ngraphs inspired by transit networks, and we introduce a novel dataset that\namends an existing one by adding images of graphs that resemble metro lines.\nOur modular neuro-symbolic approach combines optical graph recognition for\ngraph parsing, a pretrained optical character recognition neural network for\nparsing labels, Large Language Models (LLMs) for language processing, and ASP\nfor reasoning. This method serves as a first baseline and achieves an overall\naverage accuracy of 73% on the dataset. Our evaluation provides further\nevidence of the potential of modular neuro-symbolic systems, in particular with\npretrained models that do not involve any further training and logic\nprogramming for reasoning, to solve complex VQA tasks.\n","authors":["Jakob Johannes Bauer","Thomas Eiter","Nelson Higuera Ruiz","Johannes Oetsch"],"pdf_url":"https://arxiv.org/pdf/2502.09211v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453. This work was partially\n  funded from the Bosch Center for AI"},{"id":"http://arxiv.org/abs/2502.09202v1","updated":"2025-02-13T11:40:46Z","published":"2025-02-13T11:40:46Z","title":"Faster than real-time detection of shot boundaries, sampling structure\n  and dynamic keyframes in video","summary":"  The detection of shot boundaries (hardcuts and short dissolves), sampling\nstructure (progressive / interlaced / pulldown) and dynamic keyframes in a\nvideo are fundamental video analysis tasks which have to be done before any\nfurther high-level analysis tasks. We present a novel algorithm which does all\nthese analysis tasks in an unified way, by utilizing a combination of\ninter-frame and intra-frame measures derived from the motion field and\nnormalized cross correlation. The algorithm runs four times faster than\nreal-time due to sparse and selective calculation of these measures. An initial\nevaluation furthermore shows that the proposed algorithm is extremely robust\neven for challenging content showing large camera or object motion,\nflashlights, flicker or low contrast / noise.\n","authors":["Hannes Fassold"],"pdf_url":"https://arxiv.org/pdf/2502.09202v1.pdf","comment":"Accepted for ICISPC 2024"},{"id":"http://arxiv.org/abs/2502.07838v2","updated":"2025-02-13T11:13:14Z","published":"2025-02-11T02:31:45Z","title":"NanoVLMs: How small can we go and still make coherent Vision Language\n  Models?","summary":"  Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have\ngarnered significant research attention for their ability to leverage Large\nLanguage Models (LLMs) in multimodal tasks. However, their potential is\nconstrained by inherent challenges, including proprietary restrictions,\nsubstantial computational demands, and limited accessibility. Smaller models,\nsuch as GIT and BLIP, exhibit marked limitations, often failing to generate\ncoherent and consistent text beyond a few tokens, even with extensive training.\nThis underscores a pivotal inquiry: how small can a VLM be and still produce\nfluent and consistent text? Drawing inspiration from the exceptional learning\nprocess of 3-4 year old children, who rely heavily on visual cues for\nunderstanding and communication, we introduce two novel datasets: ShortDesc\n(featuring concise image descriptions) and LongDesc (containing more detailed\nimage descriptions). These datasets consist of image-text pairs where the text\nis restricted to the simple vocabulary and syntax typically used by young\nchildren, generated with a scaled- down model, GPT-4o. Using these datasets, we\ndemonstrate that it is possible to train VLMs that are significantly smaller,\nup to 10 times smaller than state of the art(SOTA) small VLMs while maintaining\narchitectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade\nthe text, as if stories written by students, on creativity, meaningfulness, and\nconsistency, assigning scores out of 10. This method addresses limitations of\nstandard benchmarks by accommodating unstructured outputs and providing a\nmultidimensional evaluation of the model capabilities. Our findings contribute\nto the development of lightweight, accessible multimodal models for resource\nconstrained environments.\n","authors":["Mukund Agarwalla","Himanshu Kumar","Raj Dandekar","Rajat Dandekar","Sreedath Panat"],"pdf_url":"https://arxiv.org/pdf/2502.07838v2.pdf","comment":"11 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09164v1","updated":"2025-02-13T10:48:11Z","published":"2025-02-13T10:48:11Z","title":"E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization","summary":"  We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.\n","authors":["Trung X. Pham","Zhang Kang","Ji Woo Hong","Xuran Zheng","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2502.09164v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2406.02548v3","updated":"2025-02-13T10:46:38Z","published":"2024-06-04T17:59:31Z","title":"Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance\n  Segmentation","summary":"  Recent works on open-vocabulary 3D instance segmentation show strong promise,\nbut at the cost of slow inference speed and high computation requirements. This\nhigh computation cost is typically due to their heavy reliance on 3D clip\nfeatures, which require computationally expensive 2D foundation models like\nSegment Anything (SAM) and CLIP for multi-view aggregation into 3D. As a\nconsequence, this hampers their applicability in many real-world applications\nthat require both fast and accurate predictions. To this end, we propose a fast\nyet accurate open-vocabulary 3D instance segmentation approach, named Open-YOLO\n3D, that effectively leverages only 2D object detection from multi-view RGB\nimages for open-vocabulary 3D instance segmentation. We address this task by\ngenerating class-agnostic 3D masks for objects in the scene and associating\nthem with text prompts. We observe that the projection of class-agnostic 3D\npoint cloud instances already holds instance information; thus, using SAM might\nonly result in redundancy that unnecessarily increases the inference time. We\nempirically find that a better performance of matching text prompts to 3D masks\ncan be achieved in a faster fashion with a 2D object detector. We validate our\nOpen-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios:\n(i) with ground truth masks, where labels are required for given object\nproposals, and (ii) with class-agnostic 3D proposals generated from a 3D\nproposal network. Our Open-YOLO 3D achieves state-of-the-art performance on\nboth datasets while obtaining up to $\\sim$16$\\times$ speedup compared to the\nbest existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D\nachieves mean average precision (mAP) of 24.7\\% while operating at 22 seconds\nper scene. Code and model are available at github.com/aminebdj/OpenYOLO3D.\n","authors":["Mohamed El Amine Boudjoghra","Angela Dai","Jean Lahoud","Hisham Cholakkal","Rao Muhammad Anwer","Salman Khan","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2406.02548v3.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.09150v1","updated":"2025-02-13T10:25:52Z","published":"2025-02-13T10:25:52Z","title":"Shortcut Learning Susceptibility in Vision Classifiers","summary":"  Shortcut learning, where machine learning models exploit spurious\ncorrelations in data instead of capturing meaningful features, poses a\nsignificant challenge to building robust and generalizable models. This\nphenomenon is prevalent across various machine learning applications, including\nvision, natural language processing, and speech recognition, where models may\nfind unintended cues that minimize training loss but fail to capture the\nunderlying structure of the data. Vision classifiers such as Convolutional\nNeural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers\n(ViTs) leverage distinct architectural principles to process spatial and\nstructural information, making them differently susceptible to shortcut\nlearning. In this study, we systematically evaluate these architectures by\nintroducing deliberate shortcuts into the dataset that are positionally\ncorrelated with class labels, creating a controlled setup to assess whether\nmodels rely on these artificial cues or learn actual distinguishing features.\nWe perform both quantitative evaluation by training on the shortcut-modified\ndataset and testing them on two different test sets -- one containing the same\nshortcuts and another without them -- to determine the extent of reliance on\nshortcuts. Additionally, qualitative evaluation is performed by using network\ninversion-based reconstruction techniques to analyze what the models\ninternalize in their weights, aiming to reconstruct the training data as\nperceived by the classifiers. We evaluate shortcut learning behavior across\nmultiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and\nCIFAR-10, to compare the susceptibility of different vision classifier\narchitectures to shortcut reliance and assess their varying degrees of\nsensitivity to spurious correlations.\n","authors":["Pirzada Suhail","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2502.09150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09148v1","updated":"2025-02-13T10:23:45Z","published":"2025-02-13T10:23:45Z","title":"Multimodal HIE Lesion Segmentation in Neonates: A Comparative Study of\n  Loss Functions","summary":"  Segmentation of Hypoxic-Ischemic Encephalopathy (HIE) lesions in neonatal MRI\nis a crucial but challenging task due to diffuse multifocal lesions with\nvarying volumes and the limited availability of annotated HIE lesion datasets.\nUsing the BONBID-HIE dataset, we implemented a 3D U-Net with optimized\npreprocessing, augmentation, and training strategies to overcome data\nconstraints. The goal of this study is to identify the optimal loss function\nspecifically for the HIE lesion segmentation task. To this end, we evaluated\nvarious loss functions, including Dice, Dice-Focal, Tversky, Hausdorff Distance\n(HausdorffDT) Loss, and two proposed compound losses -- Dice-Focal-HausdorffDT\nand Tversky-HausdorffDT -- to enhance segmentation performance. The results\nshow that different loss functions predict distinct segmentation masks, with\ncompound losses outperforming standalone losses. Tversky-HausdorffDT Loss\nachieves the highest Dice and Normalized Surface Dice scores, while\nDice-Focal-HausdorffDT Loss minimizes Mean Surface Distance. This work\nunderscores the significance of task-specific loss function optimization,\ndemonstrating that combining region-based and boundary-aware losses leads to\nmore accurate HIE lesion segmentation, even with limited training data.\n","authors":["Annayah Usman","Abdul Haseeb","Tahir Syed"],"pdf_url":"https://arxiv.org/pdf/2502.09148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09143v1","updated":"2025-02-13T10:18:44Z","published":"2025-02-13T10:18:44Z","title":"Feature-based Graph Attention Networks Improve Online Continual Learning","summary":"  Online continual learning for image classification is crucial for models to\nadapt to new data while retaining knowledge of previously learned tasks. This\ncapability is essential to address real-world challenges involving dynamic\nenvironments and evolving data distributions. Traditional approaches\npredominantly employ Convolutional Neural Networks, which are limited to\nprocessing images as grids and primarily capture local patterns rather than\nrelational information. Although the emergence of transformer architectures has\nimproved the ability to capture relationships, these models often require\nsignificantly larger resources. In this paper, we present a novel online\ncontinual learning framework based on Graph Attention Networks (GATs), which\neffectively capture contextual relationships and dynamically update the\ntask-specific representation via learned attention weights. Our approach\nutilizes a pre-trained feature extractor to convert images into graphs using\nhierarchical feature maps, representing information at varying levels of\ngranularity. These graphs are then processed by a GAT and incorporate an\nenhanced global pooling strategy to improve classification performance for\ncontinual learning. In addition, we propose the rehearsal memory duplication\ntechnique that improves the representation of the previous tasks while\nmaintaining the memory budget. Comprehensive evaluations on benchmark datasets,\nincluding SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the\nsuperiority of our method compared to the state-of-the-art methods.\n","authors":["Adjovi Sim","Zhengkui Wang","Aik Beng Ng","Shalini De Mello","Simon See","Wonmin Byeon"],"pdf_url":"https://arxiv.org/pdf/2502.09143v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2407.03635v2","updated":"2025-02-13T10:18:34Z","published":"2024-07-04T04:55:14Z","title":"SSP-IR: Semantic and Structure Priors for Diffusion-based Realistic\n  Image Restoration","summary":"  Realistic image restoration is a crucial task in computer vision, and\ndiffusion-based models for image restoration have garnered significant\nattention due to their ability to produce realistic results. Restoration can be\nseen as a controllable generation conditioning on priors. However, due to the\nseverity of image degradation, existing diffusion-based restoration methods\ncannot fully exploit priors from low-quality images and still have many\nchallenges in perceptual quality, semantic fidelity, and structure accuracy.\nBased on the challenges, we introduce a novel image restoration method, SSP-IR.\nOur approach aims to fully exploit semantic and structure priors from\nlow-quality images to guide the diffusion model in generating semantically\nfaithful and structurally accurate natural restoration results. Specifically,\nwe integrate the visual comprehension capabilities of Multimodal Large Language\nModels (explicit) and the visual representations of the original image\n(implicit) to acquire accurate semantic prior. To extract\ndegradation-independent structure prior, we introduce a Processor with RGB and\nFFT constraints to extract structure prior from the low-quality images, guiding\nthe diffusion model and preventing the generation of unreasonable artifacts.\nLastly, we employ a multi-level attention mechanism to integrate the acquired\nsemantic and structure priors. The qualitative and quantitative results\ndemonstrate that our method outperforms other state-of-the-art methods overall\non both synthetic and real-world datasets. Our project page is\nhttps://zyhrainbow.github.io/projects/SSP-IR.\n","authors":["Yuhong Zhang","Hengsheng Zhang","Zhengxue Cheng","Rong Xie","Li Song","Wenjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.03635v2.pdf","comment":"To be published in IEEE TCSVT"},{"id":"http://arxiv.org/abs/2502.09140v1","updated":"2025-02-13T10:15:16Z","published":"2025-02-13T10:15:16Z","title":"Replay-free Online Continual Learning with Self-Supervised MultiPatches","summary":"  Online Continual Learning (OCL) methods train a model on a non-stationary\ndata stream where only a few examples are available at a time, often leveraging\nreplay strategies. However, usage of replay is sometimes forbidden, especially\nin applications with strict privacy regulations. Therefore, we propose\nContinual MultiPatches (CMP), an effective plug-in for existing OCL\nself-supervised learning strategies that avoids the use of replay samples. CMP\ngenerates multiple patches from a single example and projects them into a\nshared feature space, where patches coming from the same example are pushed\ntogether without collapsing into a single point. CMP surpasses replay and other\nSSL-based strategies on OCL streams, challenging the role of replay as a go-to\nsolution for self-supervised OCL.\n","authors":["Giacomo Cignoni","Andrea Cossu","Alex Gomez-Villa","Joost van de Weijer","Antonio Carta"],"pdf_url":"https://arxiv.org/pdf/2502.09140v1.pdf","comment":"Accepted at ESANN 2025"},{"id":"http://arxiv.org/abs/2407.02371v3","updated":"2025-02-13T10:13:24Z","published":"2024-07-02T15:40:29Z","title":"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video\n  Generation","summary":"  Text-to-video (T2V) generation has recently garnered significant attention\nthanks to the large multi-modality model Sora. However, T2V generation still\nfaces two important challenges: 1) Lacking a precise open sourced high-quality\ndataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,\nare either with low quality or too large for most research institutions.\nTherefore, it is challenging but crucial to collect a precise high-quality\ntext-video pairs for T2V generation. 2) Ignoring to fully utilize textual\ninformation. Recent T2V methods have focused on vision transformers, using a\nsimple cross attention module for video generation, which falls short of\nthoroughly extracting semantic information from text prompt. To address these\nissues, we introduce OpenVid-1M, a precise high-quality dataset with expressive\ncaptions. This open-scenario dataset contains over 1 million text-video pairs,\nfacilitating research on T2V generation. Furthermore, we curate 433K 1080p\nvideos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition\nvideo generation. Additionally, we propose a novel Multi-modal Video Diffusion\nTransformer (MVDiT) capable of mining both structure information from visual\ntokens and semantic information from text tokens. Extensive experiments and\nablation studies verify the superiority of OpenVid-1M over previous datasets\nand the effectiveness of our MVDiT.\n","authors":["Kepan Nan","Rui Xie","Penghao Zhou","Tiehan Fan","Zhenheng Yang","Zhijie Chen","Xiang Li","Jian Yang","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2407.02371v3.pdf","comment":"20 pages, 15 figures, Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09125v1","updated":"2025-02-13T10:03:29Z","published":"2025-02-13T10:03:29Z","title":"Automatic Pruning via Structured Lasso with Class-wise Information","summary":"  Most pruning methods concentrate on unimportant filters of neural networks.\nHowever, they face the loss of statistical information due to a lack of\nconsideration for class-wise data. In this paper, from the perspective of\nleveraging precise class-wise information for model pruning, we utilize\nstructured lasso with guidance from Information Bottleneck theory. Our approach\nensures that statistical information is retained during the pruning process.\nWith these techniques, we introduce two innovative adaptive network pruning\nschemes: sparse graph-structured lasso pruning with Information Bottleneck\n(\\textbf{sGLP-IB}) and sparse tree-guided lasso pruning with Information\nBottleneck (\\textbf{sTLP-IB}). The key aspect is pruning model filters using\nsGLP-IB and sTLP-IB to better capture class-wise relatedness. Compared to\nmultiple state-of-the-art methods, our approaches demonstrate superior\nperformance across three datasets and six model architectures in extensive\nexperiments. For instance, using the VGG16 model on the CIFAR-10 dataset, we\nachieve a parameter reduction of 85%, a decrease in FLOPs by 61%, and maintain\nan accuracy of 94.10% (0.14% higher than the original model); we reduce the\nparameters by 55% with the accuracy at 76.12% using the ResNet architecture on\nImageNet (only drops 0.03%). In summary, we successfully reduce model size and\ncomputational resource usage while maintaining accuracy. Our codes are at\nhttps://anonymous.4open.science/r/IJCAI-8104.\n","authors":["Xiang Liu","Mingchen Li","Xia Li","Leigang Qu","Zifan Peng","Yijun Song","Zemin Liu","Linshan Jiang","Jialin Li"],"pdf_url":"https://arxiv.org/pdf/2502.09125v1.pdf","comment":"11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2310.17869v2","updated":"2025-02-13T10:02:14Z","published":"2023-10-27T03:07:05Z","title":"Grid Jigsaw Representation with CLIP: A New Perspective on Image\n  Clustering","summary":"  Unsupervised representation learning for image clustering is essential in\ncomputer vision. Although the advancement of visual models has improved image\nclustering with efficient visual representations, challenges still remain.\nFirstly, existing features often lack the ability to represent the internal\nstructure of images, hindering the accurate clustering of visually similar\nimages. Secondly, finer-grained semantic labels are often missing, limiting the\nability to capture nuanced differences and similarities between images. In this\npaper, we propose a new perspective on image clustering, the pretrain-based\nGrid Jigsaw Representation (pGJR). Inspired by human jigsaw puzzle processing,\nwe modify the traditional jigsaw learning to gain a more sequential and\nincremental understanding of image structure. We also leverage the pretrained\nCLIP to extract the prior features which can benefit from the enhanced\ncross-modal representation for richer and more nuanced semantic information and\nlabel level differentiation. Our experiments demonstrate that using the\npretrained model as a feature extractor can accelerate the convergence of\nclustering. We append the GJR module to pGJR and observe significant\nimprovements on common-use benchmark datasets. The experimental results\nhighlight the effectiveness of our approach in the clustering task, as\nevidenced by improvements in the ACC, NMI, and ARI metrics, as well as the\nsuper-fast convergence speed.\n","authors":["Zijie Song","Zhenzhen Hu","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2310.17869v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08925v3","updated":"2025-02-13T10:00:58Z","published":"2024-10-11T15:50:31Z","title":"An Overview of Prototype Formulations for Interpretable Deep Learning","summary":"  Prototypical part networks offer interpretable alternatives to black-box deep\nlearning models. However, many of these networks rely on Euclidean prototypes,\nwhich may limit their flexibility. This work provides a comprehensive overview\nof various prototype formulations. Experiments conducted on the CUB-200-2011,\nStanford Cars, and Oxford Flowers datasets demonstrate the effectiveness and\nversatility of these different formulations.\n","authors":["Maximilian Xiling Li","Korbinian Franz Rudolf","Nils Blank","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.08925v3.pdf","comment":"Equal Contribution of M.X.Li and K.F.Rudolf"},{"id":"http://arxiv.org/abs/2502.09122v1","updated":"2025-02-13T09:57:25Z","published":"2025-02-13T09:57:25Z","title":"Improving Deep Regression with Tightness","summary":"  For deep regression, preserving the ordinality of the targets with respect to\nthe feature representation improves performance across various tasks. However,\na theoretical explanation for the benefits of ordinality is still lacking. This\nwork reveals that preserving ordinality reduces the conditional entropy\n$H(Z|Y)$ of representation $Z$ conditional on the target $Y$. However, our\nfindings reveal that typical regression losses do little to reduce $H(Z|Y)$,\neven though it is vital for generalization performance. With this motivation,\nwe introduce an optimal transport-based regularizer to preserve the similarity\nrelationships of targets in the feature space to reduce $H(Z|Y)$. Additionally,\nwe introduce a simple yet efficient strategy of duplicating the regressor\ntargets, also with the aim of reducing $H(Z|Y)$. Experiments on three\nreal-world regression tasks verify the effectiveness of our strategies to\nimprove deep regression. Code:\nhttps://github.com/needylove/Regression_tightness.\n","authors":["Shihao Zhang","Yuguang Yan","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.09122v1.pdf","comment":"ICLR 2025, Code: https://github.com/needylove/Regression_tightness"},{"id":"http://arxiv.org/abs/2404.03713v2","updated":"2025-02-13T09:48:12Z","published":"2024-04-04T17:46:20Z","title":"Explaining Explainability: Recommendations for Effective Use of Concept\n  Activation Vectors","summary":"  Concept-based explanations translate the internal representations of deep\nlearning models into a language that humans are familiar with: concepts. One\npopular method for finding concepts is Concept Activation Vectors (CAVs), which\nare learnt using a probe dataset of concept exemplars. In this work, we\ninvestigate three properties of CAVs: (1) inconsistency across layers, (2)\nentanglement with other concepts, and (3) spatial dependency. Each property\nprovides both challenges and opportunities in interpreting models. We introduce\ntools designed to detect the presence of these properties, provide insight into\nhow each property can lead to misleading explanations, and provide\nrecommendations to mitigate their impact. To demonstrate practical\napplications, we apply our recommendations to a melanoma classification task,\nshowing how entanglement can lead to uninterpretable results and that the\nchoice of negative probe set can have a substantial impact on the meaning of a\nCAV. Further, we show that understanding these properties can be used to our\nadvantage. For example, we introduce spatially dependent CAVs to test if a\nmodel is translation invariant with respect to a specific concept and class.\nOur experiments are performed on natural images (ImageNet), skin lesions (ISIC\n2019), and a new synthetic dataset, Elements. Elements is designed to capture a\nknown ground truth relationship between concepts and classes. We release this\ndataset to facilitate further research in understanding and evaluating\ninterpretability methods.\n","authors":["Angus Nicolson","Lisa Schut","J. Alison Noble","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2404.03713v2.pdf","comment":"Accepted by Transactions on Machine Learning Research (02/2025)"},{"id":"http://arxiv.org/abs/2502.09111v1","updated":"2025-02-13T09:41:08Z","published":"2025-02-13T09:41:08Z","title":"DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance\n  Prior","summary":"  Gaussian SLAM systems excel in real-time rendering and fine-grained\nreconstruction compared to NeRF-based systems. However, their reliance on\nextensive keyframes is impractical for deployment in real-world robotic\nsystems, which typically operate under sparse-view conditions that can result\nin substantial holes in the map. To address these challenges, we introduce\nDenseSplat, the first SLAM system that effectively combines the advantages of\nNeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for\ninitializing primitives that densely populate maps and seamlessly fill gaps. It\nalso implements geometry-aware primitive sampling and pruning strategies to\nmanage granularity and enhance rendering efficiency. Moreover, DenseSplat\nintegrates loop closure and bundle adjustment, significantly enhancing\nframe-to-frame tracking accuracy. Extensive experiments on multiple large-scale\ndatasets demonstrate that DenseSplat achieves superior performance in tracking\nand mapping compared to current state-of-the-art methods.\n","authors":["Mingrui Li","Shuhong Liu","Tianchen Deng","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09110v1","updated":"2025-02-13T09:40:26Z","published":"2025-02-13T09:40:26Z","title":"Pulling Back the Curtain: Unsupervised Adversarial Detection via\n  Contrastive Auxiliary Networks","summary":"  Deep learning models are widely employed in safety-critical applications yet\nremain susceptible to adversarial attacks -- imperceptible perturbations that\ncan significantly degrade model performance. Conventional defense mechanisms\npredominantly focus on either enhancing model robustness or detecting\nadversarial inputs independently. In this work, we propose an Unsupervised\nadversarial detection via Contrastive Auxiliary Networks (U-CAN) to uncover\nadversarial behavior within auxiliary feature representations, without the need\nfor adversarial examples. U-CAN is embedded within selected intermediate layers\nof the target model. These auxiliary networks, comprising projection layers and\nArcFace-based linear layers, refine feature representations to more effectively\ndistinguish between benign and adversarial inputs. Comprehensive experiments\nacross multiple datasets (CIFAR-10, Mammals, and a subset of ImageNet) and\narchitectures (ResNet-50, VGG-16, and ViT) demonstrate that our method\nsurpasses existing unsupervised adversarial detection techniques, achieving\nsuperior F1 scores against four distinct attack methods. The proposed framework\nprovides a scalable and effective solution for enhancing the security and\nreliability of deep learning systems.\n","authors":["Eylon Mizrahi","Raz Lapid","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2502.09110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06657v2","updated":"2025-02-13T09:33:39Z","published":"2023-12-11T18:59:31Z","title":"Learning Naturally Aggregated Appearance for Efficient 3D Editing","summary":"  Neural radiance fields, which represent a 3D scene as a color field and a\ndensity field, have demonstrated great progress in novel view synthesis yet are\nunfavorable for editing due to the implicitness. This work studies the task of\nefficient 3D editing, where we focus on editing speed and user interactivity.\nTo this end, we propose to learn the color field as an explicit 2D appearance\naggregation, also called canonical image, with which users can easily customize\ntheir 3D editing via 2D image processing. We complement the canonical image\nwith a projection field that maps 3D points onto 2D pixels for texture query.\nThis field is initialized with a pseudo canonical camera model and optimized\nwith offset regularity to ensure the naturalness of the canonical image.\nExtensive experiments on different datasets suggest that our representation,\ndubbed AGAP, well supports various ways of 3D editing (e.g., stylization,\ninstance segmentation, and interactive drawing). Our approach demonstrates\nremarkable efficiency by being at least 20 times faster per edit compared to\nexisting NeRF-based editing methods. Project page is available at\nhttps://felixcheng97.github.io/AGAP/.\n","authors":["Ka Leong Cheng","Qiuyu Wang","Zifan Shi","Kecheng Zheng","Yinghao Xu","Hao Ouyang","Qifeng Chen","Yujun Shen"],"pdf_url":"https://arxiv.org/pdf/2312.06657v2.pdf","comment":"Project page: https://felixcheng97.github.io/AGAP/; accepted to 3DV\n  2025"},{"id":"http://arxiv.org/abs/2408.07422v2","updated":"2025-02-13T09:32:44Z","published":"2024-08-14T10:00:16Z","title":"LLMI3D: MLLM-based 3D Perception from a Single 2D Image","summary":"  Recent advancements in autonomous driving, augmented reality, robotics, and\nembodied intelligence have necessitated 3D perception algorithms. However,\ncurrent 3D perception methods, especially specialized small models, exhibit\npoor generalization in open scenarios. On the other hand, multimodal large\nlanguage models (MLLMs) excel in general capacity but underperform in 3D tasks,\ndue to weak 3D local spatial object perception, poor text-based geometric\nnumerical output, and inability to handle camera focal variations. To address\nthese challenges, we propose the following solutions: Spatial-Enhanced Local\nFeature Mining for better spatial feature extraction, 3D Query Token-Derived\nInfo Decoding for precise geometric regression, and Geometry Projection-Based\n3D Reasoning for handling camera focal length variations. We employ\nparameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a\npowerful 3D perception MLLM. Additionally, we have constructed the IG3D\ndataset, which provides fine-grained descriptions and question-answer\nannotations. Extensive experiments demonstrate that our LLMI3D achieves\nstate-of-the-art performance, outperforming other methods by a large margin.\n","authors":["Fan Yang","Sicheng Zhao","Yanhao Zhang","Hui Chen","Haonan Lu","Jungong Han","Guiguang Ding"],"pdf_url":"https://arxiv.org/pdf/2408.07422v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08299v2","updated":"2025-02-13T09:28:41Z","published":"2025-02-12T10:59:45Z","title":"When do they StOP?: A First Step Towards Automatically Identifying Team\n  Communication in the Operating Room","summary":"  Purpose: Surgical performance depends not only on surgeons' technical skills\nbut also on team communication within and across the different professional\ngroups present during the operation. Therefore, automatically identifying team\ncommunication in the OR is crucial for patient safety and advances in the\ndevelopment of computer-assisted surgical workflow analysis and intra-operative\nsupport systems. To take the first step, we propose a new task of detecting\ncommunication briefings involving all OR team members, i.e. the team Time-out\nand the StOP?-protocol, by localizing their start and end times in video\nrecordings of surgical operations. Methods: We generate an OR dataset of real\nsurgeries, called Team-OR, with more than one hundred hours of surgical videos\ncaptured by the multi-view camera system in the OR. The dataset contains\ntemporal annotations of 33 Time-out and 22 StOP?-protocol activities in total.\nWe then propose a novel group activity detection approach, where we encode both\nscene context and action features, and use an efficient neural network model to\noutput the results. Results: The experimental results on the Team-OR dataset\nshow that our approach outperforms existing state-of-the-art temporal action\ndetection approaches. It also demonstrates the lack of research on group\nactivities in the OR, proving the significance of our dataset. Conclusion: We\ninvestigate the Team Time-Out and the StOP?-protocol in the OR, by presenting\nthe first OR dataset with temporal annotations of group activities protocols,\nand introducing a novel group activity detection approach that outperforms\nexisting approaches. Code is available at\nhttps://github.com/CAMMA-public/Team-OR.\n","authors":["Keqi Chen","Lilien Schewski","Vinkle Srivastav","Jo√´l Lavanchy","Didier Mutter","Guido Beldi","Sandra Keller","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2502.08299v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09093v1","updated":"2025-02-13T09:04:28Z","published":"2025-02-13T09:04:28Z","title":"From Visuals to Vocabulary: Establishing Equivalence Between Image and\n  Text Token Through Autoregressive Pre-training in MLLMs","summary":"  While MLLMs perform well on perceptual tasks, they lack precise multimodal\nalignment, limiting performance. To address this challenge, we propose Vision\nDynamic Embedding-Guided Pretraining (VDEP), a hybrid autoregressive training\nparadigm for MLLMs. Utilizing dynamic embeddings from the MLP following the\nvisual encoder, this approach supervises image hidden states and integrates\nimage tokens into autoregressive training. Existing MLLMs primarily focused on\nrecovering information from textual inputs, often neglecting the effective\nprocessing of image data. In contrast, the key improvement of this work is the\nreinterpretation of multimodal alignment as a process of recovering information\nfrom input data, with particular emphasis on reconstructing detailed visual\nfeatures.The proposed method seamlessly integrates into standard models without\narchitectural changes. Experiments on 13 benchmarks show VDEP outperforms\nbaselines, surpassing existing methods.\n","authors":["Mingxiao Li","Fang Qu","Zhanpeng Chen","Na Su","Zhizhou Zhong","Ziyang Chen","Nan Du","Xiaolong Li"],"pdf_url":"https://arxiv.org/pdf/2502.09093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09088v1","updated":"2025-02-13T09:01:00Z","published":"2025-02-13T09:01:00Z","title":"Unsupervised Anomaly Detection on Implicit Shape representations for\n  Sarcopenia Detection","summary":"  Sarcopenia is an age-related progressive loss of muscle mass and strength\nthat significantly impacts daily life. A commonly studied criterion for\ncharacterizing the muscle mass has been the combination of 3D imaging and\nmanual segmentations. In this paper, we instead study the muscles' shape. We\nrely on an implicit neural representation (INR) to model normal muscle shapes.\nWe then introduce an unsupervised anomaly detection method to identify\nsarcopenic muscles based on the reconstruction error of the implicit model.\nRelying on a conditional INR with an auto-decoding strategy, we also learn a\nlatent representation of the muscles that clearly separates normal from\nabnormal muscles in an unsupervised fashion. Experimental results on a dataset\nof 103 segmented volumes indicate that our double anomaly detection strategy\neffectively discriminates sarcopenic and non-sarcopenic muscles.\n","authors":["Louise Piecuch","Jeremie Huet","Antoine Frouin","Antoine Nordez","Anne-Sophie Boureau","Diana Mateus"],"pdf_url":"https://arxiv.org/pdf/2502.09088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18880v3","updated":"2025-02-13T09:00:13Z","published":"2024-05-29T08:39:31Z","title":"EventZoom: A Progressive Approach to Event-Based Data Augmentation for\n  Enhanced Neuromorphic Vision","summary":"  Dynamic Vision Sensors (DVS) capture event data with high temporal resolution\nand low power consumption, presenting a more efficient solution for visual\nprocessing in dynamic and real-time scenarios compared to conventional video\ncapture methods. Event data augmentation serve as an essential method for\novercoming the limitation of scale and diversity in event datasets. Our\ncomparative experiments demonstrate that the two factors, spatial integrity and\ntemporal continuity, can significantly affect the capacity of event data\naugmentation, which are guarantee for maintaining the sparsity and high dynamic\nrange characteristics unique to event data. However, existing augmentation\nmethods often neglect the preservation of spatial integrity and temporal\ncontinuity. To address this, we developed a novel event data augmentation\nstrategy EventZoom, which employs a temporal progressive strategy, embedding\ntransformed samples into the original samples through progressive scaling and\nshifting. The scaling process avoids the spatial information loss associated\nwith cropping, while the progressive strategy prevents interruptions or abrupt\nchanges in temporal information. We validated EventZoom across various\nsupervised learning frameworks. The experimental results show that EventZoom\nconsistently outperforms existing event data augmentation methods with SOTA\nperformance. For the first time, we have concurrently employed Semi-supervised\nand Unsupervised learning to verify feasibility on event augmentation\nalgorithms, demonstrating the applicability and effectiveness of EventZoom as a\npowerful event-based data augmentation tool in handling real-world scenes with\nhigh dynamics and variability environments.\n","authors":["Yiting Dong","Xiang He","Guobin Shen","Dongcheng Zhao","Yang Li","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2405.18880v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2502.03966v3","updated":"2025-02-13T08:54:42Z","published":"2025-02-06T10:59:44Z","title":"MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation","summary":"  In this paper, we present synthetic data generation framework for flood\nhazard detection system. For high fidelity and quality, we characterize several\nreal-world properties into virtual world and simulate the flood situation by\ncontrolling them. For the sake of efficiency, recent generative models in\nimage-to-3D and urban city synthesis are leveraged to easily composite flood\nenvironments so that we avoid data bias due to the hand-crafted manner. Based\non our framework, we build the flood synthetic dataset with 5 levels, dubbed\nMultiFloodSynth which contains rich annotation types like normal map,\nsegmentation, 3D bounding box for a variety of downstream task. In experiments,\nour dataset demonstrate the enhanced performance of flood hazard detection with\non-par realism compared with real dataset.\n","authors":["YoonJe Kang","Yonghoon Jung","Wonseop Shin","Bumsoo Kim","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2502.03966v3.pdf","comment":"6 pages, 6 figures. Accepted as Oral Presentation to AAAI 2025\n  Workshop on Good-Data"},{"id":"http://arxiv.org/abs/2502.09080v1","updated":"2025-02-13T08:54:04Z","published":"2025-02-13T08:54:04Z","title":"BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian\n  Primitives for Weakly-Supervised Cross-View Localization","summary":"  This paper addresses the problem of weakly supervised cross-view\nlocalization, where the goal is to estimate the pose of a ground camera\nrelative to a satellite image with noisy ground truth annotations. A common\napproach to bridge the cross-view domain gap for pose estimation is Bird's-Eye\nView (BEV) synthesis. However, existing methods struggle with height ambiguity\ndue to the lack of depth information in ground images and satellite height\nmaps. Previous solutions either assume a flat ground plane or rely on complex\nmodels, such as cross-view transformers. We propose BevSplat, a novel method\nthat resolves height ambiguity by using feature-based Gaussian primitives. Each\npixel in the ground image is represented by a 3D Gaussian with semantic and\nspatial features, which are synthesized into a BEV feature map for relative\npose estimation. Additionally, to address challenges with panoramic query\nimages, we introduce an icosphere-based supervision strategy for the Gaussian\nprimitives. We validate our method on the widely used KITTI and VIGOR datasets,\nwhich include both pinhole and panoramic query images. Experimental results\nshow that BevSplat significantly improves localization accuracy over prior\napproaches.\n","authors":["Qiwei Wang","Shaoxun Wu","Yujiao Shi"],"pdf_url":"https://arxiv.org/pdf/2502.09080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09075v1","updated":"2025-02-13T08:45:43Z","published":"2025-02-13T08:45:43Z","title":"PTZ-Calib: Robust Pan-Tilt-Zoom Camera Calibration","summary":"  In this paper, we present PTZ-Calib, a robust two-stage PTZ camera\ncalibration method, that efficiently and accurately estimates camera parameters\nfor arbitrary viewpoints. Our method includes an offline and an online stage.\nIn the offline stage, we first uniformly select a set of reference images that\nsufficiently overlap to encompass a complete 360{\\deg} view. We then utilize\nthe novel PTZ-IBA (PTZ Incremental Bundle Adjustment) algorithm to\nautomatically calibrate the cameras within a local coordinate system.\nAdditionally, for practical application, we can further optimize camera\nparameters and align them with the geographic coordinate system using extra\nglobal reference 3D information. In the online stage, we formulate the\ncalibration of any new viewpoints as a relocalization problem. Our approach\nbalances the accuracy and computational efficiency to meet real-world demands.\nExtensive evaluations demonstrate our robustness and superior performance over\nstate-of-the-art methods on various real and synthetic datasets. Datasets and\nsource code can be accessed online at https://github.com/gjgjh/PTZ-Calib\n","authors":["Jinhui Guo","Lubin Fan","Bojian Wu","Jiaqi Gu","Shen Cao","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2502.09075v1.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2203.13310v5","updated":"2025-02-13T08:33:30Z","published":"2022-03-24T19:28:54Z","title":"MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection","summary":"  Monocular 3D object detection has long been a challenging task in autonomous\ndriving. Most existing methods follow conventional 2D detectors to first\nlocalize object centers, and then predict 3D attributes by neighboring\nfeatures. However, only using local visual features is insufficient to\nunderstand the scene-level 3D spatial structures and ignores the long-range\ninter-object depth relations. In this paper, we introduce the first DETR\nframework for Monocular DEtection with a depth-guided TRansformer, named\nMonoDETR. We modify the vanilla transformer to be depth-aware and guide the\nwhole detection process by contextual depth cues. Specifically, concurrent to\nthe visual encoder that captures object appearances, we introduce to predict a\nforeground depth map, and specialize a depth encoder to extract non-local depth\nembeddings. Then, we formulate 3D object candidates as learnable queries and\npropose a depth-guided decoder to conduct object-scene depth interactions. In\nthis way, each object query estimates its 3D attributes adaptively from the\ndepth-guided regions on the image and is no longer constrained to local visual\nfeatures. On KITTI benchmark with monocular images as input, MonoDETR achieves\nstate-of-the-art performance and requires no extra dense depth annotations.\nBesides, our depth-guided modules can also be plug-and-play to enhance\nmulti-view 3D object detectors on nuScenes dataset, demonstrating our superior\ngeneralization capacity. Code is available at\nhttps://github.com/ZrrSkywalker/MonoDETR.\n","authors":["Renrui Zhang","Han Qiu","Tai Wang","Ziyu Guo","Yiwen Tang","Xuanzhuo Xu","Ziteng Cui","Yu Qiao","Peng Gao","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2203.13310v5.pdf","comment":"Accepted by ICCV 2023. Code is available at\n  https://github.com/ZrrSkywalker/MonoDETR"},{"id":"http://arxiv.org/abs/2502.09064v1","updated":"2025-02-13T08:26:54Z","published":"2025-02-13T08:26:54Z","title":"StyleBlend: Enhancing Style-Specific Content Creation in Text-to-Image\n  Diffusion Models","summary":"  Synthesizing visually impressive images that seamlessly align both text\nprompts and specific artistic styles remains a significant challenge in\nText-to-Image (T2I) diffusion models. This paper introduces StyleBlend, a\nmethod designed to learn and apply style representations from a limited set of\nreference images, enabling content synthesis of both text-aligned and\nstylistically coherent. Our approach uniquely decomposes style into two\ncomponents, composition and texture, each learned through different strategies.\nWe then leverage two synthesis branches, each focusing on a corresponding style\ncomponent, to facilitate effective style blending through shared features\nwithout affecting content generation. StyleBlend addresses the common issues of\ntext misalignment and weak style representation that previous methods have\nstruggled with. Extensive qualitative and quantitative comparisons demonstrate\nthe superiority of our approach.\n","authors":["Zichong Chen","Shijin Wang","Yang Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.09064v1.pdf","comment":"Accepted to Eurographics 2025. Project page:\n  https://zichongc.github.io/StyleBlend/"},{"id":"http://arxiv.org/abs/2410.02130v2","updated":"2025-02-13T08:24:37Z","published":"2024-10-03T01:23:44Z","title":"MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers\n  for Open-Domain Sound Generation","summary":"  We introduce MDSGen, a novel framework for vision-guided open-domain sound\ngeneration optimized for model parameter size, memory consumption, and\ninference speed. This framework incorporates two key innovations: (1) a\nredundant video feature removal module that filters out unnecessary visual\ninformation, and (2) a temporal-aware masking strategy that leverages temporal\ncontext for enhanced audio generation accuracy. In contrast to existing\nresource-heavy Unet-based models, \\texttt{MDSGen} employs denoising masked\ndiffusion transformers, facilitating efficient generation without reliance on\npre-trained diffusion models. Evaluated on the benchmark VGGSound dataset, our\nsmallest model (5M parameters) achieves $97.9$% alignment accuracy, using\n$172\\times$ fewer parameters, $371$% less memory, and offering $36\\times$\nfaster inference than the current 860M-parameter state-of-the-art model\n($93.9$% accuracy). The larger model (131M parameters) reaches nearly $99$%\naccuracy while requiring $6.5\\times$ fewer parameters. These results highlight\nthe scalability and effectiveness of our approach. The code is available at\nhttps://bit.ly/mdsgen.\n","authors":["Trung X. Pham","Tri Ton","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2410.02130v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09057v1","updated":"2025-02-13T08:11:10Z","published":"2025-02-13T08:11:10Z","title":"Vision-Language In-Context Learning Driven Few-Shot Visual Inspection\n  Model","summary":"  We propose general visual inspection model using Vision-Language Model~(VLM)\nwith few-shot images of non-defective or defective products, along with\nexplanatory texts that serve as inspection criteria. Although existing VLM\nexhibit high performance across various tasks, they are not trained on specific\ntasks such as visual inspection. Thus, we construct a dataset consisting of\ndiverse images of non-defective and defective products collected from the web,\nalong with unified formatted output text, and fine-tune VLM. For new products,\nour method employs In-Context Learning, which allows the model to perform\ninspections with an example of non-defective or defective image and the\ncorresponding explanatory texts with visual prompts. This approach eliminates\nthe need to collect a large number of training samples and re-train the model\nfor each product. The experimental results show that our method achieves high\nperformance, with MCC of 0.804 and F1-score of 0.950 on MVTec AD in a one-shot\nmanner. Our code is available\nat~https://github.com/ia-gu/Vision-Language-In-Context-Learning-Driven-Few-Shot-Visual-Inspection-Model.\n","authors":["Shiryu Ueno","Yoshikazu Hayashi","Shunsuke Nakatsuka","Yusei Yamada","Hiroaki Aizawa","Kunihito Kato"],"pdf_url":"https://arxiv.org/pdf/2502.09057v1.pdf","comment":"VISAPP 2025"},{"id":"http://arxiv.org/abs/2212.08650v3","updated":"2025-02-13T08:06:42Z","published":"2022-12-16T18:51:41Z","title":"ColorSense: A Study on Color Vision in Machine Visual Recognition","summary":"  Color vision is essential for human visual perception, but its impact on\nmachine perception is still underexplored. There has been an intensified demand\nfor understanding its role in machine perception for safety-critical tasks such\nas assistive driving and surgery but lacking suitable datasets. To fill this\ngap, we curate multipurpose datasets ColorSense, by collecting 110,000\nnon-trivial human annotations of foreground and background color labels from\npopular visual recognition benchmarks. To investigate the impact of color\nvision on machine perception, we assign each image a color discrimination level\nbased on its dominant foreground and background colors and use it to study the\nimpact of color vision on machine perception. We validate the use of our\ndatasets by demonstrating that the level of color discrimination has a\ndominating effect on the performance of mainstream machine perception models.\nSpecifically, we examine the perception ability of machine vision by\nconsidering key factors such as model architecture, training objective, model\nsize, training data, and task complexity. Furthermore, to investigate how color\nand environmental factors affect the robustness of visual recognition in\nmachine perception, we integrate our ColorSense datasets with image corruptions\nand perform a more comprehensive visual perception evaluation. Our findings\nsuggest that object recognition tasks such as classification and localization\nare susceptible to color vision bias, especially for high-stakes cases such as\nvehicle classes, and advanced mitigation techniques such as data augmentation\nand so on only give marginal improvement. Our analyses highlight the need for\nnew approaches toward the performance evaluation of machine perception models\nin real-world applications. Lastly, we present various potential applications\nof ColorSense such as studying spurious correlations.\n","authors":["Ming-Chang Chiu","Yingfei Wang","Derrick Eui Gyu Kim","Pin-Yu Chen","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2212.08650v3.pdf","comment":"12 pages, 11 figures, Accepted at Secure and Trustworthy Machine\n  Learning"},{"id":"http://arxiv.org/abs/2502.09051v1","updated":"2025-02-13T08:05:44Z","published":"2025-02-13T08:05:44Z","title":"AIDE: Agentically Improve Visual Language Model with Domain Experts","summary":"  The enhancement of Visual Language Models (VLMs) has traditionally relied on\nknowledge distillation from larger, more capable models. This dependence\ncreates a fundamental bottleneck for improving state-of-the-art systems,\nparticularly when no superior models exist. We introduce AIDE (Agentic\nImprovement through Domain Experts), a novel framework that enables VLMs to\nautonomously enhance their capabilities by leveraging specialized domain expert\nmodels. AIDE operates through a four-stage process: (1) identifying instances\nfor refinement, (2) engaging domain experts for targeted analysis, (3)\nsynthesizing expert outputs with existing data, and (4) integrating enhanced\ninstances into the training pipeline. Experiments on multiple benchmarks,\nincluding MMMU, MME, MMBench, etc., demonstrate AIDE's ability to achieve\nnotable performance gains without relying on larger VLMs nor human supervision.\nOur framework provides a scalable, resource-efficient approach to continuous\nVLM improvement, addressing critical limitations in current methodologies,\nparticularly valuable when larger models are unavailable to access.\n","authors":["Ming-Chang Chiu","Fuxiao Liu","Karan Sapra","Andrew Tao","Yaser Jacoob","Xuezhe Ma","Zhiding Yu","Guilin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.09051v1.pdf","comment":"6 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.09045v1","updated":"2025-02-13T08:01:29Z","published":"2025-02-13T08:01:29Z","title":"Evolution of Data-driven Single- and Multi-Hazard Susceptibility Mapping\n  and Emergence of Deep Learning Methods","summary":"  Data-driven susceptibility mapping of natural hazards has harnessed the\nadvances in classification methods used on heterogeneous sources represented as\nraster images. Susceptibility mapping is an important step towards risk\nassessment for any natural hazard. Increasingly, multiple hazards co-occur\nspatially, temporally, or both, which calls for an in-depth study on\nmulti-hazard susceptibility mapping. In recent years, single-hazard\nsusceptibility mapping algorithms have become well-established and have been\nextended to multi-hazard susceptibility mapping. Deep learning is also emerging\nas a promising method for single-hazard susceptibility mapping. Here, we\ndiscuss the evolution of methods for a single hazard, their extensions to\nmulti-hazard maps as a late fusion of decisions, and the use of deep learning\nmethods in susceptibility mapping. We finally propose a vision for adapting\ndata fusion strategies in multimodal deep learning to multi-hazard\nsusceptibility mapping. From the background study of susceptibility methods, we\ndemonstrate that deep learning models are promising, untapped methods for\nmulti-hazard susceptibility mapping. Data fusion strategies provide a larger\nspace of deep learning models applicable to multi-hazard susceptibility\nmapping.\n","authors":["Jaya Sreevalsan-Nair","Aswathi Mundayatt"],"pdf_url":"https://arxiv.org/pdf/2502.09045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09039v1","updated":"2025-02-13T07:48:56Z","published":"2025-02-13T07:48:56Z","title":"Large Images are Gaussians: High-Quality Large Image Representation with\n  Levels of 2D Gaussian Splatting","summary":"  While Implicit Neural Representations (INRs) have demonstrated significant\nsuccess in image representation, they are often hindered by large training\nmemory and slow decoding speed. Recently, Gaussian Splatting (GS) has emerged\nas a promising solution in 3D reconstruction due to its high-quality novel view\nsynthesis and rapid rendering capabilities, positioning it as a valuable tool\nfor a broad spectrum of applications. In particular, a GS-based representation,\n2DGS, has shown potential for image fitting. In our work, we present\n\\textbf{L}arge \\textbf{I}mages are \\textbf{G}aussians (\\textbf{LIG}), which\ndelves deeper into the application of 2DGS for image representations,\naddressing the challenge of fitting large images with 2DGS in the situation of\nnumerous Gaussian points, through two distinct modifications: 1) we adopt a\nvariant of representation and optimization strategy, facilitating the fitting\nof a large number of Gaussian points; 2) we propose a Level-of-Gaussian\napproach for reconstructing both coarse low-frequency initialization and fine\nhigh-frequency details. Consequently, we successfully represent large images as\nGaussian points and achieve high-quality large image representation,\ndemonstrating its efficacy across various types of large images. Code is\navailable at\n{\\href{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}.\n","authors":["Lingting Zhu","Guying Lin","Jinnan Chen","Xinjie Zhang","Zhenchao Jin","Zhao Wang","Lequan Yu"],"pdf_url":"https://arxiv.org/pdf/2502.09039v1.pdf","comment":"Accepted by 39th Annual AAAI Conference on Artificial Intelligence\n  (AAAI 2025). 10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.01694v2","updated":"2025-02-13T07:42:33Z","published":"2024-12-02T16:37:50Z","title":"Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation","summary":"  This paper tackles the problem of video question answering (VideoQA), a task\nthat often requires multi-step reasoning and a profound understanding of\nspatial-temporal dynamics. While large video-language models perform well on\nbenchmarks, they often lack explainability and spatial-temporal grounding. In\nthis paper, we propose Agent-of-Thoughts Distillation (AoTD), a method that\nenhances models by incorporating automatically generated Chain-of-Thoughts\n(CoTs) into the instruction-tuning process. Specifically, we leverage an\nagent-based system to decompose complex questions into sub-tasks, and address\nthem with specialized vision models, the intermediate results are then treated\nas reasoning chains. We also introduce a verification mechanism using a large\nlanguage model (LLM) to ensure the reliability of generated CoTs. Extensive\nexperiments demonstrate that AoTD improves the performance on multiple-choice\nand open-ended benchmarks.\n","authors":["Yudi Shi","Shangzhe Di","Qirui Chen","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2412.01694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11876v3","updated":"2025-02-13T07:31:32Z","published":"2023-09-21T08:22:44Z","title":"Multi-level Asymmetric Contrastive Learning for Volumetric Medical Image\n  Segmentation Pre-training","summary":"  Medical image segmentation is a fundamental yet challenging task due to the\narduous process of acquiring large volumes of high-quality labeled data from\nexperts. Contrastive learning offers a promising but still problematic solution\nto this dilemma. Firstly existing medical contrastive learning strategies focus\non extracting image-level representation, which ignores abundant multi-level\nrepresentations. Furthermore they underutilize the decoder either by random\ninitialization or separate pre-training from the encoder, thereby neglecting\nthe potential collaboration between the encoder and decoder. To address these\nissues, we propose a novel multi-level asymmetric contrastive learning\nframework named MACL for volumetric medical image segmentation pre-training.\nSpecifically, we design an asymmetric contrastive learning structure to\npre-train encoder and decoder simultaneously to provide better initialization\nfor segmentation models. Moreover, we develop a multi-level contrastive\nlearning strategy that integrates correspondences across feature-level,\nimage-level, and pixel-level representations to ensure the encoder and decoder\ncapture comprehensive details from representations of varying scales and\ngranularities during the pre-training phase. Finally, experiments on 8 medical\nimage datasets indicate our MACL framework outperforms existing 11 contrastive\nlearning strategies. i.e. Our MACL achieves a superior performance with more\nprecise predictions from visualization figures and 1.72%, 7.87%, 2.49% and\n1.48% Dice higher than previous best results on ACDC, MMWHS, HVSMR and CHAOS\nwith 10% labeled data, respectively. And our MACL also has a strong\ngeneralization ability among 5 variant U-Net backbones. Our code will be\nreleased at https://github.com/stevezs315/MACL.\n","authors":["Shuang Zeng","Lei Zhu","Xinliang Zhang","Micky C Nnamdi","Wenqi Shi","J Ben Tamo","Qian Chen","Hangzhou He","Lujia Jin","Zifeng Tian","Qiushi Ren","Zhaoheng Xie","Yanye Lu"],"pdf_url":"https://arxiv.org/pdf/2309.11876v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09026v1","updated":"2025-02-13T07:31:03Z","published":"2025-02-13T07:31:03Z","title":"Billet Number Recognition Based on Test-Time Adaptation","summary":"  During the steel billet production process, it is essential to recognize\nmachine-printed or manually written billet numbers on moving billets in\nreal-time. To address the issue of low recognition accuracy for existing scene\ntext recognition methods, caused by factors such as image distortions and\ndistribution differences between training and test data, we propose a billet\nnumber recognition method that integrates test-time adaptation with prior\nknowledge. First, we introduce a test-time adaptation method into a model that\nuses the DB network for text detection and the SVTR network for text\nrecognition. By minimizing the model's entropy during the testing phase, the\nmodel can adapt to the distribution of test data without the need for\nsupervised fine-tuning. Second, we leverage the billet number encoding rules as\nprior knowledge to assess the validity of each recognition result. Invalid\nresults, which do not comply with the encoding rules, are replaced. Finally, we\nintroduce a validation mechanism into the CTC algorithm using prior knowledge\nto address its limitations in recognizing damaged characters. Experimental\nresults on real datasets, including both machine-printed billet numbers and\nhandwritten billet numbers, show significant improvements in evaluation\nmetrics, validating the effectiveness of the proposed method.\n","authors":["Yuan Wei","Xiuzhuang Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.09026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09020v1","updated":"2025-02-13T07:16:16Z","published":"2025-02-13T07:16:16Z","title":"EventSTR: A Benchmark Dataset and Baselines for Event Stream based Scene\n  Text Recognition","summary":"  Mainstream Scene Text Recognition (STR) algorithms are developed based on RGB\ncameras which are sensitive to challenging factors such as low illumination,\nmotion blur, and cluttered backgrounds. In this paper, we propose to recognize\nthe scene text using bio-inspired event cameras by collecting and annotating a\nlarge-scale benchmark dataset, termed EventSTR. It contains 9,928\nhigh-definition (1280 * 720) event samples and involves both Chinese and\nEnglish characters. We also benchmark multiple STR algorithms as the baselines\nfor future works to compare. In addition, we propose a new event-based scene\ntext recognition framework, termed SimC-ESTR. It first extracts the event\nfeatures using a visual encoder and projects them into tokens using a Q-former\nmodule. More importantly, we propose to augment the vision tokens based on a\nmemory mechanism before feeding into the large language models. A\nsimilarity-based error correction mechanism is embedded within the large\nlanguage model to correct potential minor errors fundamentally based on\ncontextual information. Extensive experiments on the newly proposed EventSTR\ndataset and two simulation STR datasets fully demonstrate the effectiveness of\nour proposed model. We believe that the dataset and algorithmic model can\ninnovatively propose an event-based STR task and are expected to accelerate the\napplication of event cameras in various industries. The source code and\npre-trained models will be released on https://github.com/Event-AHU/EventSTR\n","authors":["Xiao Wang","Jingtao Jiang","Dong Li","Futian Wang","Lin Zhu","Yaowei Wang","Yongyong Tian","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2502.09020v1.pdf","comment":"In Peer Review"},{"id":"http://arxiv.org/abs/2412.13655v2","updated":"2025-02-13T07:13:57Z","published":"2024-12-18T09:34:32Z","title":"VIIS: Visible and Infrared Information Synthesis for Severe Low-light\n  Image Enhancement","summary":"  Images captured in severe low-light circumstances often suffer from\nsignificant information absence. Existing singular modality image enhancement\nmethods struggle to restore image regions lacking valid information. By\nleveraging light-impervious infrared images, visible and infrared image fusion\nmethods have the potential to reveal information hidden in darkness. However,\nthey primarily emphasize inter-modal complementation but neglect intra-modal\nenhancement, limiting the perceptual quality of output images. To address these\nlimitations, we propose a novel task, dubbed visible and infrared information\nsynthesis (VIIS), which aims to achieve both information enhancement and fusion\nof the two modalities. Given the difficulty in obtaining ground truth in the\nVIIS task, we design an information synthesis pretext task (ISPT) based on\nimage augmentation. We employ a diffusion model as the framework and design a\nsparse attention-based dual-modalities residual (SADMR) conditioning mechanism\nto enhance information interaction between the two modalities. This mechanism\nenables features with prior knowledge from both modalities to adaptively and\niteratively attend to each modality's information during the denoising process.\nOur extensive experiments demonstrate that our model qualitatively and\nquantitatively outperforms not only the state-of-the-art methods in relevant\nfields but also the newly designed baselines capable of both information\nenhancement and fusion. The code is available at\nhttps://github.com/Chenz418/VIIS.\n","authors":["Chen Zhao","Mengyuan Yu","Fan Yang","Peiguang Jing"],"pdf_url":"https://arxiv.org/pdf/2412.13655v2.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2502.09018v1","updated":"2025-02-13T07:11:07Z","published":"2025-02-13T07:11:07Z","title":"Zero-shot Concept Bottleneck Models","summary":"  Concept bottleneck models (CBMs) are inherently interpretable and\nintervenable neural network models, which explain their final label prediction\nby the intermediate prediction of high-level semantic concepts. However, they\nrequire target task training to learn input-to-concept and concept-to-label\nmappings, incurring target dataset collections and training resources. In this\npaper, we present \\textit{zero-shot concept bottleneck models} (Z-CBMs), which\npredict concepts and labels in a fully zero-shot manner without training neural\nnetworks. Z-CBMs utilize a large-scale concept bank, which is composed of\nmillions of vocabulary extracted from the web, to describe arbitrary input in\nvarious domains. For the input-to-concept mapping, we introduce concept\nretrieval, which dynamically finds input-related concepts by the cross-modal\nsearch on the concept bank. In the concept-to-label inference, we apply concept\nregression to select essential concepts from the retrieved concepts by sparse\nlinear regression. Through extensive experiments, we confirm that our Z-CBMs\nprovide interpretable and intervenable concepts without any additional\ntraining. Code will be available at https://github.com/yshinya6/zcbm.\n","authors":["Shin'ya Yamaguchi","Kosuke Nishida","Daiki Chijiwa","Yasutoshi Ida"],"pdf_url":"https://arxiv.org/pdf/2502.09018v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.03628v2","updated":"2025-02-13T07:05:53Z","published":"2023-12-06T17:19:00Z","title":"Boosting Segment Anything Model Towards Open-Vocabulary Learning","summary":"  The recent Segment Anything Model (SAM) has emerged as a new paradigmatic\nvision foundation model, showcasing potent zero-shot generalization and\nflexible prompting. Despite SAM finding applications and adaptations in various\ndomains, its primary limitation lies in the inability to grasp object\nsemantics. In this paper, we present Sambor to seamlessly integrate SAM with\nthe open-vocabulary object detector in an end-to-end framework. While retaining\nall the remarkable capabilities inherent to SAM, we boost it to detect\narbitrary objects from human inputs like category names or reference\nexpressions. Building upon the SAM image encoder, we introduce a novel\nSideFormer module designed to acquire SAM features adept at perceiving objects\nand inject comprehensive semantic information for recognition. In addition, we\ndevise an Open-set RPN that leverages SAM proposals to assist in finding\npotential objects. Consequently, Sambor enables the open-vocabulary detector to\nequally focus on generalizing both localization and classification sub-tasks.\nOur approach demonstrates superior zero-shot performance across benchmarks,\nincluding COCO and LVIS, proving highly competitive against previous\nstate-of-the-art methods. We aspire for this work to serve as a meaningful\nendeavor in endowing SAM to recognize diverse object categories and advancing\nopen-vocabulary learning with the support of vision foundation models.\n","authors":["Xumeng Han","Longhui Wei","Xuehui Yu","Zhiyang Dou","Xin He","Kuiran Wang","Yingfei Sun","Zhenjun Han","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2312.03628v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.01061v2","updated":"2025-02-13T06:56:29Z","published":"2025-02-03T05:17:32Z","title":"OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human\n  Animation Models","summary":"  End-to-end human animation, such as audio-driven talking human generation,\nhas undergone notable advancements in the recent few years. However, existing\nmethods still struggle to scale up as large general video generation models,\nlimiting their potential in real applications. In this paper, we propose\nOmniHuman, a Diffusion Transformer-based framework that scales up data by\nmixing motion-related conditions into the training phase. To this end, we\nintroduce two training principles for these mixed conditions, along with the\ncorresponding model architecture and inference strategy. These designs enable\nOmniHuman to fully leverage data-driven motion generation, ultimately achieving\nhighly realistic human video generation. More importantly, OmniHuman supports\nvarious portrait contents (face close-up, portrait, half-body, full-body),\nsupports both talking and singing, handles human-object interactions and\nchallenging body poses, and accommodates different image styles. Compared to\nexisting end-to-end audio-driven methods, OmniHuman not only produces more\nrealistic videos, but also offers greater flexibility in inputs. It also\nsupports multiple driving modalities (audio-driven, video-driven and combined\ndriving signals). Video samples are provided on the ttfamily project page\n(https://omnihuman-lab.github.io)\n","authors":["Gaojie Lin","Jianwen Jiang","Jiaqi Yang","Zerong Zheng","Chao Liang"],"pdf_url":"https://arxiv.org/pdf/2502.01061v2.pdf","comment":"https://omnihuman-lab.github.io/"},{"id":"http://arxiv.org/abs/2502.08636v2","updated":"2025-02-13T06:42:15Z","published":"2025-02-12T18:53:20Z","title":"PulseCheck457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large\n  Multimodal Models","summary":"  Although large multimodal models (LMMs) have demonstrated remarkable\ncapabilities in visual scene interpretation and reasoning, their capacity for\ncomplex and precise 3-dimensional spatial reasoning remains uncertain. Existing\nbenchmarks focus predominantly on 2D spatial understanding and lack a framework\nto comprehensively evaluate 6D spatial reasoning across varying complexities.\nTo address this limitation, we present PulseCheck457, a scalable and unbiased\nsynthetic dataset designed with 4 key capability for spatial reasoning:\nmulti-object recognition, 2D location, 3D location, and 3D orientation. We\ndevelop a cascading evaluation structure, constructing 7 question types across\n5 difficulty levels that range from basic single object recognition to our new\nproposed complex 6D spatial reasoning tasks. We evaluated various large\nmultimodal models (LMMs) on PulseCheck457, observing a general decline in\nperformance as task complexity increases, particularly in 3D reasoning and 6D\nspatial tasks. To quantify these challenges, we introduce the Relative\nPerformance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning\ncapabilities. Leveraging the unbiased attribute design of our dataset, we also\nuncover prediction biases across different attributes, with similar patterns\nobserved in real-world image settings.\n","authors":["Xingrui Wang","Wufei Ma","Tiezheng Zhang","Celso M de Melo","Jieneng Chen","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2502.08636v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09000v1","updated":"2025-02-13T06:32:19Z","published":"2025-02-13T06:32:19Z","title":"Residual Transformer Fusion Network for Salt and Pepper Image Denoising","summary":"  Convolutional Neural Network (CNN) has been widely used in unstructured\ndatasets, one of which is image denoising. Image denoising is a noisy image\nreconstruction process that aims to reduce additional noise that occurs from\nthe noisy image with various strategies. Image denoising has a problem, namely\nthat some image denoising methods require some prior knowledge of information\nabout noise. To overcome this problem, a combined architecture of Convolutional\nVision Transformer (CvT) and Residual Networks (ResNet) is used which is called\nthe Residual Transformer Fusion Network (RTF-Net). In general, the process in\nthis architecture can be divided into two parts, Noise Suppression Network\n(NSN) and Structure Enhancement Network (SEN). Residual Block is used in the\nNoise Suppression Network and is used to learn the noise map in the image,\nwhile the CvT is used in the Structure Enhancement Network and is used to learn\nthe details that need to be added to the image processed by the Noise\nSuppression Network. The model was trained using the DIV2K Training Set\ndataset, and validation using the DIV2K Validation Set. After doing the\ntraining, the model was tested using Lena, Bridge, Pepper, and BSD300 images\nwith noise levels ranging from 30%, 50%, and 70% and the PSNR results were\ncompared with the DBA, NASNLM, PARIGI, NLSF, NLSF-MLP and NLSF-CNN methods. The\ntest results show that the proposed method is superior in all cases except for\nPepper's image with a noise level of 30%, where NLSF-CNN is superior with a\nPSNR value of 32.99 dB, while the proposed method gets a PSNR value of 31.70\ndB.\n","authors":["Bintang Pradana Erlangga Putra","Heri Prasetyo","Esti Suryani"],"pdf_url":"https://arxiv.org/pdf/2502.09000v1.pdf","comment":"8 pages, 17 figures"},{"id":"http://arxiv.org/abs/2411.07742v2","updated":"2025-02-13T06:27:51Z","published":"2024-11-12T12:07:27Z","title":"Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial\n  Pruning","summary":"  This paper studies point cloud perception within outdoor environments.\nExisting methods face limitations in recognizing objects located at a distance\nor occluded, due to the sparse nature of outdoor point clouds. In this work, we\nobserve a significant mitigation of this problem by accumulating multiple\ntemporally consecutive LiDAR sweeps, resulting in a remarkable improvement in\nperception accuracy. However, the computation cost also increases, hindering\nprevious approaches from utilizing a large number of LiDAR sweeps. To tackle\nthis challenge, we find that a considerable portion of points in the\naccumulated point cloud is redundant, and discarding these points has minimal\nimpact on perception accuracy. We introduce a simple yet effective Gumbel\nSpatial Pruning (GSP) layer that dynamically prunes points based on a learned\nend-to-end sampling. The GSP layer is decoupled from other network components\nand thus can be seamlessly integrated into existing point cloud network\narchitectures. Without incurring additional computational overhead, we increase\nthe number of LiDAR sweeps from 10, a common practice, to as many as 40.\nConsequently, there is a significant enhancement in perception performance. For\ninstance, in nuScenes 3D object detection and BEV map segmentation tasks, our\npruning strategy improves the vanilla TransL baseline and other baseline\nmethods.\n","authors":["Tianyu Sun","Jianhao Li","Xueqian Zhang","Zhongdao Wang","Bailan Feng","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.07742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08997v1","updated":"2025-02-13T06:24:07Z","published":"2025-02-13T06:24:07Z","title":"Hierarchical Vision Transformer with Prototypes for Interpretable\n  Medical Image Classification","summary":"  Explainability is a highly demanded requirement for applications in high-risk\nareas such as medicine. Vision Transformers have mainly been limited to\nattention extraction to provide insight into the model's reasoning. Our\napproach combines the high performance of Vision Transformers with the\nintroduction of new explainability capabilities. We present HierViT, a Vision\nTransformer that is inherently interpretable and adapts its reasoning to that\nof humans. A hierarchical structure is used to process domain-specific features\nfor prediction. It is interpretable by design, as it derives the target output\nwith human-defined features that are visualized by exemplary images\n(prototypes). By incorporating domain knowledge about these decisive features,\nthe reasoning is semantically similar to human reasoning and therefore\nintuitive. Moreover, attention heatmaps visualize the crucial regions for\nidentifying each feature, thereby providing HierViT with a versatile tool for\nvalidating predictions. Evaluated on two medical benchmark datasets, LIDC-IDRI\nfor lung nodule assessment and derm7pt for skin lesion classification, HierViT\nachieves superior and comparable prediction accuracy, respectively, while\noffering explanations that align with human reasoning.\n","authors":["Luisa Gall√©e","Catharina Silvia Lisson","Meinrad Beer","Michael G√∂tz"],"pdf_url":"https://arxiv.org/pdf/2502.08997v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08163v2","updated":"2025-02-13T06:08:21Z","published":"2025-01-14T14:41:51Z","title":"DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction","summary":"  The accelerated MRI reconstruction poses a challenging ill-posed inverse\nproblem due to the significant undersampling in k-space. Deep neural networks,\nsuch as CNNs and ViT, have shown substantial performance improvements for this\ntask while encountering the dilemma between global receptive fields and\nefficient computation. To this end, this paper pioneers exploring Mamba, a new\nparadigm for long-range dependency modeling with linear complexity, for\nefficient and effective MRI reconstruction. However, directly applying Mamba to\nMRI reconstruction faces three significant issues: (1) Mamba's row-wise and\ncolumn-wise scanning disrupts k-space's unique spectrum, leaving its potential\nin k-space learning unexplored. (2) Existing Mamba methods unfold feature maps\nwith multiple lengthy scanning paths, leading to long-range forgetting and high\ncomputational burden. (3) Mamba struggles with spatially-varying contents,\nresulting in limited diversity of local representations. To address these, we\npropose a dual-domain multi-scale Mamba for MRI reconstruction from the\nfollowing perspectives: (1) We pioneer vision Mamba in k-space learning. A\ncircular scanning is customized for spectrum unfolding, benefiting the global\nmodeling of k-space. (2) We propose a multi-scale Mamba with an efficient\nscanning strategy in both image and k-space domains. It mitigates long-range\nforgetting and achieves a better trade-off between efficiency and performance.\n(3) We develop a local diversity enhancement module to improve the\nspatially-varying representation of Mamba. Extensive experiments are conducted\non three public datasets for MRI reconstruction under various undersampling\npatterns. Comprehensive results demonstrate that our method significantly\noutperforms state-of-the-art methods with lower computational cost.\nImplementation code will be available at\nhttps://github.com/XiaoMengLiLiLi/DM-Mamba.\n","authors":["Yucong Meng","Zhiwei Yang","Zhijian Song","Yonghong Shi"],"pdf_url":"https://arxiv.org/pdf/2501.08163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08988v1","updated":"2025-02-13T05:51:41Z","published":"2025-02-13T05:51:41Z","title":"Latents of latents to delineate pixels: hybrid Matryoshka\n  autoencoder-to-U-Net pairing for segmenting large medical images in GPU-poor\n  and low-data regimes","summary":"  Medical images are often high-resolution and lose important detail if\ndownsampled, making pixel-level methods such as semantic segmentation much less\nefficient if performed on a low-dimensional image. We propose a low-rank\nMatryoshka projection and a hybrid segmenting architecture that preserves\nimportant information while retaining sufficient pixel geometry for pixel-level\ntasks. We design the Matryoshka Autoencoder (MatAE-U-Net) which combines the\nhierarchical encoding of the Matryoshka Autoencoder with the spatial\nreconstruction capabilities of a U-Net decoder, leveraging multi-scale feature\nextraction and skip connections to enhance accuracy and generalisation. We\napply it to the problem of segmenting the left ventricle (LV) in\nechocardiographic images using the Stanford EchoNet-D dataset, including 1,000\nstandardised video-mask pairs of cardiac ultrasound videos resized to 112x112\npixels. The MatAE-UNet model achieves a Mean IoU of 77.68\\%, Mean Pixel\nAccuracy of 97.46\\%, and Dice Coefficient of 86.91\\%, outperforming the\nbaseline U-Net, which attains a Mean IoU of 74.70\\%, Mean Pixel Accuracy of\n97.31\\%, and Dice Coefficient of 85.20\\%. The results highlight the potential\nof using the U-Net in the recursive Matroshka latent space for imaging problems\nwith low-contrast such as echocardiographic analysis.\n","authors":["Tahir Syed","Ariba Khan","Sawera Hanif"],"pdf_url":"https://arxiv.org/pdf/2502.08988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04207v2","updated":"2025-02-13T05:40:03Z","published":"2025-02-06T16:47:28Z","title":"Enhanced Feature-based Image Stitching for Endoscopic Videos in\n  Pediatric Eosinophilic Esophagitis","summary":"  Video endoscopy represents a major advance in the investigation of\ngastrointestinal diseases. Reviewing endoscopy videos often involves frequent\nadjustments and reorientations to piece together a complete view, which can be\nboth time-consuming and prone to errors. Image stitching techniques address\nthis issue by providing a continuous and complete visualization of the examined\narea. However, endoscopic images, particularly those of the esophagus, present\nunique challenges. The smooth surface, lack of distinct feature points, and\nnon-horizontal orientation complicate the stitching process, rendering\ntraditional feature-based methods often ineffective for these types of images.\nIn this paper, we propose a novel preprocessing pipeline designed to enhance\nendoscopic image stitching through advanced computational techniques. Our\napproach converts endoscopic video data into continuous 2D images by following\nfour key steps: (1) keyframe selection, (2) image rotation adjustment to\ncorrect distortions, (3) surface unwrapping using polar coordinate\ntransformation to generate a flat image, and (4) feature point matching\nenhanced by Adaptive Histogram Equalization for improved feature detection. We\nevaluate stitching quality through the assessment of valid feature point match\npairs. Experiments conducted on 20 pediatric endoscopy videos demonstrate that\nour method significantly improves image alignment and stitching quality\ncompared to traditional techniques, laying a robust foundation for more\neffective panoramic image creation.\n","authors":["Juming Xiong","Muyang Li","Ruining Deng","Tianyuan Yao","Shunxing Bao","Regina N Tyree","Girish Hiremath","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2502.04207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08977v1","updated":"2025-02-13T05:27:50Z","published":"2025-02-13T05:27:50Z","title":"Text-driven 3D Human Generation via Contrastive Preference Optimization","summary":"  Recent advances in Score Distillation Sampling (SDS) have improved 3D human\ngeneration from textual descriptions. However, existing methods still face\nchallenges in accurately aligning 3D models with long and complex textual\ninputs. To address this challenge, we propose a novel framework that introduces\ncontrastive preferences, where human-level preference models, guided by both\npositive and negative prompts, assist SDS for improved alignment. Specifically,\nwe design a preference optimization module that integrates multiple models to\ncomprehensively capture the full range of textual features. Furthermore, we\nintroduce a negation preference module to mitigate over-optimization of\nirrelevant details by leveraging static-dynamic negation prompts, effectively\npreventing ``reward hacking\". Extensive experiments demonstrate that our method\nachieves state-of-the-art results, significantly enhancing texture realism and\nvisual alignment with textual descriptions, particularly for long and complex\ninputs.\n","authors":["Pengfei Zhou","Xukun Shen","Yong Hu"],"pdf_url":"https://arxiv.org/pdf/2502.08977v1.pdf","comment":"8"},{"id":"http://arxiv.org/abs/2502.08974v1","updated":"2025-02-13T05:21:02Z","published":"2025-02-13T05:21:02Z","title":"Topo2Seq: Enhanced Topology Reasoning via Topology Sequence Learning","summary":"  Extracting lane topology from perspective views (PV) is crucial for planning\nand control in autonomous driving. This approach extracts potential drivable\ntrajectories for self-driving vehicles without relying on high-definition (HD)\nmaps. However, the unordered nature and weak long-range perception of the\nDETR-like framework can result in misaligned segment endpoints and limited\ntopological prediction capabilities. Inspired by the learning of contextual\nrelationships in language models, the connectivity relations in roads can be\ncharacterized as explicit topology sequences. In this paper, we introduce\nTopo2Seq, a novel approach for enhancing topology reasoning via topology\nsequences learning. The core concept of Topo2Seq is a randomized order\nprompt-to-sequence learning between lane segment decoder and topology sequence\ndecoder. The dual-decoder branches simultaneously learn the lane topology\nsequences extracted from the Directed Acyclic Graph (DAG) and the lane graph\ncontaining geometric information. Randomized order prompt-to-sequence learning\nextracts unordered key points from the lane graph predicted by the lane segment\ndecoder, which are then fed into the prompt design of the topology sequence\ndecoder to reconstruct an ordered and complete lane graph. In this way, the\nlane segment decoder learns powerful long-range perception and accurate\ntopological reasoning from the topology sequence decoder. Notably, topology\nsequence decoder is only introduced during training and does not affect the\ninference efficiency. Experimental evaluations on the OpenLane-V2 dataset\ndemonstrate the state-of-the-art performance of Topo2Seq in topology reasoning.\n","authors":["Yiming Yang","Yueru Luo","Bingkun He","Erlong Li","Zhipeng Cao","Chao Zheng","Shuqi Mei","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2502.08974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11356v2","updated":"2025-02-13T05:20:48Z","published":"2024-09-17T17:00:52Z","title":"RenderWorld: World Model with Self-Supervised 3D Label","summary":"  End-to-end autonomous driving with vision-only is not only more\ncost-effective compared to LiDAR-vision fusion but also more reliable than\ntraditional methods. To achieve a economical and robust purely visual\nautonomous driving system, we propose RenderWorld, a vision-only end-to-end\nautonomous driving framework, which generates 3D occupancy labels using a\nself-supervised gaussian-based Img2Occ Module, then encodes the labels by\nAM-VAE, and uses world model for forecasting and planning. RenderWorld employs\nGaussian Splatting to represent 3D scenes and render 2D images greatly improves\nsegmentation accuracy and reduces GPU memory consumption compared with\nNeRF-based methods. By applying AM-VAE to encode air and non-air separately,\nRenderWorld achieves more fine-grained scene element representation, leading to\nstate-of-the-art performance in both 4D occupancy forecasting and motion\nplanning from autoregressive world model.\n","authors":["Ziyang Yan","Wenzhen Dong","Yihua Shao","Yuhang Lu","Liu Haiyang","Jingwen Liu","Haozhe Wang","Zhe Wang","Yan Wang","Fabio Remondino","Yuexin Ma"],"pdf_url":"https://arxiv.org/pdf/2409.11356v2.pdf","comment":"Accepted in 2025 IEEE International Conference on Robotics and\n  Automation (ICRA)"},{"id":"http://arxiv.org/abs/2410.17610v3","updated":"2025-02-13T05:15:07Z","published":"2024-10-23T07:06:08Z","title":"ImDy: Human Inverse Dynamics from Imitated Observations","summary":"  Inverse dynamics (ID), which aims at reproducing the driven torques from\nhuman kinematic observations, has been a critical tool for gait analysis.\nHowever, it is hindered from wider application to general motion due to its\nlimited scalability. Conventional optimization-based ID requires expensive\nlaboratory setups, restricting its availability. To alleviate this problem, we\npropose to exploit the recently progressive human motion imitation algorithms\nto learn human inverse dynamics in a data-driven manner. The key insight is\nthat the human ID knowledge is implicitly possessed by motion imitators, though\nnot directly applicable. In light of this, we devise an efficient data\ncollection pipeline with state-of-the-art motion imitation algorithms and\nphysics simulators, resulting in a large-scale human inverse dynamics benchmark\nas Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint\ntorque and full-body ground reaction force data. With ImDy, we train a\ndata-driven human inverse dynamics solver ImDyS(olver) in a fully supervised\nmanner, which conducts ID and ground reaction force estimation simultaneously.\nExperiments on ImDy and real-world data demonstrate the impressive competency\nof ImDyS in human inverse dynamics and ground reaction force estimation.\nMoreover, the potential of ImDy(-S) as a fundamental motion analysis tool is\nexhibited with downstream applications. The project page is\nhttps://foruck.github.io/ImDy/.\n","authors":["Xinpeng Liu","Junxuan Liang","Zili Lin","Haowen Hou","Yong-Lu Li","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2410.17610v3.pdf","comment":"To appear in ICLR 2025. Yong-Lu Li and Cewu Lu are the corresponding\n  authors"},{"id":"http://arxiv.org/abs/2501.15001v2","updated":"2025-02-13T04:15:47Z","published":"2025-01-25T00:29:24Z","title":"What if Eye...? Computationally Recreating Vision Evolution","summary":"  Vision systems in nature show remarkable diversity, from simple\nlight-sensitive patches to complex camera eyes with lenses. While natural\nselection has produced these eyes through countless mutations over millions of\nyears, they represent just one set of realized evolutionary paths. Testing\nhypotheses about how environmental pressures shaped eye evolution remains\nchallenging since we cannot experimentally isolate individual factors.\nComputational evolution offers a way to systematically explore alternative\ntrajectories. Here we show how environmental demands drive three fundamental\naspects of visual evolution through an artificial evolution framework that\nco-evolves both physical eye structure and neural processing in embodied\nagents. First, we demonstrate computational evidence that task specific\nselection drives bifurcation in eye evolution - orientation tasks like\nnavigation in a maze leads to distributed compound-type eyes while an object\ndiscrimination task leads to the emergence of high-acuity camera-type eyes.\nSecond, we reveal how optical innovations like lenses naturally emerge to\nresolve fundamental tradeoffs between light collection and spatial precision.\nThird, we uncover systematic scaling laws between visual acuity and neural\nprocessing, showing how task complexity drives coordinated evolution of sensory\nand computational capabilities. Our work introduces a novel paradigm that\nilluminates evolutionary principles shaping vision by creating targeted\nsingle-player games where embodied agents must simultaneously evolve visual\nsystems and learn complex behaviors. Through our unified genetic encoding\nframework, these embodied agents serve as next-generation hypothesis testing\nmachines while providing a foundation for designing manufacturable bio-inspired\nvision systems. Website: http://eyes.mit.edu/\n","authors":["Kushagra Tiwary","Aaron Young","Zaid Tasneem","Tzofi Klinghoffer","Akshat Dave","Tomaso Poggio","Dan-Eric Nilsson","Brian Cheung","Ramesh Raskar"],"pdf_url":"https://arxiv.org/pdf/2501.15001v2.pdf","comment":"Website: http://eyes.mit.edu/"},{"id":"http://arxiv.org/abs/2502.08946v1","updated":"2025-02-13T04:00:03Z","published":"2025-02-13T04:00:03Z","title":"The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of\n  Physical Concept Understanding","summary":"  In a systematic way, we investigate a widely asked question: Do LLMs really\nunderstand what they say?, which relates to the more familiar term Stochastic\nParrot. To this end, we propose a summative assessment over a carefully\ndesigned physical concept understanding task, PhysiCo. Our task alleviates the\nmemorization issue via the usage of grid-format inputs that abstractly describe\nphysical phenomena. The grids represents varying levels of understanding, from\nthe core phenomenon, application examples to analogies to other abstract\npatterns in the grid world. A comprehensive study on our task demonstrates: (1)\nstate-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag\nbehind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,\nas they fail on our grid task but can describe and recognize the same concepts\nwell in natural language; (3) our task challenges the LLMs due to intrinsic\ndifficulties rather than the unfamiliar grid format, as in-context learning and\nfine-tuning on same formatted data added little to their performance.\n","authors":["Mo Yu","Lemao Liu","Junjie Wu","Tsz Ting Chung","Shunchi Zhang","Jiangnan Li","Dit-Yan Yeung","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08946v1.pdf","comment":"NAACL 2025 Main Conference. First 5 authors contributed equally.\n  Project page: https://physico-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2502.07856v2","updated":"2025-02-13T03:55:03Z","published":"2025-02-11T14:57:33Z","title":"MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE\n  Solvers","summary":"  In applications of diffusion models, controllable generation is of practical\nsignificance, but is also challenging. Current methods for controllable\ngeneration primarily focus on modifying the score function of diffusion models,\nwhile Mean Reverting (MR) Diffusion directly modifies the structure of the\nstochastic differential equation (SDE), making the incorporation of image\nconditions simpler and more natural. However, current training-free fast\nsamplers are not directly applicable to MR Diffusion. And thus MR Diffusion\nrequires hundreds of NFEs (number of function evaluations) to obtain\nhigh-quality samples. In this paper, we propose a new algorithm named MRS (MR\nSampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time\nSDE and the probability flow ordinary differential equation (PF-ODE) associated\nwith MR Diffusion, and derive semi-analytical solutions. The solutions consist\nof an analytical function and an integral parameterized by a neural network.\nBased on this solution, we can generate high-quality samples in fewer steps.\nOur approach does not require training and supports all mainstream\nparameterizations, including noise prediction, data prediction and velocity\nprediction. Extensive experiments demonstrate that MR Sampler maintains high\nsampling quality with a speedup of 10 to 20 times across ten different image\nrestoration tasks. Our algorithm accelerates the sampling procedure of MR\nDiffusion, making it more practical in controllable generation.\n","authors":["Ao Li","Wei Fang","Hongbo Zhao","Le Lu","Ge Yang","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2502.07856v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08940v1","updated":"2025-02-13T03:41:50Z","published":"2025-02-13T03:41:50Z","title":"Towards Understanding Why Data Augmentation Improves Generalization","summary":"  Data augmentation is a cornerstone technique in deep learning, widely used to\nimprove model generalization. Traditional methods like random cropping and\ncolor jittering, as well as advanced techniques such as CutOut, Mixup, and\nCutMix, have achieved notable success across various domains. However, the\nmechanisms by which data augmentation improves generalization remain poorly\nunderstood, and existing theoretical analyses typically focus on individual\ntechniques without a unified explanation. In this work, we present a unified\ntheoretical framework that elucidates how data augmentation enhances\ngeneralization through two key effects: partial semantic feature removal and\nfeature mixing. Partial semantic feature removal reduces the model's reliance\non individual feature, promoting diverse feature learning and better\ngeneralization. Feature mixing, by scaling down original semantic features and\nintroducing noise, increases training complexity, driving the model to develop\nmore robust features. Advanced methods like CutMix integrate both effects,\nachieving complementary benefits. Our theoretical insights are further\nsupported by experimental results, validating the effectiveness of this unified\nperspective.\n","authors":["Jingyang Li","Jiachun Pan","Kim-Chuan Toh","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08932v1","updated":"2025-02-13T03:29:42Z","published":"2025-02-13T03:29:42Z","title":"On the Promise for Assurance of Differentiable Neurosymbolic Reasoning\n  Paradigms","summary":"  To create usable and deployable Artificial Intelligence (AI) systems, there\nrequires a level of assurance in performance under many different conditions.\nMany times, deployed machine learning systems will require more classic logic\nand reasoning performed through neurosymbolic programs jointly with artificial\nneural network sensing. While many prior works have examined the assurance of a\nsingle component of the system solely with either the neural network alone or\nentire enterprise systems, very few works have examined the assurance of\nintegrated neurosymbolic systems. Within this work, we assess the assurance of\nend-to-end fully differentiable neurosymbolic systems that are an emerging\nmethod to create data-efficient and more interpretable models. We perform this\ninvestigation using Scallop, an end-to-end neurosymbolic library, across\nclassification and reasoning tasks in both the image and audio domains. We\nassess assurance across adversarial robustness, calibration, user performance\nparity, and interpretability of solutions for catching misaligned solutions. We\nfind end-to-end neurosymbolic methods present unique opportunities for\nassurance beyond their data efficiency through our empirical results but not\nacross the board. We find that this class of neurosymbolic models has higher\nassurance in cases where arithmetic operations are defined and where there is\nhigh dimensionality to the input space, where fully neural counterparts\nstruggle to learn robust reasoning operations. We identify the relationship\nbetween neurosymbolic models' interpretability to catch shortcuts that later\nresult in increased adversarial vulnerability despite performance parity.\nFinally, we find that the promise of data efficiency is typically only in the\ncase of class imbalanced reasoning problems.\n","authors":["Luke E. Richards","Jessie Yaros","Jasen Babcock","Coung Ly","Robin Cosbey","Timothy Doster","Cynthia Matuszek"],"pdf_url":"https://arxiv.org/pdf/2502.08932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08927v1","updated":"2025-02-13T03:23:17Z","published":"2025-02-13T03:23:17Z","title":"Dynamic watermarks in images generated by diffusion models","summary":"  High-fidelity text-to-image diffusion models have revolutionized visual\ncontent generation, but their widespread use raises significant ethical\nconcerns, including intellectual property protection and the misuse of\nsynthetic media. To address these challenges, we propose a novel multi-stage\nwatermarking framework for diffusion models, designed to establish copyright\nand trace generated images back to their source. Our multi-stage watermarking\ntechnique involves embedding: (i) a fixed watermark that is localized in the\ndiffusion model's learned noise distribution and, (ii) a human-imperceptible,\ndynamic watermark in generates images, leveraging a fine-tuned decoder. By\nleveraging the Structural Similarity Index Measure (SSIM) and cosine\nsimilarity, we adapt the watermark's shape and color to the generated content\nwhile maintaining robustness. We demonstrate that our method enables reliable\nsource verification through watermark classification, even when the dynamic\nwatermark is adjusted for content-specific variations. Source model\nverification is enabled through watermark classification. o support further\nresearch, we generate a dataset of watermarked images and introduce a\nmethodology to evaluate the statistical impact of watermarking on generated\ncontent.Additionally, we rigorously test our framework against various attack\nscenarios, demonstrating its robustness and minimal impact on image quality.\nOur work advances the field of AI-generated content security by providing a\nscalable solution for model ownership verification and misuse prevention.\n","authors":["Yunzhuo Chen","Naveed Akhtar","Nur Al Hasan Haldar","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2502.08927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14755v2","updated":"2025-02-13T03:15:41Z","published":"2024-04-23T05:36:33Z","title":"SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework\n  with Interactive Vision-Language Models","summary":"  With the continuous advancement of vision language models (VLMs) technology,\nremarkable research achievements have emerged in the dermatology field, the\nfourth most prevalent human disease category. However, despite these\nadvancements, VLM still faces explainable problems to user in diagnosis due to\nthe inherent complexity of dermatological conditions, existing tools offer\nrelatively limited support for user comprehension. We propose SkinGEN, a\ndiagnosis-to-generation framework that leverages the stable diffusion(SD) model\nto generate reference demonstrations from diagnosis results provided by VLM,\nthereby enhancing the visual explainability for users. Through extensive\nexperiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for\nskin condition image generation. We conduct a user study with 32 participants\nevaluating both the system performance and explainability. Results demonstrate\nthat SkinGEN significantly improves users' comprehension of VLM predictions and\nfosters increased trust in the diagnostic process. This work paves the way for\nmore transparent and user-centric VLM applications in dermatology and beyond.\n","authors":["Bo Lin","Yingjing Xu","Xuanwen Bao","Zhou Zhao","Zhouyang Wang","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2404.14755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11566v3","updated":"2025-02-13T03:15:37Z","published":"2024-02-18T12:27:59Z","title":"Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data\n  Augmentation and Consistency Training","summary":"  The 2D human pose estimation (HPE) is a basic visual problem. However, its\nsupervised learning requires massive keypoint labels, which is labor-intensive\nto collect. Thus, we aim at boosting a pose estimator by excavating extra\nunlabeled data with semi-supervised learning (SSL). Most previous SSHPE methods\nare consistency-based and strive to maintain consistent outputs for differently\naugmented inputs. Under this genre, we find that SSHPE can be boosted from two\ncores: advanced data augmentations and concise consistency training ways.\nSpecifically, for the first core, we discover the synergistic effects of\nexisting augmentations, and reveal novel paradigms for conveniently producing\nnew superior HPE-oriented augmentations which can more effectively add noise on\nunlabeled samples. We can therefore establish paired easy-hard augmentations\nwith larger difficulty gaps. For the second core, we propose to repeatedly\naugment unlabeled images with diverse hard augmentations, and generate\nmulti-path predictions sequentially for optimizing multi-losses in a single\nnetwork. This simple and compact design is interpretable, and easily benefits\nfrom newly found augmentations. Comparing to state-of-the-art SSL approaches,\nour method brings substantial improvements on public datasets. And we\nextensively validate the superiority and versatility of our approach on\nconventional human body images, overhead fisheye images, and human hand images.\nThe code is released in https://github.com/hnuzhy/MultiAugs.\n","authors":["Huayi Zhou","Mukun Luo","Fei Jiang","Yue Ding","Hongtao Lu","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2402.11566v3.pdf","comment":"under review. Semi-Supervised 2D Human Pose Estimation"},{"id":"http://arxiv.org/abs/2502.08921v1","updated":"2025-02-13T03:15:18Z","published":"2025-02-13T03:15:18Z","title":"Detecting Malicious Concepts Without Image Generation in AIGC","summary":"  The task of text-to-image generation has achieved tremendous success in\npractice, with emerging concept generation models capable of producing highly\npersonalized and customized content. Fervor for concept generation is\nincreasing rapidly among users, and platforms for concept sharing have sprung\nup. The concept owners may upload malicious concepts and disguise them with\nnon-malicious text descriptions and example images to deceive users into\ndownloading and generating malicious content. The platform needs a quick method\nto determine whether a concept is malicious to prevent the spread of malicious\nconcepts. However, simply relying on concept image generation to judge whether\na concept is malicious requires time and computational resources. Especially,\nas the number of concepts uploaded and downloaded on the platform continues to\nincrease, this approach becomes impractical and poses a risk of generating\nmalicious content. In this paper, we propose Concept QuickLook, the first\nsystematic work to incorporate malicious concept detection into research, which\nperforms detection based solely on concept files without generating any images.\nWe define malicious concepts and design two work modes for detection: concept\nmatching and fuzzy detection. Extensive experiments demonstrate that the\nproposed Concept QuickLook can detect malicious concepts and demonstrate\npracticality in concept sharing platforms. We also design robustness\nexperiments to further validate the effectiveness of the solution. We hope this\nwork can initiate malicious concept detection tasks and provide some\ninspiration.\n","authors":["Kun Xu","Yushu Zhang","Shuren Qi","Tao Wang","Wenying Wen","Yuming Fang"],"pdf_url":"https://arxiv.org/pdf/2502.08921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00626v3","updated":"2025-02-13T03:11:20Z","published":"2024-02-01T14:41:20Z","title":"Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks","summary":"  Typographic attacks, adding misleading text to images, can deceive\nvision-language models (LVLMs). The susceptibility of recent large LVLMs like\nGPT4-V to such attacks is understudied, raising concerns about amplified\nmisinformation in personal assistant applications. Previous attacks use simple\nstrategies, such as random misleading words, which don't fully exploit LVLMs'\nlanguage reasoning abilities. We introduce an experimental setup for testing\ntypographic attacks on LVLMs and propose two novel self-generated attacks: (1)\nClass-based attacks, where the model identifies a similar class to deceive\nitself, and (2) Reasoned attacks, where an advanced LVLM suggests an attack\ncombining a deceiving class and description. Our experiments show these attacks\nsignificantly reduce classification performance by up to 60\\% and are effective\nacross different models, including InstructBLIP and MiniGPT4. Code:\nhttps://github.com/mqraitem/Self-Gen-Typo-Attack\n","authors":["Maan Qraitem","Nazia Tasnim","Piotr Teterwak","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2402.00626v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06855v3","updated":"2025-02-13T03:10:12Z","published":"2024-02-10T01:36:39Z","title":"For Better or For Worse? Learning Minimum Variance Features With Label\n  Augmentation","summary":"  Data augmentation has been pivotal in successfully training deep learning\nmodels on classification tasks over the past decade. An important subclass of\ndata augmentation techniques - which includes both label smoothing and Mixup -\ninvolves modifying not only the input data but also the input label during\nmodel training. In this work, we analyze the role played by the label\naugmentation aspect of such methods. We first prove that linear models on\nbinary classification data trained with label augmentation learn only the\nminimum variance features in the data, while standard training (which includes\nweight decay) can learn higher variance features. We then use our techniques to\nshow that even for nonlinear models and general data distributions, the label\nsmoothing and Mixup losses are lower bounded by a function of the model output\nvariance. Lastly, we demonstrate empirically that this aspect of label\nsmoothing and Mixup can be a positive and a negative. On the one hand, we show\nthat the strong performance of label smoothing and Mixup on image\nclassification benchmarks is correlated with learning low variance hidden\nrepresentations. On the other hand, we show that Mixup and label smoothing can\nbe more susceptible to low variance spurious correlations in the training data.\n","authors":["Muthu Chidambaram","Rong Ge"],"pdf_url":"https://arxiv.org/pdf/2402.06855v3.pdf","comment":"ICLR 2025, 25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.08916v1","updated":"2025-02-13T03:08:02Z","published":"2025-02-13T03:08:02Z","title":"PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic\n  Decision-Making Applied to Histopathology","summary":"  Diagnosing diseases through histopathology whole slide images (WSIs) is\nfundamental in modern pathology but is challenged by the gigapixel scale and\ncomplexity of WSIs. Trained histopathologists overcome this challenge by\nnavigating the WSI, looking for relevant patches, taking notes, and compiling\nthem to produce a final holistic diagnostic. Traditional AI approaches, such as\nmultiple instance learning and transformer-based models, fail short of such a\nholistic, iterative, multi-scale diagnostic procedure, limiting their adoption\nin the real-world. We introduce PathFinder, a multi-modal, multi-agent\nframework that emulates the decision-making process of expert pathologists.\nPathFinder integrates four AI agents, the Triage Agent, Navigation Agent,\nDescription Agent, and Diagnosis Agent, that collaboratively navigate WSIs,\ngather evidence, and provide comprehensive diagnoses with natural language\nexplanations. The Triage Agent classifies the WSI as benign or risky; if risky,\nthe Navigation and Description Agents iteratively focus on significant regions,\ngenerating importance maps and descriptive insights of sampled patches.\nFinally, the Diagnosis Agent synthesizes the findings to determine the\npatient's diagnostic classification. Our Experiments show that PathFinder\noutperforms state-of-the-art methods in skin melanoma diagnosis by 8% while\noffering inherent explainability through natural language descriptions of\ndiagnostically relevant patches. Qualitative analysis by pathologists shows\nthat the Description Agent's outputs are of high quality and comparable to\nGPT-4o. PathFinder is also the first AI-based system to surpass the average\nperformance of pathologists in this challenging melanoma classification task by\n9%, setting a new record for efficient, accurate, and interpretable AI-assisted\ndiagnostics in pathology. Data, code and models available at\nhttps://pathfinder-dx.github.io/\n","authors":["Fatemeh Ghezloo","Mehmet Saygin Seyfioglu","Rustin Soraki","Wisdom O. Ikezogwo","Beibin Li","Tejoram Vivekanandan","Joann G. Elmore","Ranjay Krishna","Linda Shapiro"],"pdf_url":"https://arxiv.org/pdf/2502.08916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08914v1","updated":"2025-02-13T03:05:42Z","published":"2025-02-13T03:05:42Z","title":"Diffusion Models Through a Global Lens: Are They Culturally Inclusive?","summary":"  Text-to-image diffusion models have recently enabled the creation of visually\ncompelling, detailed images from textual prompts. However, their ability to\naccurately represent various cultural nuances remains an open question. In our\nwork, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion\nmodels whether they can generate culturally specific images spanning ten\ncountries. We show that these models often fail to generate cultural artifacts\nin architecture, clothing, and food, especially for underrepresented country\nregions, by conducting a fine-grained analysis of different similarity aspects,\nrevealing significant disparities in cultural relevance, description fidelity,\nand realism compared to real-world reference images. With the collected human\nevaluations, we develop a neural-based image-image similarity metric, namely,\nCultDiff-S, to predict human judgment on real and generated images with\ncultural artifacts. Our work highlights the need for more inclusive generative\nAI systems and equitable dataset representation over a wide range of cultures.\n","authors":["Zahra Bayramli","Ayhan Suleymanzade","Na Min An","Huzama Ahmad","Eunsu Kim","Junyeong Park","James Thorne","Alice Oh"],"pdf_url":"https://arxiv.org/pdf/2502.08914v1.pdf","comment":"17 pages, 17 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.08905v1","updated":"2025-02-13T02:41:34Z","published":"2025-02-13T02:41:34Z","title":"DiffoRA: Enabling Parameter-Efficient LLM Fine-Tuning via Differential\n  Low-Rank Matrix Adaptation","summary":"  The Parameter-Efficient Fine-Tuning (PEFT) methods have been extensively\nresearched for large language models in the downstream tasks. Among all the\nexisting approaches, the Low-Rank Adaptation (LoRA) has gained popularity for\nits streamlined design by incorporating low-rank matrices into existing\npre-trained models. Though effective, LoRA allocates every module an identical\nlow-rank matrix, which ignores the varying properties and contributions across\ndifferent components. Moreover, the existing adaptive LoRA solutions rely\nhighly on intuitive importance scoring indicators to adjust the interior rank\nof the decomposition matrices. In this paper, we propose a new PEFT scheme\ncalled DiffoRA, which is theoretically grounded and enables module-wise\nadoption of LoRA. At the core of our DiffoRA lies a Differential Adaptation\nMatrix (DAM) to determine which module is the most suitable and essential for\nfine-tuning. We explain how the designed matrix impacts the convergence rate\nand generalization capability of a pre-trained model. Furthermore, we construct\nthe DAM via continuous relaxation and discretization with weight-sharing\noptimizations. We fully implement our DiffoRA and design comprehensive\nexperiments to evaluate its performance. The experimental results demonstrate\nthat our approach achieves the best model accuracy over all the\nstate-of-the-art baselines across various benchmarks.\n","authors":["Tangyu Jiang","Haodi Wang","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.08905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18409v4","updated":"2025-02-13T02:37:06Z","published":"2024-02-28T15:28:36Z","title":"A Cognitive Evaluation Benchmark of Image Reasoning and Description for\n  Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs), despite their recent success, are\nhardly comprehensively tested for their cognitive abilities. Inspired by the\nprevalent use of the Cookie Theft task in human cognitive tests, we propose a\nnovel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs\nusing images with rich semantics. The benchmark consists of 251 images along\nwith comprehensive annotations. It defines eight reasoning capabilities and\ncomprises an image description task and a visual question answering task. Our\nevaluation of well-known LVLMs shows that there is still a significant gap in\ncognitive abilities between LVLMs and humans.\n","authors":["Xiujie Song","Mengyue Wu","Kenny Q. Zhu","Chunhao Zhang","Yanyi Chen"],"pdf_url":"https://arxiv.org/pdf/2402.18409v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08902v1","updated":"2025-02-13T02:36:01Z","published":"2025-02-13T02:36:01Z","title":"CoL3D: Collaborative Learning of Single-view Depth and Camera Intrinsics\n  for Metric 3D Shape Recovery","summary":"  Recovering the metric 3D shape from a single image is particularly relevant\nfor robotics and embodied intelligence applications, where accurate spatial\nunderstanding is crucial for navigation and interaction with environments.\nUsually, the mainstream approaches achieve it through monocular depth\nestimation. However, without camera intrinsics, the 3D metric shape can not be\nrecovered from depth alone. In this study, we theoretically demonstrate that\ndepth serves as a 3D prior constraint for estimating camera intrinsics and\nuncover the reciprocal relations between these two elements. Motivated by this,\nwe propose a collaborative learning framework for jointly estimating depth and\ncamera intrinsics, named CoL3D, to learn metric 3D shapes from single images.\nSpecifically, CoL3D adopts a unified network and performs collaborative\noptimization at three levels: depth, camera intrinsics, and 3D point clouds.\nFor camera intrinsics, we design a canonical incidence field mechanism as a\nprior that enables the model to learn the residual incident field for enhanced\ncalibration. Additionally, we incorporate a shape similarity measurement loss\nin the point cloud space, which improves the quality of 3D shapes essential for\nrobotic applications. As a result, when training and testing on a single\ndataset with in-domain settings, CoL3D delivers outstanding performance in both\ndepth estimation and camera calibration across several indoor and outdoor\nbenchmark datasets, which leads to remarkable 3D shape quality for the\nperception capabilities of robots.\n","authors":["Chenghao Zhang","Lubin Fan","Shen Cao","Bojian Wu","Jieping Ye"],"pdf_url":"https://arxiv.org/pdf/2502.08902v1.pdf","comment":"Accepted at ICRA 2025"},{"id":"http://arxiv.org/abs/2406.06230v3","updated":"2025-02-13T02:32:43Z","published":"2024-06-10T13:00:22Z","title":"UEMM-Air: Make Unmanned Aerial Vehicles Perform More Multi-modal Tasks","summary":"  The development of multi-modal learning for Unmanned Aerial Vehicles (UAVs)\ntypically relies on a large amount of pixel-aligned multi-modal image data.\nHowever, existing datasets face challenges such as limited modalities, high\nconstruction costs, and imprecise annotations. To this end, we propose a\nsynthetic multi-modal UAV-based multi-task dataset, UEMM-Air. Specifically, we\nsimulate various UAV flight scenarios and object types using the Unreal Engine\n(UE). Then we design the UAV's flight logic to automatically collect data from\ndifferent scenarios, perspectives, and altitudes. Furthermore, we propose a\nnovel heuristic automatic annotation algorithm to generate accurate object\ndetection labels. Finally, we utilize labels to generate text descriptions of\nimages to make our UEMM-Air support more cross-modality tasks. In total, our\nUEMM-Air consists of 120k pairs of images with 6 modalities and precise\nannotations. Moreover, we conduct numerous experiments and establish new\nbenchmark results on our dataset. We also found that models pre-trained on\nUEMM-Air exhibit better performance on downstream tasks compared to other\nsimilar datasets. The dataset is publicly available\n(https://github.com/1e12Leon/UEMM-Air) to support the research of multi-modal\ntasks on UAVs.\n","authors":["Liang Yao","Fan Liu","Shengxiang Xu","Chuanyi Zhang","Xing Ma","Jianyu Jiang","Zequan Wang","Shimin Di","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.06230v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08884v1","updated":"2025-02-13T01:52:02Z","published":"2025-02-13T01:52:02Z","title":"ShapeLib: designing a library of procedural 3D shape abstractions with\n  Large Language Models","summary":"  Procedural representations are desirable, versatile, and popular shape\nencodings. Authoring them, either manually or using data-driven procedures,\nremains challenging, as a well-designed procedural representation should be\ncompact, intuitive, and easy to manipulate. A long-standing problem in shape\nanalysis studies how to discover a reusable library of procedural functions,\nwith semantically aligned exposed parameters, that can explain an entire shape\nfamily. We present ShapeLib as the first method that leverages the priors of\nfrontier LLMs to design a library of 3D shape abstraction functions. Our system\naccepts two forms of design intent: text descriptions of functions to include\nin the library and a seed set of exemplar shapes. We discover procedural\nabstractions that match this design intent by proposing, and then validating,\nfunction applications and implementations. The discovered shape functions in\nthe library are not only expressive but also generalize beyond the seed set to\na full family of shapes. We train a recognition network that learns to infer\nshape programs based on our library from different visual modalities\n(primitives, voxels, point clouds). Our shape functions have parameters that\nare semantically interpretable and can be modified to produce plausible shape\nvariations. We show that this allows inferred programs to be successfully\nmanipulated by an LLM given a text prompt. We evaluate ShapeLib on different\ndatasets and show clear advantages over existing methods and alternative\nformulations.\n","authors":["R. Kenny Jones","Paul Guerrero","Niloy J. Mitra","Daniel Ritchie"],"pdf_url":"https://arxiv.org/pdf/2502.08884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08869v1","updated":"2025-02-13T00:42:11Z","published":"2025-02-13T00:42:11Z","title":"Harnessing Vision Models for Time Series Analysis: A Survey","summary":"  Time series analysis has witnessed the inspiring development from traditional\nautoregressive models, deep learning models, to recent Transformers and Large\nLanguage Models (LLMs). Efforts in leveraging vision models for time series\nanalysis have also been made along the way but are less visible to the\ncommunity due to the predominant research on sequence modeling in this domain.\nHowever, the discrepancy between continuous time series and the discrete token\nspace of LLMs, and the challenges in explicitly modeling the correlations of\nvariates in multivariate time series have shifted some research attentions to\nthe equally successful Large Vision Models (LVMs) and Vision Language Models\n(VLMs). To fill the blank in the existing literature, this survey discusses the\nadvantages of vision models over LLMs in time series analysis. It provides a\ncomprehensive and in-depth overview of the existing methods, with dual views of\ndetailed taxonomy that answer the key research questions including how to\nencode time series as images and how to model the imaged time series for\nvarious tasks. Additionally, we address the challenges in the pre- and\npost-processing steps involved in this framework and outline future directions\nto further advance time series analysis with vision models.\n","authors":["Jingchao Ni","Ziming Zhao","ChengAo Shen","Hanghang Tong","Dongjin Song","Wei Cheng","Dongsheng Luo","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08869v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.09622v1","updated":"2025-02-13T18:59:47Z","published":"2025-02-13T18:59:47Z","title":"Theoretical Benefit and Limitation of Diffusion Language Model","summary":"  Diffusion language models have emerged as a promising approach for text\ngeneration. One would naturally expect this method to be an efficient\nreplacement for autoregressive models since multiple tokens can be sampled in\nparallel during each diffusion step. However, its efficiency-accuracy trade-off\nis not yet well understood. In this paper, we present a rigorous theoretical\nanalysis of a widely used type of diffusion language model, the Masked\nDiffusion Model (MDM), and find that its effectiveness heavily depends on the\ntarget evaluation metric. Under mild conditions, we prove that when using\nperplexity as the metric, MDMs can achieve near-optimal perplexity in sampling\nsteps regardless of sequence length, demonstrating that efficiency can be\nachieved without sacrificing performance. However, when using the sequence\nerror rate--which is important for understanding the \"correctness\" of a\nsequence, such as a reasoning chain--we show that the required sampling steps\nmust scale linearly with sequence length to obtain \"correct\" sequences, thereby\neliminating MDM's efficiency advantage over autoregressive models. Our analysis\nestablishes the first theoretical foundation for understanding the benefits and\nlimitations of MDMs. All theoretical findings are supported by empirical\nstudies.\n","authors":["Guhao Feng","Yihan Geng","Jian Guan","Wei Wu","Liwei Wang","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.09622v1.pdf","comment":"32 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.09619v1","updated":"2025-02-13T18:59:44Z","published":"2025-02-13T18:59:44Z","title":"Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights","summary":"  With the increasing numbers of publicly available models, there are probably\npretrained, online models for most tasks users require. However, current model\nsearch methods are rudimentary, essentially a text-based search in the\ndocumentation, thus users cannot find the relevant models. This paper presents\nProbeLog, a method for retrieving classification models that can recognize a\ntarget concept, such as \"Dog\", without access to model metadata or training\ndata. Differently from previous probing methods, ProbeLog computes a descriptor\nfor each output dimension (logit) of each model, by observing its responses on\na fixed set of inputs (probes). Our method supports both logit-based retrieval\n(\"find more logits like this\") and zero-shot, text-based retrieval (\"find all\nlogits corresponding to dogs\"). As probing-based representations require\nmultiple costly feedforward passes through the model, we develop a method,\nbased on collaborative filtering, that reduces the cost of encoding\nrepositories by 3x. We demonstrate that ProbeLog achieves high retrieval\naccuracy, both in real-world and fine-grained search tasks and is scalable to\nfull-size repositories.\n","authors":["Jonathan Kahana","Or Nathan","Eliahu Horwitz","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2502.09619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09616v1","updated":"2025-02-13T18:59:15Z","published":"2025-02-13T18:59:15Z","title":"Variational Rectified Flow Matching","summary":"  We study Variational Rectified Flow Matching, a framework that enhances\nclassic rectified flow matching by modeling multi-modal velocity vector-fields.\nAt inference time, classic rectified flow matching 'moves' samples from a\nsource distribution to the target distribution by solving an ordinary\ndifferential equation via integration along a velocity vector-field. At\ntraining time, the velocity vector-field is learnt by linearly interpolating\nbetween coupled samples one drawn from the source and one drawn from the target\ndistribution randomly. This leads to ''ground-truth'' velocity vector-fields\nthat point in different directions at the same location, i.e., the velocity\nvector-fields are multi-modal/ambiguous. However, since training uses a\nstandard mean-squared-error loss, the learnt velocity vector-field averages\n''ground-truth'' directions and isn't multi-modal. In contrast, variational\nrectified flow matching learns and samples from multi-modal flow directions. We\nshow on synthetic data, MNIST, CIFAR-10, and ImageNet that variational\nrectified flow matching leads to compelling results.\n","authors":["Pengsheng Guo","Alexander G. Schwing"],"pdf_url":"https://arxiv.org/pdf/2502.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09614v1","updated":"2025-02-13T18:59:13Z","published":"2025-02-13T18:59:13Z","title":"DexTrack: Towards Generalizable Neural Tracking Control for Dexterous\n  Manipulation from Human References","summary":"  We address the challenge of developing a generalizable neural tracking\ncontroller for dexterous manipulation from human references. This controller\naims to manage a dexterous robot hand to manipulate diverse objects for various\npurposes defined by kinematic human-object interactions. Developing such a\ncontroller is complicated by the intricate contact dynamics of dexterous\nmanipulation and the need for adaptivity, generalizability, and robustness.\nCurrent reinforcement learning and trajectory optimization methods often fall\nshort due to their dependence on task-specific rewards or precise system\nmodels. We introduce an approach that curates large-scale successful robot\ntracking demonstrations, comprising pairs of human references and robot\nactions, to train a neural controller. Utilizing a data flywheel, we\niteratively enhance the controller's performance, as well as the number and\nquality of successful tracking demonstrations. We exploit available tracking\ndemonstrations and carefully integrate reinforcement learning and imitation\nlearning to boost the controller's performance in dynamic environments. At the\nsame time, to obtain high-quality tracking demonstrations, we individually\noptimize per-trajectory tracking by leveraging the learned tracking controller\nin a homotopy optimization method. The homotopy optimization, mimicking\nchain-of-thought, aids in solving challenging trajectory tracking problems to\nincrease demonstration diversity. We showcase our success by training a\ngeneralizable neural controller and evaluating it in both simulation and real\nworld. Our method achieves over a 10% improvement in success rates compared to\nleading baselines. The project website with animated results is available at\nhttps://meowuu7.github.io/DexTrack/.\n","authors":["Xueyi Liu","Jianibieke Adalibieke","Qianwei Han","Yuzhe Qin","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2502.09614v1.pdf","comment":"Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/\n  Code: https://github.com/Meowuu7/DexTrack/ Video:\n  https://youtu.be/zru1Z-DaiWE"},{"id":"http://arxiv.org/abs/2402.17767v2","updated":"2025-02-13T18:59:11Z","published":"2024-02-27T18:58:54Z","title":"Opening Articulated Objects in the Real World","summary":"  What does it take to build mobile manipulation systems that can competently\noperate on previously unseen objects in previously unseen environments? This\nwork answers this question using opening of articulated objects as a mobile\nmanipulation testbed. Specifically, our focus is on the end-to-end performance\non this task without any privileged information, i.e. the robot starts at a\nlocation with the novel target articulated object in view, and has to approach\nthe object and successfully open it. We first develop a system for this task,\nand then conduct 100+ end-to-end system tests across 13 real world test sites.\nOur large-scale study reveals a number of surprising findings: a) modular\nsystems outperform end-to-end learned systems for this task, even when the\nend-to-end learned systems are trained on 1000+ demonstrations, b) perception,\nand not precise end-effector control, is the primary bottleneck to task\nsuccess, and c) state-of-the-art articulation parameter estimation models\ndeveloped in isolation struggle when faced with robot-centric viewpoints.\nOverall, our findings highlight the limitations of developing components of the\npipeline in isolation and underscore the need for system-level research,\nproviding a pragmatic roadmap for building generalizable mobile manipulation\nsystems. Videos, code, and models are available on the project website:\nhttps://arjung128.github.io/opening-articulated-objects/\n","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"pdf_url":"https://arxiv.org/pdf/2402.17767v2.pdf","comment":"Project webpage:\n  https://arjung128.github.io/opening-articulated-objects/"},{"id":"http://arxiv.org/abs/2403.06925v2","updated":"2025-02-13T18:58:58Z","published":"2024-03-11T17:12:09Z","title":"Transformers Learn Low Sensitivity Functions: Investigations and\n  Implications","summary":"  Transformers achieve state-of-the-art accuracy and robustness across many\ntasks, but an understanding of their inductive biases and how those biases\ndiffer from other neural network architectures remains elusive. In this work,\nwe identify the sensitivity of the model to token-wise random perturbations in\nthe input as a unified metric which explains the inductive bias of transformers\nacross different data modalities and distinguishes them from other\narchitectures. We show that transformers have lower sensitivity than MLPs,\nCNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show\nthat this low-sensitivity bias has important implications: i) lower sensitivity\ncorrelates with improved robustness; it can also be used as an efficient\nintervention to further improve the robustness of transformers; ii) it\ncorresponds to flatter minima in the loss landscape; and iii) it can serve as a\nprogress measure for grokking. We support these findings with theoretical\nresults showing (weak) spectral bias of transformers in the NTK regime, and\nimproved robustness due to the lower sensitivity. The code is available at\nhttps://github.com/estija/sensitivity.\n","authors":["Bhavya Vasudeva","Deqing Fu","Tianyi Zhou","Elliott Kau","Youqi Huang","Vatsal Sharan"],"pdf_url":"https://arxiv.org/pdf/2403.06925v2.pdf","comment":"ICLR 2025. 24 pages, 19 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.09611v1","updated":"2025-02-13T18:58:15Z","published":"2025-02-13T18:58:15Z","title":"Designing a Conditional Prior Distribution for Flow-Based Generative\n  Models","summary":"  Flow-based generative models have recently shown impressive performance for\nconditional generation tasks, such as text-to-image generation. However,\ncurrent methods transform a general unimodal noise distribution to a specific\nmode of the target data distribution. As such, every point in the initial\nsource distribution can be mapped to every point in the target distribution,\nresulting in long average paths. To this end, in this work, we tap into a\nnon-utilized property of conditional flow-based models: the ability to design a\nnon-trivial prior distribution. Given an input condition, such as a text\nprompt, we first map it to a point lying in data space, representing an\n``average\" data point with the minimal average distance to all data points of\nthe same conditional mode (e.g., class). We then utilize the flow matching\nformulation to map samples from a parametric distribution centered around this\npoint to the conditional target distribution. Experimentally, our method\nsignificantly improves training times and generation efficiency (FID, KID and\nCLIP alignment scores) compared to baselines, producing high quality samples\nusing fewer sampling steps.\n","authors":["Noam Issachar","Mohammad Salama","Raanan Fattal","Sagie Benaim"],"pdf_url":"https://arxiv.org/pdf/2502.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13904v3","updated":"2025-02-13T18:58:14Z","published":"2025-01-23T18:34:09Z","title":"Privacy-Preserving Personalized Federated Prompt Learning for Multimodal\n  Large Language Models","summary":"  Multimodal Large Language Models (LLMs) are pivotal in revolutionizing\ncustomer support and operations by integrating multiple modalities such as\ntext, images, and audio. Federated Prompt Learning (FPL) is a recently proposed\napproach that combines pre-trained multimodal LLMs such as vision-language\nmodels with federated learning to create personalized, privacy-preserving AI\nsystems. However, balancing the competing goals of personalization,\ngeneralization, and privacy remains a significant challenge.\nOver-personalization can lead to overfitting, reducing generalizability, while\nstringent privacy measures, such as differential privacy, can hinder both\npersonalization and generalization. In this paper, we propose a Differentially\nPrivate Federated Prompt Learning (DP-FPL) approach to tackle this challenge by\nleveraging a low-rank factorization scheme to capture generalization while\nmaintaining a residual term that preserves expressiveness for personalization.\nTo ensure privacy, we introduce a novel method where we apply local\ndifferential privacy to the two low-rank components of the local prompt, and\nglobal differential privacy to the global prompt. Our approach mitigates the\nimpact of privacy noise on the model performance while balancing the tradeoff\nbetween personalization and generalization. Extensive experiments demonstrate\nthe effectiveness of our approach over other benchmarks.\n","authors":["Linh Tran","Wei Sun","Stacy Patterson","Ana Milanova"],"pdf_url":"https://arxiv.org/pdf/2501.13904v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09609v1","updated":"2025-02-13T18:57:20Z","published":"2025-02-13T18:57:20Z","title":"Score-of-Mixture Training: Training One-Step Generative Models Made\n  Simple","summary":"  We propose Score-of-Mixture Training (SMT), a novel framework for training\none-step generative models by minimizing a class of divergences called the\n$\\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the score\nof mixture distributions between real and fake samples across multiple noise\nlevels. Similar to consistency models, our approach supports both training from\nscratch (SMT) and distillation using a pretrained diffusion model, which we\ncall Score-of-Mixture Distillation (SMD). It is simple to implement, requires\nminimal hyperparameter tuning, and ensures stable training. Experiments on\nCIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even\noutperform existing methods.\n","authors":["Tejas Jayashankar","J. Jon Ryu","Gregory Wornell"],"pdf_url":"https://arxiv.org/pdf/2502.09609v1.pdf","comment":"27 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09606v1","updated":"2025-02-13T18:55:56Z","published":"2025-02-13T18:55:56Z","title":"Human-LLM Coevolution: Evidence from Academic Writing","summary":"  With a statistical analysis of arXiv paper abstracts, we report a marked drop\nin the frequency of several words previously identified as overused by ChatGPT,\nsuch as \"delve\", starting soon after they were pointed out in early 2024. The\nfrequency of certain other words favored by ChatGPT, such as \"significant\", has\ninstead kept increasing. These phenomena suggest that some authors of academic\npapers have adapted their use of large language models (LLMs), for example, by\nselecting outputs or applying modifications to the LLM-generated content. Such\ncoevolution and cooperation of humans and LLMs thus introduce additional\nchallenges to the detection of machine-generated text in real-world scenarios.\nEstimating the impact of LLMs on academic writing by examining word frequency\nremains feasible, and more attention should be paid to words that were already\nfrequently employed, including those that have decreased in frequency.\n","authors":["Mingmeng Geng","Roberto Trotta"],"pdf_url":"https://arxiv.org/pdf/2502.09606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09604v1","updated":"2025-02-13T18:55:13Z","published":"2025-02-13T18:55:13Z","title":"SelfCite: Self-Supervised Alignment for Context Attribution in Large\n  Language Models","summary":"  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to\ngenerate high-quality, fine-grained, sentence-level citations for the\nstatements in their generated responses. Instead of only relying on costly and\nlabor-intensive annotations, SelfCite leverages a reward signal provided by the\nLLM itself through context ablation: If a citation is necessary, removing the\ncited text from the context should prevent the same response; if sufficient,\nretaining the cited text alone should preserve the same response. This reward\ncan guide the inference-time best-of-N sampling strategy to improve citation\nquality significantly, as well as be used in preference optimization to\ndirectly fine-tune the models for generating better citations. The\neffectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3\npoints on the LongBench-Cite benchmark across five long-form question answering\ntasks.\n","authors":["Yung-Sung Chuang","Benjamin Cohen-Wang","Shannon Zejiang Shen","Zhaofeng Wu","Hu Xu","Xi Victoria Lin","James Glass","Shang-Wen Li","Wen-tau Yih"],"pdf_url":"https://arxiv.org/pdf/2502.09604v1.pdf","comment":"Implementation available at https://github.com/voidism/SelfCite"},{"id":"http://arxiv.org/abs/2502.09597v1","updated":"2025-02-13T18:52:03Z","published":"2025-02-13T18:52:03Z","title":"Do LLMs Recognize Your Preferences? Evaluating Personalized Preference\n  Following in LLMs","summary":"  Large Language Models (LLMs) are increasingly used as chatbots, yet their\nability to personalize responses to user preferences remains limited. We\nintroduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize\nand adhere to user preferences in a long-context conversational setting.\nPrefEval comprises 3,000 manually curated user preference and query pairs\nspanning 20 topics. PrefEval contains user personalization or preference\ninformation in both explicit and implicit forms, and evaluates LLM performance\nusing a generation and a classification task. With PrefEval, we evaluated the\naforementioned preference following capabilities of 10 open-source and\nproprietary LLMs in multi-session conversations with varying context lengths up\nto 100k tokens. We benchmark with various prompting, iterative feedback, and\nretrieval-augmented generation methods. Our benchmarking effort reveals that\nstate-of-the-art LLMs face significant challenges in proactively following\nusers' preferences during conversations. In particular, in zero-shot settings,\npreference following accuracy falls below 10% at merely 10 turns (~3k tokens)\nacross most evaluated models. Even with advanced prompting and retrieval\nmethods, preference following still deteriorates in long-context conversations.\nFurthermore, we show that fine-tuning on PrefEval significantly improves\nperformance. We believe PrefEval serves as a valuable resource for measuring,\nunderstanding, and enhancing LLMs' preference following abilities, paving the\nway for personalized conversational agents. Our code and dataset are available\nat https://prefeval.github.io/.\n","authors":["Siyan Zhao","Mingyi Hong","Yang Liu","Devamanyu Hazarika","Kaixiang Lin"],"pdf_url":"https://arxiv.org/pdf/2502.09597v1.pdf","comment":"Accepted at ICLR 2025 as oral presentation. Code and data at:\n  https://prefeval.github.io/"},{"id":"http://arxiv.org/abs/2502.09591v1","updated":"2025-02-13T18:48:04Z","published":"2025-02-13T18:48:04Z","title":"Censor Dependent Variational Inference","summary":"  This paper provides a comprehensive analysis of variational inference in\nlatent variable models for survival analysis, emphasizing the distinctive\nchallenges associated with applying variational methods to survival data. We\nidentify a critical weakness in the existing methodology, demonstrating how a\npoorly designed variational distribution may hinder the objective of survival\nanalysis tasks--modeling time-to-event distributions. We prove that the optimal\nvariational distribution, which perfectly bounds the log-likelihood, may depend\non the censoring mechanism. To address this issue, we propose censor-dependent\nvariational inference (CDVI), tailored for latent variable models in survival\nanalysis. More practically, we introduce CD-CVAE, a V-structure Variational\nAutoencoder (VAE) designed for the scalable implementation of CDVI. Further\ndiscussion extends some existing theories and training techniques to survival\nanalysis. Extensive experiments validate our analysis and demonstrate\nsignificant improvements in the estimation of individual survival\ndistributions.\n","authors":["Chuanhui Liu","Xiao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09587v1","updated":"2025-02-13T18:45:56Z","published":"2025-02-13T18:45:56Z","title":"Rolling Ahead Diffusion for Traffic Scene Simulation","summary":"  Realistic driving simulation requires that NPCs not only mimic natural\ndriving behaviors but also react to the behavior of other simulated agents.\nRecent developments in diffusion-based scenario generation focus on creating\ndiverse and realistic traffic scenarios by jointly modelling the motion of all\nthe agents in the scene. However, these traffic scenarios do not react when the\nmotion of agents deviates from their modelled trajectories. For example, the\nego-agent can be controlled by a stand along motion planner. To produce\nreactive scenarios with joint scenario models, the model must regenerate the\nscenario at each timestep based on new observations in a Model Predictive\nControl (MPC) fashion. Although reactive, this method is time-consuming, as one\ncomplete possible future for all NPCs is generated per simulation step.\nAlternatively, one can utilize an autoregressive model (AR) to predict only the\nimmediate next-step future for all NPCs. Although faster, this method lacks the\ncapability for advanced planning. We present a rolling diffusion based traffic\nscene generation model which mixes the benefits of both methods by predicting\nthe next step future and simultaneously predicting partially noised further\nfuture steps at the same time. We show that such model is efficient compared to\ndiffusion model based AR, achieving a beneficial compromise between reactivity\nand computational efficiency.\n","authors":["Yunpeng Liu","Matthew Niedoba","William Harvey","Adam Scibior","Berend Zwartsenberg","Frank Wood"],"pdf_url":"https://arxiv.org/pdf/2502.09587v1.pdf","comment":"Accepted to Workshop on Machine Learning for Autonomous Driving at\n  AAAI 2025"},{"id":"http://arxiv.org/abs/2502.09583v1","updated":"2025-02-13T18:41:55Z","published":"2025-02-13T18:41:55Z","title":"Learning to Coordinate with Experts","summary":"  When deployed in dynamic environments, AI agents will inevitably encounter\nchallenges that exceed their individual capabilities. Leveraging assistance\nfrom expert agents-whether human or AI-can significantly enhance safety and\nperformance in such situations. However, querying experts is often costly,\nnecessitating the development of agents that can efficiently request and\nutilize expert guidance. In this paper, we introduce a fundamental coordination\nproblem called Learning to Yield and Request Control (YRC), where the objective\nis to learn a strategy that determines when to act autonomously and when to\nseek expert assistance. We consider a challenging practical setting in which an\nagent does not interact with experts during training but must adapt to novel\nenvironmental changes and expert interventions at test time. To facilitate\nempirical research, we introduce YRC-Bench, an open-source benchmark featuring\ndiverse domains. YRC-Bench provides a standardized Gym-like API, simulated\nexperts, evaluation pipeline, and implementation of competitive baselines.\nTowards tackling the YRC problem, we propose a novel validation approach and\ninvestigate the performance of various learning methods across diverse\nenvironments, yielding insights that can guide future research.\n","authors":["Mohamad H. Danesh","Tu Trinh","Benjamin Plaut","Nguyen X. Khanh"],"pdf_url":"https://arxiv.org/pdf/2502.09583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20092v2","updated":"2025-02-13T18:38:13Z","published":"2024-10-26T06:06:08Z","title":"OGBench: Benchmarking Offline Goal-Conditioned RL","summary":"  Offline goal-conditioned reinforcement learning (GCRL) is a major problem in\nreinforcement learning (RL) because it provides a simple, unsupervised, and\ndomain-agnostic way to acquire diverse behaviors and representations from\nunlabeled data without rewards. Despite the importance of this setting, we lack\na standard benchmark that can systematically evaluate the capabilities of\noffline GCRL algorithms. In this work, we propose OGBench, a new, high-quality\nbenchmark for algorithms research in offline goal-conditioned RL. OGBench\nconsists of 8 types of environments, 85 datasets, and reference implementations\nof 6 representative offline GCRL algorithms. We have designed these challenging\nand realistic environments and datasets to directly probe different\ncapabilities of algorithms, such as stitching, long-horizon reasoning, and the\nability to handle high-dimensional inputs and stochasticity. While\nrepresentative algorithms may rank similarly on prior benchmarks, our\nexperiments reveal stark strengths and weaknesses in these different\ncapabilities, providing a strong foundation for building new algorithms.\nProject page: https://seohong.me/projects/ogbench\n","authors":["Seohong Park","Kevin Frans","Benjamin Eysenbach","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2410.20092v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09573v1","updated":"2025-02-13T18:31:17Z","published":"2025-02-13T18:31:17Z","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt\n  Engineering","summary":"  In this study, we tackle industry challenges in video content classification\nby exploring and optimizing GPT-based models for zero-shot classification\nacross seven critical categories of video quality. We contribute a novel\napproach to improving GPT's performance through prompt optimization and policy\nrefinement, demonstrating that simplifying complex policies significantly\nreduces false negatives. Additionally, we introduce a new\ndecomposition-aggregation-based prompt engineering technique, which outperforms\ntraditional single-prompt methods. These experiments, conducted on real\nindustry problems, show that thoughtful prompt design can substantially enhance\nGPT's performance without additional finetuning, offering an effective and\nscalable solution for improving video classification systems across various\ndomains in industry.\n","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"pdf_url":"https://arxiv.org/pdf/2502.09573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09571v1","updated":"2025-02-13T18:29:48Z","published":"2025-02-13T18:29:48Z","title":"DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra","summary":"  Mass spectrometry plays a fundamental role in elucidating the structures of\nunknown molecules and subsequent scientific discoveries. One formulation of the\nstructure elucidation task is the conditional $\\textit{de novo}$ generation of\nmolecular structure given a mass spectrum. Toward a more accurate and efficient\nscientific discovery pipeline for small molecules, we present DiffMS, a\nformula-restricted encoder-decoder generative network that achieves\nstate-of-the-art performance on this task. The encoder utilizes a transformer\narchitecture and models mass spectra domain knowledge such as peak formulae and\nneutral losses, and the decoder is a discrete graph diffusion model restricted\nby the heavy-atom composition of a known chemical formula. To develop a robust\ndecoder that bridges latent embeddings and molecular structures, we pretrain\nthe diffusion decoder with fingerprint-structure pairs, which are available in\nvirtually infinite quantities, compared to structure-spectrum pairs that number\nin the tens of thousands. Extensive experiments on established benchmarks show\nthat DiffMS outperforms existing models on $\\textit{de novo}$ molecule\ngeneration. We provide several ablations to demonstrate the effectiveness of\nour diffusion and pretraining approaches and show consistent performance\nscaling with increasing pretraining dataset size. DiffMS code is publicly\navailable at https://github.com/coleygroup/DiffMS.\n","authors":["Montgomery Bohde","Mrunali Manjrekar","Runzhong Wang","Shuiwang Ji","Connor W. Coley"],"pdf_url":"https://arxiv.org/pdf/2502.09571v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09570v1","updated":"2025-02-13T18:28:17Z","published":"2025-02-13T18:28:17Z","title":"Enhancing the Utility of Higher-Order Information in Relational Learning","summary":"  Higher-order information is crucial for relational learning in many domains\nwhere relationships extend beyond pairwise interactions. Hypergraphs provide a\nnatural framework for modeling such relationships, which has motivated recent\nextensions of graph neural net- work architectures to hypergraphs. However,\ncomparisons between hypergraph architectures and standard graph-level models\nremain limited. In this work, we systematically evaluate a selection of\nhypergraph-level and graph-level architectures, to determine their\neffectiveness in leveraging higher-order information in relational learning.\nOur results show that graph-level architectures applied to hypergraph\nexpansions often outperform hypergraph- level ones, even on inputs that are\nnaturally parametrized as hypergraphs. As an alternative approach for\nleveraging higher-order information, we propose hypergraph-level encodings\nbased on classical hypergraph characteristics. While these encodings do not\nsignificantly improve hypergraph architectures, they yield substantial\nperformance gains when combined with graph-level models. Our theoretical\nanalysis shows that hypergraph-level encodings provably increase the\nrepresentational power of message-passing graph neural networks beyond that of\ntheir graph-level counterparts.\n","authors":["Raphael Pellegrin","Lukas Fesser","Melanie Weber"],"pdf_url":"https://arxiv.org/pdf/2502.09570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08593v2","updated":"2025-02-13T18:24:40Z","published":"2025-02-12T17:32:23Z","title":"Toward Universal Laws of Outlier Propagation","summary":"  We argue that Algorithmic Information Theory (AIT) admits a principled way to\nquantify outliers in terms of so-called randomness deficiency. For the\nprobability distribution generated by a causal Bayesian network, we show that\nthe randomness deficiency of the joint state decomposes into randomness\ndeficiencies of each causal mechanism, subject to the Independence of\nMechanisms Principle. Accordingly, anomalous joint observations can be\nquantitatively attributed to their root causes, i.e., the mechanisms that\nbehaved anomalously. As an extension of Levin's law of randomness conservation,\nwe show that weak outliers cannot cause strong ones when Independence of\nMechanisms holds. We show how these information theoretic laws provide a better\nunderstanding of the behaviour of outliers defined with respect to existing\nscores.\n","authors":["Aram Ebtekar","Yuhao Wang","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2502.08593v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10238v2","updated":"2025-02-13T18:22:34Z","published":"2024-07-14T15:11:13Z","title":"Asymptotic Normality of Generalized Low-Rank Matrix Sensing via\n  Riemannian Geometry","summary":"  We prove an asymptotic normality guarantee for generalized low-rank matrix\nsensing -- i.e., matrix sensing under a general convex loss $\\bar\\ell(\\langle\nX,M\\rangle,y^*)$, where $M\\in\\mathbb{R}^{d\\times d}$ is the unknown rank-$k$\nmatrix, $X$ is a measurement matrix, and $y^*$ is the corresponding\nmeasurement. Our analysis relies on tools from Riemannian geometry to handle\ndegeneracy of the Hessian of the loss due to rotational symmetry in the\nparameter space. In particular, we parameterize the manifold of low-rank\nmatrices by $\\bar\\theta\\bar\\theta^\\top$, where\n$\\bar\\theta\\in\\mathbb{R}^{d\\times k}$. Then, assuming the minimizer of the\nempirical loss $\\bar\\theta^0\\in\\mathbb{R}^{d\\times k}$ is in a constant size\nball around the true parameters $\\bar\\theta^*$, we prove\n$\\sqrt{n}(\\phi^0-\\phi^*)\\xrightarrow{D}N(0,(H^*)^{-1})$ as $n\\to\\infty$, where\n$\\phi^0$ and $\\phi^*$ are representations of $\\bar\\theta^*$ and $\\bar\\theta^0$\nin the horizontal space of the Riemannian quotient manifold\n$\\mathbb{R}^{d\\times k}/\\text{O}(k)$, and $H^*$ is the Hessian of the true loss\nin the same representation.\n","authors":["Osbert Bastani"],"pdf_url":"https://arxiv.org/pdf/2407.10238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09566v1","updated":"2025-02-13T18:21:15Z","published":"2025-02-13T18:21:15Z","title":"Zero-shot generation of synthetic neurosurgical data with large language\n  models","summary":"  Clinical data is fundamental to advance neurosurgical research, but access is\noften constrained by data availability, small sample sizes, privacy\nregulations, and resource-intensive preprocessing and de-identification\nprocedures. Synthetic data offers a potential solution to challenges associated\nwith accessing and using real-world data (RWD). This study aims to evaluate the\ncapability of zero-shot generation of synthetic neurosurgical data with a large\nlanguage model (LLM), GPT-4o, by benchmarking with the conditional tabular\ngenerative adversarial network (CTGAN). Synthetic datasets were compared to\nreal-world neurosurgical data to assess fidelity (means, proportions,\ndistributions, and bivariate correlations), utility (ML classifier performance\non RWD), and privacy (duplication of records from RWD). The GPT-4o-generated\ndatasets matched or exceeded CTGAN performance, despite no fine-tuning or\naccess to RWD for pre-training. Datasets demonstrated high univariate and\nbivariate fidelity to RWD without directly exposing any real patient records,\neven at amplified sample size. Training an ML classifier on GPT-4o-generated\ndata and testing on RWD for a binary prediction task showed an F1 score (0.706)\nwith comparable performance to training on the CTGAN data (0.705) for\npredicting postoperative functional status deterioration. GPT-4o demonstrated a\npromising ability to generate high-fidelity synthetic neurosurgical data. These\nfindings also indicate that data synthesized with GPT-4o can effectively\naugment clinical data with small sample sizes, and train ML models for\nprediction of neurosurgical outcomes. Further investigation is necessary to\nimprove the preservation of distributional characteristics and boost classifier\nperformance.\n","authors":["Austin A. Barr","Eddie Guo","Emre Sezgin"],"pdf_url":"https://arxiv.org/pdf/2502.09566v1.pdf","comment":"13 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2502.09564v1","updated":"2025-02-13T18:17:03Z","published":"2025-02-13T18:17:03Z","title":"Diffusing DeBias: a Recipe for Turning a Bug into a Feature","summary":"  Deep learning model effectiveness in classification tasks is often challenged\nby the quality and quantity of training data which, whenever containing strong\nspurious correlations between specific attributes and target labels, can result\nin unrecoverable biases in model predictions. Tackling these biases is crucial\nin improving model generalization and trust, especially in real-world\nscenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting\nas a plug-in for common methods in model debiasing while exploiting the\ninherent bias-learning tendency of diffusion models. Our approach leverages\nconditional diffusion models to generate synthetic bias-aligned images, used to\ntrain a bias amplifier model, to be further employed as an auxiliary method in\ndifferent unsupervised debiasing approaches. Our proposed method, which also\ntackles the common issue of training set memorization typical of this type of\ntech- niques, beats current state-of-the-art in multiple benchmark datasets by\nsignificant margins, demonstrating its potential as a versatile and effective\ntool for tackling dataset bias in deep learning applications.\n","authors":["Massimiliano Ciranni","Vito Paolo Pastore","Roberto Di Via","Enzo Tartaglione","Francesca Odone","Vittorio Murino"],"pdf_url":"https://arxiv.org/pdf/2502.09564v1.pdf","comment":"29 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2502.07864v2","updated":"2025-02-13T18:07:04Z","published":"2025-02-11T18:20:18Z","title":"TransMLA: Multi-Head Latent Attention Is All You Need","summary":"  Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.\n","authors":["Fanxu Meng","Zengwei Yao","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07864v2.pdf","comment":"https://github.com/fxmeng/TransMLA"},{"id":"http://arxiv.org/abs/2502.09553v1","updated":"2025-02-13T18:05:12Z","published":"2025-02-13T18:05:12Z","title":"SyntheticPop: Attacking Speaker Verification Systems With Synthetic\n  VoicePops","summary":"  Voice Authentication (VA), also known as Automatic Speaker Verification\n(ASV), is a widely adopted authentication method, particularly in automated\nsystems like banking services, where it serves as a secondary layer of user\nauthentication. Despite its popularity, VA systems are vulnerable to various\nattacks, including replay, impersonation, and the emerging threat of deepfake\naudio that mimics the voice of legitimate users. To mitigate these risks,\nseveral defense mechanisms have been proposed. One such solution, Voice Pops,\naims to distinguish an individual's unique phoneme pronunciations during the\nenrollment process. While promising, the effectiveness of VA+VoicePop against a\nbroader range of attacks, particularly logical or adversarial attacks, remains\ninsufficiently explored. We propose a novel attack method, which we refer to as\nSyntheticPop, designed to target the phoneme recognition capabilities of the\nVA+VoicePop system. The SyntheticPop attack involves embedding synthetic \"pop\"\nnoises into spoofed audio samples, significantly degrading the model's\nperformance. We achieve an attack success rate of over 95% while poisoning 20%\nof the training dataset. Our experiments demonstrate that VA+VoicePop achieves\n69% accuracy under normal conditions, 37% accuracy when subjected to a baseline\nlabel flipping attack, and just 14% accuracy under our proposed SyntheticPop\nattack, emphasizing the effectiveness of our method.\n","authors":["Eshaq Jamdar","Amith Kamath Belman"],"pdf_url":"https://arxiv.org/pdf/2502.09553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18970v3","updated":"2025-02-13T17:57:28Z","published":"2024-10-24T17:59:16Z","title":"WASP: A Weight-Space Approach to Detecting Learned Spuriousness","summary":"  It is of crucial importance to train machine learning models such that they\nclearly understand what defines each class in a given task. Though there is a\nsum of works dedicated to identifying the spurious correlations featured by a\ndataset that may impact the model's understanding of the classes, all current\napproaches rely solely on data or error analysis. That is, they cannot point\nout spurious correlations learned by the model that are not already pointed out\nby the counterexamples featured in the validation or training sets. We propose\na method that transcends this limitation, switching the focus from analyzing a\nmodel's predictions to analyzing the model's weights, the mechanism behind the\nmaking of the decisions, which proves to be more insightful. Our proposed\nWeight-space Approach to detecting Spuriousness (WASP) relies on analyzing the\nweights of foundation models as they drift towards capturing various (spurious)\ncorrelations while being fine-tuned on a given dataset. We demonstrate that\ndifferent from previous works, our method (i) can expose spurious correlations\nfeatured by a dataset even when they are not exposed by training or validation\ncounterexamples, (ii) it works for multiple modalities such as image and text,\nand (iii) it can uncover previously untapped spurious correlations learned by\nImageNet-1k classifiers.\n","authors":["Cristian Daniel PƒÉduraru","Antonio BƒÉrbƒÉlau","Radu Filipescu","Andrei Liviu Nicolicioiu","Elena Burceanu"],"pdf_url":"https://arxiv.org/pdf/2410.18970v3.pdf","comment":"8 pages, 4 figures, 6 tables, under review"},{"id":"http://arxiv.org/abs/2502.09534v1","updated":"2025-02-13T17:50:27Z","published":"2025-02-13T17:50:27Z","title":"Fast Tensor Completion via Approximate Richardson Iteration","summary":"  We study tensor completion (TC) through the lens of low-rank tensor\ndecomposition (TD). Many TD algorithms use fast alternating minimization\nmethods, which solve highly structured linear regression problems at each step\n(e.g., for CP, Tucker, and tensor-train decompositions). However, such\nalgebraic structure is lost in TC regression problems, making direct extensions\nunclear. To address this, we propose a lifting approach that approximately\nsolves TC regression problems using structured TD regression algorithms as\nblackbox subroutines, enabling sublinear-time methods. We theoretically analyze\nthe convergence rate of our approximate Richardson iteration based algorithm,\nand we demonstrate on real-world tensors that its running time can be 100x\nfaster than direct methods for CP completion.\n","authors":["Mehrdad Ghadiri","Matthew Fahrbach","Yunbum Kook","Ali Jadbabaie"],"pdf_url":"https://arxiv.org/pdf/2502.09534v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.09525v1","updated":"2025-02-13T17:37:42Z","published":"2025-02-13T17:37:42Z","title":"Robust Learning of Multi-index Models via Iterative Subspace\n  Approximation","summary":"  We study the task of learning Multi-Index Models (MIMs) with label noise\nunder the Gaussian distribution. A $K$-MIM is any function $f$ that only\ndepends on a $K$-dimensional subspace. We focus on well-behaved MIMs with\nfinite ranges that satisfy certain regularity properties. Our main contribution\nis a general robust learner that is qualitatively optimal in the Statistical\nQuery (SQ) model. Our algorithm iteratively constructs better approximations to\nthe defining subspace by computing low-degree moments conditional on the\nprojection to the subspace computed thus far, and adding directions with\nrelatively large empirical moments. This procedure efficiently finds a subspace\n$V$ so that $f(\\mathbf{x})$ is close to a function of the projection of\n$\\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional\nmoments do not help, we prove an SQ lower bound suggesting that no efficient\nlearner exists.\n  As applications, we provide faster robust learners for the following concept\nclasses:\n  * {\\bf Multiclass Linear Classifiers} We give a constant-factor approximate\nagnostic learner with sample complexity $N = O(d)\n2^{\\mathrm{poly}(K/\\epsilon)}$ and computational complexity $\\mathrm{poly}(N\n,d)$. This is the first constant-factor agnostic learner for this class whose\ncomplexity is a fixed-degree polynomial in $d$.\n  * {\\bf Intersections of Halfspaces} We give an approximate agnostic learner\nfor this class achieving 0-1 error $K \\tilde{O}(\\mathrm{OPT}) + \\epsilon$ with\nsample complexity $N=O(d^2) 2^{\\mathrm{poly}(K/\\epsilon)}$ and computational\ncomplexity $\\mathrm{poly}(N ,d)$. This is the first agnostic learner for this\nclass with near-linear error dependence and complexity a fixed-degree\npolynomial in $d$.\n  Furthermore, we show that in the presence of random classification noise, the\ncomplexity of our algorithm scales polynomially with $1/\\epsilon$.\n","authors":["Ilias Diakonikolas","Giannis Iakovidis","Daniel M. Kane","Nikos Zarifis"],"pdf_url":"https://arxiv.org/pdf/2502.09525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09511v1","updated":"2025-02-13T17:22:50Z","published":"2025-02-13T17:22:50Z","title":"Diffusion Models for Molecules: A Survey of Methods and Tasks","summary":"  Generative tasks about molecules, including but not limited to molecule\ngeneration, are crucial for drug discovery and material design, and have\nconsistently attracted significant attention. In recent years, diffusion models\nhave emerged as an impressive class of deep generative models, sparking\nextensive research and leading to numerous studies on their application to\nmolecular generative tasks. Despite the proliferation of related work, there\nremains a notable lack of up-to-date and systematic surveys in this area.\nParticularly, due to the diversity of diffusion model formulations, molecular\ndata modalities, and generative task types, the research landscape is\nchallenging to navigate, hindering understanding and limiting the area's\ngrowth. To address this, this paper conducts a comprehensive survey of\ndiffusion model-based molecular generative methods. We systematically review\nthe research from the perspectives of methodological formulations, data\nmodalities, and task types, offering a novel taxonomy. This survey aims to\nfacilitate understanding and further flourishing development in this area. The\nrelevant papers are summarized at:\nhttps://github.com/AzureLeon1/awesome-molecular-diffusion-models.\n","authors":["Liang Wang","Chao Song","Zhiyuan Liu","Yu Rong","Qiang Liu","Shu Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09509v1","updated":"2025-02-13T17:21:51Z","published":"2025-02-13T17:21:51Z","title":"EQ-VAE: Equivariance Regularized Latent Space for Improved Generative\n  Image Modeling","summary":"  Latent generative models have emerged as a leading approach for high-quality\nimage synthesis. These models rely on an autoencoder to compress images into a\nlatent space, followed by a generative model to learn the latent distribution.\nWe identify that existing autoencoders lack equivariance to semantic-preserving\ntransformations like scaling and rotation, resulting in complex latent spaces\nthat hinder generative performance. To address this, we propose EQ-VAE, a\nsimple regularization approach that enforces equivariance in the latent space,\nreducing its complexity without degrading reconstruction quality. By finetuning\npre-trained autoencoders with EQ-VAE, we enhance the performance of several\nstate-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,\nachieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.\nEQ-VAE is compatible with both continuous and discrete autoencoders, thus\noffering a versatile enhancement for a wide range of latent generative models.\nProject page and code: https://eq-vae.github.io/.\n","authors":["Theodoros Kouzelis","Ioannis Kakogeorgiou","Spyros Gidaris","Nikos Komodakis"],"pdf_url":"https://arxiv.org/pdf/2502.09509v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.09507v1","updated":"2025-02-13T17:21:37Z","published":"2025-02-13T17:21:37Z","title":"When and How Does CLIP Enable Domain and Compositional Generalization?","summary":"  The remarkable generalization performance of contrastive vision-language\nmodels like CLIP is often attributed to the diversity of their training\ndistributions. However, key questions remain unanswered: Can CLIP generalize to\nan entirely unseen domain when trained on a diverse mixture of domains (domain\ngeneralization)? Can it generalize to unseen classes within partially seen\ndomains (compositional generalization)? What factors affect such\ngeneralization? To answer these questions, we trained CLIP models on\nsystematically constructed training distributions with controlled domain\ndiversity and object class exposure. Our experiments show that domain diversity\nis essential for both domain and compositional generalization, yet\ncompositional generalization can be surprisingly weaker than domain\ngeneralization when the training distribution contains a suboptimal subset of\nthe test domain. Through data-centric and mechanistic analyses, we find that\nsuccessful generalization requires learning of shared representations already\nin intermediate layers and shared circuitry.\n","authors":["Elias Kempf","Simon Schrodi","Max Argus","Thomas Brox"],"pdf_url":"https://arxiv.org/pdf/2502.09507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09503v1","updated":"2025-02-13T17:15:26Z","published":"2025-02-13T17:15:26Z","title":"AttentionSmithy: A Modular Framework for Rapid Transformer Development\n  and Customization","summary":"  Transformer architectures have transformed AI applications but remain complex\nto customize for domain experts lacking low-level implementation expertise. We\nintroduce AttentionSmithy, a modular software package that simplifies\ntransformer innovation by breaking down key components into reusable building\nblocks: attention modules, feed-forward networks, normalization layers, and\npositional encodings. Users can rapidly prototype and evaluate transformer\nvariants without extensive coding. Our framework supports four positional\nencoding strategies and integrates with neural architecture search for\nautomated design. We validate AttentionSmithy by replicating the original\ntransformer under resource constraints and optimizing translation performance\nby combining positional encodings. Additionally, we demonstrate its\nadaptability in gene-specific modeling, achieving over 95% accuracy in cell\ntype classification. These case studies highlight AttentionSmithy's potential\nto accelerate research across diverse fields by removing framework\nimplementation barriers.\n","authors":["Caleb Cranney","Jesse G. Meyer"],"pdf_url":"https://arxiv.org/pdf/2502.09503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09502v1","updated":"2025-02-13T17:14:18Z","published":"2025-02-13T17:14:18Z","title":"Scalable First-order Method for Certifying Optimal k-Sparse GLMs","summary":"  This paper investigates the problem of certifying optimality for sparse\ngeneralized linear models (GLMs), where sparsity is enforced through an\n$\\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can\ncertify optimality by pruning nodes using dual bounds, existing methods for\ncomputing these bounds are either computationally intensive or exhibit slow\nconvergence, limiting their scalability to large-scale problems. To address\nthis challenge, we propose a first-order proximal gradient algorithm designed\nto solve the perspective relaxation of the problem within a BnB framework.\nSpecifically, we formulate the relaxed problem as a composite optimization\nproblem and demonstrate that the proximal operator of the non-smooth component\ncan be computed exactly in log-linear time complexity, eliminating the need to\nsolve a computationally expensive second-order cone program. Furthermore, we\nintroduce a simple restart strategy that enhances convergence speed while\nmaintaining low per-iteration complexity. Extensive experiments on synthetic\nand real-world datasets show that our approach significantly accelerates dual\nbound computations and is highly effective in providing optimality certificates\nfor large-scale problems.\n","authors":["Jiachang Liu","Soroosh Shafiee","Andrea Lodi"],"pdf_url":"https://arxiv.org/pdf/2502.09502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09500v1","updated":"2025-02-13T17:10:43Z","published":"2025-02-13T17:10:43Z","title":"Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting","summary":"  Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\n\\href{https://github.com/amazon-science/eideticnet-training}{this https URL}.\n","authors":["Nicholas Dronen","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2502.09500v1.pdf","comment":"16 pages, 6 figures; code is available at\n  https://github.com/amazon-science/eideticnet-training"},{"id":"http://arxiv.org/abs/2501.14346v2","updated":"2025-02-13T17:03:04Z","published":"2025-01-24T09:17:57Z","title":"HorNets: Learning from Discrete and Continuous Signals with Routing\n  Neural Networks","summary":"  Construction of neural network architectures suitable for learning from both\ncontinuous and discrete tabular data is a challenging research endeavor.\nContemporary high-dimensional tabular data sets are often characterized by a\nrelatively small instance count, requiring data-efficient learning. We propose\nHorNets (Horn Networks), a neural network architecture with state-of-the-art\nperformance on synthetic and real-life data sets from scarce-data tabular\ndomains. HorNets are based on a clipped polynomial-like activation function,\nextended by a custom discrete-continuous routing mechanism that decides which\npart of the neural network to optimize based on the input's cardinality. By\nexplicitly modeling parts of the feature combination space or combining whole\nspace in a linear attention-like manner, HorNets dynamically decide which mode\nof operation is the most suitable for a given piece of data with no explicit\nsupervision. This architecture is one of the few approaches that reliably\nretrieves logical clauses (including noisy XNOR) and achieves state-of-the-art\nclassification performance on 14 real-life biomedical high-dimensional data\nsets. HorNets are made freely available under a permissive license alongside a\nsynthetic generator of categorical benchmarks.\n","authors":["Boshko Koloski","Nada Lavraƒç","Bla≈æ ≈†krlj"],"pdf_url":"https://arxiv.org/pdf/2501.14346v2.pdf","comment":"Accepted to the ACML conference journal track with the Machine\n  Learning journal. The first and the last authors share an equal contribution"},{"id":"http://arxiv.org/abs/2502.09496v1","updated":"2025-02-13T17:03:03Z","published":"2025-02-13T17:03:03Z","title":"On Agnostic PAC Learning in the Small Error Regime","summary":"  Binary classification in the classic PAC model exhibits a curious phenomenon:\nEmpirical Risk Minimization (ERM) learners are suboptimal in the realizable\ncase yet optimal in the agnostic case. Roughly speaking, this owes itself to\nthe fact that non-realizable distributions $\\mathcal{D}$ are simply more\ndifficult to learn than realizable distributions -- even when one discounts a\nlearner's error by $\\mathrm{err}(h^*_{\\mathcal{D}})$, the error of the best\nhypothesis in $\\mathcal{H}$ for $\\mathcal{D}$. Thus, optimal agnostic learners\nare permitted to incur excess error on (easier-to-learn) distributions\n$\\mathcal{D}$ for which $\\tau = \\mathrm{err}(h^*_{\\mathcal{D}})$ is small.\n  Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this\nshortcoming by including $\\tau$ itself as a parameter in the agnostic error\nterm. In this more fine-grained model, they demonstrate tightness of the error\nlower bound $\\tau + \\Omega \\left(\\sqrt{\\frac{\\tau (d + \\log(1 / \\delta))}{m}} +\n\\frac{d + \\log(1 / \\delta)}{m} \\right)$ in a regime where $\\tau > d/m$, and\nleave open the question of whether there may be a higher lower bound when $\\tau\n\\approx d/m$, with $d$ denoting $\\mathrm{VC}(\\mathcal{H})$. In this work, we\nresolve this question by exhibiting a learner which achieves error $c \\cdot\n\\tau + O \\left(\\sqrt{\\frac{\\tau (d + \\log(1 / \\delta))}{m}} + \\frac{d + \\log(1\n/ \\delta)}{m} \\right)$ for a constant $c \\leq 2.1$, thus matching the lower\nbound when $\\tau \\approx d/m$. Further, our learner is computationally\nefficient and is based upon careful aggregations of ERM classifiers, making\nprogress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS\n`24). We leave open the interesting question of whether our approach can be\nrefined to lower the constant from 2.1 to 1, which would completely settle the\ncomplexity of agnostic learning.\n","authors":["Julian Asilis","Mikael M√∏ller H√∏gsgaard","Grigoris Velegkas"],"pdf_url":"https://arxiv.org/pdf/2502.09496v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2502.09495v1","updated":"2025-02-13T17:01:45Z","published":"2025-02-13T17:01:45Z","title":"Cracking the Code: Enhancing Development finance understanding with\n  artificial intelligence","summary":"  Analyzing development projects is crucial for understanding donors aid\nstrategies, recipients priorities, and to assess development finance capacity\nto adress development issues by on-the-ground actions. In this area, the\nOrganisation for Economic Co-operation and Developments (OECD) Creditor\nReporting System (CRS) dataset is a reference data source. This dataset\nprovides a vast collection of project narratives from various sectors\n(approximately 5 million projects). While the OECD CRS provides a rich source\nof information on development strategies, it falls short in informing project\npurposes due to its reporting process based on donors self-declared main\nobjectives and pre-defined industrial sectors. This research employs a novel\napproach that combines Machine Learning (ML) techniques, specifically Natural\nLanguage Processing (NLP), an innovative Python topic modeling technique called\nBERTopic, to categorise (cluster) and label development projects based on their\nnarrative descriptions. By revealing existing yet hidden topics of development\nfinance, this application of artificial intelligence enables a better\nunderstanding of donor priorities and overall development funding and provides\nmethods to analyse public and private projects narratives.\n","authors":["Pierre Beaucoral"],"pdf_url":"https://arxiv.org/pdf/2502.09495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09494v1","updated":"2025-02-13T17:00:11Z","published":"2025-02-13T17:00:11Z","title":"Communicating Likelihoods with Normalising Flows","summary":"  We present a machine-learning-based workflow to model an unbinned likelihood\nfrom its samples. A key advancement over existing approaches is the validation\nof the learned likelihood using rigorous statistical tests of the joint\ndistribution, such as the Kolmogorov-Smirnov test of the joint distribution.\nOur method enables the reliable communication of experimental and\nphenomenological likelihoods for subsequent analyses. We demonstrate its\neffectiveness through three case studies in high-energy physics. To support\nbroader adoption, we provide an open-source reference implementation, nabu.\n","authors":["Jack Y. Araz","Anja Beck","M√©ril Reboud","Michael Spannowsky","Danny van Dyk"],"pdf_url":"https://arxiv.org/pdf/2502.09494v1.pdf","comment":"4 pages + references, 1 figure"},{"id":"http://arxiv.org/abs/2502.09490v1","updated":"2025-02-13T16:57:07Z","published":"2025-02-13T16:57:07Z","title":"Inverse Design with Dynamic Mode Decomposition","summary":"  We introduce a computationally efficient method for the automation of inverse\ndesign in science and engineering. Based on simple least-square regression, the\nunderlying dynamic mode decomposition algorithm can be used to construct a\nlow-rank subspace spanning multiple experiments in parameter space. The\nproposed inverse design dynamic mode composition (ID-DMD) algorithm leverages\nthe computed low-dimensional subspace to enable fast digital design and\noptimization on laptop-level computing, including the potential to prescribe\nthe dynamics themselves. Moreover, the method is robust to noise, physically\ninterpretable, and can provide uncertainty quantification metrics. The\narchitecture can also efficiently scale to large-scale design problems using\nrandomized algorithms in the ID-DMD. The simplicity of the method and its\nimplementation are highly attractive in practice, and the ID-DMD has been\ndemonstrated to be an order of magnitude more accurate than competing methods\nwhile simultaneously being 3-5 orders faster on challenging engineering design\nproblems ranging from structural vibrations to fluid dynamics. Due to its\nspeed, robustness, interpretability, and ease-of-use, ID-DMD in comparison with\nother leading machine learning methods represents a significant advancement in\ndata-driven methods for inverse design and optimization, promising a paradigm\nshift in how to approach inverse design in practice.\n","authors":["Yunpeng Zhu","Liangliang Cheng","Anping Jing","Hanyu Huo","Ziqiang Lang","Bo Zhang","J. Nathan Kutz"],"pdf_url":"https://arxiv.org/pdf/2502.09490v1.pdf","comment":"29 pages, 19 figures"},{"id":"http://arxiv.org/abs/2502.09487v1","updated":"2025-02-13T16:52:06Z","published":"2025-02-13T16:52:06Z","title":"Objective quantification of mood states using large language models","summary":"  Emotional states influence human behaviour and cognition, leading to diverse\nthought trajectories. Similarly, Large Language Models (LLMs) showcase an\nexcellent level of response consistency across wide-ranging contexts (prompts).\nWe leverage these parallels to establish a framework for quantifying mental\nstates. Our approach utilises self-report questionnaires that reliably assess\nthese states due to their inherent sensitivity to patterns of co-occurring\nresponses. Specifically, we recruited a large sample of participants (N=422) to\ninvestigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set\nof depressive mood states measured with participants' open-ended responses to a\ndepression questionnaire. We show LLM responses to held-out multiple-choice\nquestions, given participants' open-ended answers, correlate strongly (r:\n0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation\nfrom mood representations. We explore a link between these representations and\nfactor analysis. Using ridge regression, we find depression-related subspaces\nwithin LLM hidden states. We show these subspaces to be predictive of\nparticipants' \"Depression\" and \"Somatic & Emotional Distress\" factor scores, as\nwell as suicidality severity. Overall, LLMs can provide quantitative measures\nof mental states. The reliability of these hinges upon how informative the\nquestions we ask participants are. Used correctly, this approach could\nsupplement mental state assessment in a variety of settings.\n","authors":["Jakub Onysk","Quentin Huys"],"pdf_url":"https://arxiv.org/pdf/2502.09487v1.pdf","comment":"main text - 9 pages, 5 figures;"},{"id":"http://arxiv.org/abs/2410.13879v2","updated":"2025-02-13T16:43:47Z","published":"2024-10-03T00:48:07Z","title":"Mixed-curvature decision trees and random forests","summary":"  Decision trees (DTs) and their random forest (RF) extensions are workhorses\nof classification and regression in Euclidean spaces. However, algorithms for\nlearning in non-Euclidean spaces are still limited. We extend DT and RF\nalgorithms to product manifolds: Cartesian products of several hyperbolic,\nhyperspherical, or Euclidean components. Such manifolds handle heterogeneous\ncurvature while still factorizing neatly into simpler components, making them\ncompelling embedding spaces for complex datasets. Our novel angular\nreformulation of DTs respects the geometry of the product manifold, yielding\nsplits that are geodesically convex, maximum-margin, and composable. In the\nspecial cases of single-component manifolds, our method simplifies to its\nEuclidean or hyperbolic counterparts, or introduces hyperspherical DT\nalgorithms, depending on the curvature. We benchmark our method on various\nclassification, regression, and link prediction tasks on synthetic data, graph\nembeddings, mixed-curvature variational autoencoder latent spaces, and\nempirical data. Compared to 7 other classifiers, product RFs ranked first on 25\nout of 57 benchmarks, and placed in the top 2 for 46 out of 57. This highlights\nthe value of product RFs as straightforward yet powerful new tools for data\nanalysis in product manifolds. Code for our paper is available at\nhttps://github.com/pchlenski/manify.\n","authors":["Philippe Chlenski","Quentin Chu","Raiyan R. Khan","Kaizhu Du","Antonio Khalil Moretti","Itsik Pe'er"],"pdf_url":"https://arxiv.org/pdf/2410.13879v2.pdf","comment":"27 pages, 11 figures. Submitted to ICML 2025"},{"id":"http://arxiv.org/abs/2502.09479v1","updated":"2025-02-13T16:43:32Z","published":"2025-02-13T16:43:32Z","title":"Assessing Generative AI value in a public sector context: evidence from\n  a field experiment","summary":"  The emergence of Generative AI (Gen AI) has motivated an interest in\nunderstanding how it could be used to enhance productivity across various\ntasks. We add to research results for the performance impact of Gen AI on\ncomplex knowledge-based tasks in a public sector setting. In a pre-registered\nexperiment, after establishing a baseline level of performance, we find mixed\nevidence for two types of composite tasks related to document understanding and\ndata analysis. For the Documents task, the treatment group using Gen AI had a\n17% improvement in answer quality scores (as judged by human evaluators) and a\n34% improvement in task completion time compared to a control group. For the\nData task, we find the Gen AI treatment group experienced a 12% reduction in\nquality scores and no significant difference in mean completion time compared\nto the control group. These results suggest that the benefits of Gen AI may be\ntask and potentially respondent dependent. We also discuss field notes and\nlessons learned, as well as supplementary insights from a post-trial survey and\nfeedback workshop with participants.\n","authors":["Trevor Fitzpatrick","Seamus Kelly","Patrick Carey","David Walsh","Ruairi Nugent"],"pdf_url":"https://arxiv.org/pdf/2502.09479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09477v1","updated":"2025-02-13T16:41:44Z","published":"2025-02-13T16:41:44Z","title":"DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation\n  Networks for Quantitative Nanomaterial Analysis through Differentiable\n  Rendering and Generative Modelling","summary":"  Nanomaterials exhibit distinctive properties governed by parameters such as\nsize, shape, and surface characteristics, which critically influence their\napplications and interactions across technological, biological, and\nenvironmental contexts. Accurate quantification and understanding of these\nmaterials are essential for advancing research and innovation. In this regard,\ndeep learning segmentation networks have emerged as powerful tools that enable\nautomated insights and replace subjective methods with precise quantitative\nanalysis. However, their efficacy depends on representative annotated datasets,\nwhich are challenging to obtain due to the costly imaging of nanoparticles and\nthe labor-intensive nature of manual annotations. To overcome these\nlimitations, we introduce DiffRenderGAN, a novel generative model designed to\nproduce annotated synthetic data. By integrating a differentiable renderer into\na Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes\ntextural rendering parameters to generate realistic, annotated nanoparticle\nimages from non-annotated real microscopy images. This approach reduces the\nneed for manual intervention and enhances segmentation performance compared to\nexisting synthetic data methods by generating diverse and realistic data.\nTested on multiple ion and electron microscopy cases, including titanium\ndioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),\nDiffRenderGAN bridges the gap between synthetic and real data, advancing the\nquantification and understanding of complex nanomaterial systems.\n","authors":["Dennis Possart","Leonid Mill","Florian Vollnhals","Tor Hildebrand","Peter Suter","Mathis Hoffmann","Jonas Utz","Daniel Augsburger","Mareike Thies","Mingxuan Wu","Fabian Wagner","George Sarau","Silke Christiansen","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2502.09477v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16333v2","updated":"2025-02-13T16:41:13Z","published":"2024-10-19T15:42:49Z","title":"Conformal Predictive Portfolio Selection","summary":"  This study examines portfolio selection using predictive models for portfolio\nreturns. Portfolio selection is a fundamental task in finance, and a variety of\nmethods have been developed to achieve this goal. For instance, the\nmean-variance approach constructs portfolios by balancing the trade-off between\nthe mean and variance of asset returns, while the quantile-based approach\noptimizes portfolios by considering tail risk. These methods often depend on\ndistributional information estimated from historical data using predictive\nmodels, each of which carries its own uncertainty. To address this, we propose\na framework for predictive portfolio selection via conformal prediction ,\ncalled \\emph{Conformal Predictive Portfolio Selection} (CPPS). Our approach\nforecasts future portfolio returns, computes the corresponding prediction\nintervals, and selects the portfolio of interest based on these intervals. The\nframework is flexible and can accommodate a wide range of predictive models,\nincluding autoregressive (AR) models, random forests, and neural networks. We\ndemonstrate the effectiveness of the CPPS framework by applying it to an AR\nmodel and validate its performance through empirical studies, showing that it\ndelivers superior returns compared to simpler strategies.\n","authors":["Masahiro Kato"],"pdf_url":"https://arxiv.org/pdf/2410.16333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09473v1","updated":"2025-02-13T16:36:25Z","published":"2025-02-13T16:36:25Z","title":"Learning to Predict Global Atrial Fibrillation Dynamics from Sparse\n  Measurements","summary":"  Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all\ntreatment with limited success in persistent AF. This may be due to our\ninability to map the dynamics of AF with the limited resolution and coverage\nprovided by sequential contact mapping catheters, preventing effective patient\nphenotyping for personalised, targeted ablation. Here we introduce FibMap, a\ngraph recurrent neural network model that reconstructs global AF dynamics from\nsparse measurements. Trained and validated on 51 non-contact whole atria\nrecordings, FibMap reconstructs whole atria dynamics from 10% surface coverage,\nachieving a 210% lower mean absolute error and an order of magnitude higher\nperformance in tracking phase singularities compared to baseline methods.\nClinical utility of FibMap is demonstrated on real-world contact mapping\nrecordings, achieving reconstruction fidelity comparable to non-contact\nmapping. FibMap's state-spaces and patient-specific parameters offer insights\nfor electrophenotyping AF. Integrating FibMap into clinical practice could\nenable personalised AF care and improve outcomes.\n","authors":["Alexander Jenkins","Andrea Cini","Joseph Barker","Alexander Sharp","Arunashis Sau","Varun Valentine","Srushti Valasang","Xinyang Li","Tom Wong","Timothy Betts","Danilo Mandic","Cesare Alippi","Fu Siong Ng"],"pdf_url":"https://arxiv.org/pdf/2502.09473v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.20440v2","updated":"2025-02-13T16:35:17Z","published":"2024-09-30T16:00:23Z","title":"Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits","summary":"  Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret\nfor adversarial as well as stochastic bandit problems and allow for a\nstreamlined analysis. Nonetheless, FTRL algorithms require the solution of an\noptimization problem in every iteration and are thus computationally\nchallenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achieve\ncomputational efficiency by perturbing the estimates of the rewards of the\narms, but their regret analysis is cumbersome. We propose a new FTPL algorithm\nthat generates optimal policies for both adversarial and stochastic multi-armed\nbandits. Like FTRL, our algorithm admits a unified regret analysis, and similar\nto FTPL, it offers low computational costs. Unlike existing FTPL algorithms\nthat rely on independent additive disturbances governed by a \\textit{known}\ndistribution, we allow for disturbances governed by an \\textit{ambiguous}\ndistribution that is only known to belong to a given set and propose a\nprinciple of optimism in the face of ambiguity. Consequently, our framework\ngeneralizes existing FTPL algorithms. It also encapsulates a broad range of\nFTRL methods as special cases, including several optimal ones, which appears to\nbe impossible with current FTPL methods. Finally, we use techniques from\ndiscrete choice theory to devise an efficient bisection algorithm for computing\nthe optimistic arm sampling probabilities. This algorithm is up to $10^4$ times\nfaster than standard FTRL algorithms that solve an optimization problem in\nevery iteration. Our results not only settle existing conjectures but also\nprovide new insights into the impact of perturbations by mapping FTRL to FTPL.\n","authors":["Mengmeng Li","Daniel Kuhn","Bahar Ta≈ükesen"],"pdf_url":"https://arxiv.org/pdf/2409.20440v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17163v2","updated":"2025-02-13T16:32:55Z","published":"2024-05-27T13:36:50Z","title":"Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep\n  Graph Networks","summary":"  The dynamics of information diffusion within graphs is a critical open issue\nthat heavily influences graph representation learning, especially when\nconsidering long-range propagation. This calls for principled approaches that\ncontrol and regulate the degree of propagation and dissipation of information\nthroughout the neural flow. Motivated by this, we introduce (port-)Hamiltonian\nDeep Graph Networks, a novel framework that models neural information flow in\ngraphs by building on the laws of conservation of Hamiltonian dynamical\nsystems. We reconcile under a single theoretical and practical framework both\nnon-dissipative long-range propagation and non-conservative behaviors,\nintroducing tools from mechanical systems to gauge the equilibrium between the\ntwo components. Our approach can be applied to general message-passing\narchitectures, and it provides theoretical guarantees on information\nconservation in time. Empirical results prove the effectiveness of our\nport-Hamiltonian scheme in pushing simple graph convolutional architectures to\nstate-of-the-art performance in long-range benchmarks.\n","authors":["Simon Heilig","Alessio Gravina","Alessandro Trenta","Claudio Gallicchio","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2405.17163v2.pdf","comment":"Accepted at ICLR 2025 (https://openreview.net/forum?id=03EkqSCKuO)"},{"id":"http://arxiv.org/abs/2411.03263v2","updated":"2025-02-13T16:28:07Z","published":"2024-11-05T17:02:29Z","title":"Proxy-informed Bayesian transfer learning with unknown sources","summary":"  Generalization outside the scope of one's training data requires leveraging\nprior knowledge about the effects that transfer, and the effects that don't,\nbetween different data sources. Transfer learning is a framework for specifying\nand refining this knowledge about sets of source (training) and target\n(prediction) data. A challenging open problem is addressing the empirical\nphenomenon of negative transfer, whereby the transfer learner performs worse on\nthe target data after taking the source data into account than before. We first\nintroduce a Bayesian perspective on negative transfer, and then a method to\naddress it. The key insight from our formulation is that negative transfer can\nstem from misspecified prior information about non-transferable causes of the\nsource data. Our proposed method, proxy-informed robust method for\nprobabilistic transfer learning (PROMPT), does not require prior knowledge of\nthe source data (the data sources may be \"unknown\"). PROMPT is thus applicable\nwhen differences between tasks are unobserved, such as in the presence of\nlatent confounders. Moreover, the learner need not have access to observations\nin the target task (cannot \"fine-tune\"), and instead makes use of proxy\n(indirect) information. Our theoretical results show that the threat of\nnegative transfer does not depend on the informativeness of the proxy\ninformation, highlighting the usefulness of PROMPT in cases where only noisy\nindirect information, such as human feedback, is available.\n","authors":["Sabina J. Sloman","Julien Martinelli","Samuel Kaski"],"pdf_url":"https://arxiv.org/pdf/2411.03263v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09445v1","updated":"2025-02-13T16:15:43Z","published":"2025-02-13T16:15:43Z","title":"A Differentiable Rank-Based Objective For Better Feature Learning","summary":"  In this paper, we leverage existing statistical methods to better understand\nfeature learning from data. We tackle this by modifying the model-free variable\nselection method, Feature Ordering by Conditional Independence (FOCI), which is\nintroduced in \\cite{azadkia2021simple}. While FOCI is based on a non-parametric\ncoefficient of conditional dependence, we introduce its parametric,\ndifferentiable approximation. With this approximate coefficient of correlation,\nwe present a new algorithm called difFOCI, which is applicable to a wider range\nof machine learning problems thanks to its differentiable nature and learnable\nparameters. We present difFOCI in three contexts: (1) as a variable selection\nmethod with baseline comparisons to FOCI, (2) as a trainable model parametrized\nwith a neural network, and (3) as a generic, widely applicable neural network\nregularizer, one that improves feature learning with better management of\nspurious correlations. We evaluate difFOCI on increasingly complex problems\nranging from basic variable selection in toy examples to saliency map\ncomparisons in convolutional networks. We then show how difFOCI can be\nincorporated in the context of fairness to facilitate classifications without\nrelying on sensitive data.\n","authors":["Krunoslav Lehman Pavasovic","David Lopez-Paz","Giulio Biroli","Levent Sagun"],"pdf_url":"https://arxiv.org/pdf/2502.09445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19082v2","updated":"2025-02-13T16:14:34Z","published":"2025-01-31T12:15:58Z","title":"A Bias-Correction Decentralized Stochastic Gradient Algorithm with\n  Momentum Acceleration","summary":"  Distributed stochastic optimization algorithms can simultaneously process\nlarge-scale datasets, significantly accelerating model training. However, their\neffectiveness is often hindered by the sparsity of distributed networks and\ndata heterogeneity. In this paper, we propose a momentum-accelerated\ndistributed stochastic gradient algorithm, termed Exact-Diffusion with Momentum\n(EDM), which mitigates the bias from data heterogeneity and incorporates\nmomentum techniques commonly used in deep learning to enhance convergence rate.\nOur theoretical analysis demonstrates that the EDM algorithm converges\nsub-linearly to the neighborhood of the optimal solution, the radius of which\nis irrespective of data heterogeneity, when applied to non-convex objective\nfunctions; under the Polyak-Lojasiewicz condition, which is a weaker assumption\nthan strong convexity, it converges linearly to the target region. Our analysis\ntechniques employed to handle momentum in complex distributed parameter update\nstructures yield a sufficiently tight convergence upper bound, offering a new\nperspective for the theoretical analysis of other momentum-based distributed\nalgorithms.\n","authors":["Yuchen Hu","Xi Chen","Weidong Liu","Xiaojun Mao"],"pdf_url":"https://arxiv.org/pdf/2501.19082v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09443v1","updated":"2025-02-13T16:12:17Z","published":"2025-02-13T16:12:17Z","title":"Relational Conformal Prediction for Correlated Time Series","summary":"  We address the problem of uncertainty quantification in time series\nforecasting by exploiting observations at correlated sequences. Relational deep\nlearning methods leveraging graph representations are among the most effective\ntools for obtaining point estimates from spatiotemporal data and correlated\ntime series. However, the problem of exploiting relational structures to\nestimate the uncertainty of such predictions has been largely overlooked in the\nsame context. To this end, we propose a novel distribution-free approach based\non the conformal prediction framework and quantile regression. Despite the\nrecent applications of conformal prediction to sequential data, existing\nmethods operate independently on each target time series and do not account for\nrelationships among them when constructing the prediction interval. We fill\nthis void by introducing a novel conformal prediction method based on graph\ndeep learning operators. Our method, named Conformal Relational Prediction\n(CoRel), does not require the relational structure (graph) to be known as a\nprior and can be applied on top of any pre-trained time series predictor.\nAdditionally, CoRel includes an adaptive component to handle non-exchangeable\ndata and changes in the input time series. Our approach provides accurate\ncoverage and archives state-of-the-art uncertainty quantification in relevant\nbenchmarks.\n","authors":["Andrea Cini","Alexander Jenkins","Danilo Mandic","Cesare Alippi","Filippo Maria Bianchi"],"pdf_url":"https://arxiv.org/pdf/2502.09443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17438v2","updated":"2025-02-13T16:11:35Z","published":"2023-05-27T10:26:23Z","title":"On the Importance of Backbone to the Adversarial Robustness of Object\n  Detectors","summary":"  Object detection is a critical component of various security-sensitive\napplications, such as autonomous driving and video surveillance. However,\nexisting object detectors are vulnerable to adversarial attacks, which poses a\nsignificant challenge to their reliability and security. Through experiments,\nfirst, we found that existing works on improving the adversarial robustness of\nobject detectors give a false sense of security. Second, we found that\nadversarially pre-trained backbone networks were essential for enhancing the\nadversarial robustness of object detectors. We then proposed a simple yet\neffective recipe for fast adversarial fine-tuning on object detectors with\nadversarially pre-trained backbones. Without any modifications to the structure\nof object detectors, our recipe achieved significantly better adversarial\nrobustness than previous works. Finally, we explored the potential of different\nmodern object detector designs for improving adversarial robustness with our\nrecipe and demonstrated interesting findings, which inspired us to design\nstate-of-the-art (SOTA) robust detectors. Our empirical results set a new\nmilestone for adversarially robust object detection. Code and trained\ncheckpoints are available at https://github.com/thu-ml/oddefense.\n","authors":["Xiao Li","Hang Chen","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2305.17438v2.pdf","comment":"Accepted by IEEE TIFS"},{"id":"http://arxiv.org/abs/2410.11415v2","updated":"2025-02-13T16:02:42Z","published":"2024-10-15T09:02:55Z","title":"KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI","summary":"  A popular approach to neurosymbolic AI involves mapping logic formulas to\narithmetic circuits (computation graphs consisting of sums and products) and\npassing the outputs of a neural network through these circuits. This approach\nenforces symbolic constraints onto a neural network in a principled and\nend-to-end differentiable way. Unfortunately, arithmetic circuits are\nchallenging to run on modern AI accelerators as they exhibit a high degree of\nirregular sparsity. To address this limitation, we introduce knowledge layers\n(KLay), a new data structure to represent arithmetic circuits that can be\nefficiently parallelized on GPUs. Moreover, we contribute two algorithms used\nin the translation of traditional circuit representations to KLay and a further\nalgorithm that exploits parallelization opportunities during circuit\nevaluations. We empirically show that KLay achieves speedups of multiple orders\nof magnitude over the state of the art, thereby paving the way towards scaling\nneurosymbolic AI to larger real-world applications.\n","authors":["Jaron Maene","Vincent Derkinderen","Pedro Zuidberg Dos Martires"],"pdf_url":"https://arxiv.org/pdf/2410.11415v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09432v1","updated":"2025-02-13T15:55:00Z","published":"2025-02-13T15:55:00Z","title":"Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes","summary":"  We study robust Markov decision processes (RMDPs) with non-rectangular\nuncertainty sets, which capture interdependencies across states unlike\ntraditional rectangular models. While non-rectangular robust policy evaluation\nis generally NP-hard, even in approximation, we identify a powerful class of\n$L_p$-bounded uncertainty sets that avoid these complexity barriers due to\ntheir structural simplicity. We further show that this class can be decomposed\ninto infinitely many \\texttt{sa}-rectangular $L_p$-bounded sets and leverage\nits structural properties to derive a novel dual formulation for $L_p$ RMDPs.\nThis formulation provides key insights into the adversary's strategy and\nenables the development of the first robust policy evaluation algorithms for\nnon-rectangular RMDPs. Empirical results demonstrate that our approach\nsignificantly outperforms brute-force methods, establishing a promising\nfoundation for future investigation into non-rectangular robust MDPs.\n","authors":["Navdeep Kumar","Adarsh Gupta","Maxence Mohamed Elfatihi","Giorgia Ramponi","Kfir Yehuda Levy","Shie Mannor"],"pdf_url":"https://arxiv.org/pdf/2502.09432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.08097v3","updated":"2025-02-13T15:53:59Z","published":"2024-05-13T18:24:03Z","title":"A Galois theorem for machine learning: Functions on symmetric matrices\n  and point clouds via lightweight invariant features","summary":"  In this work, we present a mathematical formulation for machine learning of\n(1) functions on symmetric matrices that are invariant with respect to the\naction of permutations by conjugation, and (2) functions on point clouds that\nare invariant with respect to rotations, reflections, and permutations of the\npoints. To achieve this, we provide a general construction of generically\nseparating invariant features using ideas inspired by Galois theory. We\nconstruct $O(n^2)$ invariant features derived from generators for the field of\nrational functions on $n\\times n$ symmetric matrices that are invariant under\njoint permutations of rows and columns. We show that these invariant features\ncan separate all distinct orbits of symmetric matrices except for a measure\nzero set; such features can be used to universally approximate invariant\nfunctions on almost all weighted graphs. For point clouds in a fixed dimension,\nwe prove that the number of invariant features can be reduced, generically\nwithout losing expressivity, to $O(n)$, where $n$ is the number of points. We\ncombine these invariant features with DeepSets to learn functions on symmetric\nmatrices and point clouds with varying sizes. We empirically demonstrate the\nfeasibility of our approach on molecule property regression and point cloud\ndistance prediction.\n","authors":["Ben Blum-Smith","Ningyuan Huang","Marco Cuturi","Soledad Villar"],"pdf_url":"https://arxiv.org/pdf/2405.08097v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00315v3","updated":"2025-02-13T15:46:35Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.01706v2","updated":"2025-02-13T15:43:25Z","published":"2024-10-02T16:15:26Z","title":"Sable: a Performant, Efficient and Scalable Sequence Model for MARL","summary":"  As multi-agent reinforcement learning (MARL) progresses towards solving\nlarger and more complex problems, it becomes increasingly important that\nalgorithms exhibit the key properties of (1) strong performance, (2) memory\nefficiency and (3) scalability. In this work, we introduce Sable, a performant,\nmemory efficient and scalable sequence modeling approach to MARL. Sable works\nby adapting the retention mechanism in Retentive Networks to achieve\ncomputationally efficient processing of multi-agent observations with long\ncontext memory for temporal reasoning. Through extensive evaluations across six\ndiverse environments, we demonstrate how Sable is able to significantly\noutperform existing state-of-the-art methods in a large number of diverse tasks\n(34 out of 45 tested). Furthermore, Sable maintains performance as we scale the\nnumber of agents, handling environments with more than a thousand agents while\nexhibiting a linear increase in memory usage. Finally, we conduct ablation\nstudies to isolate the source of Sable's performance gains and confirm its\nefficient computational memory usage.\n","authors":["Omayma Mahjoub","Sasha Abramowitz","Ruan de Kock","Wiem Khlifi","Simon du Toit","Jemma Daniel","Louay Ben Nessir","Louise Beyers","Claude Formanek","Liam Clark","Arnu Pretorius"],"pdf_url":"https://arxiv.org/pdf/2410.01706v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09419v1","updated":"2025-02-13T15:42:44Z","published":"2025-02-13T15:42:44Z","title":"On multi-token prediction for efficient LLM inference","summary":"  We systematically investigate multi-token prediction (MTP) capabilities\nwithin LLMs pre-trained for next-token prediction (NTP). We first show that\nsuch models inherently possess MTP capabilities via numerical marginalization\nover intermediate token probabilities, though performance is data-dependent and\nimproves with model scale. Furthermore, we explore the challenges of\nintegrating MTP heads into frozen LLMs and find that their hidden layers are\nstrongly specialized for NTP, making adaptation non-trivial. Finally, we show\nthat while joint training of MTP heads with the backbone improves performance,\nit cannot fully overcome this barrier, prompting further research in this\ndirection. Our findings provide a deeper understanding of MTP applied to\npretrained LLMs, informing strategies for accelerating inference through\nparallel token prediction.\n","authors":["Somesh Mehra","Javier Alonso Garcia","Lukas Mauch"],"pdf_url":"https://arxiv.org/pdf/2502.09419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09417v1","updated":"2025-02-13T15:40:39Z","published":"2025-02-13T15:40:39Z","title":"A Survey of Reinforcement Learning for Optimization in Automation","summary":"  Reinforcement Learning (RL) has become a critical tool for optimization\nchallenges within automation, leading to significant advancements in several\nareas. This review article examines the current landscape of RL within\nautomation, with a particular focus on its roles in manufacturing, energy\nsystems, and robotics. It discusses state-of-the-art methods, major challenges,\nand upcoming avenues of research within each sector, highlighting RL's capacity\nto solve intricate optimization challenges. The paper reviews the advantages\nand constraints of RL-driven optimization methods in automation. It points out\nprevalent challenges encountered in RL optimization, including issues related\nto sample efficiency and scalability; safety and robustness; interpretability\nand trustworthiness; transfer learning and meta-learning; and real-world\ndeployment and integration. It further explores prospective strategies and\nfuture research pathways to navigate these challenges. Additionally, the survey\nincludes a comprehensive list of relevant research papers, making it an\nindispensable guide for scholars and practitioners keen on exploring this\ndomain.\n","authors":["Ahmad Farooq","Kamran Iqbal"],"pdf_url":"https://arxiv.org/pdf/2502.09417v1.pdf","comment":"8 pages, 4 tables, and 1 figure. Accepted at IEEE 20th International\n  Conference on Automation Science and Engineering (CASE) 2024"},{"id":"http://arxiv.org/abs/2410.08751v2","updated":"2025-02-13T15:36:19Z","published":"2024-10-11T12:10:51Z","title":"Zero-Shot Offline Imitation Learning via Optimal Transport","summary":"  Zero-shot imitation learning algorithms hold the promise of reproducing\nunseen behavior from as little as a single demonstration at test time. Existing\npractical approaches view the expert demonstration as a sequence of goals,\nenabling imitation with a high-level goal selector, and a low-level\ngoal-conditioned policy. However, this framework can suffer from myopic\nbehavior: the agent's immediate actions towards achieving individual goals may\nundermine long-term objectives. We introduce a novel method that mitigates this\nissue by directly optimizing the occupancy matching objective that is intrinsic\nto imitation learning. We propose to lift a goal-conditioned value function to\na distance between occupancies, which are in turn approximated via a learned\nworld model. The resulting method can learn from offline, suboptimal data, and\nis capable of non-myopic, zero-shot imitation, as we demonstrate in complex,\ncontinuous benchmarks.\n","authors":["Thomas Rupf","Marco Bagatella","Nico G√ºrtler","Jonas Frey","Georg Martius"],"pdf_url":"https://arxiv.org/pdf/2410.08751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08441v2","updated":"2025-02-13T15:36:14Z","published":"2025-02-12T14:32:17Z","title":"Better Embeddings with Coupled Adam","summary":"  Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.\n","authors":["Felix Stollenwerk","Tobias Stollenwerk"],"pdf_url":"https://arxiv.org/pdf/2502.08441v2.pdf","comment":"17 pages, 8 figures; figures corrected"},{"id":"http://arxiv.org/abs/2411.02280v2","updated":"2025-02-13T15:21:43Z","published":"2024-11-04T17:09:10Z","title":"The LLM Language Network: A Neuroscientific Approach for Identifying\n  Causally Task-Relevant Units","summary":"  Large language models (LLMs) exhibit remarkable capabilities on not just\nlanguage tasks, but also various tasks that are not linguistic in nature, such\nas logical reasoning and social inference. In the human brain, neuroscience has\nidentified a core language system that selectively and causally supports\nlanguage processing. We here ask whether similar specialization for language\nemerges in LLMs. We identify language-selective units within 18 popular LLMs,\nusing the same localization approach that is used in neuroscience. We then\nestablish the causal role of these units by demonstrating that ablating LLM\nlanguage-selective units -- but not random units -- leads to drastic deficits\nin language tasks. Correspondingly, language-selective LLM units are more\naligned to brain recordings from the human language system than random units.\nFinally, we investigate whether our localization method extends to other\ncognitive domains: while we find specialized networks in some LLMs for\nreasoning and social capabilities, there are substantial differences among\nmodels. These findings provide functional and causal evidence for\nspecialization in large language models, and highlight parallels with the\nfunctional organization in the brain.\n","authors":["Badr AlKhamissi","Greta Tuckute","Antoine Bosselut","Martin Schrimpf"],"pdf_url":"https://arxiv.org/pdf/2411.02280v2.pdf","comment":"NAACL 2025"},{"id":"http://arxiv.org/abs/2502.09396v1","updated":"2025-02-13T15:16:53Z","published":"2025-02-13T15:16:53Z","title":"A hierarchical approach for assessing the vulnerability of tree-based\n  classification models to membership inference attack","summary":"  Machine learning models can inadvertently expose confidential properties of\ntheir training data, making them vulnerable to membership inference attacks\n(MIA). While numerous evaluation methods exist, many require computationally\nexpensive processes, such as training multiple shadow models. This article\npresents two new complementary approaches for efficiently identifying\nvulnerable tree-based models: an ante-hoc analysis of hyperparameter choices\nand a post-hoc examination of trained model structure. While these new methods\ncannot certify whether a model is safe from MIA, they provide practitioners\nwith a means to significantly reduce the number of models that need to undergo\nexpensive MIA assessment through a hierarchical filtering approach.\n  More specifically, it is shown that the rank order of disclosure risk for\ndifferent hyperparameter combinations remains consistent across datasets,\nenabling the development of simple, human-interpretable rules for identifying\nrelatively high-risk models before training. While this ante-hoc analysis\ncannot determine absolute safety since this also depends on the specific\ndataset, it allows the elimination of unnecessarily risky configurations during\nhyperparameter tuning. Additionally, computationally inexpensive structural\nmetrics serve as indicators of MIA vulnerability, providing a second filtering\nstage to identify risky models after training but before conducting expensive\nattacks. Empirical results show that hyperparameter-based risk prediction rules\ncan achieve high accuracy in predicting the most at risk combinations of\nhyperparameters across different tree-based model types, while requiring no\nmodel training. Moreover, target model accuracy is not seen to correlate with\nprivacy risk, suggesting opportunities to optimise model configurations for\nboth performance and privacy.\n","authors":["Richard J. Preen","Jim Smith"],"pdf_url":"https://arxiv.org/pdf/2502.09396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09395v1","updated":"2025-02-13T15:16:52Z","published":"2025-02-13T15:16:52Z","title":"Robot Pouring: Identifying Causes of Spillage and Selecting Alternative\n  Action Parameters Using Probabilistic Actual Causation","summary":"  In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a\nlarge variety of objects and goals. When confronted with an unexpected or\nunwanted outcome, we take corrective actions and try again until achieving the\ndesired result. The reasoning performed to identify a cause of the observed\noutcome and to select an appropriate corrective action is a crucial aspect of\nhuman reasoning for successful task execution. Central to this reasoning is the\nassumption that a factor is responsible for producing the observed outcome. In\nthis paper, we investigate the use of probabilistic actual causation to\ndetermine whether a factor is the cause of an observed undesired outcome.\nFurthermore, we show how the actual causation probabilities can be used to find\nalternative actions to change the outcome. We apply the probabilistic actual\ncausation analysis to a robot pouring task. When spillage occurs, the analysis\nindicates whether a task parameter is the cause and how it should be changed to\navoid spillage. The analysis requires a causal graph of the task and the\ncorresponding conditional probability distributions. To fulfill these\nrequirements, we perform a complete causal modeling procedure (i.e., task\nanalysis, definition of variables, determination of the causal graph structure,\nand estimation of conditional probability distributions) using data from a\nrealistic simulation of the robot pouring task, covering a large combinatorial\nspace of task parameters. Based on the results, we discuss the implications of\nthe variables' representation and how the alternative actions suggested by the\nactual causation analysis would compare to the alternative solutions proposed\nby a human observer. The practical use of the analysis of probabilistic actual\ncausation to select alternative action parameters is demonstrated.\n","authors":["Jaime Maldonado","Jonas Krumme","Christoph Zetzsche","Vanessa Didelez","Kerstin Schill"],"pdf_url":"https://arxiv.org/pdf/2502.09395v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.09390v1","updated":"2025-02-13T15:07:20Z","published":"2025-02-13T15:07:20Z","title":"SQuARE: Sequential Question Answering Reasoning Engine for Enhanced\n  Chain-of-Thought in Large Language Models","summary":"  In the rapidly evolving field of Natural Language Processing, Large Language\nModels (LLMs) are tasked with increasingly complex reasoning challenges.\nTraditional methods like chain-of-thought prompting have shown promise but\noften fall short in fully leveraging a model's reasoning capabilities. This\npaper introduces SQuARE (Sequential Question Answering Reasoning Engine), a\nnovel prompting technique designed to improve reasoning through a\nself-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts\nmodels to generate and resolve multiple auxiliary questions before tackling the\nmain query, promoting a more thorough exploration of various aspects of a\ntopic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models\nacross multiple question-answering datasets, demonstrate that SQuARE\nsignificantly surpasses traditional CoT prompts and existing\nrephrase-and-respond methods. By systematically decomposing queries, SQuARE\nadvances LLM capabilities in reasoning tasks. The code is publicly available at\nhttps://github.com/IntelLabs/RAG-FiT/tree/square.\n","authors":["Daniel Fleischer","Moshe Berchansky","Gad Markovits","Moshe Wasserblat"],"pdf_url":"https://arxiv.org/pdf/2502.09390v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2501.14441v2","updated":"2025-02-13T14:52:43Z","published":"2025-01-24T12:22:18Z","title":"Impact of Batch Normalization on Convolutional Network Representations","summary":"  Batch normalization (BatchNorm) is a popular layer normalization technique\nused when training deep neural networks. It has been shown to enhance the\ntraining speed and accuracy of deep learning models. However, the mechanics by\nwhich BatchNorm achieves these benefits is an active area of research, and\ndifferent perspectives have been proposed. In this paper, we investigate the\neffect of BatchNorm on the resulting hidden representations, that is, the\nvectors of activation values formed as samples are processed at each hidden\nlayer. Specifically, we consider the sparsity of these representations, as well\nas their implicit clustering -- the creation of groups of representations that\nare similar to some extent. We contrast image classification models trained\nwith and without batch normalization and highlight consistent differences\nobserved. These findings highlight that BatchNorm's effect on representational\nsparsity is not a significant factor affecting generalization, while the\nrepresentations of models trained with BatchNorm tend to show more advantageous\nclustering characteristics.\n","authors":["Hermanus L. Potgieter","Coenraad Mouton","Marelie H. Davel"],"pdf_url":"https://arxiv.org/pdf/2501.14441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09376v1","updated":"2025-02-13T14:45:11Z","published":"2025-02-13T14:45:11Z","title":"LoRA Training Provably Converges to a Low-Rank Global Minimum or It\n  Fails Loudly (But it Probably Won't Fail)","summary":"  Low-rank adaptation (LoRA) has become a standard approach for fine-tuning\nlarge foundation models. However, our theoretical understanding of LoRA remains\nlimited as prior analyses of LoRA's training dynamics either rely on\nlinearization arguments or consider highly simplified setups. In this work, we\nanalyze the LoRA loss landscape without such restrictive assumptions. We define\ntwo regimes: a ``special regime'', which includes idealized setups where\nlinearization arguments hold, and a ``generic regime'' representing more\nrealistic setups where linearization arguments do not hold. In the generic\nregime, we show that LoRA training converges to a global minimizer with low\nrank and small magnitude, or a qualitatively distinct solution with high rank\nand large magnitude. Finally, we argue that the zero-initialization and weight\ndecay in LoRA training induce an implicit bias toward the low-rank,\nsmall-magnitude region of the parameter space -- where global minima lie --\nthus shedding light on why LoRA training usually succeeds in finding global\nminima.\n","authors":["Junsu Kim","Jaeyeon Kim","Ernest K. Ryu"],"pdf_url":"https://arxiv.org/pdf/2502.09376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09374v1","updated":"2025-02-13T14:43:22Z","published":"2025-02-13T14:43:22Z","title":"Mitigating multiple single-event upsets during deep neural network\n  inference using fault-aware training","summary":"  Deep neural networks (DNNs) are increasingly used in safety-critical\napplications. Reliable fault analysis and mitigation are essential to ensure\ntheir functionality in harsh environments that contain high radiation levels.\nThis study analyses the impact of multiple single-bit single-event upsets in\nDNNs by performing fault injection at the level of a DNN model. Additionally, a\nfault aware training (FAT) methodology is proposed that improves the DNNs'\nrobustness to faults without any modification to the hardware. Experimental\nresults show that the FAT methodology improves the tolerance to faults up to a\nfactor 3.\n","authors":["Toon Vinck","Na√Øn Jonckers","Gert Dekkers","Jeffrey Prinzie","Peter Karsmakers"],"pdf_url":"https://arxiv.org/pdf/2502.09374v1.pdf","comment":"7 pages, 4 figures, Topical Workshop on Electronics for Particle\n  Physics"},{"id":"http://arxiv.org/abs/2502.07465v2","updated":"2025-02-13T14:38:24Z","published":"2025-02-11T11:16:59Z","title":"Crime Forecasting: A Spatio-temporal Analysis with Deep Learning Models","summary":"  This study uses deep-learning models to predict city partition crime counts\non specific days. It helps police enhance surveillance, gather intelligence,\nand proactively prevent crimes. We formulate crime count prediction as a\nspatiotemporal sequence challenge, where both input data and prediction targets\nare spatiotemporal sequences. In order to improve the accuracy of crime\nforecasting, we introduce a new model that combines Convolutional Neural\nNetworks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a\ncomparative analysis to access the effects of various data sequences, including\nraw and binned data, on the prediction errors of four deep learning forecasting\nmodels. Directly inputting raw crime data into the forecasting model causes\nhigh prediction errors, making the model unsuitable for real - world use. The\nfindings indicate that the proposed CNN-LSTM model achieves optimal performance\nwhen crime data is categorized into 10 or 5 groups. Data binning can enhance\nforecasting model performance, but poorly defined intervals may reduce map\ngranularity. Compared to dividing into 5 bins, binning into 10 intervals\nstrikes an optimal balance, preserving data characteristics and surpassing raw\ndata in predictive modelling efficacy.\n","authors":["Li Mao","Wei Du","Shuo Wen","Qi Li","Tong Zhang","Wei Zhong"],"pdf_url":"https://arxiv.org/pdf/2502.07465v2.pdf","comment":"The paper was submitted without the consent of all co-authors. The\n  content of the paper is incomplete and requires substantial additional work\n  before it can be considered a complete and coherent submission"},{"id":"http://arxiv.org/abs/2502.09369v1","updated":"2025-02-13T14:35:40Z","published":"2025-02-13T14:35:40Z","title":"Language Agents as Digital Representatives in Collective Decision-Making","summary":"  Consider the process of collective decision-making, in which a group of\nindividuals interactively select a preferred outcome from among a universe of\nalternatives. In this context, \"representation\" is the activity of making an\nindividual's preferences present in the process via participation by a proxy\nagent -- i.e. their \"representative\". To this end, learned models of human\nbehavior have the potential to fill this role, with practical implications for\nmulti-agent scenario studies and mechanism design. In this work, we investigate\nthe possibility of training \\textit{language agents} to behave in the capacity\nof representatives of human agents, appropriately expressing the preferences of\nthose individuals whom they stand for. First, we formalize the setting of\n\\textit{collective decision-making} -- as the episodic process of interaction\nbetween a group of agents and a decision mechanism. On this basis, we then\nformalize the problem of \\textit{digital representation} -- as the simulation\nof an agent's behavior to yield equivalent outcomes from the mechanism.\nFinally, we conduct an empirical case study in the setting of\n\\textit{consensus-finding} among diverse humans, and demonstrate the\nfeasibility of fine-tuning large language models to act as digital\nrepresentatives.\n","authors":["Daniel Jarrett","Miruna P√Æslar","Michiel A. Bakker","Michael Henry Tessler","Raphael K√∂ster","Jan Balaguer","Romuald Elie","Christopher Summerfield","Andrea Tacchetti"],"pdf_url":"https://arxiv.org/pdf/2502.09369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09365v1","updated":"2025-02-13T14:33:02Z","published":"2025-02-13T14:33:02Z","title":"Simple Path Structural Encoding for Graph Transformers","summary":"  Graph transformers extend global self-attention to graph-structured data,\nachieving notable success in graph learning. Recently, random walk structural\nencoding (RWSE) has been found to further enhance their predictive power by\nencoding both structural and positional information into the edge\nrepresentation. However, RWSE cannot always distinguish between edges that\nbelong to different local graph patterns, which reduces its ability to capture\nthe full structural complexity of graphs. This work introduces Simple Path\nStructural Encoding (SPSE), a novel method that utilizes simple path counts for\nedge encoding. We show theoretically and experimentally that SPSE overcomes the\nlimitations of RWSE, providing a richer representation of graph structures,\nparticularly for capturing local cyclic patterns. To make SPSE computationally\ntractable, we propose an efficient approximate algorithm for simple path\ncounting. SPSE demonstrates significant performance improvements over RWSE on\nvarious benchmarks, including molecular and long-range graph datasets,\nachieving statistically significant gains in discriminative tasks. These\nresults pose SPSE as a powerful edge encoding alternative for enhancing the\nexpressivity of graph transformers.\n","authors":["Louis Airale","Antonio Longa","Mattia Rigon","Andrea Passerini","Roberto Passerone"],"pdf_url":"https://arxiv.org/pdf/2502.09365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09363v1","updated":"2025-02-13T14:31:49Z","published":"2025-02-13T14:31:49Z","title":"The Accuracy Cost of Weakness: A Theoretical Analysis of Fixed-Segment\n  Weak Labeling for Events in Time","summary":"  Accurate labels are critical for deriving robust machine learning models.\nLabels are used to train supervised learning models and to evaluate most\nmachine learning paradigms. In this paper, we model the accuracy and cost of a\ncommon weak labeling process where annotators assign presence or absence labels\nto fixed-length data segments for a given event class. The annotator labels a\nsegment as \"present\" if it sufficiently covers an event from that class, e.g.,\na birdsong sound event in audio data. We analyze how the segment length affects\nthe label accuracy and the required number of annotations, and compare this\nfixed-length labeling approach with an oracle method that uses the true event\nactivations to construct the segments. Furthermore, we quantify the gap between\nthese methods and verify that in most realistic scenarios the oracle method is\nbetter than the fixed-length labeling method in both accuracy and cost. Our\nfindings provide a theoretical justification for adaptive weak labeling\nstrategies that mimic the oracle process, and a foundation for optimizing weak\nlabeling processes in sequence labeling tasks.\n","authors":["John Martinsson","Olof Mogren","Tuomas Virtanen","Maria Sandsten"],"pdf_url":"https://arxiv.org/pdf/2502.09363v1.pdf","comment":"Submitted to TMLR"},{"id":"http://arxiv.org/abs/2411.17287v2","updated":"2025-02-13T14:31:24Z","published":"2024-11-26T10:19:16Z","title":"Privacy-Preserving Federated Unsupervised Domain Adaptation for\n  Regression on Small-Scale and High-Dimensional Biological Data","summary":"  Machine learning models often struggle with generalization in small,\nheterogeneous datasets due to domain shifts caused by variations in data\ncollection and population differences. This challenge is particularly\npronounced in biological data, where data is high-dimensional, small-scale, and\ndecentralized across institutions. While federated domain adaptation methods\n(FDA) aim to address these challenges, most existing approaches rely on deep\nlearning and focus on classification tasks, making them unsuitable for\nsmall-scale, high-dimensional applications. In this work, we propose freda, a\nprivacy-preserving federated method for unsupervised domain adaptation in\nregression tasks. Unlike deep learning-based FDA approaches, freda is the first\nmethod to enable the federated training of Gaussian Processes to model complex\nfeature relationships while ensuring complete data privacy through randomized\nencoding and secure aggregation. This allows for effective domain adaptation\nwithout direct access to raw data, making it well-suited for applications\ninvolving high-dimensional, heterogeneous datasets. We evaluate freda on the\nchallenging task of age prediction from DNA methylation data, demonstrating\nthat it achieves performance comparable to the centralized state-of-the-art\nmethod while preserving complete data privacy.\n","authors":["Cem Ata Baykara","Ali Burak √únal","Nico Pfeifer","Mete Akg√ºn"],"pdf_url":"https://arxiv.org/pdf/2411.17287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09352v1","updated":"2025-02-13T14:18:41Z","published":"2025-02-13T14:18:41Z","title":"Wasserstein distributional adversarial training for deep neural networks","summary":"  Design of adversarial attacks for deep neural networks, as well as methods of\nadversarial training against them, are subject of intense research. In this\npaper, we propose methods to train against distributional attack threats,\nextending the TRADES method used for pointwise attacks. Our approach leverages\nrecent contributions and relies on sensitivity analysis for Wasserstein\ndistributionally robust optimization problems. We introduce an efficient\nfine-tuning method which can be deployed on a previously trained model. We test\nour methods on a range of pre-trained models on RobustBench. These experimental\nresults demonstrate the additional training enhances Wasserstein distributional\nrobustness, while maintaining original levels of pointwise robustness, even for\nalready very successful networks. The improvements are less marked for models\npre-trained using huge synthetic datasets of 20-100M images. However,\nremarkably, sometimes our methods are still able to improve their performance\neven when trained using only the original training dataset (50k images).\n","authors":["Xingjian Bai","Guangyi He","Yifan Jiang","Jan Obloj"],"pdf_url":"https://arxiv.org/pdf/2502.09352v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.05366v2","updated":"2025-02-13T14:16:56Z","published":"2024-06-08T06:06:20Z","title":"Regret Bounds for Episodic Risk-Sensitive Linear Quadratic Regulator","summary":"  Risk-sensitive linear quadratic regulator is one of the most fundamental\nproblems in risk-sensitive optimal control. In this paper, we study online\nadaptive control of risk-sensitive linear quadratic regulator in the finite\nhorizon episodic setting. We propose a simple least-squares greedy algorithm\nand show that it achieves $\\widetilde{\\mathcal{O}}(\\log N)$ regret under a\nspecific identifiability assumption, where $N$ is the total number of episodes.\nIf the identifiability assumption is not satisfied, we propose incorporating\nexploration noise into the least-squares-based algorithm, resulting in an\nalgorithm with $\\widetilde{\\mathcal{O}}(\\sqrt{N})$ regret. To our best\nknowledge, this is the first set of regret bounds for episodic risk-sensitive\nlinear quadratic regulator. Our proof relies on perturbation analysis of\nless-standard Riccati equations for risk-sensitive linear quadratic control,\nand a delicate analysis of the loss in the risk-sensitive performance criterion\ndue to applying the suboptimal controller in the online learning process.\n","authors":["Wenhao Xu","Xuefeng Gao","Xuedong He"],"pdf_url":"https://arxiv.org/pdf/2406.05366v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09346v1","updated":"2025-02-13T14:11:33Z","published":"2025-02-13T14:11:33Z","title":"Machine learning for modelling unstructured grid data in computational\n  physics: a review","summary":"  Unstructured grid data are essential for modelling complex geometries and\ndynamics in computational physics. Yet, their inherent irregularity presents\nsignificant challenges for conventional machine learning (ML) techniques. This\npaper provides a comprehensive review of advanced ML methodologies designed to\nhandle unstructured grid data in high-dimensional dynamical systems. Key\napproaches discussed include graph neural networks, transformer models with\nspatial attention mechanisms, interpolation-integrated ML methods, and meshless\ntechniques such as physics-informed neural networks. These methodologies have\nproven effective across diverse fields, including fluid dynamics and\nenvironmental simulations. This review is intended as a guidebook for\ncomputational scientists seeking to apply ML approaches to unstructured grid\ndata in their domains, as well as for ML researchers looking to address\nchallenges in computational physics. It places special focus on how ML methods\ncan overcome the inherent limitations of traditional numerical techniques and,\nconversely, how insights from computational physics can inform ML development.\nTo support benchmarking, this review also provides a summary of open-access\ndatasets of unstructured grid data in computational physics. Finally, emerging\ndirections such as generative models with unstructured data, reinforcement\nlearning for mesh generation, and hybrid physics-data-driven paradigms are\ndiscussed to inspire future advancements in this evolving field.\n","authors":["Sibo Cheng","Marc Bocquet","Weiping Ding","Tobias Sebastian Finn","Rui Fu","Jinlong Fu","Yike Guo","Eleda Johnson","Siyi Li","Che Liu","Eric Newton Moro","Jie Pan","Matthew Piggott","Cesar Quilodran","Prakhar Sharma","Kun Wang","Dunhui Xiao","Xiao Xue","Yong Zeng","Mingrui Zhang","Hao Zhou","Kewei Zhu","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2502.09346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04708v2","updated":"2025-02-13T14:06:51Z","published":"2024-11-07T13:45:26Z","title":"Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs","summary":"  Following the milestones in large language models (LLMs) and multimodal\nmodels, we have seen a surge in applying LLMs to biochemical tasks. Leveraging\ngraph features and molecular text representations, LLMs can tackle various\ntasks, such as predicting chemical reaction outcomes and describing molecular\nproperties. However, most current work overlooks the *multi-level nature* of\nthe graph modality, even though different chemistry tasks may benefit from\ndifferent feature levels. In this work, we first study the effect of feature\ngranularity and reveal that even reducing all GNN-generated feature tokens to a\nsingle one does not significantly impact model performance. We then investigate\nthe effect of various graph feature levels and demonstrate that both the\nquality of LLM-generated molecules and model performance across different tasks\ndepend on different graph feature levels. Therefore, we conclude with two key\ninsights: (1) current molecular-related multimodal LLMs lack a comprehensive\nunderstanding of graph features, and (2) static processing is not sufficient\nfor hierarchical graph feature. We share our findings in detail, with the hope\nof paving the way for the community to develop more advanced multimodal LLMs\nfor incorporating molecular graphs.\n","authors":["Chengxin Hu","Hao Li","Yihe Yuan","Jing Li","Ivor Tsang"],"pdf_url":"https://arxiv.org/pdf/2411.04708v2.pdf","comment":"9 pages, 4 tables, 1 figure, paper under review"},{"id":"http://arxiv.org/abs/2502.09341v1","updated":"2025-02-13T14:01:15Z","published":"2025-02-13T14:01:15Z","title":"Neural Spatiotemporal Point Processes: Trends and Challenges","summary":"  Spatiotemporal point processes (STPPs) are probabilistic models for events\noccurring in continuous space and time. Real-world event data often exhibit\nintricate dependencies and heterogeneous dynamics. By incorporating modern deep\nlearning techniques, STPPs can model these complexities more effectively than\ntraditional approaches. Consequently, the fusion of neural methods with STPPs\nhas become an active and rapidly evolving research area. In this review, we\ncategorize existing approaches, unify key design choices, and explain the\nchallenges of working with this data modality. We further highlight emerging\ntrends and diverse application domains. Finally, we identify open challenges\nand gaps in the literature.\n","authors":["Sumantrak Mukherjee","Mouad Elhamdi","George Mohler","David A. Selby","Yao Xie","Sebastian Vollmer","Gerrit Grossmann"],"pdf_url":"https://arxiv.org/pdf/2502.09341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09340v1","updated":"2025-02-13T14:00:55Z","published":"2025-02-13T14:00:55Z","title":"This looks like what? Challenges and Future Research Directions for\n  Part-Prototype Models","summary":"  The growing interest in eXplainable Artificial Intelligence (XAI) has\nprompted research into models with built-in interpretability, the most\nprominent of which are part-prototype models. Part-Prototype Models (PPMs) make\ndecisions by comparing an input image to a set of learned prototypes, providing\nhuman-understandable explanations in the form of ``this looks like that''.\nDespite their inherent interpretability, PPMS are not yet considered a valuable\nalternative to post-hoc models. In this survey, we investigate the reasons for\nthis and provide directions for future research. We analyze papers from 2019 to\n2024, and derive a taxonomy of the challenges that current PPMS face. Our\nanalysis shows that the open challenges are quite diverse. The main concern is\nthe quality and quantity of prototypes. Other concerns are the lack of\ngeneralization to a variety of tasks and contexts, and general methodological\nissues, including non-standardized evaluation. We provide ideas for future\nresearch in five broad directions: improving predictive performance, developing\nnovel architectures grounded in theory, establishing frameworks for human-AI\ncollaboration, aligning models with humans, and establishing metrics and\nbenchmarks for evaluation. We hope that this survey will stimulate research and\npromote intrinsically interpretable models for application domains. Our list of\nsurveyed papers is available at https://github.com/aix-group/ppm-survey.\n","authors":["Khawla Elhadri","Tomasz Michalski","Adam Wr√≥bel","J√∂rg Schl√∂tterer","Bartosz Zieli≈Ñski","Christin Seifert"],"pdf_url":"https://arxiv.org/pdf/2502.09340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07532v2","updated":"2025-02-13T13:55:08Z","published":"2025-02-11T13:15:16Z","title":"Diffusion-LAM: Probabilistic Limited Area Weather Forecasting with\n  Diffusion","summary":"  Machine learning methods have been shown to be effective for weather\nforecasting, based on the speed and accuracy compared to traditional numerical\nmodels. While early efforts primarily concentrated on deterministic\npredictions, the field has increasingly shifted toward probabilistic\nforecasting to better capture the forecast uncertainty. Most machine\nlearning-based models have been designed for global-scale predictions, with\nonly limited work targeting regional or limited area forecasting, which allows\nmore specialized and flexible modeling for specific locations. This work\nintroduces Diffusion-LAM, a probabilistic limited area weather model leveraging\nconditional diffusion. By conditioning on boundary data from surrounding\nregions, our approach generates forecasts within a defined area. Experimental\nresults on the MEPS limited area dataset demonstrate the potential of\nDiffusion-LAM to deliver accurate probabilistic forecasts, highlighting its\npromise for limited-area weather prediction.\n","authors":["Erik Larsson","Joel Oskarsson","Tomas Landelius","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2502.07532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09335v1","updated":"2025-02-13T13:54:58Z","published":"2025-02-13T13:54:58Z","title":"Graph Diffusion Network for Drug-Gene Prediction","summary":"  Predicting drug-gene associations is crucial for drug development and disease\ntreatment. While graph neural networks (GNN) have shown effectiveness in this\ntask, they face challenges with data sparsity and efficient contrastive\nlearning implementation. We introduce a graph diffusion network for drug-gene\nprediction (GDNDGP), a framework that addresses these limitations through two\nkey innovations. First, it employs meta-path-based homogeneous graph learning\nto capture drug-drug and gene-gene relationships, ensuring similar entities\nshare embedding spaces. Second, it incorporates a parallel diffusion network\nthat generates hard negative samples during training, eliminating the need for\nexhaustive negative sample retrieval. Our model achieves superior performance\non the DGIdb 4.0 dataset and demonstrates strong generalization capability on\ntripartite drug-gene-disease networks. Results show significant improvements\nover existing methods in drug-gene prediction tasks, particularly in handling\ncomplex heterogeneous relationships. The source code is publicly available at\nhttps://github.com/csjywu1/GDNDGP.\n","authors":["Jiayang Wu","Wensheng Gan","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.09335v1.pdf","comment":"IEEE/ACM TCBB. 14 pages"},{"id":"http://arxiv.org/abs/2412.05000v2","updated":"2025-02-13T13:53:58Z","published":"2024-12-06T12:52:24Z","title":"Noise Matters: Diffusion Model-based Urban Mobility Generation with\n  Collaborative Noise Priors","summary":"  With global urbanization, the focus on sustainable cities has largely grown,\ndriving research into equity, resilience, and urban planning, which often\nrelies on mobility data. The rise of web-based apps and mobile devices has\nprovided valuable user data for mobility-related research. However, real-world\nmobility data is costly and raises privacy concerns. To protect privacy while\nretaining key features of real-world movement, the demand for synthetic data\nhas steadily increased. Recent advances in diffusion models have shown great\npotential for mobility trajectory generation due to their ability to model\nrandomness and uncertainty. However, existing approaches often directly apply\nidentically distributed (i.i.d.) noise sampling from image generation\ntechniques, which fail to account for the spatiotemporal correlations and\nsocial interactions that shape urban mobility patterns. In this paper, we\npropose CoDiffMob, a diffusion model for urban mobility generation with\ncollaborative noise priors, we emphasize the critical role of noise in\ndiffusion models for generating mobility data. By leveraging both individual\nmovement characteristics and population-wide dynamics, we construct novel\ncollaborative noise priors that provide richer and more informative guidance\nthroughout the generation process. Extensive experiments demonstrate the\nsuperiority of our method, with generated data accurately capturing both\nindividual preferences and collective patterns, achieving an improvement of\nover 32%. Furthermore, it can effectively replace web-derived mobility data to\nbetter support downstream applications, while safeguarding user privacy and\nfostering a more secure and ethical web. This highlights its tremendous\npotential for applications in sustainable city-related research. The code and\ndata are available at https://github.com/tsinghua-fib-lab/CoDiffMob.\n","authors":["Yuheng Zhang","Yuan Yuan","Jingtao Ding","Jian Yuan","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2412.05000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09332v1","updated":"2025-02-13T13:49:52Z","published":"2025-02-13T13:49:52Z","title":"Full Swap Regret and Discretized Calibration","summary":"  We study the problem of minimizing swap regret in structured normal-form\ngames. Players have a very large (potentially infinite) number of pure actions,\nbut each action has an embedding into $d$-dimensional space and payoffs are\ngiven by bilinear functions of these embeddings. We provide an efficient\nlearning algorithm for this setting that incurs at most\n$\\tilde{O}(T^{(d+1)/(d+3)})$ swap regret after $T$ rounds.\n  To achieve this, we introduce a new online learning problem we call\n\\emph{full swap regret minimization}. In this problem, a learner repeatedly\ntakes a (randomized) action in a bounded convex $d$-dimensional action set\n$\\mathcal{K}$ and then receives a loss from the adversary, with the goal of\nminimizing their regret with respect to the \\emph{worst-case} swap function\nmapping $\\mathcal{K}$ to $\\mathcal{K}$. For varied assumptions about the\nconvexity and smoothness of the loss functions, we design algorithms with full\nswap regret bounds ranging from $O(T^{d/(d+2)})$ to $O(T^{(d+1)/(d+2)})$.\n  Finally, we apply these tools to the problem of online forecasting to\nminimize calibration error, showing that several notions of calibration can be\nviewed as specific instances of full swap regret. In particular, we design\nefficient algorithms for online forecasting that guarantee at most $O(T^{1/3})$\n$\\ell_2$-calibration error and $O(\\max(\\sqrt{\\epsilon T}, T^{1/3}))$\n\\emph{discretized-calibration} error (when the forecaster is restricted to\npredicting multiples of $\\epsilon$).\n","authors":["Maxwell Fishelson","Robert Kleinberg","Princewill Okoroafor","Renato Paes Leme","Jon Schneider","Yifeng Teng"],"pdf_url":"https://arxiv.org/pdf/2502.09332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09329v1","updated":"2025-02-13T13:43:52Z","published":"2025-02-13T13:43:52Z","title":"Bayesian Optimization for Simultaneous Selection of Machine Learning\n  Algorithms and Hyperparameters on Shared Latent Space","summary":"  Selecting the optimal combination of a machine learning (ML) algorithm and\nits hyper-parameters is crucial for the development of high-performance ML\nsystems. However, since the combination of ML algorithms and hyper-parameters\nis enormous, the exhaustive validation requires a significant amount of time.\nMany existing studies use Bayesian optimization (BO) for accelerating the\nsearch. On the other hand, a significant difficulty is that, in general, there\nexists a different hyper-parameter space for each one of candidate ML\nalgorithms. BO-based approaches typically build a surrogate model independently\nfor each hyper-parameter space, by which sufficient observations are required\nfor all candidate ML algorithms. In this study, our proposed method embeds\ndifferent hyper-parameter spaces into a shared latent space, in which a\nsurrogate multi-task model for BO is estimated. This approach can share\ninformation of observations from different ML algorithms by which efficient\noptimization is expected with a smaller number of total observations. We\nfurther propose the pre-training of the latent space embedding with an\nadversarial regularization, and a ranking model for selecting an effective\npre-trained embedding for a given target dataset. Our empirical study\ndemonstrates effectiveness of the proposed method through datasets from OpenML.\n","authors":["Kazuki Ishikawa","Ryota Ozaki","Yohei Kanzaki","Ichiro Takeuchi","Masayuki Karasuyama"],"pdf_url":"https://arxiv.org/pdf/2502.09329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16839v3","updated":"2025-02-13T13:39:26Z","published":"2025-01-28T10:28:17Z","title":"Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans","summary":"  Among generative neural models, flow matching techniques stand out for their\nsimple applicability and good scaling properties. Here, velocity fields of\ncurves connecting a simple latent and a target distribution are learned. Then\nthe corresponding ordinary differential equation can be used to sample from a\ntarget distribution, starting in samples from the latent one. This paper\nreviews from a mathematical point of view different techniques to learn the\nvelocity fields of absolutely continuous curves in the Wasserstein geometry. We\nshow how the velocity fields can be characterized and learned via i) transport\nplans (couplings) between latent and target distributions, ii) Markov kernels\nand iii) stochastic processes, where the latter two include the coupling\napproach, but are in general broader. Besides this main goal, we show how flow\nmatching can be used for solving Bayesian inverse problems, where the\ndefinition of conditional Wasserstein distances plays a central role. Finally,\nwe briefly address continuous normalizing flows and score matching techniques,\nwhich approach the learning of velocity fields of curves from other directions.\n","authors":["Christian Wald","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2501.16839v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09324v1","updated":"2025-02-13T13:37:52Z","published":"2025-02-13T13:37:52Z","title":"Depth-Bounds for Neural Networks via the Braid Arrangement","summary":"  We contribute towards resolving the open question of how many hidden layers\nare required in ReLU networks for exactly representing all continuous and\npiecewise linear functions on $\\mathbb{R}^d$. While the question has been\nresolved in special cases, the best known lower bound in general is still 2. We\nfocus on neural networks that are compatible with certain polyhedral complexes,\nmore precisely with the braid fan. For such neural networks, we prove a\nnon-constant lower bound of $\\Omega(\\log\\log d)$ hidden layers required to\nexactly represent the maximum of $d$ numbers. Additionally, under our\nassumption, we provide a combinatorial proof that 3 hidden layers are necessary\nto compute the maximum of 5 numbers; this had only been verified with an\nexcessive computation so far. Finally, we show that a natural generalization of\nthe best known upper bound to maxout networks is not tight, by demonstrating\nthat a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to\nrepresent the maximum of 7 numbers.\n","authors":["Moritz Grillo","Christoph Hertrich","Georg Loho"],"pdf_url":"https://arxiv.org/pdf/2502.09324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09319v1","updated":"2025-02-13T13:33:45Z","published":"2025-02-13T13:33:45Z","title":"Bridging Jensen Gap for Max-Min Group Fairness Optimization in\n  Recommendation","summary":"  Group max-min fairness (MMF) is commonly used in fairness-aware recommender\nsystems (RS) as an optimization objective, as it aims to protect marginalized\nitem groups and ensures a fair competition platform. However, our theoretical\nanalysis indicates that integrating MMF constraint violates the assumption of\nsample independence during optimization, causing the loss function to deviate\nfrom linear additivity. Such nonlinearity property introduces the Jensen gap\nbetween the model's convergence point and the optimal point if mini-batch\nsampling is applied. Both theoretical and empirical studies show that as the\nmini-batch size decreases and the group size increases, the Jensen gap will\nwiden accordingly. Some methods using heuristic re-weighting or debiasing\nstrategies have the potential to bridge the Jensen gap. However, they either\nlack theoretical guarantees or suffer from heavy computational costs. To\novercome these limitations, we first theoretically demonstrate that the\nMMF-constrained objective can be essentially reformulated as a group-weighted\noptimization objective. Then we present an efficient and effective algorithm\nnamed FairDual, which utilizes a dual optimization technique to minimize the\nJensen gap. Our theoretical analysis demonstrates that FairDual can achieve a\nsub-linear convergence rate to the globally optimal solution and the Jensen gap\ncan be well bounded under a mini-batch sampling strategy with random shuffle.\nExtensive experiments conducted using six large-scale RS backbone models on\nthree publicly available datasets demonstrate that FairDual outperforms all\nbaselines in terms of both accuracy and fairness. Our data and codes are shared\nat https://github.com/XuChen0427/FairDual.\n","authors":["Chen Xu","Yuxin Li","Wenjie Wang","Liang Pang","Jun Xu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.09319v1.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09318v1","updated":"2025-02-13T13:33:35Z","published":"2025-02-13T13:33:35Z","title":"SigGate: Enhancing Recurrent Neural Networks with Signature-Based Gating\n  Mechanisms","summary":"  In this paper, we propose a novel approach that enhances recurrent neural\nnetworks (RNNs) by incorporating path signatures into their gating mechanisms.\nOur method modifies both Long Short-Term Memory (LSTM) and Gated Recurrent Unit\n(GRU) architectures by replacing their forget and reset gates, respectively,\nwith learnable path signatures. These signatures, which capture the geometric\nfeatures of the entire path history, provide a richer context for controlling\ninformation flow through the network's memory. This modification allows the\nnetworks to make memory decisions based on the full historical context rather\nthan just the current input and state. Through experimental studies, we\ndemonstrate that our Signature-LSTM (SigLSTM) and Signature-GRU (SigGRU) models\noutperform their traditional counterparts across various sequential learning\ntasks. By leveraging path signatures in recurrent architectures, this method\noffers new opportunities to enhance performance in time series analysis and\nforecasting applications.\n","authors":["R√©mi Genet","Hugo Inzirillo"],"pdf_url":"https://arxiv.org/pdf/2502.09318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04750v2","updated":"2025-02-13T13:33:19Z","published":"2025-02-07T08:33:28Z","title":"Tighter sparse variational Gaussian processes","summary":"  Sparse variational Gaussian process (GP) approximations based on inducing\npoints have become the de facto standard for scaling GPs to large datasets,\nowing to their theoretical elegance, computational efficiency, and ease of\nimplementation. This paper introduces a provably tighter variational\napproximation by relaxing the standard assumption that the conditional\napproximate posterior given the inducing points must match that in the prior.\nThe key innovation is to modify the conditional posterior to have smaller\nvariances than that of the prior at the training points. We derive the\ncollapsed bound for the regression case, describe how to use the proposed\napproximation in large data settings, and discuss its application to handle\northogonally structured inducing points and GP latent variable models.\nExtensive experiments on regression benchmarks, classification, and latent\nvariable models demonstrate that the proposed approximation consistently\nmatches or outperforms standard sparse variational GPs while maintaining the\nsame computational cost. An implementation will be made available in all\npopular GP packages.\n","authors":["Thang D. Bui","Matthew Ashman","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2502.04750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03345v3","updated":"2025-02-13T13:25:18Z","published":"2024-06-05T15:04:27Z","title":"Feature contamination: Neural networks learn uncorrelated features and\n  fail to generalize","summary":"  Learning representations that generalize under distribution shifts is\ncritical for building robust machine learning models. However, despite\nsignificant efforts in recent years, algorithmic advances in this direction\nhave been limited. In this work, we seek to understand the fundamental\ndifficulty of out-of-distribution generalization with deep neural networks. We\nfirst empirically show that perhaps surprisingly, even allowing a neural\nnetwork to explicitly fit the representations obtained from a teacher network\nthat can generalize out-of-distribution is insufficient for the generalization\nof the student network. Then, by a theoretical study of two-layer ReLU networks\noptimized by stochastic gradient descent (SGD) under a structured feature\nmodel, we identify a fundamental yet unexplored feature learning proclivity of\nneural networks, feature contamination: neural networks can learn uncorrelated\nfeatures together with predictive features, resulting in generalization failure\nunder distribution shifts. Notably, this mechanism essentially differs from the\nprevailing narrative in the literature that attributes the generalization\nfailure to spurious correlations. Overall, our results offer new insights into\nthe non-linear feature learning dynamics of neural networks and highlight the\nnecessity of considering inductive biases in out-of-distribution\ngeneralization.\n","authors":["Tianren Zhang","Chujie Zhao","Guanyu Chen","Yizhou Jiang","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.03345v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2502.09306v1","updated":"2025-02-13T13:18:30Z","published":"2025-02-13T13:18:30Z","title":"Non-asymptotic Analysis of Diffusion Annealed Langevin Monte Carlo for\n  Generative Modelling","summary":"  We investigate the theoretical properties of general diffusion\n(interpolation) paths and their Langevin Monte Carlo implementation, referred\nto as diffusion annealed Langevin Monte Carlo (DALMC), under weak conditions on\nthe data distribution. Specifically, we analyse and provide non-asymptotic\nerror bounds for the annealed Langevin dynamics where the path of distributions\nis defined as Gaussian convolutions of the data distribution as in diffusion\nmodels. We then extend our results to recently proposed heavy-tailed (Student's\nt) diffusion paths, demonstrating their theoretical properties for heavy-tailed\ndata distributions for the first time. Our analysis provides theoretical\nguarantees for a class of score-based generative models that interpolate\nbetween a simple distribution (Gaussian or Student's t) and the data\ndistribution in finite time. This approach offers a broader perspective\ncompared to standard score-based diffusion approaches, which are typically\nbased on a forward Ornstein-Uhlenbeck (OU) noising process.\n","authors":["Paula Cordero-Encinar","O. Deniz Akyildiz","Andrew B. Duncan"],"pdf_url":"https://arxiv.org/pdf/2502.09306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09303v1","updated":"2025-02-13T13:16:10Z","published":"2025-02-13T13:16:10Z","title":"Towards Seamless Hierarchical Federated Learning under Intermittent\n  Client Participation: A Stagewise Decision-Making Methodology","summary":"  Federated Learning (FL) offers a pioneering distributed learning paradigm\nthat enables devices/clients to build a shared global model. This global model\nis obtained through frequent model transmissions between clients and a central\nserver, which may cause high latency, energy consumption, and congestion over\nbackhaul links. To overcome these drawbacks, Hierarchical Federated Learning\n(HFL) has emerged, which organizes clients into multiple clusters and utilizes\nedge nodes (e.g., edge servers) for intermediate model aggregations between\nclients and the central server. Current research on HFL mainly focus on\nenhancing model accuracy, latency, and energy consumption in scenarios with a\nstable/fixed set of clients. However, addressing the dynamic availability of\nclients -- a critical aspect of real-world scenarios -- remains underexplored.\nThis study delves into optimizing client selection and client-to-edge\nassociations in HFL under intermittent client participation so as to minimize\noverall system costs (i.e., delay and energy), while achieving fast model\nconvergence. We unveil that achieving this goal involves solving a complex\nNP-hard problem. To tackle this, we propose a stagewise methodology that splits\nthe solution into two stages, referred to as Plan A and Plan B. Plan A focuses\non identifying long-term clients with high chance of participation in\nsubsequent model training rounds. Plan B serves as a backup, selecting\nalternative clients when long-term clients are unavailable during model\ntraining rounds. This stagewise methodology offers a fresh perspective on\nclient selection that can enhance both HFL and conventional FL via enabling\nlow-overhead decision-making processes. Through evaluations on MNIST and\nCIFAR-10 datasets, we show that our methodology outperforms existing benchmarks\nin terms of model accuracy and system costs.\n","authors":["Minghong Wu","Minghui Liwang","Yuhan Su","Li Li","Seyyedali Hosseinalipour","Xianbin Wang","Huaiyu Dai","Zhenzhen Jiao"],"pdf_url":"https://arxiv.org/pdf/2502.09303v1.pdf","comment":"20 pages, 8 figures,5 tables"},{"id":"http://arxiv.org/abs/2108.12113v3","updated":"2025-02-13T13:12:41Z","published":"2021-08-27T04:18:45Z","title":"A method of supervised learning from conflicting data with hidden\n  contexts","summary":"  Conventional supervised learning assumes a stable input-output relationship.\nHowever, this assumption fails in open-ended training settings where the\ninput-output relationship depends on hidden contexts. In this work, we\nformulate a more general supervised learning problem in which training data is\ndrawn from multiple unobservable domains, each potentially exhibiting distinct\ninput-output maps. This inherent conflict in data renders standard empirical\nrisk minimization training ineffective. To address this challenge, we propose a\nmethod LEAF that introduces an allocation function, which learns to assign\nconflicting data to different predictive models. We establish a connection\nbetween LEAF and a variant of the Expectation-Maximization algorithm, allowing\nus to derive an analytical expression for the allocation function. Finally, we\nprovide a theoretical analysis of LEAF and empirically validate its\neffectiveness on both synthetic and real-world tasks involving conflicting\ndata.\n","authors":["Tianren Zhang","Yizhou Jiang","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2108.12113v3.pdf","comment":"35 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.09298v1","updated":"2025-02-13T13:12:16Z","published":"2025-02-13T13:12:16Z","title":"Convex Is Back: Solving Belief MDPs With Convexity-Informed Deep\n  Reinforcement Learning","summary":"  We present a novel method for Deep Reinforcement Learning (DRL),\nincorporating the convex property of the value function over the belief space\nin Partially Observable Markov Decision Processes (POMDPs). We introduce hard-\nand soft-enforced convexity as two different approaches, and compare their\nperformance against standard DRL on two well-known POMDP environments, namely\nthe Tiger and FieldVisionRockSample problems. Our findings show that including\nthe convexity feature can substantially increase performance of the agents, as\nwell as increase robustness over the hyperparameter space, especially when\ntesting on out-of-distribution domains. The source code for this work can be\nfound at https://github.com/Dakout/Convex_DRL.\n","authors":["Daniel Koutas","Daniel Hettegger","Kostas G. Papakonstantinou","Daniel Straub"],"pdf_url":"https://arxiv.org/pdf/2502.09298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09297v1","updated":"2025-02-13T13:11:54Z","published":"2025-02-13T13:11:54Z","title":"When do neural networks learn world models?","summary":"  Humans develop world models that capture the underlying generation process of\ndata. Whether neural networks can learn similar world models remains an open\nproblem. In this work, we provide the first theoretical results for this\nproblem, showing that in a multi-task setting, models with a low-degree bias\nprovably recover latent data-generating variables under mild assumptions --\neven if proxy tasks involve complex, non-linear functions of the latents.\nHowever, such recovery is also sensitive to model architecture. Our analysis\nleverages Boolean models of task solutions via the Fourier-Walsh transform and\nintroduces new techniques for analyzing invertible Boolean transforms, which\nmay be of independent interest. We illustrate the algorithmic implications of\nour results and connect them to related research areas, including\nself-supervised learning, out-of-distribution generalization, and the linear\nrepresentation hypothesis in large language models.\n","authors":["Tianren Zhang","Guanyu Chen","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.09297v1.pdf","comment":"28 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.16106v2","updated":"2025-02-13T13:11:46Z","published":"2024-10-21T15:34:44Z","title":"Statistical Inference for Temporal Difference Learning with Linear\n  Function Approximation","summary":"  Statistical inference with finite-sample validity for the value function of a\ngiven policy in Markov decision processes (MDPs) is crucial for ensuring the\nreliability of reinforcement learning. Temporal Difference (TD) learning,\narguably the most widely used algorithm for policy evaluation, serves as a\nnatural framework for this purpose. In this paper, we study the consistency\nproperties of TD learning with Polyak-Ruppert averaging and linear function\napproximation, and obtain three significant improvements over existing results.\nFirst, we derive a novel sharp high-dimensional probability convergence\nguarantee that depends explicitly on the asymptotic variance and holds under\nweak conditions. We further establish refined high-dimensional Berry-Esseen\nbounds over the class of convex sets that guarantee faster rates than those in\nthe literature. Finally, we propose a plug-in estimator for the asymptotic\ncovariance matrix, designed for efficient online computation. These results\nenable the construction of confidence regions and simultaneous confidence\nintervals for the linear parameters of the value function, with guaranteed\nfinite-sample coverage. We demonstrate the applicability of our theoretical\nfindings through numerical experiments.\n","authors":["Weichen Wu","Gen Li","Yuting Wei","Alessandro Rinaldo"],"pdf_url":"https://arxiv.org/pdf/2410.16106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09291v1","updated":"2025-02-13T13:08:11Z","published":"2025-02-13T13:08:11Z","title":"Joint Attention Mechanism Learning to Facilitate Opto-physiological\n  Monitoring during Physical Activity","summary":"  Opto-physiological monitoring is a non-contact technique for measuring\ncardiac signals, i.e., photoplethysmography (PPG). Quality PPG signals directly\nlead to reliable physiological readings. However, PPG signal acquisition\nprocedures are often accompanied by spurious motion artefacts (MAs), especially\nduring low-to-high-intensity physical activity. This study proposes a practical\nadversarial learning approach for opto-physiological monitoring by using a\ngenerative adversarial network with an attention mechanism (AM-GAN) to model\nmotion noise and to allow MA removal. The AM-GAN learns an MA-resistant mapping\nfrom raw and noisy signals to clear PPG signals in an adversarial manner,\nguided by an attention mechanism to directly translate the motion reference of\ntriaxial acceleration to the MAs appearing in the raw signal. The AM-GAN was\nexperimented with three various protocols engaged with 39 subjects in various\nphysical activities. The average absolute error for heart rate (HR) derived\nfrom the MA-free PPG signal via the AM-GAN, is 1.81 beats/min for the IEEE-SPC\ndataset and 3.86 beats/min for the PPGDalia dataset. The same procedure applied\nto an in-house LU dataset resulted in average absolute errors for HR and\nrespiratory rate (RR) of less than 1.37 beats/min and 2.49 breaths/min,\nrespectively. The study demonstrates the robustness and resilience of AM-GAN,\nparticularly during low-to-high-intensity physical activities.\n","authors":["Xiaoyu Zheng","Sijung Hu","Vincent Dwyer","Mahsa Derakhshani","Laura Barrett"],"pdf_url":"https://arxiv.org/pdf/2502.09291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09290v1","updated":"2025-02-13T13:06:56Z","published":"2025-02-13T13:06:56Z","title":"Dynamic Rolling Horizon Optimization for Network-Constrained V2X Value\n  Stacking of Electric Vehicles Under Uncertainties","summary":"  Electric vehicle (EV) coordination can provide significant benefits through\nvehicle-to-everything (V2X) by interacting with the grid, buildings, and other\nEVs. This work aims to develop a V2X value-stacking framework, including\nvehicle-to-building (V2B), vehicle-to-grid (V2G), and energy trading, to\nmaximize economic benefits for residential communities while maintaining\ndistribution voltage. This work also seeks to quantify the impact of prediction\nerrors related to building load, renewable energy, and EV arrivals. A dynamic\nrolling-horizon optimization (RHO) method is employed to leverage multiple\nrevenue streams and maximize the potential of EV coordination. To address\nenergy uncertainties, including hourly local building load, local photovoltaic\n(PV) generation, and EV arrivals, this work develops a Transformer-based\nforecasting model named Gated Recurrent Units-Encoder-Temporal Fusion Decoder\n(GRU-EN-TFD). The simulation results, using real data from Australia's National\nElectricity Market, and the Independent System Operators in New England and New\nYork in the US, reveal that V2X value stacking can significantly reduce energy\ncosts. The proposed GRU-EN-TFD model outperforms the benchmark forecast model.\nUncertainties in EV arrivals have a more substantial impact on value-stacking\nperformance, highlighting the significance of its accurate forecast. This work\nprovides new insights into the dynamic interactions among residential\ncommunities, unlocking the full potential of EV batteries.\n","authors":["Canchen Jiang","Ariel Liebman","Bo Jie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09290v1.pdf","comment":"21 pages, accepted by Renewable Energy"},{"id":"http://arxiv.org/abs/2502.09287v1","updated":"2025-02-13T13:01:46Z","published":"2025-02-13T13:01:46Z","title":"An Uncertainty Principle for Linear Recurrent Neural Networks","summary":"  We consider linear recurrent neural networks, which have become a key\nbuilding block of sequence modeling due to their ability for stable and\neffective long-range modeling. In this paper, we aim at characterizing this\nability on a simple but core copy task, whose goal is to build a linear filter\nof order $S$ that approximates the filter that looks $K$ time steps in the past\n(which we refer to as the shift-$K$ filter), where $K$ is larger than $S$.\nUsing classical signal models and quadratic cost, we fully characterize the\nproblem by providing lower bounds of approximation, as well as explicit filters\nthat achieve this lower bound up to constants. The optimal performance\nhighlights an uncertainty principle: the optimal filter has to average values\naround the $K$-th time step in the past with a range~(width) that is\nproportional to $K/S$.\n","authors":["Alexandre Fran√ßois","Antonio Orvieto","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2502.09287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07115v2","updated":"2025-02-13T12:54:36Z","published":"2025-02-10T23:11:44Z","title":"Online Scheduling for LLM Inference with KV Cache Constraints","summary":"  Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.\n","authors":["Patrick Jaillet","Jiashuo Jiang","Chara Podimata","Zijie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07115v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09282v1","updated":"2025-02-13T12:54:13Z","published":"2025-02-13T12:54:13Z","title":"FE-LWS: Refined Image-Text Representations via Decoder Stacking and\n  Fused Encodings for Remote Sensing Image Captioning","summary":"  Remote sensing image captioning aims to generate descriptive text from remote\nsensing images, typically employing an encoder-decoder framework. In this\nsetup, a convolutional neural network (CNN) extracts feature representations\nfrom the input image, which then guide the decoder in a sequence-to-sequence\ncaption generation process. Although much research has focused on refining the\ndecoder, the quality of image representations from the encoder remains crucial\nfor accurate captioning. This paper introduces a novel approach that integrates\nfeatures from two distinct CNN based encoders, capturing complementary\ninformation to enhance caption generation. Additionally, we propose a weighted\naveraging technique to combine the outputs of all GRUs in the stacked decoder.\nFurthermore, a comparison-based beam search strategy is incorporated to refine\ncaption selection. The results demonstrate that our fusion-based approach,\nalong with the enhanced stacked decoder, significantly outperforms both the\ntransformer-based state-of-the-art model and other LSTM-based baselines.\n","authors":["Swadhin Das","Raksha Sharma"],"pdf_url":"https://arxiv.org/pdf/2502.09282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09795v4","updated":"2025-02-13T12:35:53Z","published":"2024-10-13T10:48:22Z","title":"WGFormer: An SE(3)-Transformer Driven by Wasserstein Gradient Flows for\n  Molecular Ground-State Conformation Prediction","summary":"  Predicting molecular ground-state conformation (i.e., energy-minimized\nconformation) is crucial for many chemical applications such as molecular\ndocking and property prediction. Classic energy-based simulation is\ntime-consuming when solving this problem while existing learning-based methods\nhave advantages in computational efficiency but sacrifice accuracy and\ninterpretability. In this work, we propose a novel and effective method to\nbridge the energy-based simulation and the learning-based strategy, which\ndesigns and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called\nWGFormer, for molecular ground-state conformation prediction. Specifically, our\nmethod tackles this task within an auto-encoding framework, which encodes\nlow-quality conformations by the proposed WGFormer and decodes corresponding\nground-state conformations by an MLP. The architecture of WGFormer corresponds\nto Wasserstein gradient flows -- it optimizes molecular conformations by\nminimizing an energy function defined on the latent mixture models of atoms,\nthereby significantly improving performance and interpretability. Extensive\nexperiments show that our method consistently outperforms state-of-the-art\ncompetitors, providing a new and insightful paradigm to predict molecular\nground-state conformation.\n","authors":["Fanmeng Wang","Minjie Cheng","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.09795v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09271v1","updated":"2025-02-13T12:33:39Z","published":"2025-02-13T12:33:39Z","title":"LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via\n  Subgraph Injection","summary":"  Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in\nmodeling data with graph structures, yet recent research reveals their\nsusceptibility to adversarial attacks. Traditional attack methodologies, which\nrely on manipulating the original graph or adding links to artificially created\nnodes, often prove impractical in real-world settings. This paper introduces a\nnovel adversarial scenario involving the injection of an isolated subgraph to\ndeceive both the link recommender and the node classifier within a GNN system.\nSpecifically, the link recommender is mislead to propose links between targeted\nvictim nodes and the subgraph, encouraging users to unintentionally establish\nconnections and that would degrade the node classification accuracy, thereby\nfacilitating a successful attack. To address this, we present the LiSA\nframework, which employs a dual surrogate model and bi-level optimization to\nsimultaneously meet two adversarial objectives. Extensive experiments on\nreal-world datasets demonstrate the effectiveness of our method.\n","authors":["Wenlun Zhang","Enyan Dai","Kentaro Yoshioka"],"pdf_url":"https://arxiv.org/pdf/2502.09271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17460v3","updated":"2025-02-13T12:32:45Z","published":"2024-01-30T21:46:09Z","title":"Rendering Wireless Environments Useful for Gradient Estimators: A\n  Zero-Order Stochastic Federated Learning Method","summary":"  Cross-device federated learning (FL) is a growing machine learning setting\nwhereby multiple edge devices collaborate to train a model without disclosing\ntheir raw data. With the great number of mobile devices participating in more\nFL applications via the wireless environment, the practical implementation of\nthese applications will be hindered due to the limited uplink capacity of\ndevices, causing critical bottlenecks. In this work, we propose a novel doubly\ncommunication-efficient zero-order (ZO) method with a one-point gradient\nestimator that replaces communicating long vectors with scalar values and that\nharnesses the nature of the wireless communication channel, overcoming the need\nto know the channel state coefficient. It is the first method that includes the\nwireless channel in the learning algorithm itself instead of wasting resources\nto analyze it and remove its impact. We then offer a thorough analysis of the\nproposed zero-order federated learning (ZOFL) framework and prove that our\nmethod converges \\textit{almost surely}, which is a novel result in nonconvex\nZO optimization. We further prove a convergence rate of\n$O(\\frac{1}{\\sqrt[3]{K}})$ in the nonconvex setting. We finally demonstrate the\npotential of our algorithm with experimental results.\n","authors":["Elissa Mhanna","Mohamad Assaad"],"pdf_url":"https://arxiv.org/pdf/2401.17460v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09268v1","updated":"2025-02-13T12:29:50Z","published":"2025-02-13T12:29:50Z","title":"GEVRM: Goal-Expressive Video Generation Model For Robust Visual\n  Manipulation","summary":"  With the rapid development of embodied artificial intelligence, significant\nprogress has been made in vision-language-action (VLA) models for general robot\ndecision-making. However, the majority of existing VLAs fail to account for the\ninevitable external perturbations encountered during deployment. These\nperturbations introduce unforeseen state information to the VLA, resulting in\ninaccurate actions and consequently, a significant decline in generalization\nperformance. The classic internal model control (IMC) principle demonstrates\nthat a closed-loop system with an internal model that includes external input\nsignals can accurately track the reference input and effectively offset the\ndisturbance. We propose a novel closed-loop VLA method GEVRM that integrates\nthe IMC principle to enhance the robustness of robot visual manipulation. The\ntext-guided video generation model in GEVRM can generate highly expressive\nfuture visual planning goals. Simultaneously, we evaluate perturbations by\nsimulating responses, which are called internal embeddings and optimized\nthrough prototype contrastive learning. This allows the model to implicitly\ninfer and distinguish perturbations from the external environment. The proposed\nGEVRM achieves state-of-the-art performance on both standard and perturbed\nCALVIN benchmarks and shows significant improvements in realistic robot tasks.\n","authors":["Hongyin Zhang","Pengxiang Ding","Shangke Lyu","Ying Peng","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.09268v1.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09263v1","updated":"2025-02-13T12:24:23Z","published":"2025-02-13T12:24:23Z","title":"Unlocking the Potential of Classic GNNs for Graph-level Tasks: Simple\n  Architectures Meet Excellence","summary":"  Message-passing Graph Neural Networks (GNNs) are often criticized for their\nlimited expressiveness, issues like over-smoothing and over-squashing, and\nchallenges in capturing long-range dependencies, while Graph Transformers (GTs)\nare considered superior due to their global attention mechanisms. Literature\nfrequently suggests that GTs outperform GNNs, particularly in graph-level tasks\nsuch as graph classification and regression. In this study, we explore the\nuntapped potential of GNNs through an enhanced framework, GNN+, which\nintegrates six widely used techniques: edge feature integration, normalization,\ndropout, residual connections, feed-forward networks, and positional encoding,\nto effectively tackle graph-level tasks. We conduct a systematic evaluation of\nthree classic GNNs, namely GCN, GIN, and GatedGCN, enhanced by the GNN+\nframework across 14 well-known graph-level datasets. Our results show that,\ncontrary to the prevailing belief, classic GNNs excel in graph-level tasks,\nsecuring top three rankings across all datasets and achieving first place in\neight, while also demonstrating greater efficiency than GTs. This highlights\nthe potential of simple GNN architectures, challenging the belief that complex\nmechanisms in GTs are essential for superior graph-level performance.\n","authors":["Yuankai Luo","Lei Shi","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2105.09266v5","updated":"2025-02-13T12:15:30Z","published":"2021-05-19T17:22:47Z","title":"Copyright in Generative Deep Learning","summary":"  Machine-generated artworks are now part of the contemporary art scene: they\nare attracting significant investments and they are presented in exhibitions\ntogether with those created by human artists. These artworks are mainly based\non generative deep learning techniques, which have seen a formidable\ndevelopment and remarkable refinement in the very recent years. Given the\ninherent characteristics of these techniques, a series of novel legal problems\narise. In this article, we consider a set of key questions in the area of\ngenerative deep learning for the arts, including the following: is it possible\nto use copyrighted works as training set for generative models? How do we\nlegally store their copies in order to perform the training process? Who (if\nsomeone) will own the copyright on the generated data? We try to answer these\nquestions considering the law in force in both the United States of America and\nthe European Union, and potential future alternatives. We then extend our\nanalysis to code generation, which is an emerging area of generative deep\nlearning. Finally, we also formulate a set of practical guidelines for artists\nand developers working on deep learning generated art, as well as some policy\nsuggestions for policymakers.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2105.09266v5.pdf","comment":"Published in Data & Policy at\n  https://www.cambridge.org/core/journals/data-and-policy/article/copyright-in-generative-deep-learning/C401539FDF79A6AC6CEE8C5256508B5E"},{"id":"http://arxiv.org/abs/2502.09257v1","updated":"2025-02-13T12:13:25Z","published":"2025-02-13T12:13:25Z","title":"Bandit Multiclass List Classification","summary":"  We study the problem of multiclass list classification with (semi-)bandit\nfeedback, where input examples are mapped into subsets of size $m$ of a\ncollection of $K$ possible labels, and the feedback consists of the predicted\nlabels which lie in the set of true labels of the given example. Our main\nresult is for the $(\\varepsilon,\\delta)$-PAC variant of the problem for which\nwe design an algorithm that returns an $\\varepsilon$-optimal hypothesis with\nhigh probability using a sample complexity of $O \\big( (\\mathrm{poly}(K/m) + sm\n/ \\varepsilon^2) \\log (|H|/\\delta) \\big)$ where $H$ is the underlying (finite)\nhypothesis class and $s$ is an upper bound on the number of true labels for a\ngiven example. This bound improves upon known bounds for combinatorial\nsemi-bandits whenever $s \\ll K$. Moreover, in the regime where $s = O(1)$ the\nleading terms in our bound match the corresponding full-information rates,\nimplying that bandit feedback essentially comes at no cost. Our PAC learning\nalgorithm is also computationally efficient given access to an ERM oracle for\n$H$. Additionally, we consider the regret minimization setting where data can\nbe generated adversarially, and establish a regret bound of $\\widetilde O(|H| +\n\\sqrt{smT \\log |H|})$. Our results generalize and extend those of Erez et al.\n(2024) who consider the simpler single-label setting corresponding to $s=m=1$,\nand in fact hold for the more general contextual combinatorial semi-bandit\nproblem with $s$-sparse rewards.\n","authors":["Liad Erez","Tomer Koren"],"pdf_url":"https://arxiv.org/pdf/2502.09257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.02726v7","updated":"2025-02-13T12:12:28Z","published":"2021-04-06T18:00:06Z","title":"Creativity and Machine Learning: A Survey","summary":"  There is a growing interest in the area of machine learning and creativity.\nThis survey presents an overview of the history and the state of the art of\ncomputational creativity theories, key machine learning techniques (including\ngenerative deep learning), and corresponding automatic evaluation methods.\nAfter presenting a critical discussion of the key contributions in this area,\nwe outline the current research challenges and emerging opportunities in this\nfield.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2104.02726v7.pdf","comment":"Published in ACM Computing Surveys at\n  https://dl.acm.org/doi/10.1145/3664595"},{"id":"http://arxiv.org/abs/2502.07005v3","updated":"2025-02-13T12:11:58Z","published":"2025-02-10T20:10:25Z","title":"Geometry-aware RL for Manipulation of Varying Shapes and Deformable\n  Objects","summary":"  Manipulating objects with varying geometries and deformable objects is a\nmajor challenge in robotics. Tasks such as insertion with different objects or\ncloth hanging require precise control and effective modelling of complex\ndynamics. In this work, we frame this problem through the lens of a\nheterogeneous graph that comprises smaller sub-graphs, such as actuators and\nobjects, accompanied by different edge types describing their interactions.\nThis graph representation serves as a unified structure for both rigid and\ndeformable objects tasks, and can be extended further to tasks comprising\nmultiple actuators. To evaluate this setup, we present a novel and challenging\nreinforcement learning benchmark, including rigid insertion of diverse objects,\nas well as rope and cloth manipulation with multiple end-effectors. These tasks\npresent a large search space, as both the initial and target configurations are\nuniformly sampled in 3D space. To address this issue, we propose a novel\ngraph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi),\nutilizing $SE(3)$ equivariant message passing networks as the main backbone to\nexploit the geometric symmetry. In addition, by modeling explicit\nheterogeneity, HEPi can outperform Transformer-based and non-heterogeneous\nequivariant policies in terms of average returns, sample efficiency, and\ngeneralization to unseen objects.\n","authors":["Tai Hoang","Huy Le","Philipp Becker","Vien Anh Ngo","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2502.07005v3.pdf","comment":"Accepted at ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.09254v1","updated":"2025-02-13T12:10:05Z","published":"2025-02-13T12:10:05Z","title":"AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection","summary":"  Graph anomaly detection (GAD) aims to identify abnormal nodes that differ\nfrom the majority of the nodes in a graph, which has been attracting\nsignificant attention in recent years. Existing generalist graph models have\nachieved remarkable success in different graph tasks but struggle to generalize\nto the GAD task. This limitation arises from their difficulty in learning\ngeneralized knowledge for capturing the inherently infrequent, irregular and\nheterogeneous abnormality patterns in graphs from different domains. To address\nthis challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model\nthat supports zero-shot inference and few-shot prompt tuning for GAD in diverse\ngraph datasets. One key insight is that graph-agnostic representations for\nnormal and abnormal classes are required to support effective zero/few-shot GAD\nacross different graphs. Motivated by this, AnomalyGFM is pre-trained to align\ndata-independent, learnable normal and abnormal class prototypes with node\nrepresentation residuals (i.e., representation deviation of a node from its\nneighbors). The residual features essentially project the node information into\na unified feature space where we can effectively measure the abnormality of\nnodes from different graphs in a consistent way. This provides a driving force\nfor the learning of graph-agnostic, discriminative prototypes for the normal\nand abnormal classes, which can be used to enable zero-shot GAD on new graphs,\nincluding very large-scale graphs. If there are few-shot labeled normal nodes\navailable in the new graphs, AnomalyGFM can further support prompt tuning to\nleverage these nodes for better adaptation. Comprehensive experiments on 11\nwidely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM\nsignificantly outperforms state-of-the-art competing methods under both zero-\nand few-shot GAD settings.\n","authors":["Hezhe Qiao","Chaoxi Niu","Ling Chen","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2502.09254v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.09252v1","updated":"2025-02-13T12:09:17Z","published":"2025-02-13T12:09:17Z","title":"On the Importance of Embedding Norms in Self-Supervised Learning","summary":"  Self-supervised learning (SSL) allows training data representations without a\nsupervised signal and has become an important paradigm in machine learning.\nMost SSL methods employ the cosine similarity between embedding vectors and\nhence effectively embed data on a hypersphere. While this seemingly implies\nthat embedding norms cannot play any role in SSL, a few recent works have\nsuggested that embedding norms have properties related to network convergence\nand confidence. In this paper, we resolve this apparent contradiction and\nsystematically establish the embedding norm's role in SSL training. Using\ntheoretical analysis, simulations, and experiments, we show that embedding\nnorms (i) govern SSL convergence rates and (ii) encode network confidence, with\nsmaller norms corresponding to unexpected samples. Additionally, we show that\nmanipulating embedding norms can have large effects on convergence speed. Our\nfindings demonstrate that SSL embedding norms are integral to understanding and\noptimizing network behavior.\n","authors":["Andrew Draganov","Sharvaree Vadgama","Sebastian Damrich","Jan Niklas B√∂hm","Lucas Maes","Dmitry Kobak","Erik Bekkers"],"pdf_url":"https://arxiv.org/pdf/2502.09252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07477v2","updated":"2025-02-13T12:06:33Z","published":"2024-12-10T12:50:25Z","title":"Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution\n  Simulations for Time-Efficient Fine-Resolution Policy Learning","summary":"  In earthwork and construction, excavators often encounter large rocks mixed\nwith various soil conditions, requiring skilled operators. This paper presents\na framework for achieving autonomous excavation using reinforcement learning\n(RL) through a rock excavation simulator. In the simulation, resolution can be\ndefined by the particle size/number in the whole soil space. Fine-resolution\nsimulations closely mimic real-world behavior but demand significant\ncalculation time and challenging sample collection, while coarse-resolution\nsimulations enable faster sample collection but deviate from real-world\nbehavior. To combine the advantages of both resolutions, we explore using\npolicies developed in coarse-resolution simulations for pre-training in\nfine-resolution simulations. To this end, we propose a novel policy learning\nframework called Progressive-Resolution Policy Distillation (PRPD), which\nprogressively transfers policies through some middle-resolution simulations\nwith conservative policy transfer to avoid domain gaps that could lead to\npolicy transfer failure. Validation in a rock excavation simulator and nine\nreal-world rock environments demonstrated that PRPD reduced sampling time to\nless than 1/7 while maintaining task success rates comparable to those achieved\nthrough policy learning in a fine-resolution simulation.\n","authors":["Yuki Kadokawa","Hirotaka Tahara","Takamitsu Matsubara"],"pdf_url":"https://arxiv.org/pdf/2412.07477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09245v1","updated":"2025-02-13T12:00:50Z","published":"2025-02-13T12:00:50Z","title":"You Do Not Fully Utilize Transformer's Representation Capacity","summary":"  In contrast to RNNs, which compress previous tokens into a single hidden\nstate, Transformers can attend to all previous tokens directly. However,\nstandard Transformers only use representations from the immediately preceding\nlayer. In this paper, we show that this design choice causes representation\ncollapse and leads to suboptimal performance. To address this issue, we\nintroduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that\npreserves the model's overall memory footprint while expanding its\nrepresentational capacity by allowing access to hidden states from earlier\nlayers. Through extensive experiments across various architectures and\ndifferent lookup mechanisms, we demonstrate consistent performance improvements\non a wide range of tasks. Moreover, our analysis of the learned representation\ndynamics and our exploration of depthwise circuits reveal how LIMe integrates\ninformation across layers, pointing to promising directions for future\nresearch.\n","authors":["Gleb Gerasimov","Yaroslav Aksenov","Nikita Balagansky","Viacheslav Sinii","Daniil Gavrilov"],"pdf_url":"https://arxiv.org/pdf/2502.09245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18458v4","updated":"2025-02-13T11:59:20Z","published":"2024-05-28T17:27:20Z","title":"Asymmetrical estimator for training encapsulated deep photonic neural\n  networks","summary":"  Photonic neural networks (PNNs) are fast in-propagation and high bandwidth\nparadigms that aim to popularize reproducible NN acceleration with higher\nefficiency and lower cost. However, the training of PNN is known to be\nchallenging, where the device-to-device and system-to-system variations create\nimperfect knowledge of the PNN. Despite backpropagation (BP)-based training\nalgorithms being the industry standard for their robustness, generality, and\nfast gradient convergence for digital training, existing PNN-BP methods rely\nheavily on accurate intermediate state extraction or extensive computational\nresources for deep PNNs (DPNNs). The truncated photonic signal propagation and\nthe computation overhead bottleneck DPNN's operation efficiency and increase\nsystem construction cost. Here, we introduce the asymmetrical training (AsyT)\nmethod, tailored for encapsulated DPNNs, where the signal is preserved in the\nanalogue photonic domain for the entire structure. AsyT offers a lightweight\nsolution for DPNNs with minimum readouts, fast and energy-efficient operation,\nand minimum system footprint. AsyT's ease of operation, error tolerance, and\ngenerality aim to promote PNN acceleration in a widened operational scenario\ndespite the fabrication variations and imperfect controls. We demonstrated AsyT\nfor encapsulated DPNN with integrated photonic chips, repeatably enhancing the\nperformance from in-silico BP for different network structures and datasets.\n","authors":["Yizhi Wang","Minjia Chen","Chunhui Yao","Jie Ma","Ting Yan","Richard Penty","Qixiang Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.18458v4.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.09219v1","updated":"2025-02-13T11:50:04Z","published":"2025-02-13T11:50:04Z","title":"Abduction of Domain Relationships from Data for VQA","summary":"  In this paper, we study the problem of visual question answering (VQA) where\nthe image and query are represented by ASP programs that lack domain data. We\nprovide an approach that is orthogonal and complementary to existing knowledge\naugmentation techniques where we abduce domain relationships of image\nconstructs from past examples. After framing the abduction problem, we provide\na baseline approach, and an implementation that significantly improves the\naccuracy of query answering yet requires few examples.\n","authors":["Al Mehdi Saadat Chowdhury","Paulo Shakarian","Gerardo I. Simari"],"pdf_url":"https://arxiv.org/pdf/2502.09219v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09213v1","updated":"2025-02-13T11:48:46Z","published":"2025-02-13T11:48:46Z","title":"Neuro-Symbolic Contrastive Learning for Cross-domain Inference","summary":"  Pre-trained language models (PLMs) have made significant advances in natural\nlanguage inference (NLI) tasks, however their sensitivity to textual\nperturbations and dependence on large datasets indicate an over-reliance on\nshallow heuristics. In contrast, inductive logic programming (ILP) excels at\ninferring logical relationships across diverse, sparse and limited datasets,\nbut its discrete nature requires the inputs to be precisely specified, which\nlimits their application. This paper proposes a bridge between the two\napproaches: neuro-symbolic contrastive learning. This allows for smooth and\ndifferentiable optimisation that improves logical accuracy across an otherwise\ndiscrete, noisy, and sparse topological space of logical functions. We show\nthat abstract logical relationships can be effectively embedded within a\nneuro-symbolic paradigm, by representing data as logic programs and sets of\nlogic rules. The embedding space captures highly varied textual information\nwith similar semantic logical relations, but can also separate similar textual\nrelations that have dissimilar logical relations. Experimental results\ndemonstrate that our approach significantly improves the inference capabilities\nof the models in terms of generalisation and reasoning.\n","authors":["Mingyue Liu","Ryo Ueda","Zhen Wan","Katsumi Inoue","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2502.09213v1.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2410.13914v5","updated":"2025-02-13T11:47:51Z","published":"2024-10-17T03:08:28Z","title":"Exogenous Matching: Learning Good Proposals for Tractable Counterfactual\n  Estimation","summary":"  We propose an importance sampling method for tractable and efficient\nestimation of counterfactual expressions in general settings, named Exogenous\nMatching. By minimizing a common upper bound of counterfactual estimators, we\ntransform the variance minimization problem into a conditional distribution\nlearning problem, enabling its integration with existing conditional\ndistribution modeling approaches. We validate the theoretical results through\nexperiments under various types and settings of Structural Causal Models (SCMs)\nand demonstrate the outperformance on counterfactual estimation tasks compared\nto other existing importance sampling methods. We also explore the impact of\ninjecting structural prior knowledge (counterfactual Markov boundaries) on the\nresults. Finally, we apply this method to identifiable proxy SCMs and\ndemonstrate the unbiasedness of the estimates, empirically illustrating the\napplicability of the method to practical scenarios.\n","authors":["Yikang Chen","Dehui Du","Lili Tian"],"pdf_url":"https://arxiv.org/pdf/2410.13914v5.pdf","comment":"51 pages, 15 figures. Accepted at NeurIPS 2024, see\n  https://papers.nips.cc/paper_files/paper/2024/hash/ee94bf235482e4c1f689c04c81656dbf-Abstract-Conference.html"},{"id":"http://arxiv.org/abs/2407.06447v2","updated":"2025-02-13T11:46:41Z","published":"2024-07-08T23:11:47Z","title":"Geospatial Trajectory Generation via Efficient Abduction: Deployment for\n  Independent Testing","summary":"  The ability to generate artificial human movement patterns while meeting\nlocation and time constraints is an important problem in the security\ncommunity, particularly as it enables the study of the analog problem of\ndetecting such patterns while maintaining privacy. We frame this problem as an\ninstance of abduction guided by a novel parsimony function represented as an\naggregate truth value over an annotated logic program. This approach has the\nadded benefit of affording explainability to an analyst user. By showing that\nany subset of such a program can provide a lower bound on this parsimony\nrequirement, we are able to abduce movement trajectories efficiently through an\ninformed (i.e., A*) search. We describe how our implementation was enhanced\nwith the application of multiple techniques in order to be scaled and\nintegrated with a cloud-based software stack that included bottom-up rule\nlearning, geolocated knowledge graph retrieval/management, and interfaces with\ngovernment systems for independently conducted government-run tests for which\nwe provide results. We also report on our own experiments showing that we not\nonly provide exact results but also scale to very large scenarios and provide\nrealistic agent trajectories that can go undetected by machine learning anomaly\ndetectors.\n","authors":["Divyagna Bavikadi","Dyuman Aditya","Devendra Parkar","Paulo Shakarian","Graham Mueller","Chad Parvis","Gerardo I. Simari"],"pdf_url":"https://arxiv.org/pdf/2407.06447v2.pdf","comment":"In Proceedings ICLP 2024, arXiv:2502.08453"},{"id":"http://arxiv.org/abs/2502.09203v1","updated":"2025-02-13T11:43:43Z","published":"2025-02-13T11:43:43Z","title":"Revisiting Euclidean Alignment for Transfer Learning in EEG-Based\n  Brain-Computer Interfaces","summary":"  Due to the non-stationarity and large individual differences of EEG signals,\nEEG-based brain-computer interfaces (BCIs) usually need subject-specific\ncalibration to tailor the decoding algorithm for each new subject, which is\ntime-consuming and user-unfriendly, hindering their real-world applications.\nTransfer learning (TL) has been extensively used to expedite the calibration,\nby making use of EEG data from other subjects/sessions. An important\nconsideration in TL for EEG-based BCIs is to reduce the data distribution\ndiscrepancies among different subjects/session, to avoid negative transfer.\nEuclidean alignment (EA) was proposed in 2020 to address this challenge.\nNumerous experiments from 10 different BCI paradigms demonstrated its\neffectiveness and efficiency. This paper revisits the EA, explaining its\nprocedure and correct usage, introducing its applications and extensions, and\npointing out potential new research directions. It should be very helpful to\nBCI researchers, especially those who are working on EEG signal decoding.\n","authors":["Dongrui Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06913v2","updated":"2025-02-13T11:42:53Z","published":"2025-02-10T09:26:57Z","title":"A Simple yet Effective DDG Predictor is An Unsupervised Antibody\n  Optimizer and Explainer","summary":"  The proteins that exist today have been optimized over billions of years of\nnatural evolution, during which nature creates random mutations and selects\nthem. The discovery of functionally promising mutations is challenged by the\nlimited evolutionary accessible regions, i.e., only a small region on the\nfitness landscape is beneficial. There have been numerous priors used to\nconstrain protein evolution to regions of landscapes with high-fitness\nvariants, among which the change in binding free energy (DDG) of protein\ncomplexes upon mutations is one of the most commonly used priors. However, the\nhuge mutation space poses two challenges: (1) how to improve the efficiency of\nDDG prediction for fast mutation screening; and (2) how to explain mutation\npreferences and efficiently explore accessible evolutionary regions. To address\nthese challenges, we propose a lightweight DDG predictor (Light-DDG), which\nadopts a structure-aware Transformer as the backbone and enhances it by\nknowledge distilled from existing powerful but computationally heavy DDG\npredictors. Additionally, we augmented, annotated, and released a large-scale\ndataset containing millions of mutation data for pre-training Light-DDG. We\nfind that such a simple yet effective Light-DDG can serve as a good\nunsupervised antibody optimizer and explainer. For the target antibody, we\npropose a novel Mutation Explainer to learn mutation preferences, which\naccounts for the marginal benefit of each mutation per residue. To further\nexplore accessible evolutionary regions, we conduct preference-guided antibody\noptimization and evaluate antibody candidates quickly using Light-DDG to\nidentify desirable mutations.\n","authors":["Lirong Wu","Yunfan Liu","Haitao Lin","Yufei Huang","Guojiang Zhao","Zhifeng Gao","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2502.06913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09198v1","updated":"2025-02-13T11:37:55Z","published":"2025-02-13T11:37:55Z","title":"Understanding High-Dimensional Bayesian Optimization","summary":"  Recent work reported that simple Bayesian optimization methods perform well\nfor high-dimensional real-world tasks, seemingly contradicting prior work and\ntribal knowledge. This paper investigates the 'why'. We identify fundamental\nchallenges that arise in high-dimensional Bayesian optimization and explain why\nrecent methods succeed. Our analysis shows that vanishing gradients caused by\nGaussian process initialization schemes play a major role in the failures of\nhigh-dimensional Bayesian optimization and that methods that promote local\nsearch behaviors are better suited for the task. We find that maximum\nlikelihood estimation of Gaussian process length scales suffices for\nstate-of-the-art performance. Based on this, we propose a simple variant of\nmaximum likelihood estimation called MSR that leverages these findings to\nachieve state-of-the-art performance on a comprehensive set of real-world\napplications. We also present targeted experiments to illustrate and confirm\nour findings.\n","authors":["Leonard Papenmeier","Matthias Poloczek","Luigi Nardi"],"pdf_url":"https://arxiv.org/pdf/2502.09198v1.pdf","comment":"19 pages, 20 figures"},{"id":"http://arxiv.org/abs/2502.09193v1","updated":"2025-02-13T11:33:17Z","published":"2025-02-13T11:33:17Z","title":"Generalizability through Explainability: Countering Overfitting with\n  Counterfactual Examples","summary":"  Overfitting is a well-known issue in machine learning that occurs when a\nmodel struggles to generalize its predictions to new, unseen data beyond the\nscope of its training set. Traditional techniques to mitigate overfitting\ninclude early stopping, data augmentation, and regularization. In this work, we\ndemonstrate that the degree of overfitting of a trained model is correlated\nwith the ability to generate counterfactual examples. The higher the\noverfitting, the easier it will be to find a valid counterfactual example for a\nrandomly chosen input data point. Therefore, we introduce CF-Reg, a novel\nregularization term in the training loss that controls overfitting by ensuring\nenough margin between each instance and its corresponding counterfactual.\nExperiments conducted across multiple datasets and models show that our\ncounterfactual regularizer generally outperforms existing regularization\ntechniques.\n","authors":["Flavio Giorgi","Fabiano Veglianti","Fabrizio Silvestri","Gabriele Tolomei"],"pdf_url":"https://arxiv.org/pdf/2502.09193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11619v3","updated":"2025-02-13T11:10:54Z","published":"2024-08-21T13:46:58Z","title":"Data-driven Modeling of Combined Sewer Systems for Urban Sustainability:\n  An Empirical Evaluation","summary":"  Climate change poses complex challenges, with extreme weather events becoming\nincreasingly frequent and difficult to model. Examples include the dynamics of\nCombined Sewer Systems (CSS). Overburdened CSS during heavy rainfall will\noverflow untreated wastewater into surface water bodies. Classical approaches\nto modeling the impact of extreme rainfall events rely on physical simulations,\nwhich are particularly challenging to create for large urban infrastructures.\nDeep Learning (DL) models offer a cost-effective alternative for modeling the\ncomplex dynamics of sewer systems. In this study, we present a comprehensive\nempirical evaluation of several state-of-the-art DL time series models for\npredicting sewer system dynamics in a large urban infrastructure, utilizing\nthree years of measurement data. We especially investigate the potential of DL\nmodels to maintain predictive precision during network outages by comparing\nglobal models, which have access to all variables within the sewer system, and\nlocal models, which are limited to data from a restricted set of local sensors.\nOur findings demonstrate that DL models can accurately predict the dynamics of\nsewer system load, even under network outage conditions. These results suggest\nthat DL models can effectively aid in balancing the load redistribution in CSS,\nthereby enhancing the sustainability and resilience of urban infrastructures.\n","authors":["Vipin Singh","Tianheng Ling","Teodor Chiaburu","Felix Biessmann"],"pdf_url":"https://arxiv.org/pdf/2408.11619v3.pdf","comment":"8 pages, 4 figures, accepted at 2nd Workshop on 'Public Interest AI'\n  co-located with 47th German Conference on Artificial Intelligence, Wuerzburg\n  23rd September 2024"},{"id":"http://arxiv.org/abs/2404.18573v2","updated":"2025-02-13T11:09:19Z","published":"2024-04-29T10:28:28Z","title":"Predicting Safety Misbehaviours in Autonomous Driving Systems using\n  Uncertainty Quantification","summary":"  The automated real-time recognition of unexpected situations plays a crucial\nrole in the safety of autonomous vehicles, especially in unsupported and\nunpredictable scenarios. This paper evaluates different Bayesian uncertainty\nquantification methods from the deep learning domain for the anticipatory\ntesting of safety-critical misbehaviours during system-level simulation-based\ntesting. Specifically, we compute uncertainty scores as the vehicle executes,\nfollowing the intuition that high uncertainty scores are indicative of\nunsupported runtime conditions that can be used to distinguish safe from\nfailure-inducing driving behaviors. In our study, we conducted an evaluation of\nthe effectiveness and computational overhead associated with two Bayesian\nuncertainty quantification methods, namely MC- Dropout and Deep Ensembles, for\nmisbehaviour avoidance. Overall, for three benchmarks from the Udacity\nsimulator comprising both out-of-distribution and unsafe conditions introduced\nvia mutation testing, both methods successfully detected a high number of\nout-of-bounds episodes providing early warnings several seconds in advance,\noutperforming two state-of-the-art misbehaviour prediction methods based on\nautoencoders and attention maps in terms of effectiveness and efficiency.\nNotably, Deep Ensembles detected most misbehaviours without any false alarms\nand did so even when employing a relatively small number of models, making them\ncomputationally feasible for real-time detection. Our findings suggest that\nincorporating uncertainty quantification methods is a viable approach for\nbuilding fail-safe mechanisms in deep neural network-based autonomous vehicles.\n","authors":["Ruben Grewal","Paolo Tonella","Andrea Stocco"],"pdf_url":"https://arxiv.org/pdf/2404.18573v2.pdf","comment":"In proceedings of the 17th IEEE International Conference on Software\n  Testing, Verification and Validation 2024 (ICST '24)"},{"id":"http://arxiv.org/abs/2502.09173v1","updated":"2025-02-13T10:57:25Z","published":"2025-02-13T10:57:25Z","title":"Two-Stage Representation Learning for Analyzing Movement Behavior\n  Dynamics in People Living with Dementia","summary":"  In remote healthcare monitoring, time series representation learning reveals\ncritical patient behavior patterns from high-frequency data. This study\nanalyzes home activity data from individuals living with dementia by proposing\na two-stage, self-supervised learning approach tailored to uncover low-rank\nstructures. The first stage converts time-series activities into text sequences\nencoded by a pre-trained language model, providing a rich, high-dimensional\nlatent state space using a PageRank-based method. This PageRank vector captures\nlatent state transitions, effectively compressing complex behaviour data into a\nsuccinct form that enhances interpretability. This low-rank representation not\nonly enhances model interpretability but also facilitates clustering and\ntransition analysis, revealing key behavioral patterns correlated with\nclinicalmetrics such as MMSE and ADAS-COG scores. Our findings demonstrate the\nframework's potential in supporting cognitive status prediction, personalized\ncare interventions, and large-scale health monitoring.\n","authors":["Jin Cui","Alexander Capstick","Payam Barnaghi","Gregory Scott"],"pdf_url":"https://arxiv.org/pdf/2502.09173v1.pdf","comment":"AAAI 2025 Workshop on Large Language Models and Generative AI for\n  Health"},{"id":"http://arxiv.org/abs/2502.09172v1","updated":"2025-02-13T10:56:58Z","published":"2025-02-13T10:56:58Z","title":"LOB-Bench: Benchmarking Generative AI for Finance - an Application to\n  Limit Order Book Data","summary":"  While financial data presents one of the most challenging and interesting\nsequence modelling tasks due to high noise, heavy tails, and strategic\ninteractions, progress in this area has been hindered by the lack of consensus\non quantitative evaluation paradigms. To address this, we present LOB-Bench, a\nbenchmark, implemented in python, designed to evaluate the quality and realism\nof generative message-by-order data for limit order books (LOB) in the LOBSTER\nformat. Our framework measures distributional differences in conditional and\nunconditional statistics between generated and real LOB data, supporting\nflexible multivariate statistical evaluation. The benchmark also includes\nfeatures commonly used LOB statistics such as spread, order book volumes, order\nimbalance, and message inter-arrival times, along with scores from a trained\ndiscriminator network. Lastly, LOB-Bench contains \"market impact metrics\", i.e.\nthe cross-correlations and price response functions for specific events in the\ndata. We benchmark generative autoregressive state-space models, a (C)GAN, as\nwell as a parametric LOB model and find that the autoregressive GenAI approach\nbeats traditional model classes.\n","authors":["Peer Nagy","Sascha Frey","Kang Li","Bidipta Sarkar","Svitlana Vyetrenko","Stefan Zohren","Ani Calinescu","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2502.09172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09164v1","updated":"2025-02-13T10:48:11Z","published":"2025-02-13T10:48:11Z","title":"E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot\n  Object Customization","summary":"  We propose E-MD3C ($\\underline{E}$fficient $\\underline{M}$asked\n$\\underline{D}$iffusion Transformer with Disentangled $\\underline{C}$onditions\nand $\\underline{C}$ompact $\\underline{C}$ollector), a highly efficient\nframework for zero-shot object image customization. Unlike prior works reliant\non resource-intensive Unet architectures, our approach employs lightweight\nmasked diffusion transformers operating on latent patches, offering\nsignificantly improved computational efficiency. The framework integrates three\ncore components: (1) an efficient masked diffusion transformer for processing\nautoencoder latents, (2) a disentangled condition design that ensures\ncompactness while preserving background alignment and fine details, and (3) a\nlearnable Conditions Collector that consolidates multiple inputs into a compact\nrepresentation for efficient denoising and learning. E-MD3C outperforms the\nexisting approach on the VITON-HD dataset across metrics such as PSNR, FID,\nSSIM, and LPIPS, demonstrating clear advantages in parameters, memory\nefficiency, and inference speed. With only $\\frac{1}{4}$ of the parameters, our\nTransformer-based 468M model delivers $2.5\\times$ faster inference and uses\n$\\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent\ndiffusion model.\n","authors":["Trung X. Pham","Zhang Kang","Ji Woo Hong","Xuran Zheng","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2502.09164v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.14630v2","updated":"2025-02-13T10:43:50Z","published":"2024-10-18T17:30:20Z","title":"On the Regularization of Learnable Embeddings for Time Series\n  Forecasting","summary":"  In forecasting multiple time series, accounting for the individual features\nof each sequence can be challenging. To address this, modern deep learning\nmethods for time series analysis combine a shared (global) model with local\nlayers, specific to each time series, often implemented as learnable\nembeddings. Ideally, these local embeddings should encode meaningful\nrepresentations of the unique dynamics of each sequence. However, when these\nare learned end-to-end as parameters of a forecasting model, they may end up\nacting as mere sequence identifiers. Shared processing blocks may then become\nreliant on such identifiers, limiting their transferability to new contexts. In\nthis paper, we address this issue by investigating methods to regularize the\nlearning of local learnable embeddings for time series processing.\nSpecifically, we perform the first extensive empirical study on the subject and\nshow how such regularizations consistently improve performance in widely\nadopted architectures. Furthermore, we show that methods attempting to prevent\nthe co-adaptation of local and global parameters by means of embeddings\nperturbation are particularly effective in this context. In this regard, we\ninclude in the comparison several perturbation-based regularization methods,\ngoing as far as periodically resetting the embeddings during training. The\nobtained results provide an important contribution to understanding the\ninterplay between learnable local parameters and shared processing layers: a\nkey challenge in modern time series processing models and a step toward\ndeveloping effective foundation models for time series.\n","authors":["Luca Butera","Giovanni De Felice","Andrea Cini","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2410.14630v2.pdf","comment":"Accepted at TMLR"},{"id":"http://arxiv.org/abs/2502.09152v1","updated":"2025-02-13T10:29:31Z","published":"2025-02-13T10:29:31Z","title":"Vertical Federated Continual Learning via Evolving Prototype Knowledge","summary":"  Vertical Federated Learning (VFL) has garnered significant attention as a\nprivacy-preserving machine learning framework for sample-aligned feature\nfederation. However, traditional VFL approaches do not address the challenges\nof class and feature continual learning, resulting in catastrophic forgetting\nof knowledge from previous tasks. To address the above challenge, we propose a\nnovel vertical federated continual learning method, named Vertical Federated\nContinual Learning via Evolving Prototype Knowledge (V-LETO), which primarily\nfacilitates the transfer of knowledge from previous tasks through the evolution\nof prototypes. Specifically, we propose an evolving prototype knowledge method,\nenabling the global model to retain both previous and current task knowledge.\nFurthermore, we introduce a model optimization technique that mitigates the\nforgetting of previous task knowledge by restricting updates to specific\nparameters of the local model, thereby enhancing overall performance. Extensive\nexperiments conducted in both CIL and FIL settings demonstrate that our method,\nV-LETO, outperforms the other state-of-the-art methods. For example, our method\noutperforms the state-of-the-art method by 10.39% and 35.15% for CIL and FIL\ntasks, respectively. Our code is available at\nhttps://anonymous.4open.science/r/V-LETO-0108/README.md.\n","authors":["Shuo Wang","Keke Gai","Jing Yu","Liehuang Zhu","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2502.09152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05279v2","updated":"2025-02-13T10:28:42Z","published":"2025-01-09T14:43:08Z","title":"Learning convolution operators on compact Abelian groups","summary":"  We consider the problem of learning convolution operators associated to\ncompact Abelian groups. We study a regularization-based approach and provide\ncorresponding learning guarantees, discussing natural regularity condition on\nthe convolution kernel. More precisely, we assume the convolution kernel is a\nfunction in a translation invariant Hilbert space and analyze a natural ridge\nregression (RR) estimator. Building on existing results for RR, we characterize\nthe accuracy of the estimator in terms of finite sample bounds. Interestingly,\nregularity assumptions which are classical in the analysis of RR, have a novel\nand natural interpretation in terms of space/frequency localization.\nTheoretical results are illustrated by numerical simulations.\n","authors":["Emilia Magnani","Ernesto De Vito","Philipp Hennig","Lorenzo Rosasco"],"pdf_url":"https://arxiv.org/pdf/2501.05279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09151v1","updated":"2025-02-13T10:27:30Z","published":"2025-02-13T10:27:30Z","title":"Regularization can make diffusion models more efficient","summary":"  Diffusion models are one of the key architectures of generative AI. Their\nmain drawback, however, is the computational costs. This study indicates that\nthe concept of sparsity, well known especially in statistics, can provide a\npathway to more efficient diffusion pipelines. Our mathematical guarantees\nprove that sparsity can reduce the input dimension's influence on the\ncomputational complexity to that of a much smaller intrinsic dimension of the\ndata. Our empirical findings confirm that inducing sparsity can indeed lead to\nbetter samples at a lower cost.\n","authors":["Mahsa Taheri","Johannes Lederer"],"pdf_url":"https://arxiv.org/pdf/2502.09151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15421v2","updated":"2025-02-13T10:26:45Z","published":"2024-05-24T10:36:23Z","title":"Model-free reinforcement learning with noisy actions for automated\n  experimental control in optics","summary":"  Setting up and controlling optical systems is often a challenging and tedious\ntask. The high number of degrees of freedom to control mirrors, lenses, or\nphases of light makes automatic control challenging, especially when the\ncomplexity of the system cannot be adequately modeled due to noise or\nnon-linearities. Here, we show that reinforcement learning (RL) can overcome\nthese challenges when coupling laser light into an optical fiber, using a\nmodel-free RL approach that trains directly on the experiment without\npre-training. By utilizing the sample-efficient algorithms Soft Actor-Critic\n(SAC) or Truncated Quantile Critics (TQC), our agent learns to couple with 90%\nefficiency, comparable to the human expert. We demonstrate that direct training\non an experiment can replace extensive system modeling. Our result exemplifies\nRL's potential to tackle problems in optics, paving the way for more complex\napplications where full noise modeling is not feasible.\n","authors":["Lea Richtmann","Viktoria-S. Schmiesing","Dennis Wilken","Jan Heine","Aaron Tranter","Avishek Anand","Tobias J. Osborne","Mich√®le Heurs"],"pdf_url":"https://arxiv.org/pdf/2405.15421v2.pdf","comment":"10 pages + 12 pages appendices, 2 + 12 figures"},{"id":"http://arxiv.org/abs/2502.09150v1","updated":"2025-02-13T10:25:52Z","published":"2025-02-13T10:25:52Z","title":"Shortcut Learning Susceptibility in Vision Classifiers","summary":"  Shortcut learning, where machine learning models exploit spurious\ncorrelations in data instead of capturing meaningful features, poses a\nsignificant challenge to building robust and generalizable models. This\nphenomenon is prevalent across various machine learning applications, including\nvision, natural language processing, and speech recognition, where models may\nfind unintended cues that minimize training loss but fail to capture the\nunderlying structure of the data. Vision classifiers such as Convolutional\nNeural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers\n(ViTs) leverage distinct architectural principles to process spatial and\nstructural information, making them differently susceptible to shortcut\nlearning. In this study, we systematically evaluate these architectures by\nintroducing deliberate shortcuts into the dataset that are positionally\ncorrelated with class labels, creating a controlled setup to assess whether\nmodels rely on these artificial cues or learn actual distinguishing features.\nWe perform both quantitative evaluation by training on the shortcut-modified\ndataset and testing them on two different test sets -- one containing the same\nshortcuts and another without them -- to determine the extent of reliance on\nshortcuts. Additionally, qualitative evaluation is performed by using network\ninversion-based reconstruction techniques to analyze what the models\ninternalize in their weights, aiming to reconstruct the training data as\nperceived by the classifiers. We evaluate shortcut learning behavior across\nmultiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and\nCIFAR-10, to compare the susceptibility of different vision classifier\narchitectures to shortcut reliance and assess their varying degrees of\nsensitivity to spurious correlations.\n","authors":["Pirzada Suhail","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2502.09150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09143v1","updated":"2025-02-13T10:18:44Z","published":"2025-02-13T10:18:44Z","title":"Feature-based Graph Attention Networks Improve Online Continual Learning","summary":"  Online continual learning for image classification is crucial for models to\nadapt to new data while retaining knowledge of previously learned tasks. This\ncapability is essential to address real-world challenges involving dynamic\nenvironments and evolving data distributions. Traditional approaches\npredominantly employ Convolutional Neural Networks, which are limited to\nprocessing images as grids and primarily capture local patterns rather than\nrelational information. Although the emergence of transformer architectures has\nimproved the ability to capture relationships, these models often require\nsignificantly larger resources. In this paper, we present a novel online\ncontinual learning framework based on Graph Attention Networks (GATs), which\neffectively capture contextual relationships and dynamically update the\ntask-specific representation via learned attention weights. Our approach\nutilizes a pre-trained feature extractor to convert images into graphs using\nhierarchical feature maps, representing information at varying levels of\ngranularity. These graphs are then processed by a GAT and incorporate an\nenhanced global pooling strategy to improve classification performance for\ncontinual learning. In addition, we propose the rehearsal memory duplication\ntechnique that improves the representation of the previous tasks while\nmaintaining the memory budget. Comprehensive evaluations on benchmark datasets,\nincluding SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the\nsuperiority of our method compared to the state-of-the-art methods.\n","authors":["Adjovi Sim","Zhengkui Wang","Aik Beng Ng","Shalini De Mello","Simon See","Wonmin Byeon"],"pdf_url":"https://arxiv.org/pdf/2502.09143v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.09140v1","updated":"2025-02-13T10:15:16Z","published":"2025-02-13T10:15:16Z","title":"Replay-free Online Continual Learning with Self-Supervised MultiPatches","summary":"  Online Continual Learning (OCL) methods train a model on a non-stationary\ndata stream where only a few examples are available at a time, often leveraging\nreplay strategies. However, usage of replay is sometimes forbidden, especially\nin applications with strict privacy regulations. Therefore, we propose\nContinual MultiPatches (CMP), an effective plug-in for existing OCL\nself-supervised learning strategies that avoids the use of replay samples. CMP\ngenerates multiple patches from a single example and projects them into a\nshared feature space, where patches coming from the same example are pushed\ntogether without collapsing into a single point. CMP surpasses replay and other\nSSL-based strategies on OCL streams, challenging the role of replay as a go-to\nsolution for self-supervised OCL.\n","authors":["Giacomo Cignoni","Andrea Cossu","Alex Gomez-Villa","Joost van de Weijer","Antonio Carta"],"pdf_url":"https://arxiv.org/pdf/2502.09140v1.pdf","comment":"Accepted at ESANN 2025"},{"id":"http://arxiv.org/abs/2502.09137v1","updated":"2025-02-13T10:13:20Z","published":"2025-02-13T10:13:20Z","title":"Trust Me, I Know the Way: Predictive Uncertainty in the Presence of\n  Shortcut Learning","summary":"  The correct way to quantify predictive uncertainty in neural networks remains\na topic of active discussion. In particular, it is unclear whether the\nstate-of-the art entropy decomposition leads to a meaningful representation of\nmodel, or epistemic, uncertainty (EU) in the light of a debate that pits\nignorance against disagreement perspectives. We aim to reconcile the\nconflicting viewpoints by arguing that both are valid but arise from different\nlearning situations. Notably, we show that the presence of shortcuts is\ndecisive for EU manifesting as disagreement.\n","authors":["Lisa Wimmer","Bernd Bischl","Ludwig Bothmann"],"pdf_url":"https://arxiv.org/pdf/2502.09137v1.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2502.09135v1","updated":"2025-02-13T10:11:36Z","published":"2025-02-13T10:11:36Z","title":"Interpreting and Steering Protein Language Models through Sparse\n  Autoencoders","summary":"  The rapid advancements in transformer-based language models have\nrevolutionized natural language processing, yet understanding the internal\nmechanisms of these models remains a significant challenge. This paper explores\nthe application of sparse autoencoders (SAE) to interpret the internal\nrepresentations of protein language models, specifically focusing on the ESM-2\n8M parameter model. By performing a statistical analysis on each latent\ncomponent's relevance to distinct protein annotations, we identify potential\ninterpretations linked to various protein characteristics, including\ntransmembrane regions, binding sites, and specialized motifs.\n  We then leverage these insights to guide sequence generation, shortlisting\nthe relevant latent components that can steer the model towards desired targets\nsuch as zinc finger domains. This work contributes to the emerging field of\nmechanistic interpretability in biological sequence models, offering new\nperspectives on model steering for sequence design.\n","authors":["Edith Natalia Villegas Garcia","Alessio Ansuini"],"pdf_url":"https://arxiv.org/pdf/2502.09135v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.09130v1","updated":"2025-02-13T10:07:35Z","published":"2025-02-13T10:07:35Z","title":"Finite-Time Analysis of Discrete-Time Stochastic Interpolants","summary":"  The stochastic interpolant framework offers a powerful approach for\nconstructing generative models based on ordinary differential equations (ODEs)\nor stochastic differential equations (SDEs) to transform arbitrary data\ndistributions. However, prior analyses of this framework have primarily focused\non the continuous-time setting, assuming a perfect solution of the underlying\nequations. In this work, we present the first discrete-time analysis of the\nstochastic interpolant framework, where we introduce an innovative\ndiscrete-time sampler and derive a finite-time upper bound on its distribution\nestimation error. Our result provides a novel quantification of how different\nfactors, including the distance between source and target distributions and\nestimation accuracy, affect the convergence rate and also offers a new\nprincipled way to design efficient schedules for convergence acceleration.\nFinally, numerical experiments are conducted on the discrete-time sampler to\ncorroborate our theoretical findings.\n","authors":["Yuhao Liu","Yu Chen","Rui Hu","Longbo Huang"],"pdf_url":"https://arxiv.org/pdf/2502.09130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09128v1","updated":"2025-02-13T10:05:44Z","published":"2025-02-13T10:05:44Z","title":"A Novel Dialect-Aware Framework for the Classification of Arabic\n  Dialects and Emotions","summary":"  Arabic is one of the oldest languages still in use today. As a result,\nseveral Arabic-speaking regions have developed dialects that are unique to\nthem. Dialect and emotion recognition have various uses in Arabic text\nanalysis, such as determining an online customer's origin based on their\ncomments. Furthermore, intelligent chatbots that are aware of a user's emotions\ncan respond appropriately to the user. Current research in emotion detection in\nthe Arabic language lacks awareness of how emotions are exhibited in different\ndialects, which motivates the work found in this study. This research addresses\nthe problems of dialect and emotion classification in Arabic. Specifically,\nthis is achieved by building a novel framework that can identify and predict\nArabic dialects and emotions from a given text. The framework consists of three\nmodules: A text-preprocessing module, a classification module, and a clustering\nmodule with the novel capability of building new dialect-aware emotion\nlexicons. The proposed framework generated a new emotional lexicon for\ndifferent dialects. It achieved an accuracy of 88.9% in classifying Arabic\ndialects, which outperforms the state-of-the-art results by 6.45 percentage\npoints. Furthermore, the framework achieved 89.1-79% accuracy in detecting\nemotions in the Egyptian and Gulf dialects, respectively.\n","authors":["Nasser A Alsadhan"],"pdf_url":"https://arxiv.org/pdf/2502.09128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08925v3","updated":"2025-02-13T10:00:58Z","published":"2024-10-11T15:50:31Z","title":"An Overview of Prototype Formulations for Interpretable Deep Learning","summary":"  Prototypical part networks offer interpretable alternatives to black-box deep\nlearning models. However, many of these networks rely on Euclidean prototypes,\nwhich may limit their flexibility. This work provides a comprehensive overview\nof various prototype formulations. Experiments conducted on the CUB-200-2011,\nStanford Cars, and Oxford Flowers datasets demonstrate the effectiveness and\nversatility of these different formulations.\n","authors":["Maximilian Xiling Li","Korbinian Franz Rudolf","Nils Blank","Rudolf Lioutikov"],"pdf_url":"https://arxiv.org/pdf/2410.08925v3.pdf","comment":"Equal Contribution of M.X.Li and K.F.Rudolf"},{"id":"http://arxiv.org/abs/2303.03388v3","updated":"2025-02-13T10:00:15Z","published":"2023-03-03T07:09:17Z","title":"Multi-modal Multi-kernel Graph Learning for Autism Prediction and\n  Biomarker Discovery","summary":"  Due to its complexity, graph learning-based multi-modal integration and\nclassification is one of the most challenging obstacles for disease prediction.\nTo effectively offset the negative impact between modalities in the process of\nmulti-modal integration and extract heterogeneous information from graphs, we\npropose a novel method called MMKGL (Multi-modal Multi-Kernel Graph Learning).\nFor the problem of negative impact between modalities, we propose a multi-modal\ngraph embedding module to construct a multi-modal graph. Different from\nconventional methods that manually construct static graphs for all modalities,\neach modality generates a separate graph by adaptive learning, where a function\ngraph and a supervision graph are introduced for optimization during the\nmulti-graph fusion embedding process. We then propose a multi-kernel graph\nlearning module to extract heterogeneous information from the multi-modal\ngraph. The information in the multi-modal graph at different levels is\naggregated by convolutional kernels with different receptive field sizes,\nfollowed by generating a cross-kernel discovery tensor for disease prediction.\nOur method is evaluated on the benchmark Autism Brain Imaging Data Exchange\n(ABIDE) dataset and outperforms the state-of-the-art methods. In addition,\ndiscriminative brain regions associated with autism are identified by our\nmodel, providing guidance for the study of autism pathology.\n","authors":["Jin Liu","Junbin Mao","Hanhe Lin","Hulin Kuang","Shirui Pan","Xusheng Wu","Shan Xie","Fei Liu","Yi Pan"],"pdf_url":"https://arxiv.org/pdf/2303.03388v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09624v2","updated":"2025-02-13T09:57:50Z","published":"2024-06-13T23:19:48Z","title":"DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational\n  Fluid Dynamics Simulations and Deep Learning Benchmarks","summary":"  We present DrivAerNet++, the largest and most comprehensive multimodal\ndataset for aerodynamic car design. DrivAerNet++ comprises 8,000 diverse car\ndesigns modeled with high-fidelity computational fluid dynamics (CFD)\nsimulations. The dataset includes diverse car configurations such as fastback,\nnotchback, and estateback, with different underbody and wheel designs to\nrepresent both internal combustion engines and electric vehicles. Each entry in\nthe dataset features detailed 3D meshes, parametric models, aerodynamic\ncoefficients, and extensive flow and surface field data, along with segmented\nparts for car classification and point cloud data. This dataset supports a wide\narray of machine learning applications including data-driven design\noptimization, generative modeling, surrogate model training, CFD simulation\nacceleration, and geometric classification. With more than 39 TB of publicly\navailable engineering data, DrivAerNet++ fills a significant gap in available\nresources, providing high-quality, diverse data to enhance model training,\npromote generalization, and accelerate automotive design processes. Along with\nrigorous dataset validation, we also provide ML benchmarking results on the\ntask of aerodynamic drag prediction, showcasing the breadth of applications\nsupported by our dataset. This dataset is set to significantly impact\nautomotive design and broader engineering disciplines by fostering innovation\nand improving the fidelity of aerodynamic evaluations. Dataset and code\navailable at: https://github.com/Mohamedelrefaie/DrivAerNet.\n","authors":["Mohamed Elrefaie","Florin Morar","Angela Dai","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2406.09624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09122v1","updated":"2025-02-13T09:57:25Z","published":"2025-02-13T09:57:25Z","title":"Improving Deep Regression with Tightness","summary":"  For deep regression, preserving the ordinality of the targets with respect to\nthe feature representation improves performance across various tasks. However,\na theoretical explanation for the benefits of ordinality is still lacking. This\nwork reveals that preserving ordinality reduces the conditional entropy\n$H(Z|Y)$ of representation $Z$ conditional on the target $Y$. However, our\nfindings reveal that typical regression losses do little to reduce $H(Z|Y)$,\neven though it is vital for generalization performance. With this motivation,\nwe introduce an optimal transport-based regularizer to preserve the similarity\nrelationships of targets in the feature space to reduce $H(Z|Y)$. Additionally,\nwe introduce a simple yet efficient strategy of duplicating the regressor\ntargets, also with the aim of reducing $H(Z|Y)$. Experiments on three\nreal-world regression tasks verify the effectiveness of our strategies to\nimprove deep regression. Code:\nhttps://github.com/needylove/Regression_tightness.\n","authors":["Shihao Zhang","Yuguang Yan","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.09122v1.pdf","comment":"ICLR 2025, Code: https://github.com/needylove/Regression_tightness"},{"id":"http://arxiv.org/abs/2404.03713v2","updated":"2025-02-13T09:48:12Z","published":"2024-04-04T17:46:20Z","title":"Explaining Explainability: Recommendations for Effective Use of Concept\n  Activation Vectors","summary":"  Concept-based explanations translate the internal representations of deep\nlearning models into a language that humans are familiar with: concepts. One\npopular method for finding concepts is Concept Activation Vectors (CAVs), which\nare learnt using a probe dataset of concept exemplars. In this work, we\ninvestigate three properties of CAVs: (1) inconsistency across layers, (2)\nentanglement with other concepts, and (3) spatial dependency. Each property\nprovides both challenges and opportunities in interpreting models. We introduce\ntools designed to detect the presence of these properties, provide insight into\nhow each property can lead to misleading explanations, and provide\nrecommendations to mitigate their impact. To demonstrate practical\napplications, we apply our recommendations to a melanoma classification task,\nshowing how entanglement can lead to uninterpretable results and that the\nchoice of negative probe set can have a substantial impact on the meaning of a\nCAV. Further, we show that understanding these properties can be used to our\nadvantage. For example, we introduce spatially dependent CAVs to test if a\nmodel is translation invariant with respect to a specific concept and class.\nOur experiments are performed on natural images (ImageNet), skin lesions (ISIC\n2019), and a new synthetic dataset, Elements. Elements is designed to capture a\nknown ground truth relationship between concepts and classes. We release this\ndataset to facilitate further research in understanding and evaluating\ninterpretability methods.\n","authors":["Angus Nicolson","Lisa Schut","J. Alison Noble","Yarin Gal"],"pdf_url":"https://arxiv.org/pdf/2404.03713v2.pdf","comment":"Accepted by Transactions on Machine Learning Research (02/2025)"},{"id":"http://arxiv.org/abs/2502.08644v2","updated":"2025-02-13T09:48:02Z","published":"2025-02-12T18:58:34Z","title":"Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and\n  learning in neural networks","summary":"  The brain can rapidly adapt to new contexts and learn from limited data, a\ncoveted characteristic that artificial intelligence algorithms have struggled\nto mimic. Inspired by oscillatory rhythms of the mechanical structures of\nneural cells, we developed a learning paradigm that is based on oscillations in\nlink strengths and associates learning with the coordination of these\noscillations. We find that this paradigm yields rapid adaptation and learning\nin artificial neural networks. Link oscillations can rapidly change\ncoordination, endowing the network with the ability to sense subtle context\nchanges in an unsupervised manner. In other words, the network generates the\nmissing contextual tokens required to perform as a generalist AI architecture\ncapable of predicting dynamics in multiple contexts. Oscillations also allow\nthe network to extrapolate dynamics to never-seen-before contexts. These\ncapabilities make our learning paradigm a powerful starting point for novel\nmodels of learning and cognition. Furthermore, learning through link\ncoordination is agnostic to the specifics of the neural network architecture,\nhence our study opens the door for introducing rapid adaptation and learning\ncapabilities into leading AI models.\n","authors":["Hoony Kang","Wolfgang Losert"],"pdf_url":"https://arxiv.org/pdf/2502.08644v2.pdf","comment":"13 pages, 3 figures v.2 comments: Updated email, updated typo on\n  p.11: h -> h^2 for RMSE"},{"id":"http://arxiv.org/abs/2501.19334v2","updated":"2025-02-13T09:47:38Z","published":"2025-01-31T17:34:53Z","title":"The Value of Prediction in Identifying the Worst-Off","summary":"  Machine learning is increasingly used in government programs to identify and\nsupport the most vulnerable individuals, prioritizing assistance for those at\ngreatest risk over optimizing aggregate outcomes. This paper examines the\nwelfare impacts of prediction in equity-driven contexts, and how they compare\nto other policy levers, such as expanding bureaucratic capacity. Through\nmathematical models and a real-world case study on long-term unemployment\namongst German residents, we develop a comprehensive understanding of the\nrelative effectiveness of prediction in surfacing the worst-off. Our findings\nprovide clear analytical frameworks and practical, data-driven tools that\nempower policymakers to make principled decisions when designing these systems.\n","authors":["Unai Fischer-Abaigar","Christoph Kern","Juan Carlos Perdomo"],"pdf_url":"https://arxiv.org/pdf/2501.19334v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02308v2","updated":"2025-02-13T09:38:00Z","published":"2025-02-04T13:24:28Z","title":"Real-Time Operator Takeover for Visuomotor Diffusion Policy Training","summary":"  We present a Real-Time Operator Takeover (RTOT) paradigm enabling operators\nto seamlessly take control of a live visuomotor diffusion policy, guiding the\nsystem back into desirable states or reinforcing specific demonstrations. We\npresent new insights in using the Mahalonobis distance to automatically\nidentify undesirable states. Once the operator has intervened and redirected\nthe system, the control is seamlessly returned to the policy, which resumes\ngenerating actions until further intervention is required. We demonstrate that\nincorporating the targeted takeover demonstrations significantly improves\npolicy performance compared to training solely with an equivalent number of,\nbut longer, initial demonstrations. We provide an in-depth analysis of using\nthe Mahalanobis distance to detect out-of-distribution states, illustrating its\nutility for identifying critical failure points during execution. Supporting\nmaterials, including videos of initial and takeover demonstrations and all rice\nscooping experiments, are available on the project website:\nhttps://operator-takeover.github.io/\n","authors":["Nils Ingelhag","Jesper Munkeby","Michael C. Welle","Marco Moletta","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2502.02308v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06099v2","updated":"2025-02-13T09:35:44Z","published":"2024-06-10T08:34:13Z","title":"Sequential Binary Classification for Intrusion Detection","summary":"  Network Intrusion Detection Systems (IDS) have become increasingly important\nas networks become more vulnerable to new and sophisticated attacks. Machine\nLearning (ML)-based IDS are increasingly seen as the most effective approach to\nhandle this issue. However, IDS datasets suffer from high class imbalance,\nwhich impacts the performance of standard ML models. Different from existing\ndata-driven techniques to handling class imbalance, this paper explores a\nstructural approach to handling class imbalance in multi-class classification\n(MCC) problems. The proposed approach - Sequential Binary Classification (SBC),\nis a hierarchical cascade of (regular) binary classifiers. Experiments on\nbenchmark IDS datasets demonstrate that the structural approach to handling\nclass-imbalance, as exemplified by SBC, is a viable approach to handling the\nissue.\n","authors":["Shrihari Vasudevan","Ishan Chokshi","Raaghul Ranganathan","Nachiappan Sundaram"],"pdf_url":"https://arxiv.org/pdf/2406.06099v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09106v1","updated":"2025-02-13T09:29:04Z","published":"2025-02-13T09:29:04Z","title":"Scaling Law for Stochastic Gradient Descent in Quadratically\n  Parameterized Linear Regression","summary":"  In machine learning, the scaling law describes how the model performance\nimproves with the model and data size scaling up. From a learning theory\nperspective, this class of results establishes upper and lower generalization\nbounds for a specific learning algorithm. Here, the exact algorithm running\nusing a specific model parameterization often offers a crucial implicit\nregularization effect, leading to good generalization. To characterize the\nscaling law, previous theoretical studies mainly focus on linear models,\nwhereas, feature learning, a notable process that contributes to the remarkable\nempirical success of neural networks, is regretfully vacant. This paper studies\nthe scaling law over a linear regression with the model being quadratically\nparameterized. We consider infinitely dimensional data and slope ground truth,\nboth signals exhibiting certain power-law decay rates. We study convergence\nrates for Stochastic Gradient Descent and demonstrate the learning rates for\nvariables will automatically adapt to the ground truth. As a result, in the\ncanonical linear regression, we provide explicit separations for generalization\ncurves between SGD with and without feature learning, and the\ninformation-theoretical lower bound that is agnostic to parametrization method\nand the algorithm. Our analysis for decaying ground truth provides a new\ncharacterization for the learning dynamic of the model.\n","authors":["Shihong Ding","Haihan Zhang","Hanzhen Zhao","Cong Fang"],"pdf_url":"https://arxiv.org/pdf/2502.09106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09104v1","updated":"2025-02-13T09:26:44Z","published":"2025-02-13T09:26:44Z","title":"One-shot Federated Learning Methods: A Practical Guide","summary":"  One-shot Federated Learning (OFL) is a distributed machine learning paradigm\nthat constrains client-server communication to a single round, addressing\nprivacy and communication overhead issues associated with multiple rounds of\ndata exchange in traditional Federated Learning (FL). OFL demonstrates the\npractical potential for integration with future approaches that require\ncollaborative training models, such as large language models (LLMs). However,\ncurrent OFL methods face two major challenges: data heterogeneity and model\nheterogeneity, which result in subpar performance compared to conventional FL\nmethods. Worse still, despite numerous studies addressing these limitations, a\ncomprehensive summary is still lacking. To address these gaps, this paper\npresents a systematic analysis of the challenges faced by OFL and thoroughly\nreviews the current methods. We also offer an innovative categorization method\nand analyze the trade-offs of various techniques. Additionally, we discuss the\nmost promising future directions and the technologies that should be integrated\ninto the OFL field. This work aims to provide guidance and insights for future\nresearch.\n","authors":["Xiang Liu","Zhenheng Tang","Xia Li","Yijun Song","Sijie Ji","Zemin Liu","Bo Han","Linshan Jiang","Jialin Li"],"pdf_url":"https://arxiv.org/pdf/2502.09104v1.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2501.18975v2","updated":"2025-02-13T09:25:43Z","published":"2025-01-31T09:11:06Z","title":"Meta-learning of shared linear representations beyond well-specified\n  linear regression","summary":"  Motivated by multi-task and meta-learning approaches, we consider the problem\nof learning structure shared by tasks or users, such as shared low-rank\nrepresentations or clustered structures. While all previous works focus on\nwell-specified linear regression, we consider more general convex objectives,\nwhere the structural low-rank and cluster assumptions are expressed on the\noptima of each function. We show that under mild assumptions such as\n\\textit{Hessian concentration} and \\textit{noise concentration at the optimum},\nrank and clustered regularized estimators recover such structure, provided the\nnumber of samples per task and the number of tasks are large enough. We then\nstudy the problem of recovering the subspace in which all the solutions lie, in\nthe setting where there is only a single sample per task: we show that in that\ncase, the rank-constrained estimator can recover the subspace, but that the\nnumber of tasks needs to scale exponentially large with the dimension of the\nsubspace. Finally, we provide a polynomial-time algorithm via nuclear norm\nconstraints for learning a shared linear representation in the context of\nconvex learning objectives.\n","authors":["Mathieu Even","Laurent Massouli√©"],"pdf_url":"https://arxiv.org/pdf/2501.18975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01084v2","updated":"2025-02-13T09:25:03Z","published":"2025-02-03T05:53:59Z","title":"Continuous Autoregressive Modeling with Stochastic Monotonic Alignment\n  for Speech Synthesis","summary":"  We propose a novel autoregressive modeling approach for speech synthesis,\ncombining a variational autoencoder (VAE) with a multi-modal latent space and\nan autoregressive model that uses Gaussian Mixture Models (GMM) as the\nconditional probability distribution. Unlike previous methods that rely on\nresidual vector quantization, our model leverages continuous speech\nrepresentations from the VAE's latent space, greatly simplifying the training\nand inference pipelines. We also introduce a stochastic monotonic alignment\nmechanism to enforce strict monotonic alignments. Our approach significantly\noutperforms the state-of-the-art autoregressive model VALL-E in both subjective\nand objective evaluations, achieving these results with only 10.3\\% of VALL-E's\nparameters. This demonstrates the potential of continuous speech language\nmodels as a more efficient alternative to existing quantization-based speech\nlanguage models. Sample audio can be found at https://tinyurl.com/gmm-lm-tts.\n","authors":["Weiwei Lin","Chenghan He"],"pdf_url":"https://arxiv.org/pdf/2502.01084v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.10609v3","updated":"2025-02-13T09:13:46Z","published":"2024-10-14T15:16:33Z","title":"Lambda-Skip Connections: the architectural component that prevents Rank\n  Collapse","summary":"  Rank collapse, a phenomenon where embedding vectors in sequence models\nrapidly converge to a uniform token or equilibrium state, has recently gained\nattention in the deep learning literature. This phenomenon leads to reduced\nexpressivity and potential training instabilities due to vanishing gradients.\nEmpirical evidence suggests that architectural components like skip\nconnections, LayerNorm, and MultiLayer Perceptrons (MLPs) play critical roles\nin mitigating rank collapse. While this issue is well-documented for\ntransformers, alternative sequence models, such as State Space Models (SSMs),\nwhich have recently gained prominence, have not been thoroughly examined for\nsimilar vulnerabilities. This paper extends the theory of rank collapse from\ntransformers to SSMs using a unifying framework that captures both\narchitectures. We study how a parametrized version of the classic skip\nconnection component, which we call \\emph{lambda-skip connections}, provides\nguarantees for rank collapse prevention. Through analytical results, we present\na sufficient condition to guarantee prevention of rank collapse across all the\naforementioned architectures. We also study the necessity of this condition via\nablation studies and analytical examples. To our knowledge, this is the first\nstudy that provides a general guarantee to prevent rank collapse, and that\ninvestigates rank collapse in the context of SSMs, offering valuable\nunderstanding for both theoreticians and practitioners. Finally, we validate\nour findings with experiments demonstrating the crucial role of architectural\ncomponents such as skip connections and gating mechanisms in preventing rank\ncollapse.\n","authors":["Federico Arangath Joseph","Jerome Sieber","Melanie N. Zeilinger","Carmen Amo Alonso"],"pdf_url":"https://arxiv.org/pdf/2410.10609v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02856v2","updated":"2025-02-13T09:11:32Z","published":"2025-02-05T03:25:32Z","title":"PH-VAE: A Polynomial Hierarchical Variational Autoencoder Towards\n  Disentangled Representation Learning","summary":"  The variational autoencoder (VAE) is a simple and efficient generative\nartificial intelligence method for modeling complex probability distributions\nof various types of data, such as images and texts. However, it suffers some\nmain shortcomings, such as lack of interpretability in the latent variables,\ndifficulties in tuning hyperparameters while training, producing blurry,\nunrealistic downstream outputs or loss of information due to how it calculates\nloss functions and recovers data distributions, overfitting, and origin gravity\neffect for small data sets, among other issues. These and other limitations\nhave caused unsatisfactory generation effects for the data with complex\ndistributions. In this work, we proposed and developed a polynomial\nhierarchical variational autoencoder (PH-VAE), in which we used a polynomial\nhierarchical date format to generate or to reconstruct the data distributions.\nIn doing so, we also proposed a novel Polynomial Divergence in the loss\nfunction to replace or generalize the Kullback-Leibler (KL) divergence, which\nresults in systematic and drastic improvements in both accuracy and\nreproducibility of the re-constructed distribution function as well as the\nquality of re-constructed data images while keeping the dataset size the same\nbut capturing fine resolution of the data. Moreover, we showed that the\nproposed PH-VAE has some form of disentangled representation learning ability.\n","authors":["Xi Chen","Shaofan Li"],"pdf_url":"https://arxiv.org/pdf/2502.02856v2.pdf","comment":"15 pages,14 figures"},{"id":"http://arxiv.org/abs/2410.13166v4","updated":"2025-02-13T09:08:42Z","published":"2024-10-17T02:47:10Z","title":"An Evolved Universal Transformer Memory","summary":"  Prior methods propose to offset the escalating costs of modern foundation\nmodels by dropping specific parts of their contexts with hand-designed rules,\nwhile attempting to preserve their original performance. We overcome this\ntrade-off with Neural Attention Memory Models (NAMMs), introducing a learned\nnetwork for memory management that improves both the performance and efficiency\nof transformers. We evolve NAMMs atop pre-trained transformers to provide\ndifferent latent contexts focusing on the most relevant information for\nindividual layers and attention heads. NAMMs are universally applicable to any\nmodel using self-attention as they condition exclusively on the values in the\nproduced attention matrices. Learning NAMMs on a small set of problems, we\nachieve substantial performance improvements across multiple long-context\nbenchmarks while cutting the model's input contexts up to a fraction of the\noriginal sizes. We show the generality of our conditioning enables zero-shot\ntransfer of NAMMs trained only on language to entirely new transformer\narchitectures even across input modalities, with their benefits carrying over\nto vision and reinforcement learning.\n","authors":["Edoardo Cetin","Qi Sun","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.13166v4.pdf","comment":"Published at ICLR 2025. Source code available at\n  https://github.com/SakanaAI/evo-memory"},{"id":"http://arxiv.org/abs/2502.09088v1","updated":"2025-02-13T09:01:00Z","published":"2025-02-13T09:01:00Z","title":"Unsupervised Anomaly Detection on Implicit Shape representations for\n  Sarcopenia Detection","summary":"  Sarcopenia is an age-related progressive loss of muscle mass and strength\nthat significantly impacts daily life. A commonly studied criterion for\ncharacterizing the muscle mass has been the combination of 3D imaging and\nmanual segmentations. In this paper, we instead study the muscles' shape. We\nrely on an implicit neural representation (INR) to model normal muscle shapes.\nWe then introduce an unsupervised anomaly detection method to identify\nsarcopenic muscles based on the reconstruction error of the implicit model.\nRelying on a conditional INR with an auto-decoding strategy, we also learn a\nlatent representation of the muscles that clearly separates normal from\nabnormal muscles in an unsupervised fashion. Experimental results on a dataset\nof 103 segmented volumes indicate that our double anomaly detection strategy\neffectively discriminates sarcopenic and non-sarcopenic muscles.\n","authors":["Louise Piecuch","Jeremie Huet","Antoine Frouin","Antoine Nordez","Anne-Sophie Boureau","Diana Mateus"],"pdf_url":"https://arxiv.org/pdf/2502.09088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09084v1","updated":"2025-02-13T08:59:04Z","published":"2025-02-13T08:59:04Z","title":"Application of Tabular Transformer Architectures for Operating System\n  Fingerprinting","summary":"  Operating System (OS) fingerprinting is essential for network management and\ncybersecurity, enabling accurate device identification based on network traffic\nanalysis. Traditional rule-based tools such as Nmap and p0f face challenges in\ndynamic environments due to frequent OS updates and obfuscation techniques.\nWhile Machine Learning (ML) approaches have been explored, Deep Learning (DL)\nmodels, particularly Transformer architectures, remain unexploited in this\ndomain. This study investigates the application of Tabular Transformer\narchitectures-specifically TabTransformer and FT-Transformer-for OS\nfingerprinting, leveraging structured network data from three publicly\navailable datasets. Our experiments demonstrate that FT-Transformer generally\noutperforms traditional ML models, previous approaches and TabTransformer\nacross multiple classification levels (OS family, major, and minor versions).\nThe results establish a strong foundation for DL-based OS fingerprinting,\nimproving accuracy and adaptability in complex network environments.\nFurthermore, we ensure the reproducibility of our research by providing an\nopen-source implementation.\n","authors":["Rub√©n P√©rez-Jove","Cristian R. Munteanu","Alejandro Pazos","Jose V√°zquez-Naya"],"pdf_url":"https://arxiv.org/pdf/2502.09084v1.pdf","comment":"Submitted as a preprint (not peer reviewed). 22 pages, 9 figures.\n  Code and datasets available at:\n  https://github.com/rubenpjove/tabularT-OS-fingerprinting"},{"id":"http://arxiv.org/abs/2410.17770v2","updated":"2025-02-13T08:58:17Z","published":"2024-10-23T11:19:08Z","title":"Small Singular Values Matter: A Random Matrix Analysis of Transformer\n  Models","summary":"  As large language models (LLMs) become increasingly central to AI\napplications, understanding their inner workings is essential. In this work, we\nanalyze the spectra of weight matrices in pretrained transformer models through\nthe lens of random matrix theory (RMT) to uncover learned structures. We find\nthat certain regions of the weight matrix spectra deviate markedly from RMT\npredictions, indicating richer feature encoding. By comparing the corresponding\nsingular vectors to eigenvectors of activation covariance matrices, we observe\nsubstantial overlap precisely where the spectra deviate from RMT expectations.\nOur analysis further reveals the important role of small singular values in\nLLMs, showing that these values contain significant information, a claim\nsupported by increased perplexity when they are removed from the model.\nAlthough these small values may appear unimportant prior to task-specific\nfine-tuning, removing them afterward significantly degrades performance,\nrevealing that fine-tuning refines the model primarily in these spectral\nregions. These results emphasize the critical role of small singular values,\nsuggesting that removing them in an already aligned transformer can be\ndetrimental, as it may compromise model alignment.\n","authors":["Max Staats","Matthias Thamm","Bernd Rosenow"],"pdf_url":"https://arxiv.org/pdf/2410.17770v2.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.03966v3","updated":"2025-02-13T08:54:42Z","published":"2025-02-06T10:59:44Z","title":"MultiFloodSynth: Multi-Annotated Flood Synthetic Dataset Generation","summary":"  In this paper, we present synthetic data generation framework for flood\nhazard detection system. For high fidelity and quality, we characterize several\nreal-world properties into virtual world and simulate the flood situation by\ncontrolling them. For the sake of efficiency, recent generative models in\nimage-to-3D and urban city synthesis are leveraged to easily composite flood\nenvironments so that we avoid data bias due to the hand-crafted manner. Based\non our framework, we build the flood synthetic dataset with 5 levels, dubbed\nMultiFloodSynth which contains rich annotation types like normal map,\nsegmentation, 3D bounding box for a variety of downstream task. In experiments,\nour dataset demonstrate the enhanced performance of flood hazard detection with\non-par realism compared with real dataset.\n","authors":["YoonJe Kang","Yonghoon Jung","Wonseop Shin","Bumsoo Kim","Sanghyun Seo"],"pdf_url":"https://arxiv.org/pdf/2502.03966v3.pdf","comment":"6 pages, 6 figures. Accepted as Oral Presentation to AAAI 2025\n  Workshop on Good-Data"},{"id":"http://arxiv.org/abs/2502.05454v2","updated":"2025-02-13T08:54:06Z","published":"2025-02-08T05:26:29Z","title":"Temporal Representation Alignment: Successor Features Enable Emergent\n  Compositionality in Robot Instruction Following","summary":"  Effective task representations should facilitate compositionality, such that\nafter learning a variety of basic tasks, an agent can perform compound tasks\nconsisting of multiple steps simply by composing the representations of the\nconstituent steps together. While this is conceptually simple and appealing, it\nis not clear how to automatically learn representations that enable this sort\nof compositionality. We show that learning to associate the representations of\ncurrent and future states with a temporal alignment loss can improve\ncompositional generalization, even in the absence of any explicit subtask\nplanning or reinforcement learning. We evaluate our approach across diverse\nrobotic manipulation tasks as well as in simulation, showing substantial\nimprovements for tasks specified with either language or goal images.\n","authors":["Vivek Myers","Bill Chunyuan Zheng","Anca Dragan","Kuan Fang","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2502.05454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09079v1","updated":"2025-02-13T08:53:13Z","published":"2025-02-13T08:53:13Z","title":"Quantifying Cryptocurrency Unpredictability: A Comprehensive Study of\n  Complexity and Forecasting","summary":"  This paper offers a thorough examination of the univariate predictability in\ncryptocurrency time-series. By exploiting a combination of complexity measure\nand model predictions we explore the cryptocurrencies time-series forecasting\ntask focusing on the exchange rate in USD of Litecoin, Binance Coin, Bitcoin,\nEthereum, and XRP. On one hand, to assess the complexity and the randomness of\nthese time-series, a comparative analysis has been performed using Brownian and\ncolored noises as a benchmark. The results obtained from the Complexity-Entropy\ncausality plane and power density spectrum analysis reveal that cryptocurrency\ntime-series exhibit characteristics closely resembling those of Brownian noise\nwhen analyzed in a univariate context. On the other hand, the application of a\nwide range of statistical, machine and deep learning models for time-series\nforecasting demonstrates the low predictability of cryptocurrencies. Notably,\nour analysis reveals that simpler models such as Naive models consistently\noutperform the more complex machine and deep learning ones in terms of\nforecasting accuracy across different forecast horizons and time windows. The\ncombined study of complexity and forecasting accuracies highlights the\ndifficulty of predicting the cryptocurrency market. These findings provide\nvaluable insights into the inherent characteristics of the cryptocurrency data\nand highlight the need to reassess the challenges associated with predicting\ncryptocurrency's price movements.\n","authors":["Francesco Puoti","Fabrizio Pittorino","Manuel Roveri"],"pdf_url":"https://arxiv.org/pdf/2502.09079v1.pdf","comment":"This is the author's accepted manuscript, modified per ACM\n  self-archiving policy. The definitive Version of Record is available at\n  https://doi.org/10.1145/3703412.3703420"},{"id":"http://arxiv.org/abs/2502.09067v1","updated":"2025-02-13T08:32:24Z","published":"2025-02-13T08:32:24Z","title":"FlowAR: une plateforme uniformis√©e pour la reconnaissance des\n  activit√©s humaines √† partir de capteurs binaires","summary":"  This demo showcases a platform for developing human activity recognition (AR)\nsystems, focusing on daily activities using sensor data, like binary sensors.\nWith a data-driven approach, this platform, named FlowAR, features a three-step\npipeline (flow): data cleaning, segmentation, and personalized classification.\nIts modularity allows flexibility to test methods, datasets, and ensure\nrigorous evaluations. A concrete use case demonstrates its effectiveness.\n","authors":["Ali Ncibi","Luc Bouganim","Philippe Pucheral"],"pdf_url":"https://arxiv.org/pdf/2502.09067v1.pdf","comment":"in French language https://editions-rnti.fr/?inprocid=1003044"},{"id":"http://arxiv.org/abs/2502.09061v1","updated":"2025-02-13T08:23:42Z","published":"2025-02-13T08:23:42Z","title":"CRANE: Reasoning with constrained LLM generation","summary":"  Code generation, symbolic math reasoning, and other tasks require LLMs to\nproduce outputs that are both syntactically and semantically correct.\nConstrained LLM generation is a promising direction to enforce adherence to\nformal grammar, but prior works have empirically observed that strict\nenforcement of formal constraints often diminishes the reasoning capabilities\nof LLMs. In this work, we first provide a theoretical explanation for why\nconstraining LLM outputs to very restrictive grammars that only allow\nsyntactically valid final answers reduces the reasoning capabilities of the\nmodel. Second, we demonstrate that by augmenting the output grammar with\ncarefully designed additional rules, it is always possible to preserve the\nreasoning capabilities of the LLM while ensuring syntactic and semantic\ncorrectness in its outputs. Building on these theoretical insights, we propose\na reasoning-augmented constrained decoding algorithm, CRANE, which effectively\nbalances the correctness of constrained generation with the flexibility of\nunconstrained generation. Experiments on multiple open-source LLMs and\nbenchmarks show that CRANE significantly outperforms both state-of-the-art\nconstrained decoding strategies and standard unconstrained decoding, showing up\nto 10% points accuracy improvement over baselines on challenging symbolic\nreasoning benchmarks GSM-symbolic and FOLIO.\n","authors":["Debangshu Banerjee","Tarun Suresh","Shubham Ugare","Sasa Misailovic","Gagandeep Singh"],"pdf_url":"https://arxiv.org/pdf/2502.09061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13835v2","updated":"2025-02-13T08:13:52Z","published":"2024-01-24T22:21:04Z","title":"What Large Language Models Know and What People Think They Know","summary":"  As artificial intelligence (AI) systems, particularly large language models\n(LLMs), become increasingly integrated into decision-making processes, the\nability to trust their outputs is crucial. To earn human trust, LLMs must be\nwell calibrated such that they can accurately assess and communicate the\nlikelihood of their predictions being correct. Whereas recent work has focused\non LLMs' internal confidence, less is understood about how effectively they\nconvey uncertainty to users. Here we explore the calibration gap, which refers\nto the difference between human confidence in LLM-generated answers and the\nmodels' actual confidence, and the discrimination gap, which reflects how well\nhumans and models can distinguish between correct and incorrect answers. Our\nexperiments with multiple-choice and short-answer questions reveal that users\ntend to overestimate the accuracy of LLM responses when provided with default\nexplanations. Moreover, longer explanations increased user confidence, even\nwhen the extra length did not improve answer accuracy. By adjusting LLM\nexplanations to better reflect the models' internal confidence, both the\ncalibration gap and the discrimination gap narrowed, significantly improving\nuser perception of LLM accuracy. These findings underscore the importance of\naccurate uncertainty communication and highlight the effect of explanation\nlength in influencing user trust in AI-assisted decision-making environments.\nCode and Data can be found at https://osf.io/y7pr6/ . Journal publication can\nbe found on Nature Machine Intelligence at\nhttps://www.nature.com/articles/s42256-024-00976-7 .\n","authors":["Mark Steyvers","Heliodoro Tejeda","Aakriti Kumar","Catarina Belem","Sheer Karny","Xinyue Hu","Lukas Mayer","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2401.13835v2.pdf","comment":"27 pages, 10 figures For the journal publication on Nature Machine\n  Intelligence see https://www.nature.com/articles/s42256-024-00976-7 For the\n  data and code see https://osf.io/y7pr6/"},{"id":"http://arxiv.org/abs/2401.11817v2","updated":"2025-02-13T08:11:25Z","published":"2024-01-22T10:26:14Z","title":"Hallucination is Inevitable: An Innate Limitation of Large Language\n  Models","summary":"  Hallucination has been widely recognized to be a significant drawback for\nlarge language models (LLMs). There have been many works that attempt to reduce\nthe extent of hallucination. These efforts have mostly been empirical so far,\nwhich cannot answer the fundamental question whether it can be completely\neliminated. In this paper, we formalize the problem and show that it is\nimpossible to eliminate hallucination in LLMs. Specifically, we define a formal\nworld where hallucination is defined as inconsistencies between a computable\nLLM and a computable ground truth function. By employing results from learning\ntheory, we show that LLMs cannot learn all the computable functions and will\ntherefore inevitably hallucinate if used as general problem solvers. Since the\nformal world is a part of the real world which is much more complicated,\nhallucinations are also inevitable for real world LLMs. Furthermore, for real\nworld LLMs constrained by provable time complexity, we describe the\nhallucination-prone tasks and empirically validate our claims. Finally, using\nthe formal world framework, we discuss the possible mechanisms and efficacies\nof existing hallucination mitigators as well as the practical implications on\nthe safe deployment of LLMs.\n","authors":["Ziwei Xu","Sanjay Jain","Mohan Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2401.11817v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09053v1","updated":"2025-02-13T08:08:27Z","published":"2025-02-13T08:08:27Z","title":"Game Theory Meets Large Language Models: A Systematic Survey","summary":"  Game theory establishes a fundamental framework for analyzing strategic\ninteractions among rational decision-makers. The rapid advancement of large\nlanguage models (LLMs) has sparked extensive research exploring the\nintersection of these two fields. Specifically, game-theoretic methods are\nbeing applied to evaluate and enhance LLM capabilities, while LLMs themselves\nare reshaping classic game models. This paper presents a comprehensive survey\nof the intersection of these fields, exploring a bidirectional relationship\nfrom three perspectives: (1) Establishing standardized game-based benchmarks\nfor evaluating LLM behavior; (2) Leveraging game-theoretic methods to improve\nLLM performance through algorithmic innovations; (3) Characterizing the\nsocietal impacts of LLMs through game modeling. Among these three aspects, we\nalso highlight how the equilibrium analysis for traditional game models is\nimpacted by LLMs' advanced language understanding, which in turn extends the\nstudy of game theory. Finally, we identify key challenges and future research\ndirections, assessing their feasibility based on the current state of the\nfield. By bridging theoretical rigor with emerging AI capabilities, this survey\naims to foster interdisciplinary collaboration and drive progress in this\nevolving research area.\n","authors":["Haoran Sun","Yusen Wu","Yukun Cheng","Xu Chu"],"pdf_url":"https://arxiv.org/pdf/2502.09053v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.22967v2","updated":"2025-02-13T08:05:22Z","published":"2024-10-30T12:26:02Z","title":"Adaptive NAD: Online and Self-adaptive Unsupervised Network Anomaly\n  Detector","summary":"  The widespread usage of the Internet of Things (IoT) has raised the risks of\ncyber threats, thus developing Anomaly Detection Systems (ADSs) that can adapt\nto evolving or new attacks is critical. Previous studies primarily focused on\noffline unsupervised learning methods to safeguard ADSs, which is not\napplicable in practical real-world applications. Besides, most of them strongly\nrely on assumptions of known legitimates and fail to satisfy the interpretable\nrequirements in security applications, creating barriers to the adoption in\npractice. In this paper, we design Adaptive NAD, a general framework to improve\nand interpret online unsupervised anomaly detection in security domains. An\ninterpretable two-layer anomaly detection strategy is proposed to generate\nreliable high-confidence pseudo-labels. Then, an online learning scheme is\nintroduced to update Adaptive NAD by a novel threshold calculation technique to\nadapt to new threats. Experimental results demonstrate that Adaptive NAD\nachieves more than 5.4%, 23.0%, and 3.2% improvements in SPAUC compared with\nstate-of-the-art solutions on the CIC-Darknet2020, CIC-DoHBrw-2020, and\nEdge-IIoTset datasets, respectively. The code is released at\nhttps://github.com/MyLearnCodeSpace/Adaptive-NAD.\n","authors":["Yachao Yuan","Yu Huang","Yali Yuan","Jin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.22967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09050v1","updated":"2025-02-13T08:05:14Z","published":"2025-02-13T08:05:14Z","title":"Leveraging Member-Group Relations via Multi-View Graph Filtering for\n  Effective Group Recommendation","summary":"  Group recommendation aims at providing optimized recommendations tailored to\ndiverse groups, enabling groups to enjoy appropriate items. On the other hand,\nmost existing group recommendation methods are built upon deep neural network\n(DNN) architectures designed to capture the intricate relationships between\nmember-level and group-level interactions. While these DNN-based approaches\nhave proven their effectiveness, they require complex and expensive training\nprocedures to incorporate group-level interactions in addition to member-level\ninteractions. To overcome such limitations, we introduce Group-GF, a new\napproach for extremely fast recommendations of items to each group via\nmulti-view graph filtering (GF) that offers a holistic view of complex\nmember-group dynamics, without the need for costly model training.\nSpecifically, in Group-GF, we first construct three item similarity graphs\nmanifesting different viewpoints for GF. Then, we discover a distinct\npolynomial graph filter for each similarity graph and judiciously aggregate the\nthree graph filters. Extensive experiments demonstrate the effectiveness of\nGroup-GF in terms of significantly reducing runtime and achieving\nstate-of-the-art recommendation accuracy.\n","authors":["Chae-Hyun Kim","Yoon-Ryung Choi","Jin-Duk Park","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2502.09050v1.pdf","comment":"5 pages, 3 figures, 4 tables; ACM Web Conference (WWW 2025) (to\n  appear) (Please cite our conference version.)"},{"id":"http://arxiv.org/abs/2502.09047v1","updated":"2025-02-13T08:02:15Z","published":"2025-02-13T08:02:15Z","title":"Optimal Algorithms in Linear Regression under Covariate Shift: On the\n  Importance of Precondition","summary":"  A common pursuit in modern statistical learning is to attain satisfactory\ngeneralization out of the source data distribution (OOD). In theory, the\nchallenge remains unsolved even under the canonical setting of covariate shift\nfor the linear model. This paper studies the foundational (high-dimensional)\nlinear regression where the ground truth variables are confined to an\nellipse-shape constraint and addresses two fundamental questions in this\nregime: (i) given the target covariate matrix, what is the min-max\n\\emph{optimal} algorithm under covariate shift? (ii) for what kinds of target\nclasses, the commonly-used SGD-type algorithms achieve optimality? Our analysis\nstarts with establishing a tight lower generalization bound via a Bayesian\nCramer-Rao inequality. For (i), we prove that the optimal estimator can be\nsimply a certain linear transformation of the best estimator for the source\ndistribution. Given the source and target matrices, we show that the\ntransformation can be efficiently computed via a convex program. The min-max\noptimal analysis for SGD leverages the idea that we recognize both the\naccumulated updates of the applied algorithms and the ideal transformation as\npreconditions on the learning variables. We provide sufficient conditions when\nSGD with its acceleration variants attain optimality.\n","authors":["Yuanshi Liu","Haihan Zhang","Qian Chen","Cong Fang"],"pdf_url":"https://arxiv.org/pdf/2502.09047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09046v1","updated":"2025-02-13T08:01:38Z","published":"2025-02-13T08:01:38Z","title":"Criteria-Aware Graph Filtering: Extremely Fast Yet Accurate\n  Multi-Criteria Recommendation","summary":"  Multi-criteria (MC) recommender systems, which utilize MC rating information\nfor recommendation, are increasingly widespread in various e-commerce domains.\nHowever, the MC recommendation using training-based collaborative filtering,\nrequiring consideration of multiple ratings compared to single-criterion\ncounterparts, often poses practical challenges in achieving state-of-the-art\nperformance along with scalable model training. To solve this problem, we\npropose CA-GF, a training-free MC recommendation method, which is built upon\ncriteria-aware graph filtering for efficient yet accurate MC recommendations.\nSpecifically, first, we construct an item-item similarity graph using an MC\nuser-expansion graph. Next, we design CA-GF composed of the following key\ncomponents, including 1) criterion-specific graph filtering where the optimal\nfilter for each criterion is found using various types of polynomial low-pass\nfilters and 2) criteria preference-infused aggregation where the smoothed\nsignals from each criterion are aggregated. We demonstrate that CA-GF is (a)\nefficient: providing the computational efficiency, offering the extremely fast\nruntime of less than 0.2 seconds even on the largest benchmark dataset, (b)\naccurate: outperforming benchmark MC recommendation methods, achieving\nsubstantial accuracy gains up to 24% compared to the best competitor, and (c)\ninterpretable: providing interpretations for the contribution of each criterion\nto the model prediction based on visualizations.\n","authors":["Jin-Duk Park","Jaemin Yoo","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2502.09046v1.pdf","comment":"12 pages, 8 figures, 7 tables; ACM Web Conference (WWW 2025) (to\n  appear) (Please cite our conference version.)"},{"id":"http://arxiv.org/abs/2502.00511v2","updated":"2025-02-13T07:35:08Z","published":"2025-02-01T18:09:49Z","title":"Bridging Internal Probability and Self-Consistency for Effective and\n  Efficient LLM Reasoning","summary":"  Recent advancements in large language models (LLMs) have demonstrated\nremarkable reasoning capabilities. However, single-shot inference often yields\nunreliable results for complex reasoning tasks, leading researchers to explore\nmultiple reasoning paths through methods such as perplexity and\nself-consistency. In this paper, we present the first theoretical error\ndecomposition analysis of these techniques, breaking down their error into\nestimation error and model error. Our analysis reveals a fundamental trade-off:\nperplexity methods suffer from substantial model error due to the absence of a\nproper consistency function, while self-consistency exhibits high estimation\nerror due to a slow error convergence rate. To overcome these limitations, we\npropose Reasoning-Pruning Perplexity Consistency (RPC). This approach combines\nPerplexity Consistency, which seamlessly integrates LLM perplexity with\nself-consistency, and Reasoning Pruning, which eliminates low-probability\nreasoning paths to effectively prevent the degeneration of estimation error\nreduction. Theoretical analysis demonstrates that RPC not only accelerates the\nconvergence rate of estimation error to an exponential level but also holds\nstrong potential for further reducing model error. Extensive empirical\nevaluations on seven benchmark datasets confirm that RPC can significantly\nimprove reasoning performance, sample efficiency, and confidence reliability.\n","authors":["Zhi Zhou","Tan Yuhao","Zenan Li","Yuan Yao","Lan-Zhe Guo","Xiaoxing Ma","Yu-Feng Li"],"pdf_url":"https://arxiv.org/pdf/2502.00511v2.pdf","comment":"Preliminary work"},{"id":"http://arxiv.org/abs/2402.02364v2","updated":"2025-02-13T07:29:46Z","published":"2024-02-04T06:23:05Z","title":"Loss Landscape Degeneracy Drives Stagewise Development in Transformers","summary":"  Deep learning involves navigating a high-dimensional loss landscape over the\nneural network parameter space. Over the course of training, complex\ncomputational structures form and re-form inside the neural network, leading to\nshifts in input/output behavior. It is a priority for the science of deep\nlearning to uncover principles governing the development of neural network\nstructure and behavior. Drawing on the framework of singular learning theory,\nwe propose that model development is deeply linked to degeneracy in the local\ngeometry of the loss landscape. We investigate this link by monitoring loss\nlandscape degeneracy throughout training, as quantified by the local learning\ncoefficient, for a transformer language model and an in-context linear\nregression transformer. We show that training can be divided into distinct\nperiods of change in loss landscape degeneracy, and that these changes in\ndegeneracy coincide with significant changes in the internal computational\nstructure and the input/output behavior of the transformers. This finding\nunderscores the potential of a degeneracy-based perspective for understanding\nmodern deep learning.\n","authors":["Jesse Hoogland","George Wang","Matthew Farrugia-Roberts","Liam Carroll","Susan Wei","Daniel Murfet"],"pdf_url":"https://arxiv.org/pdf/2402.02364v2.pdf","comment":"Material on essential dynamics from v1 of this preprint has been\n  removed from v2 and developed in arXiv:2501.17745"},{"id":"http://arxiv.org/abs/2502.09018v1","updated":"2025-02-13T07:11:07Z","published":"2025-02-13T07:11:07Z","title":"Zero-shot Concept Bottleneck Models","summary":"  Concept bottleneck models (CBMs) are inherently interpretable and\nintervenable neural network models, which explain their final label prediction\nby the intermediate prediction of high-level semantic concepts. However, they\nrequire target task training to learn input-to-concept and concept-to-label\nmappings, incurring target dataset collections and training resources. In this\npaper, we present \\textit{zero-shot concept bottleneck models} (Z-CBMs), which\npredict concepts and labels in a fully zero-shot manner without training neural\nnetworks. Z-CBMs utilize a large-scale concept bank, which is composed of\nmillions of vocabulary extracted from the web, to describe arbitrary input in\nvarious domains. For the input-to-concept mapping, we introduce concept\nretrieval, which dynamically finds input-related concepts by the cross-modal\nsearch on the concept bank. In the concept-to-label inference, we apply concept\nregression to select essential concepts from the retrieved concepts by sparse\nlinear regression. Through extensive experiments, we confirm that our Z-CBMs\nprovide interpretable and intervenable concepts without any additional\ntraining. Code will be available at https://github.com/yshinya6/zcbm.\n","authors":["Shin'ya Yamaguchi","Kosuke Nishida","Daiki Chijiwa","Yasutoshi Ida"],"pdf_url":"https://arxiv.org/pdf/2502.09018v1.pdf","comment":"14 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.09017v1","updated":"2025-02-13T07:11:01Z","published":"2025-02-13T07:11:01Z","title":"Diversity Enhances an LLM's Performance in RAG and Long-context Task","summary":"  The rapid advancements in large language models (LLMs) have highlighted the\nchallenge of context window limitations, primarily due to the quadratic time\ncomplexity of the self-attention mechanism (\\(O(N^2)\\), where \\(N\\) denotes the\ncontext window length). This constraint impacts tasks such as\nretrieval-augmented generation (RAG) in question answering (Q\\&A) and long\ncontext summarization. A common approach involves selecting content with the\nhighest similarity to the query; however, this often leads to redundancy and\nthe exclusion of diverse yet relevant information. Building on principles from\nMaximal Marginal Relevance (MMR) and Farthest Point Sampling (FPS), we\nintegrate diversity into the content selection process. Our findings reveal\nthat incorporating diversity substantially increases the recall of selecting\nrelevant sentences or chunks before LLM-based Q\\&A and summarization. These\nresults highlight the importance of maintaining diversity in future LLM\napplications to further improve summarization and Q\\&A outcomes.\n","authors":["Zhchao Wang","Bin Bi","Yanqi Luo","Sitaram Asur","Claire Na Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.09017v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14349v4","updated":"2025-02-13T07:05:51Z","published":"2025-01-24T09:19:15Z","title":"Online Inverse Linear Optimization: Improved Regret Bound, Robustness to\n  Suboptimality, and Toward Tight Regret Analysis","summary":"  We study an online learning problem where, over $T$ rounds, a learner\nobserves both time-varying sets of feasible actions and an agent's optimal\nactions, selected by solving linear optimization over the feasible actions. The\nlearner sequentially makes predictions of the agent's underlying linear\nobjective function, and their quality is measured by the regret, the cumulative\ngap between optimal objective values and those achieved by following the\nlearner's predictions. A seminal work by B\\\"armann et al. (ICML 2017) showed\nthat online learning methods can be applied to this problem to achieve regret\nbounds of $O(\\sqrt{T})$. Recently, Besbes et al. (COLT 2021, Oper. Res. 2023)\nsignificantly improved the result by achieving an $O(n^4\\ln T)$ regret bound,\nwhere $n$ is the dimension of the ambient space of objective vectors. Their\nmethod, based on the ellipsoid method, runs in polynomial time but is\ninefficient for large $n$ and $T$. In this paper, we obtain an $O(n\\ln T)$\nregret bound, improving upon the previous bound of $O(n^4\\ln T)$ by a factor of\n$n^3$. Our method is simple and efficient: we apply the online Newton step\n(ONS) to appropriate exp-concave loss functions. Moreover, for the case where\nthe agent's actions are possibly suboptimal, we establish an $O(n\\ln\nT+\\sqrt{\\Delta_Tn\\ln T})$ regret bound, where $\\Delta_T$ is the cumulative\nsuboptimality of the agent's actions. This bound is achieved by using MetaGrad,\nwhich runs ONS with $\\Theta(\\ln T)$ different learning rates in parallel. We\nalso provide a simple instance that implies an $\\Omega(n)$ lower bound, showing\nthat our $O(n\\ln T)$ bound is tight up to an $O(\\ln T)$ factor. This gives rise\nto a natural question: can the $O(\\ln T)$ factor in the upper bound be removed?\nFor the special case of $n=2$, we show that an $O(1)$ regret bound is possible,\nwhile we delineate challenges in extending this result to higher dimensions.\n","authors":["Shinsaku Sakaue","Taira Tsuchiya","Han Bao","Taihei Oki"],"pdf_url":"https://arxiv.org/pdf/2501.14349v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00730v2","updated":"2025-02-13T06:59:37Z","published":"2024-09-01T14:43:47Z","title":"Generating Physical Dynamics under Priors","summary":"  Generating physically feasible dynamics in a data-driven context is\nchallenging, especially when adhering to physical priors expressed in specific\nequations or formulas. Existing methodologies often overlook the integration of\nphysical priors, resulting in violation of basic physical laws and suboptimal\nperformance. In this paper, we introduce a novel framework that seamlessly\nincorporates physical priors into diffusion-based generative models to address\nthis limitation. Our approach leverages two categories of priors: 1)\ndistributional priors, such as roto-translational invariance, and 2) physical\nfeasibility priors, including energy and momentum conservation laws and PDE\nconstraints. By embedding these priors into the generative process, our method\ncan efficiently generate physically realistic dynamics, encompassing\ntrajectories and flows. Empirical evaluations demonstrate that our method\nproduces high-quality dynamics across a diverse array of physical phenomena\nwith remarkable robustness, underscoring its potential to advance data-driven\nstudies in AI4Physics. Our contributions signify a substantial advancement in\nthe field of generative modeling, offering a robust solution to generate\naccurate and physically consistent dynamics.\n","authors":["Zihan Zhou","Xiaoxue Wang","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2409.00730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09004v1","updated":"2025-02-13T06:49:14Z","published":"2025-02-13T06:49:14Z","title":"Hope vs. Hate: Understanding User Interactions with LGBTQ+ News Content\n  in Mainstream US News Media through the Lens of Hope Speech","summary":"  This paper makes three contributions. First, via a substantial corpus of\n1,419,047 comments posted on 3,161 YouTube news videos of major US cable news\noutlets, we analyze how users engage with LGBTQ+ news content. Our analyses\nfocus both on positive and negative content. In particular, we construct a\nfine-grained hope speech classifier that detects positive (hope speech),\nnegative, neutral, and irrelevant content. Second, in consultation with a\npublic health expert specializing on LGBTQ+ health, we conduct an annotation\nstudy with a balanced and diverse political representation and release a\ndataset of 3,750 instances with fine-grained labels and detailed annotator\ndemographic information. Finally, beyond providing a vital resource for the\nLGBTQ+ community, our annotation study and subsequent in-the-wild assessments\nreveal (1) strong association between rater political beliefs and how they rate\ncontent relevant to a marginalized community; (2) models trained on individual\npolitical beliefs exhibit considerable in-the-wild disagreement; and (3)\nzero-shot large language models (LLMs) align more with liberal raters.\n","authors":["Jonathan Pofcher","Christopher M. Homan","Randall Sell","Ashiqur R. KhudaBukhsh"],"pdf_url":"https://arxiv.org/pdf/2502.09004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03945v3","updated":"2025-02-13T06:47:41Z","published":"2024-07-04T14:02:10Z","title":"A fast neural hybrid Newton solver adapted to implicit methods for\n  nonlinear dynamics","summary":"  The use of implicit time-stepping schemes for the numerical approximation of\nsolutions to stiff nonlinear time-evolution equations brings well-known\nadvantages including, typically, better stability behaviour and corresponding\nsupport of larger time steps, and better structure preservation properties.\nHowever, this comes at the price of having to solve a nonlinear equation at\nevery time step of the numerical scheme. In this work, we propose a novel deep\nlearning based hybrid Newton's method to accelerate this solution of the\nnonlinear time step system for stiff time-evolution nonlinear equations. We\npropose a targeted learning strategy which facilitates robust unsupervised\nlearning in an offline phase and provides a highly efficient initialisation for\nthe Newton iteration leading to consistent acceleration of Newton's method. A\nquantifiable rate of improvement in Newton's method achieved by improved\ninitialisation is provided and we analyse the upper bound of the generalisation\nerror of our unsupervised learning strategy. These theoretical results are\nsupported by extensive numerical results, demonstrating the efficiency of our\nproposed neural hybrid solver both in one- and two-dimensional cases.\n","authors":["Tianyu Jin","Georg Maierhofer","Katharina Schratz","Yang Xiang"],"pdf_url":"https://arxiv.org/pdf/2407.03945v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09003v1","updated":"2025-02-13T06:44:33Z","published":"2025-02-13T06:44:33Z","title":"RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models","summary":"  Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.\n","authors":["Quan Wei","Chung-Yiu Yau","Hoi-To Wai"," Yang"," Zhao","Dongyeop Kang","Youngsuk Park","Mingyi Hong"],"pdf_url":"https://arxiv.org/pdf/2502.09003v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.09002v1","updated":"2025-02-13T06:43:46Z","published":"2025-02-13T06:43:46Z","title":"End-to-End triplet loss based fine-tuning for network embedding in\n  effective PII detection","summary":"  There are many approaches in mobile data ecosystem that inspect network\ntraffic generated by applications running on user's device to detect personal\ndata exfiltration from the user's device. State-of-the-art methods rely on\nfeatures extracted from HTTP requests and in this context, machine learning\ninvolves training classifiers on these features and making predictions using\nlabelled packet traces. However, most of these methods include external feature\nselection before model training. Deep learning, on the other hand, typically\ndoes not require such techniques, as it can autonomously learn and identify\npatterns in the data without external feature extraction or selection\nalgorithms. In this article, we propose a novel deep learning based end-to-end\nlearning framework for prediction of exposure of personally identifiable\ninformation (PII) in mobile packets. The framework employs a pre-trained large\nlanguage model (LLM) and an autoencoder to generate embedding of network\npackets and then uses a triplet-loss based fine-tuning method to train the\nmodel, increasing detection effectiveness using two real-world datasets. We\ncompare our proposed detection framework with other state-of-the-art works in\ndetecting PII leaks from user's device.\n","authors":["Rishika Kohli","Shaifu Gupta","Manoj Singh Gaur"],"pdf_url":"https://arxiv.org/pdf/2502.09002v1.pdf","comment":"13 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2502.07255v2","updated":"2025-02-13T06:35:29Z","published":"2025-02-11T04:45:31Z","title":"Beyond Confidence: Adaptive Abstention in Dual-Threshold Conformal\n  Prediction for Autonomous System Perception","summary":"  Safety-critical perception systems require both reliable uncertainty\nquantification and principled abstention mechanisms to maintain safety under\ndiverse operational conditions. We present a novel dual-threshold\nconformalization framework that provides statistically-guaranteed uncertainty\nestimates while enabling selective prediction in high-risk scenarios. Our\napproach uniquely combines a conformal threshold ensuring valid prediction sets\nwith an abstention threshold optimized through ROC analysis, providing\ndistribution-free coverage guarantees (>= 1 - alpha) while identifying\nunreliable predictions. Through comprehensive evaluation on CIFAR-100,\nImageNet1K, and ModelNet40 datasets, we demonstrate superior robustness across\ncamera and LiDAR modalities under varying environmental perturbations. The\nframework achieves exceptional detection performance (AUC: 0.993 to 0.995)\nunder severe conditions while maintaining high coverage (>90.0%) and enabling\nadaptive abstention (13.5% to 63.4% +/- 0.5) as environmental severity\nincreases. For LiDAR-based perception, our approach demonstrates particularly\nstrong performance, maintaining robust coverage (>84.5%) while appropriately\nabstaining from unreliable predictions. Notably, the framework shows remarkable\nstability under heavy perturbations, with detection performance (AUC: 0.995 +/-\n0.001) significantly outperforming existing methods across all modalities. Our\nunified approach bridges the gap between theoretical guarantees and practical\ndeployment needs, offering a robust solution for safety-critical autonomous\nsystems operating in challenging real-world conditions.\n","authors":["Divake Kumar","Nastaran Darabi","Sina Tayebati","Amit Ranjan Trivedi"],"pdf_url":"https://arxiv.org/pdf/2502.07255v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09001v1","updated":"2025-02-13T06:33:16Z","published":"2025-02-13T06:33:16Z","title":"Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:\n  Balancing Security and Data Protection","summary":"  Privacy-preserving network anomaly detection has become an essential area of\nresearch due to growing concerns over the protection of sensitive data.\nTraditional anomaly de- tection models often prioritize accuracy while\nneglecting the critical aspect of privacy. In this work, we propose a hybrid\nensemble model that incorporates privacy-preserving techniques to address both\ndetection accuracy and data protection. Our model combines the strengths of\nseveral machine learning algo- rithms, including K-Nearest Neighbors (KNN),\nSupport Vector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN),\nto create a robust system capable of identifying network anomalies while\nensuring privacy. The proposed approach in- tegrates advanced preprocessing\ntechniques that enhance data quality and address the challenges of small sample\nsizes and imbalanced datasets. By embedding privacy measures into the model\ndesign, our solution offers a significant advancement over existing methods,\nensuring both enhanced detection performance and strong privacy safeguards.\n","authors":["Shaobo Liu","Zihao Zhao","Weijie He","Jiren Wang","Jing Peng","Haoyuan Ma"],"pdf_url":"https://arxiv.org/pdf/2502.09001v1.pdf","comment":"Accepted by 2024 5th International Conference on Big Data, Artificial\n  Intelligence and Internet of Things Engineering(ICBAIE 2024)"},{"id":"http://arxiv.org/abs/2502.09000v1","updated":"2025-02-13T06:32:19Z","published":"2025-02-13T06:32:19Z","title":"Residual Transformer Fusion Network for Salt and Pepper Image Denoising","summary":"  Convolutional Neural Network (CNN) has been widely used in unstructured\ndatasets, one of which is image denoising. Image denoising is a noisy image\nreconstruction process that aims to reduce additional noise that occurs from\nthe noisy image with various strategies. Image denoising has a problem, namely\nthat some image denoising methods require some prior knowledge of information\nabout noise. To overcome this problem, a combined architecture of Convolutional\nVision Transformer (CvT) and Residual Networks (ResNet) is used which is called\nthe Residual Transformer Fusion Network (RTF-Net). In general, the process in\nthis architecture can be divided into two parts, Noise Suppression Network\n(NSN) and Structure Enhancement Network (SEN). Residual Block is used in the\nNoise Suppression Network and is used to learn the noise map in the image,\nwhile the CvT is used in the Structure Enhancement Network and is used to learn\nthe details that need to be added to the image processed by the Noise\nSuppression Network. The model was trained using the DIV2K Training Set\ndataset, and validation using the DIV2K Validation Set. After doing the\ntraining, the model was tested using Lena, Bridge, Pepper, and BSD300 images\nwith noise levels ranging from 30%, 50%, and 70% and the PSNR results were\ncompared with the DBA, NASNLM, PARIGI, NLSF, NLSF-MLP and NLSF-CNN methods. The\ntest results show that the proposed method is superior in all cases except for\nPepper's image with a noise level of 30%, where NLSF-CNN is superior with a\nPSNR value of 32.99 dB, while the proposed method gets a PSNR value of 31.70\ndB.\n","authors":["Bintang Pradana Erlangga Putra","Heri Prasetyo","Esti Suryani"],"pdf_url":"https://arxiv.org/pdf/2502.09000v1.pdf","comment":"8 pages, 17 figures"},{"id":"http://arxiv.org/abs/2502.06876v2","updated":"2025-02-13T06:28:33Z","published":"2025-02-08T11:56:58Z","title":"Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and\n  Harmlessness of Large Language Model via Model Merging","summary":"  Achieving balanced alignment of large language models (LLMs) in terms of\nHelpfulness, Honesty, and Harmlessness (3H optimization) constitutes a\ncornerstone of responsible AI, with existing methods like data mixture\nstrategies facing limitations including reliance on expert knowledge and\nconflicting optimization signals. While model merging offers a promising\nalternative by integrating specialized models, its potential for 3H\noptimization remains underexplored. This paper establishes the first\ncomprehensive benchmark for model merging in 3H-aligned LLMs, systematically\nevaluating 15 methods (12 training-free merging and 3 data mixture techniques)\nacross 10 datasets associated with 5 annotation dimensions, 2 LLM families, and\n2 training paradigms. Our analysis reveals three pivotal insights: (i)\npreviously overlooked collaborative/conflicting relationships among 3H\ndimensions, (ii) the consistent superiority of model merging over data mixture\napproaches in balancing alignment trade-offs, and (iii) the critical role of\nparameter-level conflict resolution through redundant component pruning and\noutlier mitigation. Building on these findings, we propose R-TSVM, a\nReweighting-enhanced Task Singular Vector Merging method that incorporates\noutlier-aware parameter weighting and sparsity-adaptive rank selection\nstrategies adapted to the heavy-tailed parameter distribution and sparsity for\nLLMs, further improving LLM alignment across multiple evaluations. We release\nour trained models for further exploration.\n","authors":["Jinluan Yang","Dingnan Jin","Anke Tang","Li Shen","Didi Zhu","Zhengyu Chen","Daixin Wang","Qing Cui","Zhiqiang Zhang","Jun Zhou","Fei Wu","Kun Kuang"],"pdf_url":"https://arxiv.org/pdf/2502.06876v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12469v4","updated":"2025-02-13T06:18:46Z","published":"2024-11-19T12:51:17Z","title":"AI Flow at the Network Edge","summary":"  Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow.\n","authors":["Jiawei Shao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2411.12469v4.pdf","comment":"This paper has been accepted to IEEE Network Magazine"},{"id":"http://arxiv.org/abs/2502.08993v1","updated":"2025-02-13T06:11:29Z","published":"2025-02-13T06:11:29Z","title":"Off-Policy Evaluation for Recommendations with Missing-Not-At-Random\n  Rewards","summary":"  Unbiased recommender learning (URL) and off-policy evaluation/learning\n(OPE/L) techniques are effective in addressing the data bias caused by display\nposition and logging policies, thereby consistently improving the performance\nof recommendations. However, when both bias exits in the logged data, these\nestimators may suffer from significant bias. In this study, we first analyze\nthe position bias of the OPE estimator when rewards are missing not at random.\nTo mitigate both biases, we propose a novel estimator that leverages two\nprobabilities of logging policies and reward observations as propensity scores.\nOur experiments demonstrate that the proposed estimator achieves superior\nperformance compared to other estimators, even as the levels of bias in reward\nobservations increases.\n","authors":["Tatsuki Takahashi","Chihiro Maru","Hiroko Shoji"],"pdf_url":"https://arxiv.org/pdf/2502.08993v1.pdf","comment":"4pages"},{"id":"http://arxiv.org/abs/2502.08991v1","updated":"2025-02-13T06:08:01Z","published":"2025-02-13T06:08:01Z","title":"Task Generalization With AutoRegressive Compositional Structure: Can\n  Learning From $\\d$ Tasks Generalize to $\\d^{T}$ Tasks?","summary":"  Large language models (LLMs) exhibit remarkable task generalization, solving\ntasks they were never explicitly trained on with only a few demonstrations.\nThis raises a fundamental question: When can learning from a small set of tasks\ngeneralize to a large task family? In this paper, we investigate task\ngeneralization through the lens of AutoRegressive Compositional (ARC)\nstructure, where each task is a composition of $T$ operations, and each\noperation is among a finite family of $\\d$ subtasks. This yields a total class\nof size~\\( \\d^\\TT \\). We first show that generalization to all \\( \\d^\\TT \\)\ntasks is theoretically achievable by training on only \\( \\tilde{O}(\\d) \\)\ntasks. Empirically, we demonstrate that Transformers achieve such exponential\ntask generalization on sparse parity functions via in-context learning (ICL)\nand Chain-of-Thought (CoT) reasoning. We further demonstrate this\ngeneralization in arithmetic and language translation, extending beyond parity\nfunctions.\n","authors":["Amirhesam Abedsoltan","Huaqing Zhang","Kaiyue Wen","Hongzhou Lin","Jingzhao Zhang","Mikhail Belkin"],"pdf_url":"https://arxiv.org/pdf/2502.08991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08987v1","updated":"2025-02-13T05:50:13Z","published":"2025-02-13T05:50:13Z","title":"Neural Force Field: Learning Generalized Physical Representation from a\n  Few Examples","summary":"  Physical reasoning is a remarkable human ability that enables rapid learning\nand generalization from limited experience. Current AI models, despite\nextensive training, still struggle to achieve similar generalization,\nespecially in Out-of-distribution (OOD) settings. This limitation stems from\ntheir inability to abstract core physical principles from observations. A key\nchallenge is developing representations that can efficiently learn and\ngeneralize physical dynamics from minimal data. Here we present Neural Force\nField (NFF) a modeling framework built on Neural Ordinary Differential Equation\n(NODE) that learns interpretable force field representations which can be\nefficiently integrated through an Ordinary Differential Equation ( ODE) solver\nto predict object trajectories. Unlike existing approaches that rely on\nhigh-dimensional latent spaces, NFF captures fundamental physical concepts such\nas gravity, support, and collision in an interpretable manner. Experiments on\ntwo challenging physical reasoning tasks demonstrate that NFF, trained with\nonly a few examples, achieves strong generalization to unseen scenarios. This\nphysics-grounded representation enables efficient forward-backward planning and\nrapid adaptation through interactive refinement. Our work suggests that\nincorporating physics-inspired representations into learning systems can help\nbridge the gap between artificial and human physical reasoning capabilities.\n","authors":["Shiqian Li","Ruihong Shen","Chi Zhang","Yixin Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.08987v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2502.08985v1","updated":"2025-02-13T05:47:57Z","published":"2025-02-13T05:47:57Z","title":"Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline\n  Multi-Agent Reinforcement Learning","summary":"  As a data-driven approach, offline MARL learns superior policies solely from\noffline datasets, ideal for domains rich in historical data but with high\ninteraction costs and risks. However, most existing methods are task-specific,\nrequiring retraining for new tasks, leading to redundancy and inefficiency. To\naddress this issue, in this paper, we propose a task-efficient multi-task\noffline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL).\nUnlike existing offline skill-discovery methods, SD-CQL discovers skills by\nreconstructing the next observation. It then evaluates fixed and variable\nactions separately and employs behavior-regularized conservative Q-learning to\nexecute the optimal action for each skill. This approach eliminates the need\nfor local-global alignment and enables strong multi-task generalization from\nlimited small-scale source tasks. Substantial experiments on StarCraftII\ndemonstrates the superior generalization performance and task-efficiency of\nSD-CQL. It achieves the best performance on $\\textbf{10}$ out of $14$ task\nsets, with up to $\\textbf{65%}$ improvement on individual task sets, and is\nwithin $4\\%$ of the best baseline on the remaining four.\n","authors":["Xun Wang","Zhuoran Li","Hai Zhong","Longbo Huang"],"pdf_url":"https://arxiv.org/pdf/2502.08985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01612v2","updated":"2025-02-13T05:32:54Z","published":"2025-02-03T18:45:22Z","title":"Self-Improving Transformers Overcome Easy-to-Hard and Length\n  Generalization Challenges","summary":"  Large language models often struggle with length generalization and solving\ncomplex problem instances beyond their training distribution. We present a\nself-improvement approach where models iteratively generate and learn from\ntheir own solutions, progressively tackling harder problems while maintaining a\nstandard transformer architecture. Across diverse tasks including arithmetic,\nstring manipulation, and maze solving, self-improving enables models to solve\nproblems far beyond their initial training distribution-for instance,\ngeneralizing from 10-digit to 100-digit addition without apparent saturation.\nWe observe that in some cases filtering for correct self-generated examples\nleads to exponential improvements in out-of-distribution performance across\ntraining rounds. Additionally, starting from pretrained models significantly\naccelerates this self-improvement process for several tasks. Our results\ndemonstrate how controlled weak-to-strong curricula can systematically teach a\nmodel logical extrapolation without any changes to the positional embeddings,\nor the model architecture.\n","authors":["Nayoung Lee","Ziyang Cai","Avi Schwarzschild","Kangwook Lee","Dimitris Papailiopoulos"],"pdf_url":"https://arxiv.org/pdf/2502.01612v2.pdf","comment":"Added references"},{"id":"http://arxiv.org/abs/2502.08978v1","updated":"2025-02-13T05:28:29Z","published":"2025-02-13T05:28:29Z","title":"What exactly has TabPFN learned to do?","summary":"  TabPFN [Hollmann et al., 2023], a Transformer model pretrained to perform\nin-context learning on fresh tabular classification problems, was presented at\nthe last ICLR conference. To better understand its behavior, we treat it as a\nblack-box function approximator generator and observe its generated function\napproximations on a varied selection of training datasets. Exploring its\nlearned inductive biases in this manner, we observe behavior that is at turns\neither brilliant or baffling. We conclude this post with thoughts on how these\nresults might inform the development, evaluation, and application of prior-data\nfitted networks (PFNs) in the future.\n","authors":["Calvin McCarter"],"pdf_url":"https://arxiv.org/pdf/2502.08978v1.pdf","comment":"Originally published in Blogposts Track at ICLR 2024. Appendix\n  contains re-analysis on TabPFN-v2 [Hollmann et al., 2025]"},{"id":"http://arxiv.org/abs/2502.08975v1","updated":"2025-02-13T05:24:52Z","published":"2025-02-13T05:24:52Z","title":"Small Molecule Drug Discovery Through Deep Learning:Progress,\n  Challenges, and Opportunities","summary":"  Due to their excellent drug-like and pharmacokinetic properties, small\nmolecule drugs are widely used to treat various diseases, making them a\ncritical component of drug discovery. In recent years, with the rapid\ndevelopment of deep learning (DL) techniques, DL-based small molecule drug\ndiscovery methods have achieved excellent performance in prediction accuracy,\nspeed, and complex molecular relationship modeling compared to traditional\nmachine learning approaches. These advancements enhance drug screening\nefficiency and optimization, and they provide more precise and effective\nsolutions for various drug discovery tasks. Contributing to this field's\ndevelopment, this paper aims to systematically summarize and generalize the\nrecent key tasks and representative techniques in DL-based small molecule drug\ndiscovery in recent years. Specifically, we provide an overview of the major\ntasks in small molecule drug discovery and their interrelationships. Next, we\nanalyze the six core tasks, summarizing the related methods, commonly used\ndatasets, and technological development trends. Finally, we discuss key\nchallenges, such as interpretability and out-of-distribution generalization,\nand offer our insights into future research directions for DL-assisted small\nmolecule drug discovery.\n","authors":["Kun Li","Yida Xiong","Hongzhi Zhang","Xiantao Cai","Bo Du","Wenbin Hu"],"pdf_url":"https://arxiv.org/pdf/2502.08975v1.pdf","comment":"9 pages, 1 figures, 8 tables"},{"id":"http://arxiv.org/abs/2502.02494v2","updated":"2025-02-13T05:14:49Z","published":"2025-02-04T17:09:44Z","title":"Analyzing Similarity Metrics for Data Selection for Language Model\n  Pretraining","summary":"  Similarity between training examples is used to curate pretraining datasets\nfor language models by many methods -- for diversification and to select\nexamples similar to high-quality data. However, similarity is typically\nmeasured with off-the-shelf embedding models that are generic or trained for\ntasks such as retrieval. This paper introduces a framework to analyze the\nsuitability of embedding models specifically for data curation in the language\nmodel pretraining setting. We quantify the correlation between similarity in\nthe embedding space to similarity in pretraining loss between different\ntraining examples, and how diversifying in the embedding space affects\npretraining quality. We analyze a variety of embedding models in our framework,\nwith experiments using the Pile dataset for pretraining a 1.7B parameter\ndecoder-only language model. We find that the embedding models we consider are\nall useful for pretraining data curation. Moreover, a simple approach of\naveraging per-token embeddings proves to be surprisingly competitive with more\nsophisticated embedding models -- likely because the latter are not designed\nspecifically for pretraining data curation. Indeed, we believe our analysis and\nevaluation framework can serve as a foundation for the design of embedding\nmodels that specifically reason about similarity in pretraining datasets.\n","authors":["Dylan Sam","Ayan Chakrabarti","Afshin Rostamizadeh","Srikumar Ramalingam","Gui Citovsky","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.02494v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.08969v1","updated":"2025-02-13T05:13:21Z","published":"2025-02-13T05:13:21Z","title":"SkyRover: A Modular Simulator for Cross-Domain Pathfinding","summary":"  Unmanned Aerial Vehicles (UAVs) and Automated Guided Vehicles (AGVs)\nincreasingly collaborate in logistics, surveillance, inspection tasks and etc.\nHowever, existing simulators often focus on a single domain, limiting\ncross-domain study. This paper presents the SkyRover, a modular simulator for\nUAV-AGV multi-agent pathfinding (MAPF). SkyRover supports realistic agent\ndynamics, configurable 3D environments, and convenient APIs for external\nsolvers and learning methods. By unifying ground and aerial operations, it\nfacilitates cross-domain algorithm design, testing, and benchmarking.\nExperiments highlight SkyRover's capacity for efficient pathfinding and\nhigh-fidelity simulations in UAV-AGV coordination. Project is available at\nhttps://sites.google.com/view/mapf3d/home.\n","authors":["Wenhui Ma","Wenhao Li","Bo Jin","Changhong Lu","Xiangfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08969v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08963v1","updated":"2025-02-13T04:59:01Z","published":"2025-02-13T04:59:01Z","title":"Modeling Time-evolving Causality over Data Streams","summary":"  Given an extensive, semi-infinite collection of multivariate coevolving data\nsequences (e.g., sensor/web activity streams) whose observations influence each\nother, how can we discover the time-changing cause-and-effect relationships in\nco-evolving data streams? How efficiently can we reveal dynamical patterns that\nallow us to forecast future values? In this paper, we present a novel streaming\nmethod, ModePlait, which is designed for modeling such causal relationships\n(i.e., time-evolving causality) in multivariate co-evolving data streams and\nforecasting their future values. The solution relies on characteristics of the\ncausal relationships that evolve over time in accordance with the dynamic\nchanges of exogenous variables. ModePlait has the following properties: (a)\nEffective: it discovers the time-evolving causality in multivariate co-evolving\ndata streams by detecting the transitions of distinct dynamical patterns\nadaptively. (b) Accurate: it enables both the discovery of time-evolving\ncausality and the forecasting of future values in a streaming fashion. (c)\nScalable: our algorithm does not depend on data stream length and thus is\napplicable to very large sequences. Extensive experiments on both synthetic and\nreal-world datasets demonstrate that our proposed model outperforms\nstate-of-the-art methods in terms of discovering the time-evolving causality as\nwell as forecasting.\n","authors":["Naoki Chihara","Yasuko Matsubara","Ren Fujiwara","Yasushi Sakurai"],"pdf_url":"https://arxiv.org/pdf/2502.08963v1.pdf","comment":"Accepted by KDD'25"},{"id":"http://arxiv.org/abs/2502.08960v1","updated":"2025-02-13T04:53:17Z","published":"2025-02-13T04:53:17Z","title":"A Comprehensive Survey on Imbalanced Data Learning","summary":"  With the expansion of data availability, machine learning (ML) has achieved\nremarkable breakthroughs in both academia and industry. However, imbalanced\ndata distributions are prevalent in various types of raw data and severely\nhinder the performance of ML by biasing the decision-making processes. To\ndeepen the understanding of imbalanced data and facilitate the related research\nand applications, this survey systematically analyzing various real-world data\nformats and concludes existing researches for different data formats into four\ndistinct categories: data re-balancing, feature representation, training\nstrategy, and ensemble learning. This structured analysis help researchers\ncomprehensively understand the pervasive nature of imbalance across diverse\ndata format, thereby paving a clearer path toward achieving specific research\ngoals. we provide an overview of relevant open-source libraries, spotlight\ncurrent challenges, and offer novel insights aimed at fostering future\nadvancements in this critical area of study.\n","authors":["Xinyi Gao","Dongting Xie","Yihang Zhang","Zhengren Wang","Conghui He","Hongzhi Yin","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08958v1","updated":"2025-02-13T04:51:18Z","published":"2025-02-13T04:51:18Z","title":"Biologically Plausible Brain Graph Transformer","summary":"  State-of-the-art brain graph analysis methods fail to fully encode the\nsmall-world architecture of brain graphs (accompanied by the presence of hubs\nand functional modules), and therefore lack biological plausibility to some\nextent. This limitation hinders their ability to accurately represent the\nbrain's structural and functional properties, thereby restricting the\neffectiveness of machine learning models in tasks such as brain disorder\ndetection. In this work, we propose a novel Biologically Plausible Brain Graph\nTransformer (BioBGT) that encodes the small-world architecture inherent in\nbrain graphs. Specifically, we present a network entanglement-based node\nimportance encoding technique that captures the structural importance of nodes\nin global information propagation during brain graph communication,\nhighlighting the biological properties of the brain structure. Furthermore, we\nintroduce a functional module-aware self-attention to preserve the functional\nsegregation and integration characteristics of brain graphs in the learned\nrepresentations. Experimental results on three benchmark datasets demonstrate\nthat BioBGT outperforms state-of-the-art models, enhancing biologically\nplausible brain graph representations for various brain graph analytical tasks\n","authors":["Ciyuan Peng","Yuelong Huang","Qichao Dong","Shuo Yu","Feng Xia","Chengqi Zhang","Yaochu Jin"],"pdf_url":"https://arxiv.org/pdf/2502.08958v1.pdf","comment":"27pages, 16figures, published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08953v1","updated":"2025-02-13T04:28:17Z","published":"2025-02-13T04:28:17Z","title":"Integrated Optimization and Game Theory Framework for Fair Cost\n  Allocation in Community Microgrids","summary":"  Fair cost allocation in community microgrids remains a significant challenge\ndue to the complex interactions between multiple participants with varying load\nprofiles, distributed energy resources, and storage systems. Traditional cost\nallocation methods often fail to adequately address the dynamic nature of\nparticipant contributions and benefits, leading to inequitable distribution of\ncosts and reduced participant satisfaction. This paper presents a novel\nframework integrating multi-objective optimization with cooperative game theory\nfor fair and efficient microgrid operation and cost allocation. The proposed\napproach combines mixed-integer linear programming for optimal resource\ndispatch with Shapley value analysis for equitable benefit distribution,\nensuring both system efficiency and participant satisfaction. The framework was\nvalidated using real-world data across six distinct operational scenarios,\ndemonstrating significant improvements in both technical and economic\nperformance. Results show peak demand reductions ranging from 7.8% to 62.6%,\nsolar utilization rates reaching 114.8% through effective storage integration,\nand cooperative gains of up to $1,801.01 per day. The Shapley value-based\nallocation achieved balanced benefit-cost distributions, with net positions\nranging from -16.0% to +14.2% across different load categories, ensuring\nsustainable participant cooperation.\n","authors":["K. Victor Sam Moses Babu","Pratyush Chakraborty","Mayukha Pal"],"pdf_url":"https://arxiv.org/pdf/2502.08953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08949v1","updated":"2025-02-13T04:15:20Z","published":"2025-02-13T04:15:20Z","title":"Self-Supervised Graph Contrastive Pretraining for Device-level\n  Integrated Circuits","summary":"  Self-supervised graph representation learning has driven significant\nadvancements in domains such as social network analysis, molecular design, and\nelectronics design automation (EDA). However, prior works in EDA have mainly\nfocused on the representation of gate-level digital circuits, failing to\ncapture analog and mixed-signal circuits. To address this gap, we introduce\nDICE: Device-level Integrated Circuits Encoder, the first self-supervised\npretrained graph neural network (GNN) model for any circuit expressed at the\ndevice level. DICE is a message-passing neural network (MPNN) trained through\ngraph contrastive learning, and its pretraining process is simulation-free,\nincorporating two novel data augmentation techniques. Experimental results\ndemonstrate that DICE achieves substantial performance gains across three\ndownstream tasks, underscoring its effectiveness for both analog and digital\ncircuits.\n","authors":["Sungyoung Lee","Ziyi Wang","Seunggeun Kim","Taekyun Lee","David Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2502.08949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01697v2","updated":"2025-02-13T04:10:14Z","published":"2024-08-03T07:38:04Z","title":"Invariant Graph Learning Meets Information Bottleneck for\n  Out-of-Distribution Generalization","summary":"  Graph out-of-distribution (OOD) generalization remains a major challenge in\ngraph learning since graph neural networks (GNNs) often suffer from severe\nperformance degradation under distribution shifts. Invariant learning, aiming\nto extract invariant features across varied distributions, has recently emerged\nas a promising approach for OOD generation. Despite the great success of\ninvariant learning in OOD problems for Euclidean data (i.e., images), the\nexploration within graph data remains constrained by the complex nature of\ngraphs. Existing studies, such as data augmentation or causal intervention,\neither suffer from disruptions to invariance during the graph manipulation\nprocess or face reliability issues due to a lack of supervised signals for\ncausal parts. In this work, we propose a novel framework, called Invariant\nGraph Learning based on Information bottleneck theory (InfoIGL), to extract the\ninvariant features of graphs and enhance models' generalization ability to\nunseen distributions. Specifically, InfoIGL introduces a redundancy filter to\ncompress task-irrelevant information related to environmental factors.\nCooperating with our designed multi-level contrastive learning, we maximize the\nmutual information among graphs of the same class in the downstream\nclassification tasks, preserving invariant features for prediction to a great\nextent. An appealing feature of InfoIGL is its strong generalization ability\nwithout depending on supervised signal of invariance. Experiments on both\nsynthetic and real-world datasets demonstrate that our method achieves\nstate-of-the-art performance under OOD generalization for graph classification\ntasks. The source code is available at https://github.com/maowenyu-11/InfoIGL.\n","authors":["Wenyu Mao","Jiancan Wu","Haoyang Liu","Yongduo Sui","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01697v2.pdf","comment":"The article has been accepted by Frontiers of Computer Science (FCS),\n  with the DOI: {10.1007/s11704-025-40798-3}"},{"id":"http://arxiv.org/abs/2502.08946v1","updated":"2025-02-13T04:00:03Z","published":"2025-02-13T04:00:03Z","title":"The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of\n  Physical Concept Understanding","summary":"  In a systematic way, we investigate a widely asked question: Do LLMs really\nunderstand what they say?, which relates to the more familiar term Stochastic\nParrot. To this end, we propose a summative assessment over a carefully\ndesigned physical concept understanding task, PhysiCo. Our task alleviates the\nmemorization issue via the usage of grid-format inputs that abstractly describe\nphysical phenomena. The grids represents varying levels of understanding, from\nthe core phenomenon, application examples to analogies to other abstract\npatterns in the grid world. A comprehensive study on our task demonstrates: (1)\nstate-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag\nbehind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,\nas they fail on our grid task but can describe and recognize the same concepts\nwell in natural language; (3) our task challenges the LLMs due to intrinsic\ndifficulties rather than the unfamiliar grid format, as in-context learning and\nfine-tuning on same formatted data added little to their performance.\n","authors":["Mo Yu","Lemao Liu","Junjie Wu","Tsz Ting Chung","Shunchi Zhang","Jiangnan Li","Dit-Yan Yeung","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08946v1.pdf","comment":"NAACL 2025 Main Conference. First 5 authors contributed equally.\n  Project page: https://physico-benchmark.github.io/"},{"id":"http://arxiv.org/abs/2502.07856v2","updated":"2025-02-13T03:55:03Z","published":"2025-02-11T14:57:33Z","title":"MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE\n  Solvers","summary":"  In applications of diffusion models, controllable generation is of practical\nsignificance, but is also challenging. Current methods for controllable\ngeneration primarily focus on modifying the score function of diffusion models,\nwhile Mean Reverting (MR) Diffusion directly modifies the structure of the\nstochastic differential equation (SDE), making the incorporation of image\nconditions simpler and more natural. However, current training-free fast\nsamplers are not directly applicable to MR Diffusion. And thus MR Diffusion\nrequires hundreds of NFEs (number of function evaluations) to obtain\nhigh-quality samples. In this paper, we propose a new algorithm named MRS (MR\nSampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time\nSDE and the probability flow ordinary differential equation (PF-ODE) associated\nwith MR Diffusion, and derive semi-analytical solutions. The solutions consist\nof an analytical function and an integral parameterized by a neural network.\nBased on this solution, we can generate high-quality samples in fewer steps.\nOur approach does not require training and supports all mainstream\nparameterizations, including noise prediction, data prediction and velocity\nprediction. Extensive experiments demonstrate that MR Sampler maintains high\nsampling quality with a speedup of 10 to 20 times across ten different image\nrestoration tasks. Our algorithm accelerates the sampling procedure of MR\nDiffusion, making it more practical in controllable generation.\n","authors":["Ao Li","Wei Fang","Hongbo Zhao","Le Lu","Ge Yang","Minfeng Xu"],"pdf_url":"https://arxiv.org/pdf/2502.07856v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2407.04285v3","updated":"2025-02-13T03:51:06Z","published":"2024-07-05T06:34:32Z","title":"Tackling Data Corruption in Offline Reinforcement Learning via Sequence\n  Modeling","summary":"  Learning policy from offline datasets through offline reinforcement learning\n(RL) holds promise for scaling data-driven decision-making while avoiding\nunsafe and costly online interactions. However, real-world data collected from\nsensors or humans often contains noise and errors, posing a significant\nchallenge for existing offline RL methods, particularly when the real-world\ndata is limited. Our study reveals that prior research focusing on adapting\npredominant offline RL methods based on temporal difference learning still\nfalls short under data corruption when the dataset is limited. In contrast, we\ndiscover that vanilla sequence modeling methods, such as Decision Transformer,\nexhibit robustness against data corruption, even without specialized\nmodifications. To unlock the full potential of sequence modeling, we propose\nRobust Decision Rransformer (RDT) by incorporating three simple yet effective\nrobust techniques: embedding dropout to improve the model's robustness against\nerroneous inputs, Gaussian weighted learning to mitigate the effects of\ncorrupted labels, and iterative data correction to eliminate corrupted data\nfrom the source. Extensive experiments on MuJoCo, Kitchen, and Adroit tasks\ndemonstrate RDT's superior performance under various data corruption scenarios\ncompared to prior methods. Furthermore, RDT exhibits remarkable robustness in a\nmore challenging setting that combines training-time data corruption with\ntest-time observation perturbations. These results highlight the potential of\nsequence modeling for learning from noisy or corrupted offline datasets,\nthereby promoting the reliable application of offline RL in real-world\nscenarios. Our code is available at\nhttps://github.com/jiawei415/RobustDecisionTransformer.\n","authors":["Jiawei Xu","Rui Yang","Shuang Qiu","Feng Luo","Meng Fang","Baoxiang Wang","Lei Han"],"pdf_url":"https://arxiv.org/pdf/2407.04285v3.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.08943v1","updated":"2025-02-13T03:43:33Z","published":"2025-02-13T03:43:33Z","title":"Beyond the Singular: The Essential Role of Multiple Generations in\n  Effective Benchmark Evaluation and Analysis","summary":"  Large language models (LLMs) have demonstrated significant utilities in\nreal-world applications, exhibiting impressive capabilities in natural language\nprocessing and understanding. Benchmark evaluations are crucial for assessing\nthe capabilities of LLMs as they can provide a comprehensive assessment of\ntheir strengths and weaknesses. However, current evaluation methods often\noverlook the inherent randomness of LLMs by employing deterministic generation\nstrategies or relying on a single random sample, resulting in unaccounted\nsampling variance and unreliable benchmark score estimates. In this paper, we\npropose a hierarchical statistical model that provides a more comprehensive\nrepresentation of the benchmarking process by incorporating both benchmark\ncharacteristics and LLM randomness. We show that leveraging multiple\ngenerations improves the accuracy of estimating the benchmark score and reduces\nvariance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a\nprompt-level difficulty score based on correct ratios, providing fine-grained\ninsights into individual prompts. Additionally, we create a data map that\nvisualizes difficulty and semantic prompts, enabling error detection and\nquality control in benchmark construction.\n","authors":["Wenbo Zhang","Hengrui Cai","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08943v1.pdf","comment":"10 pages, 1 table, 4 Figures"},{"id":"http://arxiv.org/abs/2502.08942v1","updated":"2025-02-13T03:43:27Z","published":"2025-02-13T03:43:27Z","title":"Language in the Flow of Time: Time-Series-Paired Texts Weaved into a\n  Unified Temporal Narrative","summary":"  While many advances in time series models focus exclusively on numerical\ndata, research on multimodal time series, particularly those involving\ncontextual textual information commonly encountered in real-world scenarios,\nremains in its infancy. Consequently, effectively integrating the text modality\nremains challenging. In this work, we highlight an intuitive yet significant\nobservation that has been overlooked by existing works: time-series-paired\ntexts exhibit periodic properties that closely mirror those of the original\ntime series. Building on this insight, we propose a novel framework, Texts as\nTime Series (TaTS), which considers the time-series-paired texts to be\nauxiliary variables of the time series. TaTS can be plugged into any existing\nnumerical-only time series models and enable them to handle time series data\nwith paired texts effectively. Through extensive experiments on both multimodal\ntime series forecasting and imputation tasks across benchmark datasets with\nvarious existing time series models, we demonstrate that TaTS can enhance\npredictive performance and achieve outperformance without modifying model\narchitectures.\n","authors":["Zihao Li","Xiao Lin","Zhining Liu","Jiaru Zou","Ziwei Wu","Lecheng Zheng","Dongqi Fu","Yada Zhu","Hendrik Hamann","Hanghang Tong","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2502.08942v1.pdf","comment":"Preprint, 37 pages"},{"id":"http://arxiv.org/abs/2502.08941v1","updated":"2025-02-13T03:43:13Z","published":"2025-02-13T03:43:13Z","title":"Analysis of Off-Policy $n$-Step TD-Learning with Linear Function\n  Approximation","summary":"  This paper analyzes multi-step temporal difference (TD)-learning algorithms\nwithin the ``deadly triad'' scenario, characterized by linear function\napproximation, off-policy learning, and bootstrapping. In particular, we prove\nthat $n$-step TD-learning algorithms converge to a solution as the sampling\nhorizon $n$ increases sufficiently. The paper is divided into two parts. In the\nfirst part, we comprehensively examine the fundamental properties of their\nmodel-based deterministic counterparts, including projected value iteration,\ngradient descent algorithms, which can be viewed as prototype deterministic\nalgorithms whose analysis plays a pivotal role in understanding and developing\ntheir model-free reinforcement learning counterparts. In particular, we prove\nthat these algorithms converge to meaningful solutions when $n$ is sufficiently\nlarge. Based on these findings, in the second part, two $n$-step TD-learning\nalgorithms are proposed and analyzed, which can be seen as the model-free\nreinforcement learning counterparts of the model-based deterministic\nalgorithms.\n","authors":["Han-Dong Lim","Donghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08941v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2402.15781"},{"id":"http://arxiv.org/abs/2501.14577v2","updated":"2025-02-13T03:43:09Z","published":"2025-01-24T15:33:05Z","title":"ZETA: Leveraging Z-order Curves for Efficient Top-k Attention","summary":"  Over recent years, the Transformer has become a fundamental building block\nfor sequence modeling architectures. Yet at its core is the use of\nself-attention, whose memory and computational cost grow quadratically with the\nsequence length $N$, rendering it prohibitively expensive for long sequences. A\npromising approach is top-$k$ attention, which selects only the $k$ most\nrelevant tokens and achieves performance comparable to vanilla self-attention\nwhile significantly reducing space and computational demands. However, causal\nmasks require the current query token to only attend to past tokens, preventing\nthe existing top-$k$ attention method from efficiently searching for the most\nrelevant tokens in parallel, thereby limiting training efficiency. In this\nwork, we propose ZETA, leveraging \\textbf{Z}-Order Curves for\n\\textbf{E}fficient \\textbf{T}op-$k$ \\textbf{A}ttention, to enable parallel\nquerying of past tokens for entire sequences. % in both space and time\ncomplexity of $\\mathcal{O}(N \\log N)$. We first theoretically show that the\nchoice of key and query dimensions involves a trade-off between the curse of\ndimensionality and the preservation of relative distances after projection. In\nlight of this insight, we propose reducing the dimensionality of keys and\nqueries in contrast to values and further leverage $Z$-order curves to map\nlow-dimensional keys and queries into \\emph{one}-dimensional space, which\npermits parallel sorting, thereby largely improving the efficiency for top-$k$\ntoken selection. Experimental results demonstrate that ZETA matches the\nperformance of standard attention on the synthetic \\textsc{Multi-Query\nAssociative Recall} task and outperforms attention and its variants on\n\\textsc{Long Range Arena} and \\textsc{WikiText-103} language modeling.\n","authors":["Qiuhao Zeng","Jerry Huang","Peng Lu","Gezheng Xu","Boxing Chen","Charles Ling","Boyu Wang"],"pdf_url":"https://arxiv.org/pdf/2501.14577v2.pdf","comment":"25 pages, 4 figures, accepted in International Conference on Learning\n  Representations (ICLR) 2025"},{"id":"http://arxiv.org/abs/2502.01755v2","updated":"2025-02-13T03:42:24Z","published":"2025-02-03T19:02:00Z","title":"Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA","summary":"  Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation\n(LoRA) optimize federated training by reducing computational and communication\ncosts. We propose RoLoRA, a federated framework using alternating optimization\nto fine-tune LoRA adapters. Our approach emphasizes the importance of learning\nup and down projection matrices to enhance expressiveness and robustness. We\nuse both theoretical analysis and extensive experiments to demonstrate the\nadvantages of RoLoRA over prior approaches that either generate imperfect model\nupdates or limit expressiveness of the model. We present theoretical analysis\non a simplified linear model to demonstrate the importance of learning both\ndown-projection and up-projection matrices in LoRA. We provide extensive\nexperimental evaluations on a toy neural network on MNIST as well as large\nlanguage models including RoBERTa-Large, Llama-2-7B on diverse tasks to\ndemonstrate the advantages of RoLoRA over other methods.\n","authors":["Shuangyi Chen","Yuanxin Guo","Yue Ju","Harik Dalal","Ashish Khisti"],"pdf_url":"https://arxiv.org/pdf/2502.01755v2.pdf","comment":"A preliminary version was in ICML24 workshop, arXiv:2409.02346"},{"id":"http://arxiv.org/abs/2502.08940v1","updated":"2025-02-13T03:41:50Z","published":"2025-02-13T03:41:50Z","title":"Towards Understanding Why Data Augmentation Improves Generalization","summary":"  Data augmentation is a cornerstone technique in deep learning, widely used to\nimprove model generalization. Traditional methods like random cropping and\ncolor jittering, as well as advanced techniques such as CutOut, Mixup, and\nCutMix, have achieved notable success across various domains. However, the\nmechanisms by which data augmentation improves generalization remain poorly\nunderstood, and existing theoretical analyses typically focus on individual\ntechniques without a unified explanation. In this work, we present a unified\ntheoretical framework that elucidates how data augmentation enhances\ngeneralization through two key effects: partial semantic feature removal and\nfeature mixing. Partial semantic feature removal reduces the model's reliance\non individual feature, promoting diverse feature learning and better\ngeneralization. Feature mixing, by scaling down original semantic features and\nintroducing noise, increases training complexity, driving the model to develop\nmore robust features. Advanced methods like CutMix integrate both effects,\nachieving complementary benefits. Our theoretical insights are further\nsupported by experimental results, validating the effectiveness of this unified\nperspective.\n","authors":["Jingyang Li","Jiachun Pan","Kim-Chuan Toh","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08938v1","updated":"2025-02-13T03:38:41Z","published":"2025-02-13T03:38:41Z","title":"Reevaluating Policy Gradient Methods for Imperfect-Information Games","summary":"  In the past decade, motivated by the putative failure of naive self-play deep\nreinforcement learning (DRL) in adversarial imperfect-information games,\nresearchers have developed numerous DRL algorithms based on fictitious play\n(FP), double oracle (DO), and counterfactual regret minimization (CFR). In\nlight of recent results of the magnetic mirror descent algorithm, we\nhypothesize that simpler generic policy gradient methods like PPO are\ncompetitive with or superior to these FP, DO, and CFR-based DRL approaches. To\nfacilitate the resolution of this hypothesis, we implement and release the\nfirst broadly accessible exact exploitability computations for four large\ngames. Using these games, we conduct the largest-ever exploitability comparison\nof DRL algorithms for imperfect-information games. Over 5600 training runs, FP,\nDO, and CFR-based approaches fail to outperform generic policy gradient\nmethods. Code is available at https://github.com/nathanlct/IIG-RL-Benchmark and\nhttps://github.com/gabrfarina/exp-a-spiel .\n","authors":["Max Rudolph","Nathan Lichtle","Sobhan Mohammadpour","Alexandre Bayen","J. Zico Kolter","Amy Zhang","Gabriele Farina","Eugene Vinitsky","Samuel Sokota"],"pdf_url":"https://arxiv.org/pdf/2502.08938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14695v2","updated":"2025-02-13T03:35:44Z","published":"2022-12-16T13:55:07Z","title":"Differentiating Student Feedbacks for Knowledge Tracing","summary":"  Knowledge tracing (KT) is a crucial task in computer-aided education and\nintelligent tutoring systems, predicting students' performance on new questions\nfrom their responses to prior ones. An accurate KT model can capture a\nstudent's mastery level of different knowledge topics, as reflected in their\npredicted performance on different questions. This helps improve the learning\nefficiency by suggesting appropriate new questions that complement students'\nknowledge states. However, current KT models have significant drawbacks that\nthey neglect the imbalanced discrimination of historical responses. A\nsignificant proportion of question responses provide limited information for\ndiscerning students' knowledge mastery, such as those that demonstrate uniform\nperformance across different students. Optimizing the prediction of these cases\nmay increase overall KT accuracy, but also negatively impact the model's\nability to trace personalized knowledge states, especially causing a deceptive\nsurge of performance. Towards this end, we propose a framework to reweight the\ncontribution of different responses based on their discrimination in training.\nAdditionally, we introduce an adaptive predictive score fusion technique to\nmaintain accuracy on less discriminative responses, achieving proper balance\nbetween student knowledge mastery and question difficulty. Experimental results\ndemonstrate that our framework enhances the performance of three mainstream KT\nmethods on three widely-used datasets.\n","authors":["Jiajun Cui","Hong Qian","Chanjin Zheng","Lu Wang","Mo Yu","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2212.14695v2.pdf","comment":"Accepted by ACM TOIS"},{"id":"http://arxiv.org/abs/2502.08933v1","updated":"2025-02-13T03:32:44Z","published":"2025-02-13T03:32:44Z","title":"AutoLike: Auditing Social Media Recommendations through User\n  Interactions","summary":"  Modern social media platforms, such as TikTok, Facebook, and YouTube, rely on\nrecommendation systems to personalize content for users based on user\ninteractions with endless streams of content, such as \"For You\" pages. However,\nthese complex algorithms can inadvertently deliver problematic content related\nto self-harm, mental health, and eating disorders. We introduce AutoLike, a\nframework to audit recommendation systems in social media platforms for topics\nof interest and their sentiments. To automate the process, we formulate the\nproblem as a reinforcement learning problem. AutoLike drives the recommendation\nsystem to serve a particular type of content through interactions (e.g.,\nliking). We apply the AutoLike framework to the TikTok platform as a case\nstudy. We evaluate how well AutoLike identifies TikTok content automatically\nacross nine topics of interest; and conduct eight experiments to demonstrate\nhow well it drives TikTok's recommendation system towards particular topics and\nsentiments. AutoLike has the potential to assist regulators in auditing\nrecommendation systems for problematic content. (Warning: This paper contains\nqualitative examples that may be viewed as offensive or harmful.)\n","authors":["Hieu Le","Salma Elmalaki","Zubair Shafiq","Athina Markopoulou"],"pdf_url":"https://arxiv.org/pdf/2502.08933v1.pdf","comment":"17 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2501.01774v2","updated":"2025-02-13T03:29:50Z","published":"2025-01-03T12:03:21Z","title":"A Unifying View of Linear Function Approximation in Off-Policy RL\n  Through Matrix Splitting and Preconditioning","summary":"  Traditionally, TD and FQI are viewed as differing in the number of updates\ntoward the target value function: TD makes one update, FQI makes an infinite\nnumber, and Partial Fitted Q-Iteration (PFQI) performs a finite number, such as\nthe use of a target network in Deep Q-Networks (DQN) in the OPE setting. This\nperspective, however, fails to capture the convergence connections between\nthese algorithms and may lead to incorrect conclusions, for example, that the\nconvergence of TD implies the convergence of FQI. In this paper, we focus on\nlinear value function approximation and offer a new perspective, unifying TD,\nFQI, and PFQI as the same iterative method for solving the Least Squares\nTemporal Difference (LSTD) system, but using different preconditioners and\nmatrix splitting schemes. TD uses a constant preconditioner, FQI employs a\ndata-feature adaptive preconditioner, and PFQI transitions between the two.\nThen, we reveal that in the context of linear function approximation,\nincreasing the number of updates under the same target value function\nessentially represents a transition from using a constant preconditioner to\ndata-feature adaptive preconditioner. This unifying perspective also simplifies\nthe analyses of the convergence conditions for these algorithms and clarifies\nmany issues. Consequently, we fully characterize the convergence of each\nalgorithm without assuming specific properties of the chosen features (e.g.,\nlinear independence). We also examine how common assumptions about feature\nrepresentations affect convergence, and discover new conditions on features\nthat are important for convergence. These convergence conditions allow us to\nestablish the convergence connections between these algorithms and to address\nimportant questions.\n","authors":["Zechen Wu","Amy Greenwald","Ronald Parr"],"pdf_url":"https://arxiv.org/pdf/2501.01774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08924v1","updated":"2025-02-13T03:20:37Z","published":"2025-02-13T03:20:37Z","title":"Escaping Collapse: The Strength of Weak Data for Large Language Model\n  Training","summary":"  Synthetically-generated data plays an increasingly larger role in training\nlarge language models. However, while synthetic data has been found to be\nuseful, studies have also shown that without proper curation it can cause LLM\nperformance to plateau, or even \"collapse\", after many training iterations. In\nthis paper, we formalize this question and develop a theoretical framework to\ninvestigate how much curation is needed in order to ensure that LLM performance\ncontinually improves. We find that the requirements are nearly minimal. We\ndescribe a training procedure that converges to an optimal LLM even if almost\nall of the non-synthetic training data is of poor quality. Our analysis is\ninspired by boosting, a classic machine learning technique that leverages a\nvery weak learning algorithm to produce an arbitrarily good classifier. Our\ntraining procedure subsumes many recently proposed methods for training LLMs on\nsynthetic data, and thus our analysis sheds light on why they are successful,\nand also suggests opportunities for future improvement. We present experiments\nthat validate our theory, and show that dynamically focusing labeling resources\non the most challenging examples -- in much the same way that boosting focuses\nthe efforts of the weak learner -- leads to improved performance.\n","authors":["Kareem Amin","Sara Babakniya","Alex Bie","Weiwei Kong","Umar Syed","Sergei Vassilvitskii"],"pdf_url":"https://arxiv.org/pdf/2502.08924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08923v1","updated":"2025-02-13T03:19:50Z","published":"2025-02-13T03:19:50Z","title":"CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without\n  Compromising Quality","summary":"  We introduce CopySpec, an innovative technique designed to tackle the\ninefficiencies LLMs face when generating responses that closely resemble\nprevious outputs. CopySpec identifies repeated sequences in the model's chat\nhistory and speculates that the same tokens will follow, enabling seamless\ncopying without compromising output quality or requiring additional GPU memory.\nTo evaluate the effectiveness of our approach, we conducted experiments using\nfive LLMs and five datasets: MT-Bench, CNN/DM, GSM-8K, HumanEval, and our newly\ncreated dataset, MT-Redundant. MT-Redundant, introduced in this paper,\ntransforms the second turn of MT-Bench into a request for variations of the\nfirst turn's answer, simulating real-world scenarios where users request\nmodifications to prior responses. Our results demonstrate significant\nspeed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select\nMT-Redundant categories, and 2.66x on the third turn of GSM-8K's\nself-correction tasks. Moreover, we show that CopySpec integrates seamlessly\nwith speculative decoding, yielding an average 49% additional speed-up over\nspeculative decoding for the second turn of MT-Redundant across all eight\ncategories. While LLMs, even with speculative decoding, suffer from slower\ninference as context sizes grow, CopySpec leverages the expanded context to\naccelerate inference, making it faster as the context size increases. Our code\nand dataset are publicly available at https://github.com/RazvanDu/CopySpec.\n","authors":["Razvan-Gabriel Dumitru","Minglai Yang","Vikas Yadav","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2502.08923v1.pdf","comment":"33 pages, 18 figures, 19 tables"},{"id":"http://arxiv.org/abs/2402.00626v3","updated":"2025-02-13T03:11:20Z","published":"2024-02-01T14:41:20Z","title":"Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks","summary":"  Typographic attacks, adding misleading text to images, can deceive\nvision-language models (LVLMs). The susceptibility of recent large LVLMs like\nGPT4-V to such attacks is understudied, raising concerns about amplified\nmisinformation in personal assistant applications. Previous attacks use simple\nstrategies, such as random misleading words, which don't fully exploit LVLMs'\nlanguage reasoning abilities. We introduce an experimental setup for testing\ntypographic attacks on LVLMs and propose two novel self-generated attacks: (1)\nClass-based attacks, where the model identifies a similar class to deceive\nitself, and (2) Reasoned attacks, where an advanced LVLM suggests an attack\ncombining a deceiving class and description. Our experiments show these attacks\nsignificantly reduce classification performance by up to 60\\% and are effective\nacross different models, including InstructBLIP and MiniGPT4. Code:\nhttps://github.com/mqraitem/Self-Gen-Typo-Attack\n","authors":["Maan Qraitem","Nazia Tasnim","Piotr Teterwak","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2402.00626v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08918v1","updated":"2025-02-13T03:10:19Z","published":"2025-02-13T03:10:19Z","title":"CLEAR: Cluster-based Prompt Learning on Heterogeneous Graphs","summary":"  Prompt learning has attracted increasing attention in the graph domain as a\nmeans to bridge the gap between pretext and downstream tasks. Existing studies\non heterogeneous graph prompting typically use feature prompts to modify node\nfeatures for specific downstream tasks, which do not concern the structure of\nheterogeneous graphs. Such a design also overlooks information from the\nmeta-paths, which are core to learning the high-order semantics of the\nheterogeneous graphs. To address these issues, we propose CLEAR, a\nCluster-based prompt LEARNING model on heterogeneous graphs. We present cluster\nprompts that reformulate downstream tasks as heterogeneous graph\nreconstruction. In this way, we align the pretext and downstream tasks to share\nthe same training objective. Additionally, our cluster prompts are also\ninjected into the meta-paths such that the prompt learning process incorporates\nhigh-order semantic information entailed by the meta-paths. Extensive\nexperiments on downstream tasks confirm the superiority of CLEAR. It\nconsistently outperforms state-of-the-art models, achieving up to 5%\nimprovement on the F1 metric for node classification.\n","authors":["Feiyang Wang","Zhongbao Zhang","Junda Ye","Li Sun","Jianzhong Qi"],"pdf_url":"https://arxiv.org/pdf/2502.08918v1.pdf","comment":"accepted by PAKDD 2025"},{"id":"http://arxiv.org/abs/2402.06855v3","updated":"2025-02-13T03:10:12Z","published":"2024-02-10T01:36:39Z","title":"For Better or For Worse? Learning Minimum Variance Features With Label\n  Augmentation","summary":"  Data augmentation has been pivotal in successfully training deep learning\nmodels on classification tasks over the past decade. An important subclass of\ndata augmentation techniques - which includes both label smoothing and Mixup -\ninvolves modifying not only the input data but also the input label during\nmodel training. In this work, we analyze the role played by the label\naugmentation aspect of such methods. We first prove that linear models on\nbinary classification data trained with label augmentation learn only the\nminimum variance features in the data, while standard training (which includes\nweight decay) can learn higher variance features. We then use our techniques to\nshow that even for nonlinear models and general data distributions, the label\nsmoothing and Mixup losses are lower bounded by a function of the model output\nvariance. Lastly, we demonstrate empirically that this aspect of label\nsmoothing and Mixup can be a positive and a negative. On the one hand, we show\nthat the strong performance of label smoothing and Mixup on image\nclassification benchmarks is correlated with learning low variance hidden\nrepresentations. On the other hand, we show that Mixup and label smoothing can\nbe more susceptible to low variance spurious correlations in the training data.\n","authors":["Muthu Chidambaram","Rong Ge"],"pdf_url":"https://arxiv.org/pdf/2402.06855v3.pdf","comment":"ICLR 2025, 25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2307.02075v2","updated":"2025-02-13T02:57:47Z","published":"2023-07-05T07:32:34Z","title":"Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for\n  Entity Alignment","summary":"  Entity alignment (EA) aims at identifying equivalent entity pairs across\ndifferent knowledge graphs (KGs) that refer to the same real-world identity. To\nsystematically combat confirmation bias for pseudo-labeling-based entity\nalignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment\n(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the\naccuracy of entity alignment. UPL-EA consists of two complementary components:\n(1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling\nas an effective means to enable more accurate determination of entity\ncorrespondences across two KGs and to mitigate the adverse impact of erroneous\nmatches. A simple but highly effective criterion is further devised to derive\npseudo-labeled entity pairs that satisfy one-to-one correspondences at each\niteration. (2) The cross-iteration pseudo-label calibration operates across\nmultiple consecutive iterations to further improve the pseudo-labeling\nprecision rate by reducing the local pseudo-label selection variability with a\ntheoretical guarantee. The two components are respectively designed to\neliminate Type I and Type II pseudo-labeling errors identified through our\nanalyse. The calibrated pseudo-labels are thereafter used to augment prior\nalignment seeds to reinforce subsequent model training for alignment inference.\nThe effectiveness of UPL-EA in eliminating pseudo-labeling errors is both\ntheoretically supported and experimentally validated. The experimental results\nshow that our approach achieves competitive performance with limited prior\nalignment seeds.\n","authors":["Qijie Ding","Jie Yin","Daokun Zhang","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2307.02075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08910v1","updated":"2025-02-13T02:52:01Z","published":"2025-02-13T02:52:01Z","title":"InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU","summary":"  In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.\n","authors":["Heejun Lee","Geon Park","Jaduk Suh","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2502.08910v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2501.13252v2","updated":"2025-02-13T02:40:52Z","published":"2025-01-22T22:18:50Z","title":"Exploring the Technology Landscape through Topic Modeling, Expert\n  Involvement, and Reinforcement Learning","summary":"  In today's rapidly evolving technological landscape, organizations face the\nchallenge of integrating external insights into their decision-making processes\nto stay competitive. To address this issue, this study proposes a method that\ncombines topic modeling, expert knowledge inputs, and reinforcement learning\n(RL) to enhance the detection of technological changes. The method has four\nmain steps: (1) Build a relevant topic model, starting with textual data like\ndocuments and reports to find key themes. (2) Create aspect-based topic models.\nExperts use curated keywords to build models that showcase key domain-specific\naspects. (3) Iterative analysis and RL driven refinement: We examine metrics\nsuch as topic magnitude, similarity, entropy shifts, and how models change over\ntime. We optimize topic selection with RL. Our reward function balances the\ndiversity and similarity of the topics. (4) Synthesis and operational\nintegration: Each iteration provides insights. In the final phase, the experts\ncheck these insights and reach new conclusions. These conclusions are designed\nfor use in the firm's operational processes. The application is tested by\nforecasting trends in quantum communication. Results demonstrate the method's\neffectiveness in identifying, ranking, and tracking trends that align with\nexpert input, providing a robust tool for exploring evolving technological\nlandscapes. This research offers a scalable and adaptive solution for\norganizations to make informed strategic decisions in dynamic environments.\n","authors":["Ali Nazari","Michael Weiss"],"pdf_url":"https://arxiv.org/pdf/2501.13252v2.pdf","comment":"31 pages, 17 figures"},{"id":"http://arxiv.org/abs/2502.02315v2","updated":"2025-02-13T02:33:32Z","published":"2025-02-04T13:36:54Z","title":"VaiBot: Shuttle Between the Instructions and Parameters of Large\n  Language Models","summary":"  How to interact with LLMs through \\emph{instructions} has been widely studied\nby researchers. However, previous studies have treated the emergence of\ninstructions and the training of LLMs on task data as separate processes,\noverlooking the inherent unity between the two. This paper proposes a neural\nnetwork framework, VaiBot, that integrates VAE and VIB, designed to uniformly\nmodel, learn, and infer both deduction and induction tasks under LLMs. Through\nexperiments, we demonstrate that VaiBot performs on par with existing baseline\nmethods in terms of deductive capabilities while significantly surpassing them\nin inductive capabilities. We also find that VaiBot can scale up using general\ninstruction-following data and exhibits excellent one-shot induction abilities.\nWe finally synergistically integrate the deductive and inductive processes of\nVaiBot. Through T-SNE dimensionality reduction, we observe that its\ninductive-deductive process significantly improves the distribution of training\nparameters, enabling it to outperform baseline methods in inductive reasoning\ntasks. The code and data for this paper can be found at\nhttps://anonymous.4open.science/r/VaiBot-021F.\n","authors":["Wangtao Sun","Haotian Xu","Huanxuan Liao","Xuanqing Yu","Zhongtao Jiang","Shizhu He","Jun Zhao","Kang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.02315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06924v2","updated":"2025-02-13T02:12:41Z","published":"2025-02-10T17:33:30Z","title":"XAMBA: Enabling Efficient State Space Models on Resource-Constrained\n  Neural Processing Units","summary":"  State-Space Models (SSMs) have emerged as efficient alternatives to\ntransformers for sequential data tasks, offering linear or near-linear\nscalability with sequence length, making them ideal for long-sequence\napplications in NLP, vision, and edge AI, including real-time transcription,\ntranslation, and contextual search. These applications require lightweight,\nhigh-performance models for deployment on resource-constrained devices like\nlaptops and PCs. Designing specialized accelerators for every emerging neural\nnetwork is costly and impractical; instead, optimizing models for existing NPUs\nin AI PCs provides a scalable solution. To this end, we propose XAMBA, the\nfirst framework to enable and optimize SSMs on commercial off-the-shelf (COTS)\nstate-of-the-art (SOTA) NPUs. XAMBA follows a three-step methodology: (1)\nenabling SSMs on NPUs, (2) optimizing performance to meet KPI requirements, and\n(3) trading accuracy for additional performance gains. After enabling SSMs on\nNPUs, XAMBA mitigates key bottlenecks using CumBA and ReduBA, replacing\nsequential CumSum and ReduceSum operations with matrix-based computations,\nsignificantly improving execution speed and memory efficiency. Additionally,\nActiBA enhances performance by approximating expensive activation functions\n(e.g., Swish, Softplus) using piecewise linear mappings, reducing latency with\nminimal accuracy loss. Evaluations on an Intel Core Ultra Series 2 AI PC show\nthat XAMBA achieves up to 2.6X speed-up over the baseline. Our implementation\nis available at https://github.com/arghadippurdue/XAMBA.\n","authors":["Arghadip Das","Arnab Raha","Shamik Kundu","Soumendu Kumar Ghosh","Deepak Mathaikutty","Vijay Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2502.06924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08889v1","updated":"2025-02-13T02:05:45Z","published":"2025-02-13T02:05:45Z","title":"Linear-Time User-Level DP-SCO via Robust Statistics","summary":"  User-level differentially private stochastic convex optimization (DP-SCO) has\ngarnered significant attention due to the paramount importance of safeguarding\nuser privacy in modern large-scale machine learning applications. Current\nmethods, such as those based on differentially private stochastic gradient\ndescent (DP-SGD), often struggle with high noise accumulation and suboptimal\nutility due to the need to privatize every intermediate iterate. In this work,\nwe introduce a novel linear-time algorithm that leverages robust statistics,\nspecifically the median and trimmed mean, to overcome these challenges. Our\napproach uniquely bounds the sensitivity of all intermediate iterates of SGD\nwith gradient estimation based on robust statistics, thereby significantly\nreducing the gradient estimation noise for privacy purposes and enhancing the\nprivacy-utility trade-off. By sidestepping the repeated privatization required\nby previous methods, our algorithm not only achieves an improved theoretical\nprivacy-utility trade-off but also maintains computational efficiency. We\ncomplement our algorithm with an information-theoretic lower bound, showing\nthat our upper bound is optimal up to logarithmic factors and the dependence on\n$\\epsilon$. This work sets the stage for more robust and efficient\nprivacy-preserving techniques in machine learning, with implications for future\nresearch and application in the field.\n","authors":["Badih Ghazi","Ravi Kumar","Daogao Liu","Pasin Manurangsi"],"pdf_url":"https://arxiv.org/pdf/2502.08889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06921v2","updated":"2025-02-13T02:05:21Z","published":"2025-02-10T17:03:02Z","title":"GraNNite: Enabling High-Performance Execution of Graph Neural Networks\n  on Resource-Constrained Neural Processing Units","summary":"  Graph Neural Networks (GNNs) are vital for learning from graph-structured\ndata, enabling applications in network analysis, recommendation systems, and\nspeech analytics. Deploying them on edge devices like client PCs and laptops\nenhances real-time processing, privacy, and cloud independence. GNNs aid\nRetrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and\nenable event-based vision tasks. However, irregular memory access, sparsity,\nand dynamic structures cause high latency and energy overhead on\nresource-constrained devices. While modern edge processors integrate CPUs,\nGPUs, and NPUs, NPUs designed for data-parallel tasks struggle with irregular\nGNN computations. We introduce GraNNite, the first hardware-aware framework\noptimizing GNN execution on commercial-off-the-shelf (COTS) SOTA DNN\naccelerators via a structured three-step methodology: (1) enabling NPU\nexecution, (2) optimizing performance, and (3) trading accuracy for efficiency\ngains. Step 1 employs GraphSplit for workload distribution and StaGr for static\naggregation, while GrAd and NodePad handle dynamic graphs. Step 2 boosts\nperformance using EffOp for control-heavy tasks and GraSp for sparsity\nexploitation. Graph Convolution optimizations PreG, SymG, and CacheG reduce\nredundancy and memory transfers. Step 3 balances quality versus efficiency,\nwhere QuantGr applies INT8 quantization, and GrAx1, GrAx2, and GrAx3 accelerate\nattention, broadcast-add, and SAGE-max aggregation. On Intel Core Ultra AI PCs,\nGraNNite achieves 2.6X to 7.6X speedups over default NPU mappings and up to\n8.6X energy gains over CPUs and GPUs, delivering 10.8X and 6.7X higher\nperformance than CPUs and GPUs, respectively, across GNN models.\n","authors":["Arghadip Das","Shamik Kundu","Arnab Raha","Soumendu Ghosh","Deepak Mathaikutty","Vijay Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2502.06921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02844v2","updated":"2025-02-13T01:58:12Z","published":"2025-01-06T08:43:31Z","title":"Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification","summary":"  Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.\n","authors":["Yubo Wang","Haoyang Li","Fei Teng","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2501.02844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08882v1","updated":"2025-02-13T01:43:14Z","published":"2025-02-13T01:43:14Z","title":"2D Integrated Bayesian Tomography of Plasma Electron Density Profile for\n  HL-3 Based on Gaussian Process","summary":"  This paper introduces an integrated Bayesian model that combines line\nintegral measurements and point values using Gaussian Process (GP). The\nproposed method leverages Gaussian Process Regression (GPR) to incorporate\npoint values into 2D profiles and employs coordinate mapping to integrate\nmagnetic flux information for 2D inversion. The average relative error of the\nreconstructed profile, using the integrated Bayesian tomography model with\nnormalized magnetic flux, is as low as 3.60*10^(-4). Additionally, sensitivity\ntests were conducted on the number of grids, the standard deviation of\nsynthetic diagnostic data, and noise levels, laying a solid foundation for the\napplication of the model to experimental data. This work not only achieves\naccurate 2D inversion using the integrated Bayesian model but also provides a\nrobust framework for decoupling pressure information from equilibrium\nreconstruction, thus making it possible to optimize equilibrium reconstruction\nusing inversion results.\n","authors":["Cong Wang","Renjie Yang","Dong Li","Zongyu Yang","Zhijun Wang","Yixiong Wei","Jing Li"],"pdf_url":"https://arxiv.org/pdf/2502.08882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08881v1","updated":"2025-02-13T01:40:21Z","published":"2025-02-13T01:40:21Z","title":"WENDy for Nonlinear-in-Parameter ODEs","summary":"  The Weak-form Estimation of Non-linear Dynamics (WENDy) algorithm is extended\nto accommodate systems of ordinary differential equations that are\nnonlinear-in-parameters (NiP). The extension rests on derived analytic\nexpressions for a likelihood function, its gradient and its Hessian matrix.\nWENDy makes use of these to approximate a maximum likelihood estimator based on\noptimization routines suited for non-convex optimization problems. The\nresulting parameter estimation algorithm has better accuracy, a substantially\nlarger domain of convergence, and is often orders of magnitude faster than the\nconventional output error least squares method (based on forward solvers).\n  The WENDy.jl algorithm is efficiently implemented in Julia. We demonstrate\nthe algorithm's ability to accommodate the weak form optimization for both\nadditive normal and multiplicative log-normal noise, and present results on a\nsuite of benchmark systems of ordinary differential equations. In order to\ndemonstrate the practical benefits of our approach, we present extensive\ncomparisons between our method and output error methods in terms of accuracy,\nprecision, bias, and coverage.\n","authors":["Nic Rummel","Daniel A. Messenger","Stephen Becker","Vanja Dukic","David M. Bortz"],"pdf_url":"https://arxiv.org/pdf/2502.08881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03664v4","updated":"2025-02-13T01:25:25Z","published":"2024-02-06T03:36:05Z","title":"Partial Gromov-Wasserstein Metric","summary":"  The Gromov-Wasserstein (GW) distance has gained increasing interest in the\nmachine learning community in recent years, as it allows for the comparison of\nmeasures in different metric spaces. To overcome the limitations imposed by the\nequal mass requirements of the classical GW problem, researchers have begun\nexploring its application in unbalanced settings. However, Unbalanced GW (UGW)\ncan only be regarded as a discrepancy rather than a rigorous metric/distance\nbetween two metric measure spaces (mm-spaces). In this paper, we propose a\nparticular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW). We\nestablish that PGW is a well-defined metric between mm-spaces and discuss its\ntheoretical properties, including the existence of a minimizer for the PGW\nproblem and the relationship between PGW and GW, among others. We then propose\ntwo variants of the Frank-Wolfe algorithm for solving the PGW problem and show\nthat they are mathematically and computationally equivalent. Moreover, based on\nour PGW metric, we introduce the analogous concept of barycenters for\nmm-spaces. Finally, we validate the effectiveness of our PGW metric and related\nsolvers in applications such as shape matching, shape retrieval, and shape\ninterpolation, comparing them against existing baselines.\n","authors":["Yikun Bai","Rocio Diaz Martin","Abihith Kothapalli","Hengrong Du","Xinran Liu","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2402.03664v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08874v1","updated":"2025-02-13T01:14:30Z","published":"2025-02-13T01:14:30Z","title":"Data Sensor Fusion In Digital Twin Technology For Enhanced Capabilities\n  In A Home Environment","summary":"  This paper investigates the integration of data sensor fusion in digital twin\ntechnology to bolster home environment capabilities, particularly in the\ncontext of challenges brought on by the coronavirus pandemic and its economic\neffects. The study underscores the crucial role of digital transformation in\nnot just adapting to, but also mitigating disruptions during the fourth\nindustrial revolution. Using the Wit Motion sensor, data was collected for\nactivities such as walking, working, sitting, and lying, with sensors measuring\naccelerometers, gyroscopes, and magnetometers. The research integrates\nCyber-physical systems, IoT, AI, and robotics to fortify digital twin\ncapabilities.\n  The paper compares sensor fusion methods, including feature-level fusion,\ndecision-level fusion, and Kalman filter fusion, alongside machine learning\nmodels like SVM, GBoost, and Random Forest to assess model effectiveness.\nResults show that sensor fusion significantly improves the accuracy and\nreliability of these models, as it compensates for individual sensor\nweaknesses, particularly with magnetometers. Despite higher accuracy in ideal\nconditions, integrating data from multiple sensors ensures more consistent and\nreliable results in real-world settings, thereby establishing a robust system\nthat can be confidently applied in practical scenarios.\n","authors":["Benjamin Momoh","Salisu Yahaya"],"pdf_url":"https://arxiv.org/pdf/2502.08874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08873v1","updated":"2025-02-13T01:11:25Z","published":"2025-02-13T01:11:25Z","title":"Robust Graph-Based Semi-Supervised Learning via $p$-Conductances","summary":"  We study the problem of semi-supervised learning on graphs in the regime\nwhere data labels are scarce or possibly corrupted. We propose an approach\ncalled $p$-conductance learning that generalizes the $p$-Laplace and Poisson\nlearning methods by introducing an objective reminiscent of $p$-Laplacian\nregularization and an affine relaxation of the label constraints. This leads to\na family of probability measure mincut programs that balance sparse edge\nremoval with accurate distribution separation. Our theoretical analysis\nconnects these programs to well-known variational and probabilistic problems on\ngraphs (including randomized cuts, effective resistance, and Wasserstein\ndistance) and provides motivation for robustness when labels are diffused via\nthe heat kernel. Computationally, we develop a semismooth Newton-conjugate\ngradient algorithm and extend it to incorporate class-size estimates when\nconverting the continuous solutions into label assignments. Empirical results\non computer vision and citation datasets demonstrate that our approach achieves\nstate-of-the-art accuracy in low label-rate, corrupted-label, and partial-label\nregimes.\n","authors":["Sawyer Jack Robertson","Chester Holtz","Zhengchao Wan","Gal Mishne","Alexander Cloninger"],"pdf_url":"https://arxiv.org/pdf/2502.08873v1.pdf","comment":"29 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.08477v2","updated":"2025-02-13T01:09:58Z","published":"2024-09-13T02:07:20Z","title":"Integrating Neural Operators with Diffusion Models Improves Spectral\n  Representation in Turbulence Modeling","summary":"  We integrate neural operators with diffusion models to address the spectral\nlimitations of neural operators in surrogate modeling of turbulent flows. While\nneural operators offer computational efficiency, they exhibit deficiencies in\ncapturing high-frequency flow dynamics, resulting in overly smooth\napproximations. To overcome this, we condition diffusion models on neural\noperators to enhance the resolution of turbulent structures. Our approach is\nvalidated for different neural operators on diverse datasets, including a high\nReynolds number jet flow simulation and experimental Schlieren velocimetry. The\nproposed method significantly improves the alignment of predicted energy\nspectra with true distributions compared to neural operators alone. This\nenables the diffusion models to stabilize longer forecasts through\ndiffusion-corrected autoregressive rollouts, as we demonstrate in this work.\nAdditionally, proper orthogonal decomposition analysis demonstrates enhanced\nspectral fidelity in space-time. This work establishes a new paradigm for\ncombining generative models with neural operators to advance surrogate modeling\nof turbulent systems, and it can be used in other scientific applications that\ninvolve microstructure and high-frequency content. See our project page:\nvivekoommen.github.io/NO_DM\n","authors":["Vivek Oommen","Aniruddha Bora","Zhen Zhang","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2409.08477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12000v2","updated":"2025-02-13T01:08:18Z","published":"2024-09-18T14:12:01Z","title":"\"It Might be Technically Impressive, But It's Practically Useless to\n  us\": Motivations, Practices, Challenges, and Opportunities for\n  Cross-Functional Collaboration around AI within the News Industry","summary":"  Recently, an increasing number of news organizations have integrated\nartificial intelligence (AI) into their workflows, leading to a further influx\nof AI technologists and data workers into the news industry. This has initiated\ncross-functional collaborations between these professionals and journalists.\nAlthough prior research has explored the impact of AI-related roles entering\nthe news industry, there is a lack of studies on how internal cross-functional\ncollaboration around AI unfolds between AI professionals and journalists within\nthe news industry. Through interviews with 17 journalists, six AI\ntechnologists, and three AI workers with cross-functional experience from\nleading Chinese news organizations, we investigate the practices, challenges,\nand opportunities for internal cross-functional collaboration around AI in news\nindustry. We first study how these journalists and AI professionals perceive\nexisting internal cross-collaboration strategies. We explore the challenges of\ncross-functional collaboration and provide recommendations for enhancing future\ncross-functional collaboration around AI in the news industry.\n","authors":["Qing Xiao","Xianzhe Fan","Felix M. Simon","Bingbing Zhang","Motahhare Eslami"],"pdf_url":"https://arxiv.org/pdf/2409.12000v2.pdf","comment":"19 pages, Accepted by CHI '25"},{"id":"http://arxiv.org/abs/2502.08870v1","updated":"2025-02-13T00:49:28Z","published":"2025-02-13T00:49:28Z","title":"When and why randomised exploration works (in linear bandits)","summary":"  We provide an approach for the analysis of randomised exploration algorithms\nlike Thompson sampling that does not rely on forced optimism or posterior\ninflation. With this, we demonstrate that in the $d$-dimensional linear bandit\nsetting, when the action space is smooth and strongly convex, randomised\nexploration algorithms enjoy an $n$-step regret bound of the order $O(d\\sqrt{n}\n\\log(n))$. Notably, this shows for the first time that there exist non-trivial\nlinear bandit settings where Thompson sampling can achieve optimal dimension\ndependence in the regret.\n","authors":["Marc Abeille","David Janz","Ciara Pike-Burke"],"pdf_url":"https://arxiv.org/pdf/2502.08870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08869v1","updated":"2025-02-13T00:42:11Z","published":"2025-02-13T00:42:11Z","title":"Harnessing Vision Models for Time Series Analysis: A Survey","summary":"  Time series analysis has witnessed the inspiring development from traditional\nautoregressive models, deep learning models, to recent Transformers and Large\nLanguage Models (LLMs). Efforts in leveraging vision models for time series\nanalysis have also been made along the way but are less visible to the\ncommunity due to the predominant research on sequence modeling in this domain.\nHowever, the discrepancy between continuous time series and the discrete token\nspace of LLMs, and the challenges in explicitly modeling the correlations of\nvariates in multivariate time series have shifted some research attentions to\nthe equally successful Large Vision Models (LVMs) and Vision Language Models\n(VLMs). To fill the blank in the existing literature, this survey discusses the\nadvantages of vision models over LLMs in time series analysis. It provides a\ncomprehensive and in-depth overview of the existing methods, with dual views of\ndetailed taxonomy that answer the key research questions including how to\nencode time series as images and how to model the imaged time series for\nvarious tasks. Additionally, we address the challenges in the pre- and\npost-processing steps involved in this framework and outline future directions\nto further advance time series analysis with vision models.\n","authors":["Jingchao Ni","Ziming Zhao","ChengAo Shen","Hanghang Tong","Dongjin Song","Wei Cheng","Dongsheng Luo","Haifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01476v2","updated":"2025-02-13T00:41:05Z","published":"2023-12-03T18:23:48Z","title":"Optimizing Context-Enhanced Relational Joins","summary":"  Collecting data, extracting value, and combining insights from relational and\ncontext-rich multi-modal sources in data processing pipelines presents a\nchallenge for traditional relational DBMS. While relational operators allow\ndeclarative and optimizable query specification, they are limited to data\ntransformations unsuitable for capturing or analyzing context. On the other\nhand, representation learning models can map context-rich data into embeddings,\nallowing machine-automated context processing but requiring imperative data\ntransformation integration with the analytical query. To bridge this dichotomy,\nwe present a context-enhanced relational join and introduce an embedding\noperator composable with relational operators. This enables hybrid relational\nand context-rich vector data processing, with algebraic equivalences compatible\nwith relational algebra and corresponding logical and physical optimizations.\nWe investigate model-operator interaction with vector data processing and study\nthe characteristics of the E-join operator. Using an example of string\nembeddings, we demonstrate enabling hybrid context-enhanced processing on\nrelational join operators with vector embeddings. The importance of holistic\noptimization, from logical to physical, is demonstrated in an order of\nmagnitude execution time improvement.\n","authors":["Viktor Sanca","Manos Chatzakis","Anastasia Ailamaki"],"pdf_url":"https://arxiv.org/pdf/2312.01476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.01716v2","updated":"2025-02-13T00:33:06Z","published":"2025-01-03T09:21:27Z","title":"Beyond Non-Degeneracy: Revisiting Certainty Equivalent Heuristic for\n  Online Linear Programming","summary":"  The Certainty Equivalent heuristic (CE) is a widely-used algorithm for\nvarious dynamic resource allocation problems in OR and OM. Despite its\npopularity, existing theoretical guarantees of CE are limited to settings\nsatisfying restrictive fluid regularity conditions, particularly, the\nnon-degeneracy conditions, under the widely held belief that the violation of\nsuch conditions leads to performance deterioration and necessitates algorithmic\ninnovation beyond CE.\n  In this work, we conduct a refined performance analysis of CE within the\ngeneral framework of online linear programming. We show that CE achieves\nuniformly near-optimal regret (up to a polylogarithmic factor in $T$) under\nonly mild assumptions on the underlying distribution, without relying on any\nfluid regularity conditions. Our result implies that, contrary to prior belief,\nCE effectively beats the curse of degeneracy for a wide range of problem\ninstances with continuous conditional reward distributions, highlighting the\ndistinction of the problem's structure between discrete and non-discrete\nsettings. Our explicit regret bound interpolates between the mild $(\\log T)^2$\nregime and the worst-case $\\sqrt{T}$ regime with a parameter $\\beta$\nquantifying the minimal rate of probability accumulation of the conditional\nreward distributions, generalizing prior findings in the multisecretary\nsetting.\n  To achieve these results, we develop novel algorithmic analytical techniques.\nDrawing tools from the empirical processes theory, we establish strong\nconcentration analysis of the solutions to random linear programs, leading to\nimproved regret analysis under significantly relaxed assumptions. These\ntechniques may find potential applications in broader online decision-making\ncontexts.\n","authors":["Yilun Chen","Wenjia Wang"],"pdf_url":"https://arxiv.org/pdf/2501.01716v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05306v2","updated":"2025-02-13T00:29:05Z","published":"2024-02-07T22:53:54Z","title":"Interactive Symbolic Regression through Offline Reinforcement Learning:\n  A Co-Design Framework","summary":"  Symbolic Regression (SR) holds great potential for uncovering underlying\nmathematical and physical relationships from observed data. However, the vast\ncombinatorial space of possible expressions poses significant challenges for\nboth online search methods and pre-trained transformer models. Additionally,\ncurrent state-of-the-art approaches typically do not consider the integration\nof domain experts' prior knowledge and do not support iterative interactions\nwith the model during the equation discovery process. To address these\nchallenges, we propose the Symbolic Q-network (Sym-Q), an advanced interactive\nframework for large-scale symbolic regression. Unlike previous large-scale\ntransformer-based SR approaches, Sym-Q leverages reinforcement learning without\nrelying on a transformer-based decoder. This formulation allows the agent to\nlearn through offline reinforcement learning using any type of tree encoder,\nenabling more efficient training and inference. Furthermore, we propose a\nco-design mechanism, where the reinforcement learning-based Sym-Q facilitates\neffective interaction with domain experts at any stage of the equation\ndiscovery process. Users can dynamically modify generated nodes of the\nexpression, collaborating with the agent to tailor the mathematical expression\nto best fit the problem and align with the assumed physical laws, particularly\nwhen there is prior partial knowledge of the expected behavior. Our experiments\ndemonstrate that the pre-trained Sym-Q surpasses existing SR algorithms on the\nchallenging SSDNC benchmark. Moreover, we experimentally show on real-world\ncases that its performance can be further enhanced by the interactive co-design\nmechanism, with Sym-Q achieving greater performance gains than other\nstate-of-the-art models. Our reproducible code is available at\nhttps://github.com/EPFL-IMOS/Sym-Q.\n","authors":["Yuan Tian","Wenqi Zhou","Michele Viscione","Hao Dong","David Kammer","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2402.05306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08856v1","updated":"2025-02-13T00:14:55Z","published":"2025-02-13T00:14:55Z","title":"A Systematic Evaluation of Generative Models on Tabular Transportation\n  Data","summary":"  The sharing of large-scale transportation data is beneficial for\ntransportation planning and policymaking. However, it also raises significant\nsecurity and privacy concerns, as the data may include identifiable personal\ninformation, such as individuals' home locations. To address these concerns,\nsynthetic data generation based on real transportation data offers a promising\nsolution that allows privacy protection while potentially preserving data\nutility. Although there are various synthetic data generation techniques, they\nare often not tailored to the unique characteristics of transportation data,\nsuch as the inherent structure of transportation networks formed by all trips\nin the datasets. In this paper, we use New York City taxi data as a case study\nto conduct a systematic evaluation of the performance of widely used tabular\ndata generative models. In addition to traditional metrics such as distribution\nsimilarity, coverage, and privacy preservation, we propose a novel graph-based\nmetric tailored specifically for transportation data. This metric evaluates the\nsimilarity between real and synthetic transportation networks, providing\npotentially deeper insights into their structural and functional alignment. We\nalso introduced an improved privacy metric to address the limitations of the\ncommonly-used one. Our experimental results reveal that existing tabular data\ngenerative models often fail to perform as consistently as claimed in the\nliterature, particularly when applied to transportation data use cases.\nFurthermore, our novel graph metric reveals a significant gap between synthetic\nand real data. This work underscores the potential need to develop generative\nmodels specifically tailored to take advantage of the unique characteristics of\nemerging domains, such as transportation.\n","authors":["Chengen Wang","Alvaro Cardenas","Gurcan Comert","Murat Kantarcioglu"],"pdf_url":"https://arxiv.org/pdf/2502.08856v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.07538v2","updated":"2025-02-13T14:21:12Z","published":"2025-02-11T13:24:38Z","title":"Visual-based spatial audio generation system for multi-speaker\n  environments","summary":"  In multimedia applications such as films and video games, spatial audio\ntechniques are widely employed to enhance user experiences by simulating 3D\nsound: transforming mono audio into binaural formats. However, this process is\noften complex and labor-intensive for sound designers, requiring precise\nsynchronization of audio with the spatial positions of visual components. To\naddress these challenges, we propose a visual-based spatial audio generation\nsystem - an automated system that integrates face detection YOLOv8 for object\ndetection, monocular depth estimation, and spatial audio techniques. Notably,\nthe system operates without requiring additional binaural dataset training. The\nproposed system is evaluated against existing Spatial Audio generation system\nusing objective metrics. Experimental results demonstrate that our method\nsignificantly improves spatial consistency between audio and video, enhances\nspeech quality, and performs robustly in multi-speaker scenarios. By\nstreamlining the audio-visual alignment process, the proposed system enables\nsound engineers to achieve high-quality results efficiently, making it a\nvaluable tool for professionals in multimedia production.\n","authors":["Xiaojing Liu","Ogulcan Gurelli","Yan Wang","Joshua Reiss"],"pdf_url":"https://arxiv.org/pdf/2502.07538v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14755v2","updated":"2025-02-13T03:15:41Z","published":"2024-04-23T05:36:33Z","title":"SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework\n  with Interactive Vision-Language Models","summary":"  With the continuous advancement of vision language models (VLMs) technology,\nremarkable research achievements have emerged in the dermatology field, the\nfourth most prevalent human disease category. However, despite these\nadvancements, VLM still faces explainable problems to user in diagnosis due to\nthe inherent complexity of dermatological conditions, existing tools offer\nrelatively limited support for user comprehension. We propose SkinGEN, a\ndiagnosis-to-generation framework that leverages the stable diffusion(SD) model\nto generate reference demonstrations from diagnosis results provided by VLM,\nthereby enhancing the visual explainability for users. Through extensive\nexperiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for\nskin condition image generation. We conduct a user study with 32 participants\nevaluating both the system performance and explainability. Results demonstrate\nthat SkinGEN significantly improves users' comprehension of VLM predictions and\nfosters increased trust in the diagnostic process. This work paves the way for\nmore transparent and user-centric VLM applications in dermatology and beyond.\n","authors":["Bo Lin","Yingjing Xu","Xuanwen Bao","Zhou Zhao","Zhouyang Wang","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2404.14755v2.pdf","comment":null}]},"2025-02-12T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2502.08836v1","updated":"2025-02-12T22:57:06Z","published":"2025-02-12T22:57:06Z","title":"Survey on Single-Image Reflection Removal using Deep Learning Techniques","summary":"  The phenomenon of reflection is quite common in digital images, posing\nsignificant challenges for various applications such as computer vision,\nphotography, and image processing. Traditional methods for reflection removal\noften struggle to achieve clean results while maintaining high fidelity and\nrobustness, particularly in real-world scenarios. Over the past few decades,\nnumerous deep learning-based approaches for reflection removal have emerged,\nyielding impressive results. In this survey, we conduct a comprehensive review\nof the current literature by focusing on key venues such as ICCV, ECCV, CVPR,\nNeurIPS, etc., as these conferences and journals have been central to advances\nin the field. Our review follows a structured paper selection process, and we\ncritically assess both single-stage and two-stage deep learning methods for\nreflection removal. The contribution of this survey is three-fold: first, we\nprovide a comprehensive summary of the most recent work on single-image\nreflection removal; second, we outline task hypotheses, current deep learning\ntechniques, publicly available datasets, and relevant evaluation metrics; and\nthird, we identify key challenges and opportunities in deep learning-based\nreflection removal, highlighting the potential of this rapidly evolving\nresearch area.\n","authors":["Kangning Yang","Huiming Sun","Jie Cai","Lan Fu","Jiaming Ding","Jinlong Li","Chiu Man Ho","Zibo Meng"],"pdf_url":"https://arxiv.org/pdf/2502.08836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08821v1","updated":"2025-02-12T22:24:49Z","published":"2025-02-12T22:24:49Z","title":"DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with\n  Saliency Maps","summary":"  The recent surge in advanced generative models, such as diffusion models and\ngenerative adversarial networks (GANs), has led to an alarming rise in\nAI-generated images across various domains on the web. While such technologies\noffer benefits such as democratizing artistic creation, they also pose\nchallenges in misinformation, digital forgery, and authenticity verification.\nAdditionally, the uncredited use of AI-generated images in media and marketing\nhas sparked significant backlash from online communities. In response to this,\nwe introduce DejAIvu, a Chrome Web extension that combines real-time\nAI-generated image detection with saliency-based explainability while users\nbrowse the web. Using an ONNX-optimized deep learning model, DejAIvu\nautomatically analyzes images on websites such as Google Images, identifies\nAI-generated content using model inference, and overlays a saliency heatmap to\nhighlight AI-related artifacts. Our approach integrates efficient in-browser\ninference, gradient-based saliency analysis, and a seamless user experience,\nensuring that AI detection is both transparent and interpretable. We also\nevaluate DejAIvu across multiple pretrained architectures and benchmark\ndatasets, demonstrating high accuracy and low latency, making it a practical\nand deployable tool for enhancing AI image accountability. The code for this\nsystem can be found at https://github.com/Noodulz/dejAIvu.\n","authors":["Jocelyn Dzuong"],"pdf_url":"https://arxiv.org/pdf/2502.08821v1.pdf","comment":"5 pages, 3 figures, submitted to IJCAI 2025 demo track"},{"id":"http://arxiv.org/abs/2502.08822v1","updated":"2025-02-12T22:24:49Z","published":"2025-02-12T22:24:49Z","title":"$\\mathsf{CSMAE~}$:~Cataract Surgical Masked Autoencoder (MAE) based\n  Pre-training","summary":"  Automated analysis of surgical videos is crucial for improving surgical\ntraining, workflow optimization, and postoperative assessment. We introduce a\nCSMAE, Masked Autoencoder (MAE)-based pretraining approach, specifically\ndeveloped for Cataract Surgery video analysis, where instead of randomly\nselecting tokens for masking, they are selected based on the spatiotemporal\nimportance of the token. We created a large dataset of cataract surgery videos\nto improve the model's learning efficiency and expand its robustness in\nlow-data regimes. Our pre-trained model can be easily adapted for specific\ndownstream tasks via fine-tuning, serving as a robust backbone for further\nanalysis. Through rigorous testing on a downstream step-recognition task on two\nCataract Surgery video datasets, D99 and Cataract-101, our approach surpasses\ncurrent state-of-the-art self-supervised pretraining and adapter-based transfer\nlearning methods by a significant margin. This advancement not only\ndemonstrates the potential of our MAE-based pretraining in the field of\nsurgical video analysis but also sets a new benchmark for future research.\n","authors":["Nisarg A. Shah","Wele Gedara Chaminda Bandara","Shameema Skider","S. Swaroop Vedula","Vishal M. Patel"],"pdf_url":"https://arxiv.org/pdf/2502.08822v1.pdf","comment":"5 pages, Accepted to IEEE International Symposium on Biomedical\n  Imaging (ISBI 2025)"},{"id":"http://arxiv.org/abs/2502.08813v1","updated":"2025-02-12T21:55:26Z","published":"2025-02-12T21:55:26Z","title":"Measuring Anxiety Levels with Head Motion Patterns in Severe Depression\n  Population","summary":"  Depression and anxiety are prevalent mental health disorders that frequently\ncooccur, with anxiety significantly influencing both the manifestation and\ntreatment of depression. An accurate assessment of anxiety levels in\nindividuals with depression is crucial to develop effective and personalized\ntreatment plans. This study proposes a new noninvasive method for quantifying\nanxiety severity by analyzing head movements -specifically speed, acceleration,\nand angular displacement - during video-recorded interviews with patients\nsuffering from severe depression. Using data from a new CALYPSO Depression\nDataset, we extracted head motion characteristics and applied regression\nanalysis to predict clinically evaluated anxiety levels. Our results\ndemonstrate a high level of precision, achieving a mean absolute error (MAE) of\n0.35 in predicting the severity of psychological anxiety based on head movement\npatterns. This indicates that our approach can enhance the understanding of\nanxiety's role in depression and assist psychiatrists in refining treatment\nstrategies for individuals.\n","authors":["Fouad Boualeb","Emery Pierson","Nicolas Doudeau","Cl√©mence Nineuil","Ali Amad","Mohamed Daoudi"],"pdf_url":"https://arxiv.org/pdf/2502.08813v1.pdf","comment":"19th IEEE International Conference on Automatic Face and Gesture\n  Recognition (FG), 2025"},{"id":"http://arxiv.org/abs/2502.08786v1","updated":"2025-02-12T20:56:54Z","published":"2025-02-12T20:56:54Z","title":"MRUCT: Mixed Reality Assistance for Acupuncture Guided by Ultrasonic\n  Computed Tomography","summary":"  Chinese acupuncture practitioners primarily depend on muscle memory and\ntactile feedback to insert needles and accurately target acupuncture points, as\nthe current workflow lacks imaging modalities and visual aids. Consequently,\nnew practitioners often learn through trial and error, requiring years of\nexperience to become proficient and earn the trust of patients. Medical\nstudents face similar challenges in mastering this skill. To address these\nchallenges, we developed an innovative system, MRUCT, that integrates\nultrasonic computed tomography (UCT) with mixed reality (MR) technology to\nvisualize acupuncture points in real-time. This system offers offline image\nregistration and real-time guidance during needle insertion, enabling them to\naccurately position needles based on anatomical structures such as bones,\nmuscles, and auto-generated reference points, with the potential for clinical\nimplementation. In this paper, we outline the non-rigid registration methods\nused to reconstruct anatomical structures from UCT data, as well as the key\ndesign considerations of the MR system. We evaluated two different 3D user\ninterface (3DUI) designs and compared the performance of our system to\ntraditional workflows for both new practitioners and medical students. The\nresults highlight the potential of MR to enhance therapeutic medical practices\nand demonstrate the effectiveness of the system we developed.\n","authors":["Yue Yang","Xinkai Wang","Kehong Zhou","Xue Xie","Lifeng Zhu","Aiguo Song","Bruce Daniel"],"pdf_url":"https://arxiv.org/pdf/2502.08786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08779v1","updated":"2025-02-12T20:41:53Z","published":"2025-02-12T20:41:53Z","title":"SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models","summary":"  Stereotype biases in Large Multimodal Models (LMMs) perpetuate harmful\nsocietal prejudices, undermining the fairness and equity of AI applications. As\nLMMs grow increasingly influential, addressing and mitigating inherent biases\nrelated to stereotypes, harmful generations, and ambiguous assumptions in\nreal-world scenarios has become essential. However, existing datasets\nevaluating stereotype biases in LMMs often lack diversity and rely on synthetic\nimages, leaving a gap in bias evaluation for real-world visual contexts. To\naddress this, we introduce the Stereotype Bias Benchmark (SB-bench), the most\ncomprehensive framework to date for assessing stereotype biases across nine\ndiverse categories with non-synthetic images. SB-bench rigorously evaluates\nLMMs through carefully curated, visually grounded scenarios, challenging them\nto reason accurately about visual stereotypes. It offers a robust evaluation\nframework featuring real-world visual samples, image variations, and\nmultiple-choice question formats. By introducing visually grounded queries that\nisolate visual biases from textual ones, SB-bench enables a precise and nuanced\nassessment of a model's reasoning capabilities across varying levels of\ndifficulty. Through rigorous testing of state-of-the-art open-source and\nclosed-source LMMs, SB-bench provides a systematic approach to assessing\nstereotype biases in LMMs across key social dimensions. This benchmark\nrepresents a significant step toward fostering fairness in AI systems and\nreducing harmful biases, laying the groundwork for more equitable and socially\nresponsible LMMs. Our code and dataset are publicly available.\n","authors":["Vishal Narnaware","Ashmal Vayani","Rohit Gupta","Swetha Sirnam","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2502.08779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08774v1","updated":"2025-02-12T20:31:47Z","published":"2025-02-12T20:31:47Z","title":"Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal\n  Brain in 3D Ultrasound","summary":"  Monitoring the growth of subcortical regions of the fetal brain in ultrasound\n(US) images can help identify the presence of abnormal development. Manually\nsegmenting these regions is a challenging task, but recent work has shown that\nit can be automated using deep learning. However, applying pretrained models to\nunseen freehand US volumes often leads to a degradation of performance due to\nthe vast differences in acquisition and alignment. In this work, we first\ndemonstrate that test time adaptation (TTA) can be used to improve model\nperformance in the presence of both real and simulated domain shifts. We\nfurther propose a novel TTA method by incorporating a normative atlas as a\nprior for anatomy. In the presence of various types of domain shifts, we\nbenchmark the performance of different TTA methods and demonstrate the\nimprovements brought by our proposed approach, which may further facilitate\nautomated monitoring of fetal brain development. Our code is available at\nhttps://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation.\n","authors":["Joshua Omolegan","Pak Hei Yeung","Madeleine K. Wyburd","Linde Hesse","Monique Haak","Intergrowth-21st Consortium","Ana I. L. Namburete","Nicola K. Dinsdale"],"pdf_url":"https://arxiv.org/pdf/2502.08774v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.08769v1","updated":"2025-02-12T20:17:10Z","published":"2025-02-12T20:17:10Z","title":"Cluster and Predict Latents Patches for Improved Masked Image Modeling","summary":"  Masked Image Modeling (MIM) offers a promising approach to self-supervised\nrepresentation learning, however existing MIM models still lag behind the\nstate-of-the-art. In this paper, we systematically analyze target\nrepresentations, loss functions, and architectures, to introduce CAPI - a novel\npure-MIM framework that relies on the prediction of latent clusterings. Our\napproach leverages a clustering-based loss, which is stable to train, and\nexhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8%\naccuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes,\nsubstantially outperforming previous MIM methods and approaching the\nperformance of the current state-of-the-art, DINOv2. We release all our code\nand models.\n","authors":["Timoth√©e Darcet","Federico Baldassarre","Maxime Oquab","Julien Mairal","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2502.08769v1.pdf","comment":"13 pages, 7 figures, submitted to TMLR"},{"id":"http://arxiv.org/abs/2409.11456v3","updated":"2025-02-12T20:10:41Z","published":"2024-09-17T17:48:12Z","title":"Two Stage Segmentation of Cervical Tumors using PocketNet","summary":"  Cervical cancer remains the fourth most common malignancy amongst women\nworldwide.1 Concurrent chemoradiotherapy (CRT) serves as the mainstay\ndefinitive treatment regimen for locally advanced cervical cancers and includes\nexternal beam radiation followed by brachytherapy.2 Integral to radiotherapy\ntreatment planning is the routine contouring of both the target tumor at the\nlevel of the cervix, associated gynecologic anatomy and the adjacent organs at\nrisk (OARs). However, manual contouring of these structures is both time and\nlabor intensive and associated with known interobserver variability that can\nimpact treatment outcomes. While multiple tools have been developed to\nautomatically segment OARs and the high-risk clinical tumor volume (HR-CTV)\nusing computed tomography (CT) images,3,4,5,6 the development of deep\nlearning-based tumor segmentation tools using routine T2-weighted (T2w)\nmagnetic resonance imaging (MRI) addresses an unmet clinical need to improve\nthe routine contouring of both anatomical structures and cervical cancers,\nthereby increasing quality and consistency of radiotherapy planning. This work\napplied a novel deep-learning model (PocketNet) to segment the cervix, vagina,\nuterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecture\nwas evaluated, when trained on data via five-fold cross validation. PocketNet\nachieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% for\ntumor segmentation and 80% for organ segmentation. Validation on a publicly\navailable dataset from The Cancer Imaging Archive (TCIA) demonstrated the\nmodels robustness, achieving DSC scores of 67.3% for tumor segmentation and\n80.8% for organ segmentation. These results suggest that PocketNet is robust to\nvariations in contrast protocols, providing reliable segmentation of the\nregions of interest.\n","authors":["Awj Twam","Adrian E. Celaya","Megan C. Jacobsen","Rachel Glenn","Peng Wei","Jia Sun","Ann Klopp","Aradhana M. Venkatesan","David Fuentes"],"pdf_url":"https://arxiv.org/pdf/2409.11456v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08754v1","updated":"2025-02-12T19:51:41Z","published":"2025-02-12T19:51:41Z","title":"HistoSmith: Single-Stage Histology Image-Label Generation via\n  Conditional Latent Diffusion for Enhanced Cell Segmentation and\n  Classification","summary":"  Precise segmentation and classification of cell instances are vital for\nanalyzing the tissue microenvironment in histology images, supporting medical\ndiagnosis, prognosis, treatment planning, and studies of brain\ncytoarchitecture. However, the creation of high-quality annotated datasets for\ntraining remains a major challenge. This study introduces a novel single-stage\napproach (HistoSmith) for generating image-label pairs to augment histology\ndatasets. Unlike state-of-the-art methods that utilize diffusion models with\nseparate components for label and image generation, our approach employs a\nlatent diffusion model to learn the joint distribution of cellular layouts,\nclassification masks, and histology images. This model enables tailored data\ngeneration by conditioning on user-defined parameters such as cell types,\nquantities, and tissue types. Trained on the Conic H&E histopathology dataset\nand the Nissl-stained CytoDArk0 dataset, the model generates realistic and\ndiverse labeled samples. Experimental results demonstrate improvements in cell\ninstance segmentation and classification, particularly for underrepresented\ncell types like neutrophils in the Conic dataset. These findings underscore the\npotential of our approach to address data scarcity challenges.\n","authors":["Valentina Vadori","Jean-Marie Gra√Øc","Antonella Peruffo","Livio Finos","Ujwala Kiran Chaudhari","Enrico Grisan"],"pdf_url":"https://arxiv.org/pdf/2502.08754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06034v2","updated":"2025-02-12T19:36:57Z","published":"2025-02-09T21:14:27Z","title":"Traveling Waves Integrate Spatial Information Into Spectral\n  Representations","summary":"  Traveling waves are widely observed in the brain, but their precise\ncomputational function remains unclear. One prominent hypothesis is that they\nenable the transfer and integration of spatial information across neural\npopulations. However, few computational models have explored how traveling\nwaves might be harnessed to perform such integrative processing. Drawing\ninspiration from the famous ``Can one hear the shape of a drum?'' problem --\nwhich highlights how spectral modes encode geometric information -- we\nintroduce a set of convolutional recurrent neural networks that learn to\nproduce traveling waves in their hidden states in response to visual stimuli.\nBy applying a spectral decomposition to these wave-like activations, we obtain\na powerful new representational space that outperforms equivalently local\nfeed-forward networks on tasks requiring global spatial context. In particular,\nwe observe that traveling waves effectively expand the receptive field of\nlocally connected neurons, supporting long-range encoding and communication of\ninformation. We demonstrate that models equipped with this mechanism and\nspectral readouts solve visual semantic segmentation tasks demanding global\nintegration, where local feed-forward models fail. As a first step toward\ntraveling-wave-based representations in artificial networks, our findings\nsuggest potential efficiency benefits and offer a new framework for connecting\nto biological recordings of neural activity.\n","authors":["Mozes Jacobs","Roberto C. Budzinski","Lyle Muller","Demba Ba","T. Anderson Keller"],"pdf_url":"https://arxiv.org/pdf/2502.06034v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21144v4","updated":"2025-02-12T19:20:49Z","published":"2024-10-28T15:44:35Z","title":"Enhancing Learned Image Compression via Cross Window-based Attention","summary":"  In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods. Our code is\npublicly available at https://github.com/prmudgal/CWAM_IC_ISVC. .\n","authors":["Priyanka Mudgal","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21144v4.pdf","comment":"Paper accepted and presented in ISVC'24. Copyrights stay with ISVC\n  Our code is available at: https://github.com/prmudgal/CWAM_IC_ISVC"},{"id":"http://arxiv.org/abs/2406.04341v3","updated":"2025-02-12T19:02:07Z","published":"2024-06-06T17:59:52Z","title":"Interpreting the Second-Order Effects of Neurons in CLIP","summary":"  We interpret the function of individual neurons in CLIP by automatically\ndescribing them using text. Analyzing the direct effects (i.e. the flow from a\nneuron through the residual stream to the output) or the indirect effects\n(overall contribution) fails to capture the neurons' function in CLIP.\nTherefore, we present the \"second-order lens\", analyzing the effect flowing\nfrom a neuron through the later attention heads, directly to the output. We\nfind that these effects are highly selective: for each neuron, the effect is\nsignificant for <2% of the images. Moreover, each effect can be approximated by\na single direction in the text-image space of CLIP. We describe neurons by\ndecomposing these directions into sparse sets of text representations. The sets\nreveal polysemantic behavior - each neuron corresponds to multiple, often\nunrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, we\nmass-produce \"semantic\" adversarial examples by generating images with concepts\nspuriously correlated to the incorrect class. Additionally, we use the\nsecond-order effects for zero-shot segmentation, outperforming previous\nmethods. Our results indicate that an automated interpretation of neurons can\nbe used for model deception and for introducing new model capabilities.\n","authors":["Yossi Gandelsman","Alexei A. Efros","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2406.04341v3.pdf","comment":"project page:\n  https://yossigandelsman.github.io/clip_neurons/index.html"},{"id":"http://arxiv.org/abs/2502.08646v1","updated":"2025-02-12T18:59:43Z","published":"2025-02-12T18:59:43Z","title":"Poly-Autoregressive Prediction for Modeling Interactions","summary":"  We introduce a simple framework for predicting the behavior of an agent in\nmulti-agent settings. In contrast to autoregressive (AR) tasks, such as\nlanguage processing, our focus is on scenarios with multiple agents whose\ninteractions are shaped by physical constraints and internal motivations. To\nthis end, we propose Poly-Autoregressive (PAR) modeling, which forecasts an ego\nagent's future behavior by reasoning about the ego agent's state history and\nthe past and current states of other interacting agents. At its core, PAR\nrepresents the behavior of all agents as a sequence of tokens, each\nrepresenting an agent's state at a specific timestep. With minimal data\npre-processing changes, we show that PAR can be applied to three different\nproblems: human action forecasting in social situations, trajectory prediction\nfor autonomous vehicles, and object pose forecasting during hand-object\ninteraction. Using a small proof-of-concept transformer backbone, PAR\noutperforms AR across these three scenarios. The project website can be found\nat https://neerja.me/PAR/.\n","authors":["Neerja Thakkar","Tara Sadjadpour","Jathushan Rajasegaran","Shiry Ginosar","Jitendra Malik"],"pdf_url":"https://arxiv.org/pdf/2502.08646v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2502.08643v1","updated":"2025-02-12T18:57:22Z","published":"2025-02-12T18:57:22Z","title":"A Real-to-Sim-to-Real Approach to Robotic Manipulation with\n  VLM-Generated Iterative Keypoint Rewards","summary":"  Task specification for robotic manipulation in open-world environments is\nchallenging, requiring flexible and adaptive objectives that align with human\nintentions and can evolve through iterative feedback. We introduce Iterative\nKeypoint Reward (IKER), a visually grounded, Python-based reward function that\nserves as a dynamic task specification. Our framework leverages VLMs to\ngenerate and refine these reward functions for multi-step manipulation tasks.\nGiven RGB-D observations and free-form language instructions, we sample\nkeypoints in the scene and generate a reward function conditioned on these\nkeypoints. IKER operates on the spatial relationships between keypoints,\nleveraging commonsense priors about the desired behaviors, and enabling precise\nSE(3) control. We reconstruct real-world scenes in simulation and use the\ngenerated rewards to train reinforcement learning (RL) policies, which are then\ndeployed into the real world-forming a real-to-sim-to-real loop. Our approach\ndemonstrates notable capabilities across diverse scenarios, including both\nprehensile and non-prehensile tasks, showcasing multi-step task execution,\nspontaneous error recovery, and on-the-fly strategy adjustments. The results\nhighlight IKER's effectiveness in enabling robots to perform multi-step tasks\nin dynamic environments through iterative reward shaping.\n","authors":["Shivansh Patel","Xinchen Yin","Wenlong Huang","Shubham Garg","Hooshang Nayyeri","Li Fei-Fei","Svetlana Lazebnik","Yunzhu Li"],"pdf_url":"https://arxiv.org/pdf/2502.08643v1.pdf","comment":"ICRA 2025, Project Page: https://iker-robot.github.io/"},{"id":"http://arxiv.org/abs/2502.08642v1","updated":"2025-02-12T18:57:12Z","published":"2025-02-12T18:57:12Z","title":"SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation","summary":"  Recent advancements in large vision-language models have enabled highly\nexpressive and diverse vector sketch generation. However, state-of-the-art\nmethods rely on a time-consuming optimization process involving repeated\nfeedback from a pretrained model to determine stroke placement. Consequently,\ndespite producing impressive sketches, these methods are limited in practical\napplications. In this work, we introduce SwiftSketch, a diffusion model for\nimage-conditioned vector sketch generation that can produce high-quality\nsketches in less than a second. SwiftSketch operates by progressively denoising\nstroke control points sampled from a Gaussian distribution. Its\ntransformer-decoder architecture is designed to effectively handle the discrete\nnature of vector representation and capture the inherent global dependencies\nbetween strokes. To train SwiftSketch, we construct a synthetic dataset of\nimage-sketch pairs, addressing the limitations of existing sketch datasets,\nwhich are often created by non-artists and lack professional quality. For\ngenerating these synthetic sketches, we introduce ControlSketch, a method that\nenhances SDS-based techniques by incorporating precise spatial control through\na depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across\ndiverse concepts, efficiently producing sketches that combine high fidelity\nwith a natural and visually appealing style.\n","authors":["Ellie Arar","Yarden Frenkel","Daniel Cohen-Or","Ariel Shamir","Yael Vinker"],"pdf_url":"https://arxiv.org/pdf/2502.08642v1.pdf","comment":"https://swiftsketch.github.io/"},{"id":"http://arxiv.org/abs/2502.08640v1","updated":"2025-02-12T18:55:43Z","published":"2025-02-12T18:55:43Z","title":"Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs","summary":"  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n","authors":["Mantas Mazeika","Xuwang Yin","Rishub Tamirisa","Jaehyuk Lim","Bruce W. Lee","Richard Ren","Long Phan","Norman Mu","Adam Khoja","Oliver Zhang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08639v1","updated":"2025-02-12T18:55:36Z","published":"2025-02-12T18:55:36Z","title":"CineMaster: A 3D-Aware and Controllable Framework for Cinematic\n  Text-to-Video Generation","summary":"  In this work, we present CineMaster, a novel framework for 3D-aware and\ncontrollable text-to-video generation. Our goal is to empower users with\ncomparable controllability as professional film directors: precise placement of\nobjects within the scene, flexible manipulation of both objects and camera in\n3D space, and intuitive layout control over the rendered frames. To achieve\nthis, CineMaster operates in two stages. In the first stage, we design an\ninteractive workflow that allows users to intuitively construct 3D-aware\nconditional signals by positioning object bounding boxes and defining camera\nmovements within the 3D space. In the second stage, these control\nsignals--comprising rendered depth maps, camera trajectories and object class\nlabels--serve as the guidance for a text-to-video diffusion model, ensuring to\ngenerate the user-intended video content. Furthermore, to overcome the scarcity\nof in-the-wild datasets with 3D object motion and camera pose annotations, we\ncarefully establish an automated data annotation pipeline that extracts 3D\nbounding boxes and camera trajectories from large-scale video data. Extensive\nqualitative and quantitative experiments demonstrate that CineMaster\nsignificantly outperforms existing methods and implements prominent 3D-aware\ntext-to-video generation. Project page: https://cinemaster-dev.github.io/.\n","authors":["Qinghe Wang","Yawen Luo","Xiaoyu Shi","Xu Jia","Huchuan Lu","Tianfan Xue","Xintao Wang","Pengfei Wan","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2502.08639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08634v1","updated":"2025-02-12T18:48:12Z","published":"2025-02-12T18:48:12Z","title":"Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale\n  Implicit Neural Representation","summary":"  Purpose: To develop and validate a novel image reconstruction technique using\nimplicit neural representations (INR) for multi-view thick-slice acquisitions\nwhile reducing the scan time but maintaining high signal-to-noise ratio (SNR).\nMethods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised\nneural network-based algorithm designed to reconstruct MRI data from multi-view\nthick slices, effectively reducing scan time by 2-fold while maintaining fine\nanatomical details. We compare our method to both bicubic interpolation and the\ncurrent state-of-the-art regularized least-squares super-resolution\nreconstruction (LS-SRR) technique. Validation is performed using ground-truth\nex-vivo monkey brain data, and we demonstrate superior reconstruction quality\nacross several in-vivo human datasets. Notably, we achieve the reconstruction\nof a whole human brain in-vivo T2-weighted image with an unprecedented\n180{\\mu}m isotropic spatial resolution, accomplished in just 17 minutes of scan\ntime on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in\nterms of reconstruction quality with 22.4% lower relative error (RE) and 7.5%\nlower full-width half maximum (FWHM) indicating better preservation of fine\nstructural details in nearly half the scan time. Conclusion: ROVER-MRI offers\nan efficient and robust approach for mesoscale MR imaging, enabling rapid,\nhigh-resolution whole-brain scans. Its versatility holds great promise for\nresearch applications requiring anatomical details and time-efficient imaging.\n","authors":["Jun Lyu","Lipeng Ning","William Consagra","Qiang Liu","Richard J. Rushmore","Berkin Bilgic","Yogesh Rathi"],"pdf_url":"https://arxiv.org/pdf/2502.08634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04328v2","updated":"2025-02-12T18:40:46Z","published":"2025-02-06T18:59:55Z","title":"Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment","summary":"  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n","authors":["Zuyan Liu","Yuhao Dong","Jiahui Wang","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2502.04328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08625v1","updated":"2025-02-12T18:25:13Z","published":"2025-02-12T18:25:13Z","title":"Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN","summary":"  In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.\n","authors":["Junpeng Zhang","Lei Cheng","Qing Li","Liang Lin","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13147v2","updated":"2025-02-12T18:15:16Z","published":"2024-01-23T23:50:04Z","title":"Deep Spatiotemporal Clutter Filtering of Transthoracic Echocardiographic\n  Images: Leveraging Contextual Attention and Residual Learning","summary":"  This study presents a deep convolutional autoencoder network for filtering\nreverberation clutter from transthoracic echocardiographic (TTE) image\nsequences. Given the spatiotemporal nature of this type of clutter, the\nfiltering network employs 3D convolutional layers to suppress it throughout the\ncardiac cycle. The design of the network incorporates two key features that\ncontribute to the effectiveness of the filter: 1) an attention mechanism for\nfocusing on cluttered regions and leveraging contextual information, and 2)\nresidual learning for preserving fine image structures. To train the network, a\ndiverse set of artifact patterns was simulated and superimposed onto\nultra-realistic synthetic TTE sequences from six ultrasound vendors, generating\ninput for the filtering network. The artifact-free sequences served as\nground-truth. Performance of the filtering network was evaluated using unseen\nsynthetic and in vivo artifactual sequences. Results from the in vivo dataset\nconfirmed the network's strong generalization capabilities, despite being\ntrained solely on synthetic data and simulated artifacts. The suitability of\nthe filtered sequences for downstream processing was assessed by computing\nsegmental strain curves. A significant reduction in the discrepancy between\nstrain profiles computed from cluttered and clutter-free segments was observed\nafter filtering the cluttered sequences with the proposed network. The trained\nnetwork processes a TTE sequence in a fraction of a second, enabling real-time\nclutter filtering and potentially improving the precision of clinically\nrelevant indices derived from TTE sequences. The source code of the proposed\nmethod and example video files of the filtering results are available at:\n\\href{https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main}{https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main}.\n","authors":["Mahdi Tabassian","Somayeh Akbari","Sandro Queir√≥s","Jan D'hooge"],"pdf_url":"https://arxiv.org/pdf/2401.13147v2.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2411.05771v3","updated":"2025-02-12T17:43:56Z","published":"2024-11-08T18:33:03Z","title":"Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems","summary":"  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We then extend\nour sketched EI regularization to develop an accelerated deep internal learning\nframework, Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be\nefficiently applied for single-image and task-adapted reconstruction.\nAdditionally, for network adaptation tasks, we propose a parameter-efficient\napproach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the\nnormalization layers. Our numerical study on X-ray CT and multi-coil MRI image\nreconstruction tasks demonstrate that our approach can achieve significant\ncomputational acceleration over standard EI-based counterpart in single-input\nsetting and network adaptation at test time.\n","authors":["Guixian Xu","Jinglai Li","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2411.05771v3.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2502.08590v1","updated":"2025-02-12T17:24:19Z","published":"2025-02-12T17:24:19Z","title":"Light-A-Video: Training-free Video Relighting via Progressive Light\n  Fusion","summary":"  Recent advancements in image relighting models, driven by large-scale\ndatasets and pre-trained diffusion models, have enabled the imposition of\nconsistent lighting. However, video relighting still lags, primarily due to the\nexcessive training costs and the scarcity of diverse, high-quality video\nrelighting datasets. A simple application of image relighting models on a\nframe-by-frame basis leads to several issues: lighting source inconsistency and\nrelighted appearance inconsistency, resulting in flickers in the generated\nvideos. In this work, we propose Light-A-Video, a training-free approach to\nachieve temporally smooth video relighting. Adapted from image relighting\nmodels, Light-A-Video introduces two key techniques to enhance lighting\nconsistency. First, we design a Consistent Light Attention (CLA) module, which\nenhances cross-frame interactions within the self-attention layers to stabilize\nthe generation of the background lighting source. Second, leveraging the\nphysical principle of light transport independence, we apply linear blending\nbetween the source video's appearance and the relighted appearance, using a\nProgressive Light Fusion (PLF) strategy to ensure smooth temporal transitions\nin illumination. Experiments show that Light-A-Video improves the temporal\nconsistency of relighted video while maintaining the image quality, ensuring\ncoherent lighting transitions across frames. Project page:\nhttps://bujiazi.github.io/light-a-video.github.io/.\n","authors":["Yujie Zhou","Jiazi Bu","Pengyang Ling","Pan Zhang","Tong Wu","Qidong Huang","Jinsong Li","Xiaoyi Dong","Yuhang Zang","Yuhang Cao","Anyi Rao","Jiaqi Wang","Li Niu"],"pdf_url":"https://arxiv.org/pdf/2502.08590v1.pdf","comment":"Project Page: https://bujiazi.github.io/light-a-video.github.io/"},{"id":"http://arxiv.org/abs/2502.08580v1","updated":"2025-02-12T17:11:58Z","published":"2025-02-12T17:11:58Z","title":"Ultrasound Image Generation using Latent Diffusion Models","summary":"  Diffusion models for image generation have been a subject of increasing\ninterest due to their ability to generate diverse, high-quality images. Image\ngeneration has immense potential in medical imaging because open-source medical\nimages are difficult to obtain compared to natural images, especially for rare\nconditions. The generated images can be used later to train classification and\nsegmentation models. In this paper, we propose simulating realistic ultrasound\n(US) images by successive fine-tuning of large diffusion models on different\npublicly available databases. To do so, we fine-tuned Stable Diffusion, a\nstate-of-the-art latent diffusion model, on BUSI (Breast US Images) an\nultrasound breast image dataset. We successfully generated high-quality US\nimages of the breast using simple prompts that specify the organ and pathology,\nwhich appeared realistic to three experienced US scientists and a US\nradiologist. Additionally, we provided user control by conditioning the model\nwith segmentations through ControlNet. We will release the source code at\nhttp://code.sonography.ai/ to allow fast US image generation to the scientific\ncommunity.\n","authors":["Benoit Freiche","Anthony El-Khoury","Ali Nasiri-Sarvi","Mahdi S. Hosseini","Damien Garcia","Adrian Basarab","Mathieu Boily","Hassan Rivaz"],"pdf_url":"https://arxiv.org/pdf/2502.08580v1.pdf","comment":"6 pages conference paper for SPIE medical imaging"},{"id":"http://arxiv.org/abs/2502.08573v1","updated":"2025-02-12T17:07:43Z","published":"2025-02-12T17:07:43Z","title":"A Novel Approach to for Multimodal Emotion Recognition : Multimodal\n  semantic information fusion","summary":"  With the advancement of artificial intelligence and computer vision\ntechnologies, multimodal emotion recognition has become a prominent research\ntopic. However, existing methods face challenges such as heterogeneous data\nfusion and the effective utilization of modality correlations. This paper\nproposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on\nthe integration of contrastive learning and visual sequence compression. The\nproposed method enhances cross-modal feature fusion through contrastive\nlearning and reduces redundancy in the visual modality by leveraging visual\nsequence compression. Experimental results on two public datasets, IEMOCAP and\nMELD, demonstrate that DeepMSI-MER significantly improves the accuracy and\nrobustness of emotion recognition, validating the effectiveness of multimodal\nfeature fusion and the proposed approach.\n","authors":["Wei Dai","Dequan Zheng","Feng Yu","Yanrong Zhang","Yaohui Hou"],"pdf_url":"https://arxiv.org/pdf/2502.08573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08566v1","updated":"2025-02-12T16:56:07Z","published":"2025-02-12T16:56:07Z","title":"AR Glulam: Accurate Augmented Reality Using Multiple Fiducial Markers\n  for Glulam Fabrication","summary":"  Recent advancements in Augmented Reality (AR) have demonstrated applications\nin architecture, design, and fabrication. Compared to conventional 2D\nconstruction drawings, AR can be used to superimpose contextual instructions,\ndisplay 3D spatial information and enable on-site engagement. Despite the\npotential of AR, the widespread adoption of the technology in the industry is\nlimited by its precision. Precision is important for projects requiring strict\nconstruction tolerances, design fidelity, and fabrication feedback. For\nexample, the manufacturing of glulam beams requires tolerances of less than\n2mm. The goal of this project is to explore the industrial application of using\nmultiple fiducial markers for high-precision AR fabrication. While the method\nhas been validated in lab settings with a precision of 0.97, this paper focuses\non fabricating glulam beams in a factory setting with an industry manufacturer,\nUnalam Factory.\n","authors":["Alexander Htet Kyaw","Arvin Xu","Sasa Zivkovic","Gwyllim Jahn","Cameron Newnham","Nick Van Den Berg"],"pdf_url":"https://arxiv.org/pdf/2502.08566v1.pdf","comment":"10 Figures, Project Paper for Association for Computer Aided Design\n  in Architecture"},{"id":"http://arxiv.org/abs/2502.08560v1","updated":"2025-02-12T16:47:41Z","published":"2025-02-12T16:47:41Z","title":"Brain Latent Progression: Individual-based Spatiotemporal Disease\n  Progression on 3D Brain MRIs via Latent Diffusion","summary":"  The growing availability of longitudinal Magnetic Resonance Imaging (MRI)\ndatasets has facilitated Artificial Intelligence (AI)-driven modeling of\ndisease progression, making it possible to predict future medical scans for\nindividual patients. However, despite significant advancements in AI, current\nmethods continue to face challenges including achieving patient-specific\nindividualization, ensuring spatiotemporal consistency, efficiently utilizing\nlongitudinal data, and managing the substantial memory demands of 3D scans. To\naddress these challenges, we propose Brain Latent Progression (BrLP), a novel\nspatiotemporal model designed to predict individual-level disease progression\nin 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates\nin a small latent space, mitigating the computational challenges posed by\nhigh-dimensional imaging data; (ii) it explicitly integrates subject metadata\nto enhance the individualization of predictions; (iii) it incorporates prior\nknowledge of disease dynamics through an auxiliary model, facilitating the\nintegration of longitudinal data; and (iv) it introduces the Latent Average\nStabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in\nthe predicted progression at inference time and (b) allows us to derive a\nmeasure of the uncertainty for the prediction. We train and evaluate BrLP on\n11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its\ngeneralizability on an external test set comprising 2,257 MRIs from 962\nsubjects. Our experiments compare BrLP-generated MRI scans with real follow-up\nMRIs, demonstrating state-of-the-art accuracy compared to existing methods. The\ncode is publicly available at: https://github.com/LemuelPuglisi/BrLP.\n","authors":["Lemuel Puglisi","Daniel C. Alexander","Daniele Rav√¨"],"pdf_url":"https://arxiv.org/pdf/2502.08560v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.03328"},{"id":"http://arxiv.org/abs/2410.19702v2","updated":"2025-02-12T16:47:30Z","published":"2024-10-25T17:19:55Z","title":"TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.\n","authors":["Xiangyu Zeng","Kunchang Li","Chenting Wang","Xinhao Li","Tianxiang Jiang","Ziang Yan","Songze Li","Yansong Shi","Zhengrong Yue","Yi Wang","Yali Wang","Yu Qiao","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19702v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.08556v1","updated":"2025-02-12T16:38:40Z","published":"2025-02-12T16:38:40Z","title":"Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling","summary":"  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n","authors":["Shixiang Tang","Yizhou Wang","Lu Chen","Yuan Wang","Sida Peng","Dan Xu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2502.08556v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08549v1","updated":"2025-02-12T16:30:39Z","published":"2025-02-12T16:30:39Z","title":"Copula-based mixture model identification for subgroup clustering with\n  imaging applications","summary":"  Model-based clustering techniques have been widely applied to various\napplication areas, while most studies focus on canonical mixtures with unique\ncomponent distribution form. However, this strict assumption is often hard to\nsatisfy. In this paper, we consider the more flexible Copula-Based Mixture\nModels (CBMMs) for clustering, which allow heterogeneous component\ndistributions composed by flexible choices of marginal and copula forms. More\nspecifically, we propose an adaptation of the Generalized Iterative Conditional\nEstimation (GICE) algorithm to identify the CBMMs in an unsupervised manner,\nwhere the marginal and copula forms and their parameters are estimated\niteratively. GICE is adapted from its original version developed for switching\nMarkov model identification with the choice of realization time. Our CBMM-GICE\nclustering method is then tested on synthetic two-cluster data (N=2000 samples)\nwith discussion of the factors impacting its convergence. Finally, it is\ncompared to the Expectation Maximization identified mixture models with unique\ncomponent form on the entire MNIST database (N=70000), and on real cardiac\nmagnetic resonance data (N=276) to illustrate its value for imaging\napplications.\n","authors":["Fei Zheng","Nicolas Duchateau"],"pdf_url":"https://arxiv.org/pdf/2502.08549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08540v1","updated":"2025-02-12T16:24:22Z","published":"2025-02-12T16:24:22Z","title":"A Survey on Image Quality Assessment: Insights, Analysis, and Future\n  Outlook","summary":"  Image quality assessment (IQA) represents a pivotal challenge in\nimage-focused technologies, significantly influencing the advancement\ntrajectory of image processing and computer vision. Recently, IQA has witnessed\na notable surge in innovative research efforts, driven by the emergence of\nnovel architectural paradigms and sophisticated computational techniques. This\nsurvey delivers an extensive analysis of contemporary IQA methodologies,\norganized according to their application scenarios, serving as a beneficial\nreference for both beginners and experienced researchers. We analyze the\nadvantages and limitations of current approaches and suggest potential future\nresearch pathways. The survey encompasses both general and specific IQA\nmethodologies, including conventional statistical measures, machine learning\ntechniques, and cutting-edge deep learning models such as convolutional neural\nnetworks (CNNs) and Transformer models. The analysis within this survey\nhighlights the necessity for distortion-specific IQA methods tailored to\nvarious application scenarios, emphasizing the significance of practicality,\ninterpretability, and ease of implementation in future developments.\n","authors":["Chengqian Ma","Zhengyi Shi","Zhiqiang Lu","Shenghao Xie","Fei Chao","Yao Sui"],"pdf_url":"https://arxiv.org/pdf/2502.08540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11172v3","updated":"2025-02-12T16:23:02Z","published":"2024-09-17T13:26:17Z","title":"Annealed Winner-Takes-All for Motion Forecasting","summary":"  In autonomous driving, motion prediction aims at forecasting the future\ntrajectories of nearby agents, helping the ego vehicle to anticipate behaviors\nand drive safely. A key challenge is generating a diverse set of future\npredictions, commonly addressed using data-driven models with Multiple Choice\nLearning (MCL) architectures and Winner-Takes-All (WTA) training objectives.\nHowever, these methods face initialization sensitivity and training\ninstabilities. Additionally, to compensate for limited performance, some\napproaches rely on training with a large set of hypotheses, requiring a\npost-selection step during inference to significantly reduce the number of\npredictions. To tackle these issues, we take inspiration from annealed MCL, a\nrecently introduced technique that improves the convergence properties of MCL\nmethods through an annealed Winner-Takes-All loss (aWTA). In this paper, we\ndemonstrate how the aWTA loss can be integrated with state-of-the-art motion\nforecasting models to enhance their performance using only a minimal set of\nhypotheses, eliminating the need for the cumbersome post-selection step. Our\napproach can be easily incorporated into any trajectory prediction model\nnormally trained using WTA and yields significant improvements. To facilitate\nthe application of our approach to future motion forecasting models, the code\nis made publicly available: https://github.com/valeoai/MF_aWTA.\n","authors":["Yihong Xu","Victor Letzelter","Micka√´l Chen","√âloi Zablocki","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2409.11172v3.pdf","comment":"7 pages, 6 figures, Accepted to ICRA2025"},{"id":"http://arxiv.org/abs/2502.08528v1","updated":"2025-02-12T16:05:46Z","published":"2025-02-12T16:05:46Z","title":"BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image\n  Generation","summary":"  The properties of black holes and accretion flows can be inferred by fitting\nEvent Horizon Telescope (EHT) data to simulated images generated through\ngeneral relativistic ray tracing (GRRT). However, due to the computationally\nintensive nature of GRRT, the efficiency of generating specific radiation flux\nimages needs to be improved. This paper introduces the Branch Correction\nDenoising Diffusion Model (BCDDM), which uses a branch correction mechanism and\na weighted mixed loss function to improve the accuracy of generated black hole\nimages based on seven physical parameters of the radiatively inefficient\naccretion flow (RIAF) model. Our experiments show a strong correlation between\nthe generated images and their physical parameters. By enhancing the GRRT\ndataset with BCDDM-generated images and using ResNet50 for parameter\nregression, we achieve significant improvements in parameter prediction\nperformance. This approach reduces computational costs and provides a faster,\nmore efficient method for dataset expansion, parameter estimation, and model\nfitting.\n","authors":["Ao liu","Zelin Zhang","Songbai Chen","Cuihong Wen"],"pdf_url":"https://arxiv.org/pdf/2502.08528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08486v1","updated":"2025-02-12T15:21:18Z","published":"2025-02-12T15:21:18Z","title":"Referring Remote Sensing Image Segmentation via Bidirectional Alignment\n  Guided Joint Prediction","summary":"  Referring Remote Sensing Image Segmentation (RRSIS) is critical for\necological monitoring, urban planning, and disaster management, requiring\nprecise segmentation of objects in remote sensing imagery guided by textual\ndescriptions. This task is uniquely challenging due to the considerable\nvision-language gap, the high spatial resolution and broad coverage of remote\nsensing imagery with diverse categories and small targets, and the presence of\nclustered, unclear targets with blurred edges. To tackle these issues, we\npropose \\ours, a novel framework designed to bridge the vision-language gap,\nenhance multi-scale feature interaction, and improve fine-grained object\ndifferentiation. Specifically, \\ours introduces: (1) the Bidirectional Spatial\nCorrelation (BSC) for improved vision-language feature alignment, (2) the\nTarget-Background TwinStream Decoder (T-BTD) for precise distinction between\ntargets and non-targets, and (3) the Dual-Modal Object Learning Strategy\n(D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on\nthe benchmark datasets RefSegRS and RRSIS-D demonstrate that \\ours achieves\nstate-of-the-art performance. Specifically, \\ours improves the overall IoU\n(oIoU) by 3.76 percentage points (80.57) and 1.44 percentage points (79.23) on\nthe two datasets, respectively. Additionally, it outperforms previous methods\nin the mean IoU (mIoU) by 5.37 percentage points (67.95) and 1.84 percentage\npoints (66.04), effectively addressing the core challenges of RRSIS with\nenhanced precision and robustness.\n","authors":["Tianxiang Zhang","Zhaokun Wen","Bo Kong","Kecheng Liu","Yisi Zhang","Peixian Zhuang","Jiangyun Li"],"pdf_url":"https://arxiv.org/pdf/2502.08486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08468v1","updated":"2025-02-12T15:03:33Z","published":"2025-02-12T15:03:33Z","title":"mmE5: Improving Multimodal Multilingual Embeddings via High-quality\n  Synthetic Data","summary":"  Multimodal embedding models have gained significant attention for their\nability to map data from different modalities, such as text and images, into a\nunified representation space. However, the limited labeled multimodal data\noften hinders embedding performance. Recent approaches have leveraged data\nsynthesis to address this problem, yet the quality of synthetic data remains a\ncritical bottleneck. In this work, we identify three criteria for high-quality\nsynthetic multimodal data. First, broad scope ensures that the generated data\ncovers diverse tasks and modalities, making it applicable to various downstream\nscenarios. Second, robust cross-modal alignment makes different modalities\nsemantically consistent. Third, high fidelity ensures that the synthetic data\nmaintains realistic details to enhance its reliability. Guided by these\nprinciples, we synthesize datasets that: (1) cover a wide range of tasks,\nmodality combinations, and languages, (2) are generated via a deep thinking\nprocess within a single pass of a multimodal large language model, and (3)\nincorporate real-world images with accurate and relevant texts, ensuring\nfidelity through self-evaluation and refinement. Leveraging these high-quality\nsynthetic and labeled datasets, we train a multimodal multilingual E5 model\nmmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art\nperformance on the MMEB Benchmark and superior multilingual performance on the\nXTD benchmark. Our codes, datasets and models are released in\nhttps://github.com/haon-chen/mmE5.\n","authors":["Haonan Chen","Liang Wang","Nan Yang","Yutao Zhu","Ziliang Zhao","Furu Wei","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2502.08468v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08690v1","updated":"2025-02-12T15:03:26Z","published":"2025-02-12T15:03:26Z","title":"Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient\n  Text-to-Image Generation","summary":"  Large-scale text encoders in text-to-image (T2I) diffusion models have\ndemonstrated exceptional performance in generating high-quality images from\ntextual prompts. Unlike denoising modules that rely on multiple iterative\nsteps, text encoders require only a single forward pass to produce text\nembeddings. However, despite their minimal contribution to total inference time\nand floating-point operations (FLOPs), text encoders demand significantly\nhigher memory usage, up to eight times more than denoising modules. To address\nthis inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet\neffective pruning strategy specifically designed for text encoders in T2I\ndiffusion models. Skrr exploits the inherent redundancy in transformer blocks\nby selectively skipping or reusing certain layers in a manner tailored for T2I\ntasks, thereby reducing memory consumption without compromising performance.\nExtensive experiments demonstrate that Skrr maintains image quality comparable\nto the original model even under high sparsity levels, outperforming existing\nblockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory\nefficiency while preserving performance across multiple evaluation metrics,\nincluding the FID, CLIP, DreamSim, and GenEval scores.\n","authors":["Hoigi Seo","Wongi Jeong","Jae-sun Seo","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2502.08690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07737v2","updated":"2025-02-12T14:50:50Z","published":"2025-02-11T17:57:53Z","title":"Next Block Prediction: Video Generation via Semi-Autoregressive Modeling","summary":"  Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)\nvideo generation, but it suffers from suboptimal unidirectional dependencies\nand slow inference speed. In this work, we propose a semi-autoregressive\n(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.\nBy uniformly decomposing video content into equal-sized blocks (e.g., rows or\nframes), we shift the generation unit from individual tokens to blocks,\nallowing each token in the current block to simultaneously predict the\ncorresponding token in the next block. Unlike traditional AR modeling, our\nframework employs bidirectional attention within each block, enabling tokens to\ncapture more robust spatial dependencies. By predicting multiple tokens in\nparallel, NBP models significantly reduce the number of generation steps,\nleading to faster and more efficient inference. Our model achieves FVD scores\nof 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an\naverage of 4.4. Furthermore, thanks to the reduced number of inference steps,\nthe NBP model generates 8.89 frames (128x128 resolution) per second, achieving\nan 11x speedup. We also explored model scales ranging from 700M to 3B\nparameters, observing significant improvements in generation quality, with FVD\nscores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,\ndemonstrating the scalability of our approach.\n","authors":["Shuhuai Ren","Shuming Ma","Xu Sun","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2502.07737v2.pdf","comment":"project page: https://renshuhuai-andy.github.io/NBP-project/"},{"id":"http://arxiv.org/abs/2502.05240v2","updated":"2025-02-12T14:43:02Z","published":"2025-02-07T12:18:20Z","title":"Survey on AI-Generated Media Detection: From Non-MLLM to MLLM","summary":"  The proliferation of AI-generated media poses significant challenges to\ninformation authenticity and social trust, making reliable detection methods\nhighly demanded. Methods for detecting AI-generated media have evolved rapidly,\nparalleling the advancement of Multimodal Large Language Models (MLLMs).\nCurrent detection approaches can be categorized into two main groups:\nNon-MLLM-based and MLLM-based methods. The former employs high-precision,\ndomain-specific detectors powered by deep learning techniques, while the latter\nutilizes general-purpose detectors based on MLLMs that integrate authenticity\nverification, explainability, and localization capabilities. Despite\nsignificant progress in this field, there remains a gap in literature regarding\na comprehensive survey that examines the transition from domain-specific to\ngeneral-purpose detection methods. This paper addresses this gap by providing a\nsystematic review of both approaches, analyzing them from single-modal and\nmulti-modal perspectives. We present a detailed comparative analysis of these\ncategories, examining their methodological similarities and differences.\nThrough this analysis, we explore potential hybrid approaches and identify key\nchallenges in forgery detection, providing direction for future research.\nAdditionally, as MLLMs become increasingly prevalent in detection tasks,\nethical and security considerations have emerged as critical global concerns.\nWe examine the regulatory landscape surrounding Generative AI (GenAI) across\nvarious jurisdictions, offering valuable insights for researchers and\npractitioners in this field.\n","authors":["Yueying Zou","Peipei Li","Zekun Li","Huaibo Huang","Xing Cui","Xuannan Liu","Chenghanyu Zhang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2502.05240v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2110.14731v3","updated":"2025-02-12T14:06:45Z","published":"2021-10-27T19:33:23Z","title":"Vision Transformer for Classification of Breast Ultrasound Images","summary":"  Medical ultrasound (US) imaging has become a prominent modality for breast\ncancer imaging due to its ease-of-use, low-cost and safety. In the past decade,\nconvolutional neural networks (CNNs) have emerged as the method of choice in\nvision applications and have shown excellent potential in automatic\nclassification of US images. Despite their success, their restricted local\nreceptive field limits their ability to learn global context information.\nRecently, Vision Transformer (ViT) designs that are based on self-attention\nbetween image patches have shown great potential to be an alternative to CNNs.\nIn this study, for the first time, we utilize ViT to classify breast US images\nusing different augmentation strategies. The results are provided as\nclassification accuracy and Area Under the Curve (AUC) metrics, and the\nperformance is compared with the state-of-the-art CNNs. The results indicate\nthat the ViT models have comparable efficiency with or even better than the\nCNNs in classification of US breast images.\n","authors":["Behnaz Gheflati","Hassan Rivaz"],"pdf_url":"https://arxiv.org/pdf/2110.14731v3.pdf","comment":"5 pages, 2 figures, Published in EMBC 2022"},{"id":"http://arxiv.org/abs/2502.08417v1","updated":"2025-02-12T13:59:37Z","published":"2025-02-12T13:59:37Z","title":"Handwritten Text Recognition: A Survey","summary":"  Handwritten Text Recognition (HTR) has become an essential field within\npattern recognition and machine learning, with applications spanning historical\ndocument preservation to modern data entry and accessibility solutions. The\ncomplexity of HTR lies in the high variability of handwriting, which makes it\nchallenging to develop robust recognition systems. This survey examines the\nevolution of HTR models, tracing their progression from early heuristic-based\napproaches to contemporary state-of-the-art neural models, which leverage deep\nlearning techniques. The scope of the field has also expanded, with models\ninitially capable of recognizing only word-level content progressing to recent\nend-to-end document-level approaches. Our paper categorizes existing work into\ntwo primary levels of recognition: (1) \\emph{up to line-level}, encompassing\nword and line recognition, and (2) \\emph{beyond line-level}, addressing\nparagraph- and document-level challenges. We provide a unified framework that\nexamines research methodologies, recent advances in benchmarking, key datasets\nin the field, and a discussion of the results reported in the literature.\nFinally, we identify pressing research challenges and outline promising future\ndirections, aiming to equip researchers and practitioners with a roadmap for\nadvancing the field.\n","authors":["Carlos Garrido-Munoz","Antonio Rios-Vila","Jorge Calvo-Zaragoza"],"pdf_url":"https://arxiv.org/pdf/2502.08417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19604v2","updated":"2025-02-12T13:46:07Z","published":"2024-04-30T14:53:07Z","title":"X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image\n  Using Cross-Sectional Diffusion Models","summary":"  Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but\nhigh-resolution scans are often slow and expensive due to extensive data\nacquisition requirements. Traditional MRI reconstruction methods aim to\nexpedite this process by filling in missing frequency components in the\nK-space, performing 3D-to-3D reconstructions that demand full 3D scans. In\ncontrast, we introduce X-Diffusion, a novel cross-sectional diffusion model\nthat reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain\ninputs, achieving 2D-to-3D reconstruction from as little as a single 2D MRI\nslice or few slices. A key aspect of X-Diffusion is that it models MRI data as\nholistic 3D volumes during the cross-sectional training and inference, unlike\nprevious learning approaches that treat MRI scans as collections of 2D slices\nin standard planes (coronal, axial, sagittal). We evaluated X-Diffusion on\nbrain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank\ndataset. Our results demonstrate that X-Diffusion not only surpasses\nstate-of-the-art methods in quantitative accuracy (PSNR) on unseen data but\nalso preserves critical anatomical features such as tumor profiles, spine\ncurvature, and brain volume. Remarkably, the model generalizes beyond the\ntraining domain, successfully reconstructing knee MRIs despite being trained\nexclusively on brain data. Medical expert evaluations further confirm the\nclinical relevance and fidelity of the generated images.To our knowledge,\nX-Diffusion is the first method capable of producing detailed 3D MRIs from\nhighly limited 2D input data, potentially accelerating MRI acquisition and\nreducing associated costs. The code is available on the project website\nhttps://emmanuelleb985.github.io/XDiffusion/ .\n","authors":["Emmanuelle Bourigault","Abdullah Hamdi","Amir Jamaludin"],"pdf_url":"https://arxiv.org/pdf/2404.19604v2.pdf","comment":"preprint, project website:\n  https://emmanuelleb985.github.io/XDiffusion/"},{"id":"http://arxiv.org/abs/2104.05707v2","updated":"2025-02-12T13:35:59Z","published":"2021-04-12T17:59:22Z","title":"LocalViT: Analyzing Locality in Vision Transformers","summary":"  The aim of this paper is to study the influence of locality mechanisms in\nvision transformers. Transformers originated from machine translation and are\nparticularly good at modelling long-range dependencies within a long sequence.\nAlthough the global interaction between the token embeddings could be well\nmodelled by the self-attention mechanism of transformers, what is lacking is a\nlocality mechanism for information exchange within a local region. In this\npaper, locality mechanism is systematically investigated by carefully designed\ncontrolled experiments. We add locality to vision transformers into the\nfeed-forward network. This seemingly simple solution is inspired by the\ncomparison between feed-forward networks and inverted residual blocks. The\nimportance of locality mechanisms is validated in two ways: 1) A wide range of\ndesign choices (activation function, layer placement, expansion ratio) are\navailable for incorporating locality mechanisms and proper choices can lead to\na performance gain over the baseline, and 2) The same locality mechanism is\nsuccessfully applied to vision transformers with different architecture\ndesigns, which shows the generalization of the locality concept. For\nImageNet2012 classification, the locality-enhanced transformers outperform the\nbaselines Swin-T, DeiT-T, and PVT-T by 1.0%, 2.6% and 3.1% with a negligible\nincrease in the number of parameters and computational effort. Code is\navailable at https://github.com/ofsoundof/LocalViT.\n","authors":["Yawei Li","Kai Zhang","Jiezhang Cao","Radu Timofte","Michele Magno","Luca Benini","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2104.05707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08391v1","updated":"2025-02-12T13:28:46Z","published":"2025-02-12T13:28:46Z","title":"ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for\n  Whole Slide Image Classification","summary":"  Multiple instance learning (MIL)-based framework has become the mainstream\nfor processing the whole slide image (WSI) with giga-pixel size and\nhierarchical image context in digital pathology. However, these methods heavily\ndepend on a substantial number of bag-level labels and solely learn from the\noriginal slides, which are easily affected by variations in data distribution.\nRecently, vision language model (VLM)-based methods introduced the language\nprior by pre-training on large-scale pathological image-text pairs. However,\nthe previous text prompt lacks the consideration of pathological prior\nknowledge, therefore does not substantially boost the model's performance.\nMoreover, the collection of such pairs and the pre-training process are very\ntime-consuming and source-intensive.To solve the above problems, we propose a\ndual-scale vision-language multiple instance learning (ViLa-MIL) framework for\nwhole slide image classification. Specifically, we propose a dual-scale visual\ndescriptive text prompt based on the frozen large language model (LLM) to boost\nthe performance of VLM effectively. To transfer the VLM to process WSI\nefficiently, for the image branch, we propose a prototype-guided patch decoder\nto aggregate the patch features progressively by grouping similar patches into\nthe same prototype; for the text branch, we introduce a context-guided text\ndecoder to enhance the text features by incorporating the multi-granular image\ncontexts. Extensive studies on three multi-cancer and multi-center subtyping\ndatasets demonstrate the superiority of ViLa-MIL.\n","authors":["Jiangbo Shi","Chen Li","Tieliang Gong","Yefeng Zheng","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2502.08391v1.pdf","comment":"CVPR 2024 (Updated version with corrections for typos and errors.)"},{"id":"http://arxiv.org/abs/2502.06581v2","updated":"2025-02-12T13:25:22Z","published":"2025-02-10T15:48:11Z","title":"A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems","summary":"  The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field.\n","authors":["Linxiao Gong","Hao Yang","Gaoyun Fang","Bobo Ju","Juncen Guo","Xiaoguang Zhu","Yan Wang","Xiping Hu","Peng Sun","Azzedine Boukerche"],"pdf_url":"https://arxiv.org/pdf/2502.06581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11959v2","updated":"2025-02-12T13:25:10Z","published":"2024-12-16T16:41:51Z","title":"Gramian Multimodal Representation Learning and Alignment","summary":"  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modalities and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n","authors":["Giordano Cicchetti","Eleonora Grassucci","Luigi Sigillo","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.11959v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.15981v2","updated":"2025-02-12T13:22:54Z","published":"2024-10-21T13:06:38Z","title":"Robust Visual Representation Learning with Multi-modal Prior Knowledge\n  for Image Classification Under Distribution Shift","summary":"  Despite the remarkable success of deep neural networks (DNNs) in computer\nvision, they fail to remain high-performing when facing distribution shifts\nbetween training and testing data. In this paper, we propose Knowledge-Guided\nVisual representation learning (KGV) - a distribution-based learning approach\nleveraging multi-modal prior knowledge - to improve generalization under\ndistribution shift. It integrates knowledge from two distinct modalities: 1) a\nknowledge graph (KG) with hierarchical and association relationships; and 2)\ngenerated synthetic images of visual elements semantically represented in the\nKG. The respective embeddings are generated from the given modalities in a\ncommon latent space, i.e., visual embeddings from original and synthetic images\nas well as knowledge graph embeddings (KGEs). These embeddings are aligned via\na novel variant of translation-based KGE methods, where the node and relation\nembeddings of the KG are modeled as Gaussian distributions and translations,\nrespectively. We claim that incorporating multi-model prior knowledge enables\nmore regularized learning of image representations. Thus, the models are able\nto better generalize across different data distributions. We evaluate KGV on\ndifferent image classification tasks with major or minor distribution shifts,\nnamely road sign classification across datasets from Germany, China, and\nRussia, image classification with the mini-ImageNet dataset and its variants,\nas well as the DVM-CAR dataset. The results demonstrate that KGV consistently\nexhibits higher accuracy and data efficiency across all experiments.\n","authors":["Hongkuan Zhou","Lavdim Halilaj","Sebastian Monka","Stefan Schmid","Yuqicheng Zhu","Bo Xiong","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2410.15981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08377v1","updated":"2025-02-12T13:08:35Z","published":"2025-02-12T13:08:35Z","title":"Not All Frame Features Are Equal: Video-to-4D Generation via Decoupling\n  Dynamic-Static Features","summary":"  Recently, the generation of dynamic 3D objects from a video has shown\nimpressive results. Existing methods directly optimize Gaussians using whole\ninformation in frames. However, when dynamic regions are interwoven with static\nregions within frames, particularly if the static regions account for a large\nproportion, existing methods often overlook information in dynamic regions and\nare prone to overfitting on static regions. This leads to producing results\nwith blurry textures. We consider that decoupling dynamic-static features to\nenhance dynamic representations can alleviate this issue. Thus, we propose a\ndynamic-static feature decoupling module (DSFD). Along temporal axes, it\nregards the portions of current frame features that possess significant\ndifferences relative to reference frame features as dynamic features.\nConversely, the remaining parts are the static features. Then, we acquire\ndecoupled features driven by dynamic features and current frame features.\nMoreover, to further enhance the dynamic representation of decoupled features\nfrom different viewpoints and ensure accurate motion prediction, we design a\ntemporal-spatial similarity fusion module (TSSF). Along spatial axes, it\nadaptively selects a similar information of dynamic regions. Hinging on the\nabove, we construct a novel approach, DS4D. Experimental results verify our\nmethod achieves state-of-the-art (SOTA) results in video-to-4D. In addition,\nthe experiments on a real-world scenario dataset demonstrate its effectiveness\non the 4D scene. Our code will be publicly available.\n","authors":["Liying Yang","Chen Liu","Zhenwei Zhu","Ajian Liu","Hui Ma","Jian Nong","Yanyan Liang"],"pdf_url":"https://arxiv.org/pdf/2502.08377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08374v1","updated":"2025-02-12T13:05:35Z","published":"2025-02-12T13:05:35Z","title":"AdvSwap: Covert Adversarial Perturbation with High Frequency\n  Info-swapping for Autonomous Driving Perception","summary":"  Perception module of Autonomous vehicles (AVs) are increasingly susceptible\nto be attacked, which exploit vulnerabilities in neural networks through\nadversarial inputs, thereby compromising the AI safety. Some researches focus\non creating covert adversarial samples, but existing global noise techniques\nare detectable and difficult to deceive the human visual system. This paper\nintroduces a novel adversarial attack method, AdvSwap, which creatively\nutilizes wavelet-based high-frequency information swapping to generate covert\nadversarial samples and fool the camera. AdvSwap employs invertible neural\nnetwork for selective high-frequency information swapping, preserving both\nforward propagation and data integrity. The scheme effectively removes the\noriginal label data and incorporates the guidance image data, producing\nconcealed and robust adversarial samples. Experimental evaluations and\ncomparisons on the GTSRB and nuScenes datasets demonstrate that AdvSwap can\nmake concealed attacks on common traffic targets. The generates adversarial\nsamples are also difficult to perceive by humans and algorithms. Meanwhile, the\nmethod has strong attacking robustness and attacking transferability.\n","authors":["Yuanhao Huang","Qinfan Zhang","Jiandong Xing","Mengyue Cheng","Haiyang Yu","Yilong Ren","Xiao Xiong"],"pdf_url":"https://arxiv.org/pdf/2502.08374v1.pdf","comment":"27th IEEE International Conference on Intelligent Transportation\n  Systems (ITSC)"},{"id":"http://arxiv.org/abs/2502.08373v1","updated":"2025-02-12T13:05:24Z","published":"2025-02-12T13:05:24Z","title":"Uncertainty Aware Human-machine Collaboration in Camouflaged Object\n  Detection","summary":"  Camouflaged Object Detection (COD), the task of identifying objects concealed\nwithin their environments, has seen rapid growth due to its wide range of\npractical applications. A key step toward developing trustworthy COD systems is\nthe estimation and effective utilization of uncertainty. In this work, we\npropose a human-machine collaboration framework for classifying the presence of\ncamouflaged objects, leveraging the complementary strengths of computer vision\n(CV) models and noninvasive brain-computer interfaces (BCIs). Our approach\nintroduces a multiview backbone to estimate uncertainty in CV model\npredictions, utilizes this uncertainty during training to improve efficiency,\nand defers low-confidence cases to human evaluation via RSVP-based BCIs during\ntesting for more reliable decision-making. We evaluated the framework in the\nCAMO dataset, achieving state-of-the-art results with an average improvement of\n4.56\\% in balanced accuracy (BA) and 3.66\\% in the F1 score compared to\nexisting methods. For the best-performing participants, the improvements\nreached 7.6\\% in BA and 6.66\\% in the F1 score. Analysis of the training\nprocess revealed a strong correlation between our confidence measures and\nprecision, while an ablation study confirmed the effectiveness of the proposed\ntraining policy and the human-machine collaboration strategy. In general, this\nwork reduces human cognitive load, improves system reliability, and provides a\nstrong foundation for advancements in real-world COD applications and\nhuman-computer interaction. Our code and data are available at:\nhttps://github.com/ziyuey/Uncertainty-aware-human-machine-collaboration-in-camouflaged-object-identification.\n","authors":["Ziyue Yang","Kehan Wang","Yuhang Ming","Yong Peng","Han Yang","Qiong Chen","Wanzeng Kong"],"pdf_url":"https://arxiv.org/pdf/2502.08373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08557v3","updated":"2025-02-12T13:02:21Z","published":"2023-03-15T12:18:16Z","title":"Deep Learning for Cross-Domain Few-Shot Visual Recognition: A Survey","summary":"  While deep learning excels in computer vision tasks with abundant labeled\ndata, its performance diminishes significantly in scenarios with limited\nlabeled samples. To address this, Few-shot learning (FSL) enables models to\nperform the target tasks with very few labeled examples by leveraging prior\nknowledge from related tasks. However, traditional FSL assumes that both the\nrelated and target tasks come from the same domain, which is a restrictive\nassumption in many real-world scenarios where domain differences are common. To\novercome this limitation, Cross-domain few-shot learning (CDFSL) has gained\nattention, as it allows source and target data to come from different domains\nand label spaces. This paper presents the first comprehensive review of\nCross-domain Few-shot Learning (CDFSL), a field that has received less\nattention compared to traditional FSL due to its unique challenges. We aim to\nprovide both a position paper and a tutorial for researchers, covering key\nproblems, existing methods, and future research directions. The review begins\nwith a formal definition of CDFSL, outlining its core challenges, followed by a\nsystematic analysis of current approaches, organized under a clear taxonomy.\nFinally, we discuss promising future directions in terms of problem setups,\napplications, and theoretical advancements.\n","authors":["Huali Xu","Shuaifeng Zhi","Shuzhou Sun","Vishal M. Patel","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2303.08557v3.pdf","comment":"Accepted at ACM Computing Surveys; 35 pages, 12 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.08352v1","updated":"2025-02-12T12:27:32Z","published":"2025-02-12T12:27:32Z","title":"Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images\n  with Depth and Normal Supervision","summary":"  With advancements in satellite imaging technology, acquiring high-resolution\nmulti-view satellite imagery has become increasingly accessible, enabling rapid\nand location-independent ground model reconstruction. However, traditional\nstereo matching methods struggle to capture fine details, and while neural\nradiance fields (NeRFs) achieve high-quality reconstructions, their training\ntime is prohibitively long. Moreover, challenges such as low visibility of\nbuilding facades, illumination and style differences between pixels, and weakly\ntextured regions in satellite imagery further make it hard to reconstruct\nreasonable terrain geometry and detailed building facades. To address these\nissues, we propose Sat-DN, a novel framework leveraging a progressively trained\nmulti-resolution hash grid reconstruction architecture with explicit depth\nguidance and surface normal consistency constraints to enhance reconstruction\nquality. The multi-resolution hash grid accelerates training, while the\nprogressive strategy incrementally increases the learning frequency, using\ncoarse low-frequency geometry to guide the reconstruction of fine\nhigh-frequency details. The depth and normal constraints ensure a clear\nbuilding outline and correct planar distribution. Extensive experiments on the\nDFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving\nstate-of-the-art results in both qualitative and quantitative evaluations. The\ncode is available at https://github.com/costune/SatDN.\n","authors":["Tianle Liu","Shuangming Zhao","Wanshou Jiang","Bingxuan Guo"],"pdf_url":"https://arxiv.org/pdf/2502.08352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08347v1","updated":"2025-02-12T12:14:02Z","published":"2025-02-12T12:14:02Z","title":"Hi-End-MAE: Hierarchical encoder-driven masked autoencoders are stronger\n  vision learners for medical image segmentation","summary":"  Medical image segmentation remains a formidable challenge due to the label\nscarcity. Pre-training Vision Transformer (ViT) through masked image modeling\n(MIM) on large-scale unlabeled medical datasets presents a promising solution,\nproviding both computational efficiency and model generalization for various\ndownstream tasks. However, current ViT-based MIM pre-training frameworks\npredominantly emphasize local aggregation representations in output layers and\nfail to exploit the rich representations across different ViT layers that\nbetter capture fine-grained semantic information needed for more precise\nmedical downstream tasks. To fill the above gap, we hereby present Hierarchical\nEncoder-driven MAE (Hi-End-MAE), a simple yet effective ViT-based pre-training\nsolution, which centers on two key innovations: (1) Encoder-driven\nreconstruction, which encourages the encoder to learn more informative features\nto guide the reconstruction of masked patches; and (2) Hierarchical dense\ndecoding, which implements a hierarchical decoding structure to capture rich\nrepresentations across different layers. We pre-train Hi-End-MAE on a\nlarge-scale dataset of 10K CT scans and evaluated its performance across seven\npublic medical image segmentation benchmarks. Extensive experiments demonstrate\nthat Hi-End-MAE achieves superior transfer learning capabilities across various\ndownstream tasks, revealing the potential of ViT in medical imaging\napplications. The code is available at:\nhttps://github.com/FengheTan9/Hi-End-MAE\n","authors":["Fenghe Tang","Qingsong Yao","Wenxin Ma","Chenxu Wu","Zihang Jiang","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08347v1.pdf","comment":"19 pages, Code: https://github.com/FengheTan9/Hi-End-MAE"},{"id":"http://arxiv.org/abs/2502.08333v1","updated":"2025-02-12T11:57:11Z","published":"2025-02-12T11:57:11Z","title":"Foundation Models in Computational Pathology: A Review of Challenges,\n  Opportunities, and Impact","summary":"  From self-supervised, vision-only models to contrastive visual-language\nframeworks, computational pathology has rapidly evolved in recent years.\nGenerative AI \"co-pilots\" now demonstrate the ability to mine subtle,\nsub-visual tissue cues across the cellular-to-pathology spectrum, generate\ncomprehensive reports, and respond to complex user queries. The scale of data\nhas surged dramatically, growing from tens to millions of multi-gigapixel\ntissue images, while the number of trainable parameters in these models has\nrisen to several billion. The critical question remains: how will this new wave\nof generative and multi-purpose AI transform clinical diagnostics? In this\narticle, we explore the true potential of these innovations and their\nintegration into clinical practice. We review the rapid progress of foundation\nmodels in pathology, clarify their applications and significance. More\nprecisely, we examine the very definition of foundational models, identifying\nwhat makes them foundational, general, or multipurpose, and assess their impact\non computational pathology. Additionally, we address the unique challenges\nassociated with their development and evaluation. These models have\ndemonstrated exceptional predictive and generative capabilities, but\nestablishing global benchmarks is crucial to enhancing evaluation standards and\nfostering their widespread clinical adoption. In computational pathology, the\nbroader impact of frontier AI ultimately depends on widespread adoption and\nsocietal acceptance. While direct public exposure is not strictly necessary, it\nremains a powerful tool for dispelling misconceptions, building trust, and\nsecuring regulatory support.\n","authors":["Mohsin Bilal"," Aadam","Manahil Raza","Youssef Altherwy","Anas Alsuhaibani","Abdulrahman Abduljabbar","Fahdah Almarshad","Paul Golding","Nasir Rajpoot"],"pdf_url":"https://arxiv.org/pdf/2502.08333v1.pdf","comment":"63 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.08321v1","updated":"2025-02-12T11:37:35Z","published":"2025-02-12T11:37:35Z","title":"Screener: Self-supervised Pathology Segmentation Model for 3D Medical\n  Images","summary":"  Accurate segmentation of all pathological findings in 3D medical images\nremains a significant challenge, as supervised models are limited to detecting\nonly the few pathology classes annotated in existing datasets. To address this,\nwe frame pathology segmentation as an unsupervised visual anomaly segmentation\n(UVAS) problem, leveraging the inherent rarity of pathological patterns\ncompared to healthy ones. We enhance the existing density-based UVAS framework\nwith two key innovations: (1) dense self-supervised learning (SSL) for feature\nextraction, eliminating the need for supervised pre-training, and (2) learned,\nmasking-invariant dense features as conditioning variables, replacing\nhand-crafted positional encodings. Trained on over 30,000 unlabeled 3D CT\nvolumes, our model, Screener, outperforms existing UVAS methods on four\nlarge-scale test datasets comprising 1,820 scans with diverse pathologies. Code\nand pre-trained models will be made publicly available.\n","authors":["Mikhail Goncharov","Eugenia Soboleva","Mariia Donskova","Ivan Oseledets","Marina Munkhoeva","Maxim Panov"],"pdf_url":"https://arxiv.org/pdf/2502.08321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08317v1","updated":"2025-02-12T11:32:19Z","published":"2025-02-12T11:32:19Z","title":"Mitigating Hallucinations in Multimodal Spatial Relations through\n  Constraint-Aware Prompting","summary":"  Spatial relation hallucinations pose a persistent challenge in large\nvision-language models (LVLMs), leading to generate incorrect predictions about\nobject positions and spatial configurations within an image. To address this\nissue, we propose a constraint-aware prompting framework designed to reduce\nspatial relation hallucinations. Specifically, we introduce two types of\nconstraints: (1) bidirectional constraint, which ensures consistency in\npairwise object relations, and (2) transitivity constraint, which enforces\nrelational dependence across multiple objects. By incorporating these\nconstraints, LVLMs can produce more spatially coherent and consistent outputs.\nWe evaluate our method on three widely-used spatial relation datasets,\ndemonstrating performance improvements over existing approaches. Additionally,\na systematic analysis of various bidirectional relation analysis choices and\ntransitivity reference selections highlights greater possibilities of our\nmethods in incorporating constraints to mitigate spatial relation\nhallucinations.\n","authors":["Jiarui Wu","Zhuo Liu","Hangfeng He"],"pdf_url":"https://arxiv.org/pdf/2502.08317v1.pdf","comment":"19 pages, accepted to NAACL Findings"},{"id":"http://arxiv.org/abs/2405.08431v5","updated":"2025-02-12T11:21:50Z","published":"2024-05-14T08:51:16Z","title":"Similarity and Quality Metrics for MR Image-To-Image Translation","summary":"  Image-to-image translation can create large impact in medical imaging, as\nimages can be synthetically transformed to other modalities, sequence types,\nhigher resolutions or lower noise levels. To ensure patient safety, these\nmethods should be validated by human readers, which requires a considerable\namount of time and costs. Quantitative metrics can effectively complement such\nstudies and provide reproducible and objective assessment of synthetic images.\nIf a reference is available, the similarity of MR images is frequently\nevaluated by SSIM and PSNR metrics, even though these metrics are not or too\nsensitive regarding specific distortions. When reference images to compare with\nare not available, non-reference quality metrics can reliably detect specific\ndistortions, such as blurriness. To provide an overview on distortion\nsensitivity, we quantitatively analyze 11 similarity (reference) and 12 quality\n(non-reference) metrics for assessing synthetic images. We additionally include\na metric on a downstream segmentation task. We investigate the sensitivity\nregarding 11 kinds of distortions and typical MR artifacts, and analyze the\ninfluence of different normalization methods on each metric and distortion.\nFinally, we derive recommendations for effective usage of the analyzed\nsimilarity and quality metrics for evaluation of image-to-image translation\nmodels.\n","authors":["Melanie Dohmen","Mark A. Klemens","Ivo M. Baltruschat","Tuan Truong","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2405.08431v5.pdf","comment":"44 pages (main: 22 pages, 3 figures, supplement: 22 pages, 15\n  figures)"},{"id":"http://arxiv.org/abs/2407.21416v3","updated":"2025-02-12T11:15:25Z","published":"2024-07-31T08:04:32Z","title":"VIPeR: Visual Incremental Place Recognition with Adaptive Mining and\n  Continual Learning","summary":"  Visual place recognition (VPR) is an essential component of many autonomous\nand augmented/virtual reality systems. It enables the systems to robustly\nlocalize themselves in large-scale environments. Existing VPR methods\ndemonstrate attractive performance at the cost of heavy pre-training and\nlimited generalizability. When deployed in unseen environments, these methods\nexhibit significant performance drops. Targeting this issue, we present VIPeR,\na novel approach for visual incremental place recognition with the ability to\nadapt to new environments while retaining the performance of previous\nenvironments. We first introduce an adaptive mining strategy that balances the\nperformance within a single environment and the generalizability across\nmultiple environments. Then, to prevent catastrophic forgetting in lifelong\nlearning, we draw inspiration from human memory systems and design a novel\nmemory bank for our VIPeR. Our memory bank contains a sensory memory, a working\nmemory and a long-term memory, with the first two focusing on the current\nenvironment and the last one for all previously visited environments.\nAdditionally, we propose a probabilistic knowledge distillation to explicitly\nsafeguard the previously learned knowledge. We evaluate our proposed VIPeR on\nthree large-scale datasets, namely Oxford Robotcar, Nordland, and TartanAir.\nFor comparison, we first set a baseline performance with naive finetuning.\nThen, several more recent lifelong learning methods are compared. Our VIPeR\nachieves better performance in almost all aspects with the biggest improvement\nof 13.65% in average performance.\n","authors":["Yuhang Ming","Minyang Xu","Xingrui Yang","Weicai Ye","Weihan Wang","Yong Peng","Weichen Dai","Wanzeng Kong"],"pdf_url":"https://arxiv.org/pdf/2407.21416v3.pdf","comment":"8 pages, 4 figures. In IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2502.08297v1","updated":"2025-02-12T10:58:09Z","published":"2025-02-12T10:58:09Z","title":"BEAM: Bridging Physically-based Rendering and Gaussian Modeling for\n  Relightable Volumetric Video","summary":"  Volumetric video enables immersive experiences by capturing dynamic 3D\nscenes, enabling diverse applications for virtual reality, education, and\ntelepresence. However, traditional methods struggle with fixed lighting\nconditions, while neural approaches face trade-offs in efficiency, quality, or\nadaptability for relightable scenarios. To address these limitations, we\npresent BEAM, a novel pipeline that bridges 4D Gaussian representations with\nphysically-based rendering (PBR) to produce high-quality, relightable\nvolumetric videos from multi-view RGB footage. BEAM recovers detailed geometry\nand PBR properties via a series of available Gaussian-based techniques. It\nfirst combines Gaussian-based performance tracking with geometry-aware\nrasterization in a coarse-to-fine optimization framework to recover spatially\nand temporally consistent geometries. We further enhance Gaussian attributes by\nincorporating PBR properties step by step. We generate roughness via a\nmulti-view-conditioned diffusion model, and then derive AO and base color using\na 2D-to-3D strategy, incorporating a tailored Gaussian-based ray tracer for\nefficient visibility computation. Once recovered, these dynamic, relightable\nassets integrate seamlessly into traditional CG pipelines, supporting real-time\nrendering with deferred shading and offline rendering with ray tracing. By\noffering realistic, lifelike visualizations under diverse lighting conditions,\nBEAM opens new possibilities for interactive entertainment, storytelling, and\ncreative visualization.\n","authors":["Yu Hong","Yize Wu","Zhehao Shen","Chengcheng Guo","Yuheng Jiang","Yingliang Zhang","Jingyi Yu","Lan Xu"],"pdf_url":"https://arxiv.org/pdf/2502.08297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09388v2","updated":"2025-02-12T10:55:54Z","published":"2024-12-12T15:56:20Z","title":"All You Need in Knowledge Distillation Is a Tailored Coordinate System","summary":"  Knowledge Distillation (KD) is essential in transferring dark knowledge from\na large teacher to a small student network, such that the student can be much\nmore efficient than the teacher but with comparable accuracy. Existing KD\nmethods, however, rely on a large teacher trained specifically for the target\ntask, which is both very inflexible and inefficient. In this paper, we argue\nthat a SSL-pretrained model can effectively act as the teacher and its dark\nknowledge can be captured by the coordinate system or linear subspace where the\nfeatures lie in. We then need only one forward pass of the teacher, and then\ntailor the coordinate system (TCS) for the student network. Our TCS method is\nteacher-free and applies to diverse architectures, works well for KD and\npractical few-shot learning, and allows cross-architecture distillation with\nlarge capacity gap. Experiments show that TCS achieves significantly higher\naccuracy than state-of-the-art KD methods, while only requiring roughly half of\ntheir training time and GPU memory costs.\n","authors":["Junjie Zhou","Ke Zhu","Jianxin Wu"],"pdf_url":"https://arxiv.org/pdf/2412.09388v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2501.04304v2","updated":"2025-02-12T10:49:40Z","published":"2025-01-08T06:30:31Z","title":"DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion\n  Models","summary":"  Despite the widespread use of text-to-image diffusion models across various\ntasks, their computational and memory demands limit practical applications. To\nmitigate this issue, quantization of diffusion models has been explored. It\nreduces memory usage and computational costs by compressing weights and\nactivations into lower-bit formats. However, existing methods often struggle to\npreserve both image quality and text-image alignment, particularly in\nlower-bit($<$ 8bits) quantization. In this paper, we analyze the challenges\nassociated with quantizing text-to-image diffusion models from a distributional\nperspective. Our analysis reveals that activation outliers play a crucial role\nin determining image quality. Additionally, we identify distinctive patterns in\ncross-attention scores, which significantly affects text-image alignment. To\naddress these challenges, we propose Distribution-aware Group Quantization\n(DGQ), a method that identifies and adaptively handles pixel-wise and\nchannel-wise outliers to preserve image quality. Furthermore, DGQ applies\nprompt-specific logarithmic quantization scales to maintain text-image\nalignment. Our method demonstrates remarkable performance on datasets such as\nMS-COCO and PartiPrompts. We are the first to successfully achieve low-bit\nquantization of text-to-image diffusion models without requiring additional\nfine-tuning of weight quantization parameters. Code is available at\nhttps://github.com/ugonfor/DGQ.\n","authors":["Hyogon Ryu","NaHyeon Park","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2501.04304v2.pdf","comment":"Accepted ICLR 2025. Project page: https://ugonfor.kr/DGQ"},{"id":"http://arxiv.org/abs/2502.08287v1","updated":"2025-02-12T10:44:45Z","published":"2025-02-12T10:44:45Z","title":"CRISP: A Framework for Cryo-EM Image Segmentation and Processing with\n  Conditional Random Field","summary":"  Differentiating signals from the background in micrographs is a critical\ninitial step for cryogenic electron microscopy (cryo-EM), yet it remains\nlaborious due to low signal-to-noise ratio (SNR), the presence of contaminants\nand densely packed particles of varying sizes. Although image segmentation has\nrecently been introduced to distinguish particles at the pixel level, the low\nSNR complicates the automated generation of accurate annotations for training\nsupervised models. Moreover, platforms for systematically comparing different\ndesign choices in pipeline construction are lacking. Thus, a modular framework\nis essential to understand the advantages and limitations of this approach and\ndrive further development. To address these challenges, we present a pipeline\nthat automatically generates high-quality segmentation maps from cryo-EM data\nto serve as ground truth labels. Our modular framework enables the selection of\nvarious segmentation models and loss functions. We also integrate Conditional\nRandom Fields (CRFs) with different solvers and feature sets to refine coarse\npredictions, thereby producing fine-grained segmentation. This flexibility\nfacilitates optimal configurations tailored to cryo-EM datasets. When trained\non a limited set of micrographs, our approach achieves over 90% accuracy,\nrecall, precision, Intersection over Union (IoU), and F1-score on synthetic\ndata. Furthermore, to demonstrate our framework's efficacy in downstream\nanalyses, we show that the particles extracted by our pipeline produce 3D\ndensity maps with higher resolution than those generated by existing particle\npickers on real experimental datasets, while achieving performance comparable\nto that of manually curated datasets from experts.\n","authors":["Szu-Chi Chung","Po-Cheng Chou"],"pdf_url":"https://arxiv.org/pdf/2502.08287v1.pdf","comment":"31 pages, 28 Figures"},{"id":"http://arxiv.org/abs/2502.08285v1","updated":"2025-02-12T10:44:36Z","published":"2025-02-12T10:44:36Z","title":"Fully-Geometric Cross-Attention for Point Cloud Registration","summary":"  Point cloud registration approaches often fail when the overlap between point\nclouds is low due to noisy point correspondences. This work introduces a novel\ncross-attention mechanism tailored for Transformer-based architectures that\ntackles this problem, by fusing information from coordinates and features at\nthe super-point level between point clouds. This formulation has remained\nunexplored primarily because it must guarantee rotation and translation\ninvariance since point clouds reside in different and independent reference\nframes. We integrate the Gromov-Wasserstein distance into the cross-attention\nformulation to jointly compute distances between points across different point\nclouds and account for their geometric structure. By doing so, points from two\ndistinct point clouds can attend to each other under arbitrary rigid\ntransformations. At the point level, we also devise a self-attention mechanism\nthat aggregates the local geometric structure information into point features\nfor fine matching. Our formulation boosts the number of inlier correspondences,\nthereby yielding more precise registration results compared to state-of-the-art\napproaches. We have conducted an extensive evaluation on 3DMatch, 3DLoMatch,\nKITTI, and 3DCSR datasets.\n","authors":["Weijie Wang","Guofeng Mei","Jian Zhang","Nicu Sebe","Bruno Lepri","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2502.08285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19270v2","updated":"2025-02-12T10:37:04Z","published":"2023-05-30T17:59:32Z","title":"Learning without Forgetting for Vision-Language Models","summary":"  Class-Incremental Learning (CIL) or continual learning is a desired\ncapability in the real world, which requires a learning system to adapt to new\ntasks without forgetting former ones. While traditional CIL methods focus on\nvisual information to grasp core features, recent advances in Vision-Language\nModels (VLM) have shown promising capabilities in learning generalizable\nrepresentations with the aid of textual information. However, when continually\ntrained with new classes, VLMs often suffer from catastrophic forgetting of\nformer knowledge. Applying VLMs to CIL poses two major challenges: 1) how to\nadapt the model without forgetting; and 2) how to make full use of the\nmulti-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that\nenables VLMs to learn without forgetting. To handle the first challenge, we\npropose training task-specific projections based on the frozen image/text\nencoders. When facing new tasks, new projections are expanded and former\nprojections are fixed, alleviating the forgetting of old concepts. For the\nsecond challenge, we propose the fusion module to better utilize the\ncross-modality information. By jointly adjusting visual and textual features,\nthe model can capture semantic information with stronger representation\nability. Extensive experiments on nine benchmark datasets validate PROOF\nachieves state-of-the-art performance. Code is available at\nhttps://github.com/zhoudw-zdw/PROOF\n","authors":["Da-Wei Zhou","Yuanhan Zhang","Yan Wang","Jingyi Ning","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2305.19270v2.pdf","comment":"Accepted to TPAMI. Code is available at\n  https://github.com/zhoudw-zdw/PROOF"},{"id":"http://arxiv.org/abs/2502.08279v1","updated":"2025-02-12T10:36:55Z","published":"2025-02-12T10:36:55Z","title":"What Is That Talk About? A Video-to-Text Summarization Dataset for\n  Scientific Presentations","summary":"  Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of scientific video summarization.\n","authors":["Dongqi Liu","Chenxi Whitehouse","Xi Yu","Louis Mahon","Rohit Saxena","Zheng Zhao","Yifu Qiu","Mirella Lapata","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2502.08279v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2306.02873 by other authors"},{"id":"http://arxiv.org/abs/2309.17170v2","updated":"2025-02-12T10:09:27Z","published":"2023-09-29T12:07:08Z","title":"Robotic Grasping of Harvested Tomato Trusses Using Vision and Online\n  Learning","summary":"  Currently, truss tomato weighing and packaging require significant manual\nwork. The main obstacle to automation lies in the difficulty of developing a\nreliable robotic grasping system for already harvested trusses. We propose a\nmethod to grasp trusses that are stacked in a crate with considerable clutter,\nwhich is how they are commonly stored and transported after harvest. The method\nconsists of a deep learning-based vision system to first identify the\nindividual trusses in the crate and then determine a suitable grasping location\non the stem. To this end, we have introduced a grasp pose ranking algorithm\nwith online learning capabilities. After selecting the most promising grasp\npose, the robot executes a pinch grasp without needing touch sensors or\ngeometric models. Lab experiments with a robotic manipulator equipped with an\neye-in-hand RGB-D camera showed a 100% clearance rate when tasked to pick all\ntrusses from a pile. 93% of the trusses were successfully grasped on the first\ntry, while the remaining 7% required more attempts.\n","authors":["Luuk van den Bent","Tom√°s Coleman","Robert Babu≈°ka"],"pdf_url":"https://arxiv.org/pdf/2309.17170v2.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.06782v2","updated":"2025-02-12T10:07:07Z","published":"2025-02-10T18:58:11Z","title":"Lumina-Video: Efficient and Flexible Video Generation with Multi-scale\n  Next-DiT","summary":"  Recent advancements have established Diffusion Transformers (DiTs) as a\ndominant framework in generative modeling. Building on this success,\nLumina-Next achieves exceptional performance in the generation of\nphotorealistic images with Next-DiT. However, its potential for video\ngeneration remains largely untapped, with significant challenges in modeling\nthe spatiotemporal complexity inherent to video data. To address this, we\nintroduce Lumina-Video, a framework that leverages the strengths of Next-DiT\nwhile introducing tailored solutions for video synthesis. Lumina-Video\nincorporates a Multi-scale Next-DiT architecture, which jointly learns multiple\npatchifications to enhance both efficiency and flexibility. By incorporating\nthe motion score as an explicit condition, Lumina-Video also enables direct\ncontrol of generated videos' dynamic degree. Combined with a progressive\ntraining scheme with increasingly higher resolution and FPS, and a multi-source\ntraining scheme with mixed natural and synthetic data, Lumina-Video achieves\nremarkable aesthetic quality and motion smoothness at high training and\ninference efficiency. We additionally propose Lumina-V2A, a video-to-audio\nmodel based on Next-DiT, to create synchronized sounds for generated videos.\nCodes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.\n","authors":["Dongyang Liu","Shicheng Li","Yutong Liu","Zhen Li","Kai Wang","Xinyue Li","Qi Qin","Yufei Liu","Yi Xin","Zhongyu Li","Bin Fu","Chenyang Si","Yuewen Cao","Conghui He","Ziwei Liu","Yu Qiao","Qibin Hou","Hongsheng Li","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2502.06782v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10957v2","updated":"2025-02-12T10:06:09Z","published":"2025-01-19T06:11:02Z","title":"MARIO: A Mixed Annotation Framework For Polyp Segmentation","summary":"  Existing polyp segmentation models are limited by high labeling costs and the\nsmall size of datasets. Additionally, vast polyp datasets remain underutilized\nbecause these models typically rely on a single type of annotation. To address\nthis dilemma, we introduce MARIO, a mixed supervision model designed to\naccommodate various annotation types, significantly expanding the range of\nusable data. MARIO learns from underutilized datasets by incorporating five\nforms of supervision: pixel-level, box-level, polygon-level, scribblelevel, and\npoint-level. Each form of supervision is associated with a tailored loss that\neffectively leverages the supervision labels while minimizing the noise. This\nallows MARIO to move beyond the constraints of relying on a single annotation\ntype. Furthermore, MARIO primarily utilizes dataset with weak and cheap\nannotations, reducing the dependence on large-scale, fully annotated ones.\nExperimental results across five benchmark datasets demonstrate that MARIO\nconsistently outperforms existing methods, highlighting its efficacy in\nbalancing trade-offs between different forms of supervision and maximizing\npolyp segmentation performance\n","authors":["Haoyang Li","Yiwen Hu","Jun Wei","Zhen Li"],"pdf_url":"https://arxiv.org/pdf/2501.10957v2.pdf","comment":"Accepted by IEEE ISBI 2025 4-page paper"},{"id":"http://arxiv.org/abs/2411.01969v2","updated":"2025-02-12T09:58:15Z","published":"2024-11-04T10:44:46Z","title":"Toddlers' Active Gaze Behavior Supports Self-Supervised Object Learning","summary":"  Toddlers learn to recognize objects from different viewpoints with almost no\nsupervision. Recent works argue that toddlers develop this ability by mapping\nclose-in-time visual inputs to similar representations while interacting with\nobjects. High acuity vision is only available in the central visual field,\nwhich May explain why toddlers (much like adults) constantly move around their\ngaze during such interactions. It is unclear whether/how much toddlers curate\ntheir visual experience through these eye movements to support their learning\nof object representations. In this work, we explore whether a bio-inspired\nvisual learning model can harness toddlers' gaze behavior during a play session\nto develop view-invariant object recognition. Exploiting head-mounted eye\ntracking during dyadic play, we simulate toddlers' central visual field\nexperience by cropping image regions centered on the gaze location. This visual\nstream feeds time-based self-supervised learning algorithms. Our experiments\ndemonstrate that toddlers' gaze strategy supports the learning of invariant\nobject representations. Our analysis also reveals that the limited size of the\ncentral visual field where acuity is high is crucial for this. We further find\nthat toddlers' visual experience elicits more robust representations compared\nto adults', mostly because toddlers look at objects they hold themselves for\nlonger bouts. Overall, our work reveals how toddlers' gaze behavior supports\nself-supervised learning of view-invariant object recognition.\n","authors":["Zhengyang Yu","Arthur Aubret","Marcel C. Raabe","Jane Yang","Chen Yu","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2411.01969v2.pdf","comment":"20 pages, 15 figures"},{"id":"http://arxiv.org/abs/2404.02544v3","updated":"2025-02-12T09:52:45Z","published":"2024-04-03T08:01:00Z","title":"Semi-Supervised Unconstrained Head Pose Estimation in the Wild","summary":"  Existing research on unconstrained in-the-wild head pose estimation suffers\nfrom the flaws of its datasets, which consist of either numerous samples by\nnon-realistic synthesis or constrained collection, or small-scale natural\nimages yet with plausible manual annotations. This makes fully-supervised\nsolutions compromised due to the reliance on generous labels. To alleviate it,\nwe propose the first semi-supervised unconstrained head pose estimation method\nSemiUHPE, which can leverage abundant easily available unlabeled head images.\nTechnically, we choose semi-supervised rotation regression and adapt it to the\nerror-sensitive and label-scarce problem of unconstrained head pose. Our method\nis based on the observation that the aspect-ratio invariant cropping of wild\nheads is superior to previous landmark-based affine alignment given that\nlandmarks of unconstrained human heads are usually unavailable, especially for\nunderexplored non-frontal heads. Instead of using a pre-fixed threshold to\nfilter out pseudo labeled heads, we propose dynamic entropy based filtering to\nadaptively remove unlabeled outliers as training progresses by updating the\nthreshold in multiple stages. We then revisit the design of weak-strong\naugmentations and improve it by devising two novel head-oriented strong\naugmentations, termed pose-irrelevant cut-occlusion and pose-altering rotation\nconsistency respectively. Extensive experiments and ablation studies show that\nSemiUHPE outperforms its counterparts greatly on public benchmarks under both\nthe front-range and full-range settings. Furthermore, our proposed method is\nalso beneficial for solving other closely related problems, including generic\nobject rotation regression and 3D head reconstruction, demonstrating good\nversatility and extensibility. Code is in https://github.com/hnuzhy/SemiUHPE.\n","authors":["Huayi Zhou","Fei Jiang","Jin Yuan","Yong Rui","Hongtao Lu","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2404.02544v3.pdf","comment":"under review. Semi-Supervised Unconstrained Head Pose Estimation"},{"id":"http://arxiv.org/abs/2502.08254v1","updated":"2025-02-12T09:49:43Z","published":"2025-02-12T09:49:43Z","title":"UniCoRN: Unified Commented Retrieval Network with LMMs","summary":"  Multimodal retrieval methods have limitations in handling complex,\ncompositional queries that require reasoning about the visual content of both\nthe query and the retrieved entities. On the other hand, Large Multimodal\nModels (LMMs) can answer with language to more complex visual questions, but\nwithout the inherent ability to retrieve relevant entities to support their\nanswers. We aim to address these limitations with UniCoRN, a Unified Commented\nRetrieval Network that combines the strengths of composed multimodal retrieval\nmethods and generative language approaches, going beyond Retrieval-Augmented\nGeneration (RAG). We introduce an entity adapter module to inject the retrieved\nmultimodal entities back into the LMM, so it can attend to them while\ngenerating answers and comments. By keeping the base LMM frozen, UniCoRN\npreserves its original capabilities while being able to perform both retrieval\nand text generation tasks under a single integrated framework. To assess these\nnew abilities, we introduce the Commented Retrieval task (CoR) and a\ncorresponding dataset, with the goal of retrieving an image that accurately\nanswers a given question and generate an additional textual response that\nprovides further clarification and details about the visual information. We\ndemonstrate the effectiveness of UniCoRN on several datasets showing\nimprovements of +4.5% recall over the state of the art for composed multimodal\nretrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.\n","authors":["Maximilian Jaritz","Matthieu Guillaumin","Sabine Sternig","Loris Bazzani"],"pdf_url":"https://arxiv.org/pdf/2502.08254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02483v5","updated":"2025-02-12T09:39:06Z","published":"2024-09-04T07:20:01Z","title":"TASAR: Transfer-based Attack on Skeletal Action Recognition","summary":"  Skeletal sequence data, as a widely employed representation of human actions,\nare crucial in Human Activity Recognition (HAR). Recently, adversarial attacks\nhave been proposed in this area, which exposes potential security concerns, and\nmore importantly provides a good tool for model robustness test. Within this\nresearch, transfer-based attack is an important tool as it mimics the\nreal-world scenario where an attacker has no knowledge of the target model, but\nis under-explored in Skeleton-based HAR (S-HAR). Consequently, existing S-HAR\nattacks exhibit weak adversarial transferability and the reason remains largely\nunknown. In this paper, we investigate this phenomenon via the characterization\nof the loss function. We find that one prominent indicator of poor\ntransferability is the low smoothness of the loss function. Led by this\nobservation, we improve the transferability by properly smoothening the loss\nwhen computing the adversarial examples. This leads to the first Transfer-based\nAttack on Skeletal Action Recognition, TASAR. TASAR explores the smoothened\nmodel posterior of pre-trained surrogates, which is achieved by a new\npost-train Dual Bayesian optimization strategy. Furthermore, unlike existing\ntransfer-based methods which overlook the temporal coherence within sequences,\nTASAR incorporates motion dynamics into the Bayesian attack, effectively\ndisrupting the spatial-temporal coherence of S-HARs. For exhaustive evaluation,\nwe build the first large-scale robust S-HAR benchmark, comprising 7 S-HAR\nmodels, 10 attack methods, 3 S-HAR datasets and 2 defense models. Extensive\nresults demonstrate the superiority of TASAR. Our benchmark enables easy\ncomparisons for future studies, with the code available in the\nhttps://github.com/yunfengdiao/Skeleton-Robustness-Benchmark.\n","authors":["Yunfeng Diao","Baiqi Wu","Ruixuan Zhang","Ajian Liu","Xiaoshuai Hao","Xingxing Wei","Meng Wang","He Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02483v5.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08244v1","updated":"2025-02-12T09:38:41Z","published":"2025-02-12T09:38:41Z","title":"FloVD: Optical Flow Meets Video Diffusion Model for Enhanced\n  Camera-Controlled Video Synthesis","summary":"  This paper presents FloVD, a novel optical-flow-based video diffusion model\nfor camera-controllable video generation. FloVD leverages optical flow maps to\nrepresent motions of the camera and moving objects. This approach offers two\nkey benefits. Since optical flow can be directly estimated from videos, our\napproach allows for the use of arbitrary training videos without ground-truth\ncamera parameters. Moreover, as background optical flow encodes 3D correlation\nacross different viewpoints, our method enables detailed camera control by\nleveraging the background motion. To synthesize natural object motion while\nsupporting detailed camera control, our framework adopts a two-stage video\nsynthesis pipeline consisting of optical flow generation and flow-conditioned\nvideo synthesis. Extensive experiments demonstrate the superiority of our\nmethod over previous approaches in terms of accurate camera control and natural\nobject motion synthesis.\n","authors":["Wonjoon Jin","Qi Dai","Chong Luo","Seung-Hwan Baek","Sunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2502.08244v1.pdf","comment":"Project website: https://jinwonjoon.github.io/flovd_site/"},{"id":"http://arxiv.org/abs/2502.01993v2","updated":"2025-02-12T09:25:56Z","published":"2025-02-04T04:11:29Z","title":"One Diffusion Step to Real-World Super-Resolution via Flow Trajectory\n  Distillation","summary":"  Diffusion models (DMs) have significantly advanced the development of\nreal-world image super-resolution (Real-ISR), but the computational cost of\nmulti-step diffusion models limits their application. One-step diffusion models\ngenerate high-quality images in a one sampling step, greatly reducing\ncomputational overhead and inference latency. However, most existing one-step\ndiffusion methods are constrained by the performance of the teacher model,\nwhere poor teacher performance results in image artifacts. To address this\nlimitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique\nbased on flow matching models. We use the state-of-the-art diffusion model\nFLUX.1-dev as both the teacher model and the base model. First, we introduce\nFlow Trajectory Distillation (FTD) to distill a multi-step flow matching model\ninto a one-step Real-ISR. Second, to improve image realism and address\nhigh-frequency artifact issues in generated images, we propose TV-LPIPS as a\nperceptual loss and introduce Attention Diversification Loss (ADL) as a\nregularization term to reduce token similarity in transformer, thereby\neliminating high-frequency artifacts. Comprehensive experiments demonstrate\nthat our method outperforms existing one-step diffusion-based Real-ISR methods.\nThe code and model will be released at https://github.com/JianzeLi-114/FluxSR.\n","authors":["Jianze Li","Jiezhang Cao","Yong Guo","Wenbo Li","Yulun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.01993v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08234v1","updated":"2025-02-12T09:21:40Z","published":"2025-02-12T09:21:40Z","title":"Learning Human Skill Generators at Key-Step Levels","summary":"  We are committed to learning human skill generators at key-step levels. The\ngeneration of skills is a challenging endeavor, but its successful\nimplementation could greatly facilitate human skill learning and provide more\nexperience for embodied intelligence. Although current video generation models\ncan synthesis simple and atomic human operations, they struggle with human\nskills due to their complex procedure process. Human skills involve multi-step,\nlong-duration actions and complex scene transitions, so the existing naive\nauto-regressive methods for synthesizing long videos cannot generate human\nskills. To address this, we propose a novel task, the Key-step Skill Generation\n(KS-Gen), aimed at reducing the complexity of generating human skill videos.\nGiven the initial state and a skill description, the task is to generate video\nclips of key steps to complete the skill, rather than a full-length video. To\nsupport this task, we introduce a carefully curated dataset and define multiple\nevaluation metrics to assess performance. Considering the complexity of KS-Gen,\nwe propose a new framework for this task. First, a multimodal large language\nmodel (MLLM) generates descriptions for key steps using retrieval argument.\nSubsequently, we use a Key-step Image Generator (KIG) to address the\ndiscontinuity between key steps in skill videos. Finally, a video generation\nmodel uses these descriptions and key-step images to generate video clips of\nthe key steps with high temporal consistency. We offer a detailed analysis of\nthe results, hoping to provide more insights on human skill generation. All\nmodels and data are available at https://github.com/MCG-NJU/KS-Gen.\n","authors":["Yilu Wu","Chenhui Zhu","Shuai Wang","Hanlin Wang","Jing Wang","Zhaoxiang Zhang","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08234v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08233v1","updated":"2025-02-12T09:21:16Z","published":"2025-02-12T09:21:16Z","title":"Plantation Monitoring Using Drone Images: A Dataset and Performance\n  Review","summary":"  Automatic monitoring of tree plantations plays a crucial role in agriculture.\nFlawless monitoring of tree health helps farmers make informed decisions\nregarding their management by taking appropriate action. Use of drone images\nfor automatic plantation monitoring can enhance the accuracy of the monitoring\nprocess, while still being affordable to small farmers in developing countries\nsuch as India. Small, low cost drones equipped with an RGB camera can capture\nhigh-resolution images of agricultural fields, allowing for detailed analysis\nof the well-being of the plantations. Existing methods of automated plantation\nmonitoring are mostly based on satellite images, which are difficult to get for\nthe farmers. We propose an automated system for plantation health monitoring\nusing drone images, which are becoming easier to get for the farmers. We\npropose a dataset of images of trees with three categories: ``Good health\",\n``Stunted\", and ``Dead\". We annotate the dataset using CVAT annotation tool,\nfor use in research purposes. We experiment with different well-known CNN\nmodels to observe their performance on the proposed dataset. The initial low\naccuracy levels show the complexity of the proposed dataset. Further, our study\nrevealed that, depth-wise convolution operation embedded in a deep CNN model,\ncan enhance the performance of the model on drone dataset. Further, we apply\nstate-of-the-art object detection models to identify individual trees to better\nmonitor them automatically.\n","authors":["Yashwanth Karumanchi","Gudala Laxmi Prasanna","Snehasis Mukherjee","Nagesh Kolagani"],"pdf_url":"https://arxiv.org/pdf/2502.08233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08226v1","updated":"2025-02-12T09:12:30Z","published":"2025-02-12T09:12:30Z","title":"TRISHUL: Towards Region Identification and Screen Hierarchy\n  Understanding for Large VLM based GUI Agents","summary":"  Recent advancements in Large Vision Language Models (LVLMs) have enabled the\ndevelopment of LVLM-based Graphical User Interface (GUI) agents under various\nparadigms. Training-based approaches, such as CogAgent and SeeClick, struggle\nwith cross-dataset and cross-platform generalization due to their reliance on\ndataset-specific training. Generalist LVLMs, such as GPT-4V, employ\nSet-of-Marks (SoM) for action grounding, but obtaining SoM labels requires\nmetadata like HTML source, which is not consistently available across\nplatforms. Moreover, existing methods often specialize in singular GUI tasks\nrather than achieving comprehensive GUI understanding. To address these\nlimitations, we introduce TRISHUL, a novel, training-free agentic framework\nthat enhances generalist LVLMs for holistic GUI comprehension. Unlike prior\nworks that focus on either action grounding (mapping instructions to GUI\nelements) or GUI referring (describing GUI elements given a location), TRISHUL\nseamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen\nParsing (HSP) and the Spatially Enhanced Element Description (SEED) module,\nwhich work synergistically to provide multi-granular, spatially, and\nsemantically enriched representations of GUI elements. Our results demonstrate\nTRISHUL's superior performance in action grounding across the ScreenSpot,\nVisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring,\nTRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new\nstandard for robust and adaptable GUI comprehension.\n","authors":["Kunal Singh","Shreyas Singh","Mukund Khanna"],"pdf_url":"https://arxiv.org/pdf/2502.08226v1.pdf","comment":"Under review at ICML 2025, 8 pages 5 figures"},{"id":"http://arxiv.org/abs/2502.07183v2","updated":"2025-02-12T09:07:32Z","published":"2025-02-11T02:14:49Z","title":"Space-Aware Instruction Tuning: Dataset and Benchmark for Guide Dog\n  Robots Assisting the Visually Impaired","summary":"  Guide dog robots offer promising solutions to enhance mobility and safety for\nvisually impaired individuals, addressing the limitations of traditional guide\ndogs, particularly in perceptual intelligence and communication. With the\nemergence of Vision-Language Models (VLMs), robots are now capable of\ngenerating natural language descriptions of their surroundings, aiding in safer\ndecision-making. However, existing VLMs often struggle to accurately interpret\nand convey spatial relationships, which is crucial for navigation in complex\nenvironments such as street crossings. We introduce the Space-Aware Instruction\nTuning (SAIT) dataset and the Space-Aware Benchmark (SA-Bench) to address the\nlimitations of current VLMs in understanding physical environments. Our\nautomated data generation pipeline focuses on the virtual path to the\ndestination in 3D space and the surroundings, enhancing environmental\ncomprehension and enabling VLMs to provide more accurate guidance to visually\nimpaired individuals. We also propose an evaluation protocol to assess VLM\neffectiveness in delivering walking guidance. Comparative experiments\ndemonstrate that our space-aware instruction-tuned model outperforms\nstate-of-the-art algorithms. We have fully open-sourced the SAIT dataset and\nSA-Bench, along with the related code, at\nhttps://github.com/byungokhan/Space-awareVLM\n","authors":["ByungOk Han","Woo-han Yun","Beom-Su Seo","Jaehong Kim"],"pdf_url":"https://arxiv.org/pdf/2502.07183v2.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2502.08221v1","updated":"2025-02-12T09:01:25Z","published":"2025-02-12T09:01:25Z","title":"Take What You Need: Flexible Multi-Task Semantic Communications with\n  Channel Adaptation","summary":"  The growing demand for efficient semantic communication systems capable of\nmanaging diverse tasks and adapting to fluctuating channel conditions has\ndriven the development of robust, resource-efficient frameworks. This article\nintroduces a novel channel-adaptive and multi-task-aware semantic communication\nframework based on a masked auto-encoder architecture. Our framework optimizes\nthe transmission of meaningful information by incorporating a multi-task-aware\nscoring mechanism that identifies and prioritizes semantically significant data\nacross multiple concurrent tasks. A channel-aware extractor is employed to\ndynamically select relevant information in response to real-time channel\nconditions. By jointly optimizing semantic relevance and transmission\nefficiency, the framework ensures minimal performance degradation under\nresource constraints. Experimental results demonstrate the superior performance\nof our framework compared to conventional methods in tasks such as image\nreconstruction and object detection. These results underscore the framework's\nadaptability to heterogeneous channel environments and its scalability for\nmulti-task applications, positioning it as a promising solution for\nnext-generation semantic communication networks.\n","authors":["Xiang Chen","Shuying Gan","Chenyuan Feng","Xijun Wang","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2502.08221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08216v1","updated":"2025-02-12T08:51:33Z","published":"2025-02-12T08:51:33Z","title":"Deepfake Detection with Spatio-Temporal Consistency and Attention","summary":"  Deepfake videos are causing growing concerns among communities due to their\never-increasing realism. Naturally, automated detection of forged Deepfake\nvideos is attracting a proportional amount of interest of researchers. Current\nmethods for detecting forged videos mainly rely on global frame features and\nunder-utilize the spatio-temporal inconsistencies found in the manipulated\nvideos. Moreover, they fail to attend to manipulation-specific subtle and\nwell-localized pattern variations along both spatial and temporal dimensions.\nAddressing these gaps, we propose a neural Deepfake detector that focuses on\nthe localized manipulative signatures of the forged videos at individual frame\nlevel as well as frame sequence level. Using a ResNet backbone, it strengthens\nthe shallow frame-level feature learning with a spatial attention mechanism.\nThe spatial stream of the model is further helped by fusing texture enhanced\nshallow features with the deeper features. Simultaneously, the model processes\nframe sequences with a distance attention mechanism that further allows fusion\nof temporal attention maps with the learned features at the deeper layers. The\noverall model is trained to detect forged content as a classifier. We evaluate\nour method on two popular large data sets and achieve significant performance\nover the state-of-the-art methods.Moreover, our technique also provides memory\nand computational advantages over the competitive techniques.\n","authors":["Yunzhuo Chen","Naveed Akhtar","Nur Al Hasan Haldar","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2502.08216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06606v2","updated":"2025-02-12T08:49:38Z","published":"2025-02-10T16:04:33Z","title":"MaterialFusion: High-Quality, Zero-Shot, and Controllable Material\n  Transfer with Diffusion Models","summary":"  Manipulating the material appearance of objects in images is critical for\napplications like augmented reality, virtual prototyping, and digital content\ncreation. We present MaterialFusion, a novel framework for high-quality\nmaterial transfer that allows users to adjust the degree of material\napplication, achieving an optimal balance between new material properties and\nthe object's original features. MaterialFusion seamlessly integrates the\nmodified object into the scene by maintaining background consistency and\nmitigating boundary artifacts. To thoroughly evaluate our approach, we have\ncompiled a dataset of real-world material transfer examples and conducted\ncomplex comparative analyses. Through comprehensive quantitative evaluations\nand user studies, we demonstrate that MaterialFusion significantly outperforms\nexisting methods in terms of quality, user control, and background\npreservation. Code is available at\nhttps://github.com/ControlGenAI/MaterialFusion.\n","authors":["Kamil Garifullin","Maxim Nikolaev","Andrey Kuznetsov","Aibek Alanov"],"pdf_url":"https://arxiv.org/pdf/2502.06606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08200v1","updated":"2025-02-12T08:24:36Z","published":"2025-02-12T08:24:36Z","title":"ActiveSSF: An Active-Learning-Guided Self-Supervised Framework for\n  Long-Tailed Megakaryocyte Classification","summary":"  Precise classification of megakaryocytes is crucial for diagnosing\nmyelodysplastic syndromes. Although self-supervised learning has shown promise\nin medical image analysis, its application to classifying megakaryocytes in\nstained slides faces three main challenges: (1) pervasive background noise that\nobscures cellular details, (2) a long-tailed distribution that limits data for\nrare subtypes, and (3) complex morphological variations leading to high\nintra-class variability. To address these issues, we propose the ActiveSSF\nframework, which integrates active learning with self-supervised pretraining.\nSpecifically, our approach employs Gaussian filtering combined with K-means\nclustering and HSV analysis (augmented by clinical prior knowledge) for\naccurate region-of-interest extraction; an adaptive sample selection mechanism\nthat dynamically adjusts similarity thresholds to mitigate class imbalance; and\nprototype clustering on labeled samples to overcome morphological complexity.\nExperimental results on clinical megakaryocyte datasets demonstrate that\nActiveSSF not only achieves state-of-the-art performance but also significantly\nimproves recognition accuracy for rare subtypes. Moreover, the integration of\nthese advanced techniques further underscores the practical potential of\nActiveSSF in clinical settings. To foster further research, the code and\ndatasets will be publicly released in the future.\n","authors":["Linghao Zhuang","Ying Zhang","Gege Yuan","Xingyue Zhao","Zhiping Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.08200v1.pdf","comment":"6 pages, submitted to EMBC 2025"},{"id":"http://arxiv.org/abs/2501.10967v2","updated":"2025-02-12T08:10:06Z","published":"2025-01-19T07:00:46Z","title":"Advancing General Multimodal Capability of Vision-language Models with\n  Pyramid-descent Visual Position Encoding","summary":"  Vision-language Models (VLMs) have shown remarkable capabilities in advancing\ngeneral artificial intelligence, yet the irrational encoding of visual\npositions persists in inhibiting the models' comprehensive perception\nperformance across different levels of granularity. In this work, we propose\nPyramid-descent Visual Position Encoding (PyPE), a novel approach designed to\nenhance the perception of visual tokens within VLMs. By assigning visual\nposition indexes from the periphery to the center and expanding the central\nreceptive field incrementally, PyPE addresses the limitations of traditional\nraster-scan methods and mitigates the long-term decay effects induced by Rotary\nPosition Embedding (RoPE). Our method reduces the relative distance between\ninterrelated visual elements and instruction tokens, promoting a more rational\nallocation of attention weights and allowing for a multi-granularity perception\nof visual elements and countering the over-reliance on anchor tokens. Extensive\nexperimental evaluations demonstrate that PyPE consistently improves the\ngeneral capabilities of VLMs across various sizes. Code is available at\nhttps://github.com/SakuraTroyChen/PyPE.\n","authors":["Zhanpeng Chen","Mingxiao Li","Ziyang Chen","Nan Du","Xiaolong Li","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2501.10967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08189v1","updated":"2025-02-12T07:59:41Z","published":"2025-02-12T07:59:41Z","title":"AnyCharV: Bootstrap Controllable Character Video Generation with\n  Fine-to-Coarse Guidance","summary":"  Character video generation is a significant real-world application focused on\nproducing high-quality videos featuring specific characters. Recent\nadvancements have introduced various control signals to animate static\ncharacters, successfully enhancing control over the generation process.\nHowever, these methods often lack flexibility, limiting their applicability and\nmaking it challenging for users to synthesize a source character into a desired\ntarget scene. To address this issue, we propose a novel framework, AnyCharV,\nthat flexibly generates character videos using arbitrary source characters and\ntarget scenes, guided by pose information. Our approach involves a two-stage\ntraining process. In the first stage, we develop a base model capable of\nintegrating the source character with the target scene using pose guidance. The\nsecond stage further bootstraps controllable generation through a self-boosting\nmechanism, where we use the generated video in the first stage and replace the\nfine mask with the coarse one, enabling training outcomes with better\npreservation of character details. Experimental results demonstrate the\neffectiveness and robustness of our proposed method. Our project page is\nhttps://anycharv.github.io.\n","authors":["Zhao Wang","Hao Wen","Lingting Zhu","Chenming Shang","Yujiu Yang","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2502.08189v1.pdf","comment":"15 pages, 9 figures, 4 tables"},{"id":"http://arxiv.org/abs/2501.02385v2","updated":"2025-02-12T07:55:10Z","published":"2025-01-04T21:23:36Z","title":"Guiding Medical Vision-Language Models with Explicit Visual Prompts:\n  Framework Design and Comprehensive Exploration of Prompt Variations","summary":"  While mainstream vision-language models (VLMs) have advanced rapidly in\nunderstanding image level information, they still lack the ability to focus on\nspecific areas designated by humans. Rather, they typically rely on large\nvolumes of high-quality image-text paired data to learn and generate posterior\nattention maps. To address this critical issue, we propose leveraging visual\nprompts:simple visual markers in various forms to guide and enhance the\nformation of region-specific attention. Thus, we introduce MedVP, a pioneering\nframework that integrates medical entity extraction, visual prompt generation,\nand dataset adaptation for visual prompt guided fine-tuning. We successfully\noutperform recent state-of-the-art large models across multiple medical VQA\ndatasets. Extensive experiments and Human evaluation are conducted to analyze\nthe impact of different visual prompt forms and how they contribute to\nperformance improvement. The results demonstrate both the effectiveness and\nclinical significance of our approach.\n","authors":["Kangyu Zhu","Ziyuan Qin","Huahui Yi","Zekun Jiang","Qicheng Lao","Shaoting Zhang","Kang Li"],"pdf_url":"https://arxiv.org/pdf/2501.02385v2.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2502.08181v1","updated":"2025-02-12T07:39:44Z","published":"2025-02-12T07:39:44Z","title":"Latest Advancements Towards Catastrophic Forgetting under Data Scarcity:\n  A Comprehensive Survey on Few-Shot Class Incremental Learning","summary":"  Data scarcity significantly complicates the continual learning problem, i.e.,\nhow a deep neural network learns in dynamic environments with very few samples.\nHowever, the latest progress of few-shot class incremental learning (FSCIL)\nmethods and related studies show insightful knowledge on how to tackle the\nproblem. This paper presents a comprehensive survey on FSCIL that highlights\nseveral important aspects i.e. comprehensive and formal objectives of FSCIL\napproaches, the importance of prototype rectifications, the new learning\nparadigms based on pre-trained model and language-guided mechanism, the deeper\nanalysis of FSCIL performance metrics and evaluation, and the practical\ncontexts of FSCIL in various areas. Our extensive discussion presents the open\nchallenges, potential solutions, and future directions of FSCIL.\n","authors":["M. Anwar Ma'sum","Mahardhika Pratama","Igor Skrjanc"],"pdf_url":"https://arxiv.org/pdf/2502.08181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07381v2","updated":"2025-02-12T07:37:30Z","published":"2025-02-11T08:57:45Z","title":"Spatial Degradation-Aware and Temporal Consistent Diffusion Model for\n  Compressed Video Super-Resolution","summary":"  Due to limitations of storage and bandwidth, videos stored and transmitted on\nthe Internet are usually low-quality with low-resolution and compression noise.\nAlthough video super-resolution (VSR) is an efficient technique to enhance\nvideo resolution, relatively VSR methods focus on compressed videos. Directly\napplying general VSR approaches leads to the failure of improving practical\nvideos, especially when frames are highly compressed at a low bit rate.\nRecently, diffusion models have achieved superior performance in low-level\nvisual tasks, and their high-realism generation capability enables them to be\napplied in VSR. To synthesize more compression-lost details and refine temporal\nconsistency, we propose a novel Spatial Degradation-Aware and Temporal\nConsistent (SDATC) diffusion model for compressed VSR. Specifically, we\nintroduce a distortion Control module (DCM) to modulate diffusion model inputs\nand guide the generation. Next, the diffusion model executes the denoising\nprocess for texture generation with fine-tuned spatial prompt-based\ncompression-aware module (PCAM) and spatio-temporal attention module (STAM).\nPCAM extracts features to encode specific compression information dynamically.\nSTAM extends the spatial attention mechanism to a spatio-temporal dimension for\ncapturing temporal correlation. Extensive experimental results on benchmark\ndatasets demonstrate the effectiveness of the proposed modules in enhancing\ncompressed videos.\n","authors":["Hongyu An","Xinfeng Zhang","Shijie Zhao","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07381v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07531v2","updated":"2025-02-12T07:35:56Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08169v1","updated":"2025-02-12T07:23:26Z","published":"2025-02-12T07:23:26Z","title":"CoDynTrust: Robust Asynchronous Collaborative Perception via Dynamic\n  Feature Trust Modulus","summary":"  Collaborative perception, fusing information from multiple agents, can extend\nperception range so as to improve perception performance. However, temporal\nasynchrony in real-world environments, caused by communication delays, clock\nmisalignment, or sampling configuration differences, can lead to information\nmismatches. If this is not well handled, then the collaborative performance is\npatchy, and what's worse safety accidents may occur. To tackle this challenge,\nwe propose CoDynTrust, an uncertainty-encoded asynchronous fusion perception\nframework that is robust to the information mismatches caused by temporal\nasynchrony. CoDynTrust generates dynamic feature trust modulus (DFTM) for each\nregion of interest by modeling aleatoric and epistemic uncertainty as well as\nselectively suppressing or retaining single-vehicle features, thereby\nmitigating information mismatches. We then design a multi-scale fusion module\nto handle multi-scale feature maps processed by DFTM. Compared to existing\nworks that also consider asynchronous collaborative perception, CoDynTrust\ncombats various low-quality information in temporally asynchronous scenarios\nand allows uncertainty to be propagated to downstream tasks such as planning\nand control. Experimental results demonstrate that CoDynTrust significantly\nreduces performance degradation caused by temporal asynchrony across multiple\ndatasets, achieving state-of-the-art detection performance even with temporal\nasynchrony. The code is available at https://github.com/CrazyShout/CoDynTrust.\n","authors":["Yunjiang Xu","Lingzhi Li","Jin Wang","Benyuan Yang","Zhiwen Wu","Xinhong Chen","Jianping Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08169v1.pdf","comment":"7 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2410.01521v2","updated":"2025-02-12T07:22:41Z","published":"2024-10-02T13:10:57Z","title":"MiraGe: Editable 2D Images using Gaussian Splatting","summary":"  Implicit Neural Representations (INRs) approximate discrete data through\ncontinuous functions and are commonly used for encoding 2D images. Traditional\nimage-based INRs employ neural networks to map pixel coordinates to RGB values,\ncapturing shapes, colors, and textures within the network's weights. Recently,\nGaussianImage has been proposed as an alternative, using Gaussian functions\ninstead of neural networks to achieve comparable quality and compression. Such\na solution obtains a quality and compression ratio similar to classical INR\nmodels but does not allow image modification. In contrast, our work introduces\na novel method, MiraGe, which uses mirror reflections to perceive 2D images in\n3D space and employs flat-controlled Gaussians for precise 2D image editing.\nOur approach improves the rendering quality and allows realistic image\nmodifications, including human-inspired perception of photos in the 3D world.\nThanks to modeling images in 3D space, we obtain the illusion of 3D-based\nmodification in 2D images. We also show that our Gaussian representation can be\neasily combined with a physics engine to produce physics-based modification of\n2D images. Consequently, MiraGe allows for better quality than the standard\napproach and natural modification of 2D images\n","authors":["Joanna Waczy≈Ñska","Tomasz Szczepanik","Piotr Borycki","S≈Çawomir Tadeja","Thomas Bohn√©","Przemys≈Çaw Spurek"],"pdf_url":"https://arxiv.org/pdf/2410.01521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08167v1","updated":"2025-02-12T07:14:54Z","published":"2025-02-12T07:14:54Z","title":"DNNs May Determine Major Properties of Their Outputs Early, with Timing\n  Possibly Driven by Bias","summary":"  This paper argues that deep neural networks (DNNs) mostly determine their\noutputs during the early stages of inference, where biases inherent in the\nmodel play a crucial role in shaping this process. We draw a parallel between\nthis phenomenon and human decision-making, which often relies on fast,\nintuitive heuristics. Using diffusion models (DMs) as a case study, we\ndemonstrate that DNNs often make early-stage decision-making influenced by the\ntype and extent of bias in their design and training. Our findings offer a new\nperspective on bias mitigation, efficient inference, and the interpretation of\nmachine learning systems. By identifying the temporal dynamics of\ndecision-making in DNNs, this paper aims to inspire further discussion and\nresearch within the machine learning community.\n","authors":["Song Park","Sanghyuk Chun","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2502.08167v1.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2502.08678v1","updated":"2025-02-12T07:01:42Z","published":"2025-02-12T07:01:42Z","title":"Multispectral Remote Sensing for Weed Detection in West Australian\n  Agricultural Lands","summary":"  The Kondinin region in Western Australia faces significant agricultural\nchallenges due to pervasive weed infestations, causing economic losses and\necological impacts. This study constructs a tailored multispectral remote\nsensing dataset and an end-to-end framework for weed detection to advance\nprecision agriculture practices. Unmanned aerial vehicles were used to collect\nraw multispectral data from two experimental areas (E2 and E8) over four years,\ncovering 0.6046 km^{2} and ground truth annotations were created with\nGPS-enabled vehicles to manually label weeds and crops. The dataset is\nspecifically designed for agricultural applications in Western Australia. We\npropose an end-to-end framework for weed detection that includes extensive\npreprocessing steps, such as denoising, radiometric calibration, image\nalignment, orthorectification, and stitching. The proposed method combines\nvegetation indices (NDVI, GNDVI, EVI, SAVI, MSAVI) with multispectral channels\nto form classification features, and employs several deep learning models to\nidentify weeds based on the input features. Among these models, ResNet achieves\nthe highest performance, with a weed detection accuracy of 0.9213, an F1-Score\nof 0.8735, an mIOU of 0.7888, and an mDC of 0.8865, validating the efficacy of\nthe dataset and the proposed weed detection method.\n","authors":["Haitian Wang","Muhammad Ibrahim","Yumeng Miao","D ustin Severtson","Atif Mansoor","Ajmal S. Mian"],"pdf_url":"https://arxiv.org/pdf/2502.08678v1.pdf","comment":"8 pages, 9 figures, 1 table, Accepted for oral presentation at IEEE\n  25th International Conference on Digital Image Computing: Techniques and\n  Applications (DICTA 2024). Conference Proceeding:\n  979-8-3503-7903-7/24/\\$31.00 (C) 2024 IEEE"},{"id":"http://arxiv.org/abs/2309.14054v2","updated":"2025-02-12T07:00:51Z","published":"2023-09-25T11:36:20Z","title":"Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning\n  in Generative Adversarial Networks","summary":"  Owing to the growing concerns about privacy and regulatory compliance, it is\ndesirable to regulate the output of generative models. To that end, the\nobjective of this work is to prevent the generation of outputs containing\nundesired features from a pre-trained Generative Adversarial Network (GAN)\nwhere the underlying training data set is inaccessible. Our approach is\ninspired by the observation that the parameter space of GANs exhibits\nmeaningful directions that can be leveraged to suppress specific undesired\nfeatures. However, such directions usually result in the degradation of the\nquality of generated samples. Our proposed two-stage method, known as\n'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also\nmaintaining the quality of generated samples. In the initial stage, we adapt a\npre-trained GAN on a set of negative samples (containing undesired features)\nprovided by the user. Subsequently, we train the original pre-trained GAN using\npositive samples, along with a repulsion regularizer. This regularizer\nencourages the learned model parameters to move away from the parameters of the\nadapted model (first stage) while not degrading the generation quality. We\nprovide theoretical insights into the proposed method. To the best of our\nknowledge, our approach stands as the first method addressing unlearning within\nthe realm of high-fidelity GANs (such as StyleGAN). We validate the\neffectiveness of our method through comprehensive experiments, encompassing\nboth class-level unlearning on the MNIST and AFHQ dataset and feature-level\nunlearning tasks on the CelebA-HQ dataset. Our code and implementation is\navailable at: https://github.com/atriguha/Adapt_Unlearn.\n","authors":["Piyush Tiwary","Atri Guha","Subhodip Panda","Prathosh A. P"],"pdf_url":"https://arxiv.org/pdf/2309.14054v2.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2409.11744v2","updated":"2025-02-12T06:53:14Z","published":"2024-09-18T06:56:06Z","title":"Exploring Gaze Pattern Differences Between ASD and TD Children Using\n  Internal Cluster Validity Indices","summary":"  Autism Spectrum Disorder (ASD) affects children's social and communication\nabilities, with eye-tracking widely used to identify atypical gaze patterns.\nWhile unsupervised clustering can automate the creation of areas of interest\nfor gaze feature extraction, the use of internal cluster validity indices, like\nSilhouette Coefficient, to distinguish gaze pattern differences between ASD and\ntypically developing (TD) children remains underexplored. We explore whether\ninternal cluster validity indices can distinguish ASD from TD children.\nSpecifically, we apply seven clustering algorithms to gaze points and extract\n63 internal cluster validity indices to reveal correlations with ASD diagnosis.\nUsing these indices, we train predictive models for ASD diagnosis. Experiments\non three datasets demonstrate high predictive accuracy (81\\% AUC), validating\nthe effectiveness of these indices.\n","authors":["Weiyan Shi","Haihong Zhang","Ruiqing Ding","YongWei Zhu","Wei Wang","Kenny Tsu Wei Choo"],"pdf_url":"https://arxiv.org/pdf/2409.11744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14316v2","updated":"2025-02-12T06:48:03Z","published":"2025-01-24T08:21:35Z","title":"PAID: A Framework of Product-Centric Advertising Image Design","summary":"  Creating visually appealing advertising images is often a labor-intensive and\ntime-consuming process. Is it possible to automatically generate such images\nusing only basic product information--specifically, a product foreground image,\ntaglines, and a target size? Existing methods mainly focus on parts of the\nproblem and fail to provide a comprehensive solution. To address this gap, we\npropose a novel multistage framework called Product-Centric Advertising Image\nDesign (PAID). It consists of four sequential stages to highlight product\nforegrounds and taglines while achieving overall image aesthetics: prompt\ngeneration, layout generation, background image generation, and graphics\nrendering. Different expert models are designed and trained for the first three\nstages: First, we use a visual language model (VLM) to generate background\nprompts that match the products. Next, a VLM-based layout generation model\narranges the placement of product foregrounds, graphic elements (taglines and\ndecorative underlays), and various nongraphic elements (objects from the\nbackground prompt). Following this, we train an SDXL-based image generation\nmodel that can simultaneously accept prompts, layouts, and foreground controls.\nTo support the PAID framework, we create corresponding datasets with over\n50,000 labeled images. Extensive experimental results and online A/B tests\ndemonstrate that PAID can produce more visually appealing advertising images.\n","authors":["Hongyu Chen","Min Zhou","Jing Jiang","Jiale Chen","Yang Lu","Bo Xiao","Tiezheng Ge","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2501.14316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17133v2","updated":"2025-02-12T06:47:37Z","published":"2024-02-27T01:57:02Z","title":"SAM-DiffSR: Structure-Modulated Diffusion Model for Image\n  Super-Resolution","summary":"  Diffusion-based super-resolution (SR) models have recently garnered\nsignificant attention due to their potent restoration capabilities. But\nconventional diffusion models perform noise sampling from a single\ndistribution, constraining their ability to handle real-world scenes and\ncomplex textures across semantic regions. With the success of segment anything\nmodel (SAM), generating sufficiently fine-grained region masks can enhance the\ndetail recovery of diffusion-based SR model. However, directly integrating SAM\ninto SR models will result in much higher computational cost. In this paper, we\npropose the SAM-DiffSR model, which can utilize the fine-grained structure\ninformation from SAM in the process of sampling noise to improve the image\nquality without additional computational cost during inference. In the process\nof training, we encode structural position information into the segmentation\nmask from SAM. Then the encoded mask is integrated into the forward diffusion\nprocess by modulating it to the sampled noise. This adjustment allows us to\nindependently adapt the noise mean within each corresponding segmentation area.\nThe diffusion model is trained to estimate this modulated noise. Crucially, our\nproposed framework does NOT change the reverse diffusion process and does NOT\nrequire SAM at inference. Experimental results demonstrate the effectiveness of\nour proposed method, showcasing superior performance in suppressing artifacts,\nand surpassing existing diffusion-based methods by 0.74 dB at the maximum in\nterms of PSNR on DIV2K dataset. The code and dataset are available at\nhttps://github.com/lose4578/SAM-DiffSR.\n","authors":["Chengcheng Wang","Zhiwei Hao","Yehui Tang","Jianyuan Guo","Yujie Yang","Kai Han","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2402.17133v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16769v2","updated":"2025-02-12T06:39:07Z","published":"2024-11-25T04:17:24Z","title":"In-Context Experience Replay Facilitates Safety Red-Teaming of\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) models have shown remarkable progress, but their\npotential to generate harmful content remains a critical concern in the ML\ncommunity. While various safety mechanisms have been developed, the field lacks\nsystematic tools for evaluating their effectiveness against real-world misuse\nscenarios. In this work, we propose ICER, a novel red-teaming framework that\nleverages Large Language Models (LLMs) and a bandit optimization-based\nalgorithm to generate interpretable and semantic meaningful problematic prompts\nby learning from past successful red-teaming attempts. Our ICER efficiently\nprobes safety mechanisms across different T2I models without requiring internal\naccess or additional training, making it broadly applicable to deployed\nsystems. Through extensive experiments, we demonstrate that ICER significantly\noutperforms existing prompt attack methods in identifying model vulnerabilities\nwhile maintaining high semantic similarity with intended content. By uncovering\nthat successful jailbreaking instances can systematically facilitate the\ndiscovery of new vulnerabilities, our work provides crucial insights for\ndeveloping more robust safety mechanisms in T2I systems.\n","authors":["Zhi-Yi Chin","Mario Fritz","Pin-Yu Chen","Wei-Chen Chiu"],"pdf_url":"https://arxiv.org/pdf/2411.16769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08150v1","updated":"2025-02-12T06:30:01Z","published":"2025-02-12T06:30:01Z","title":"Force Matching with Relativistic Constraints: A Physics-Inspired\n  Approach to Stable and Efficient Generative Modeling","summary":"  This paper introduces Force Matching (ForM), a novel framework for generative\nmodeling that represents an initial exploration into leveraging special\nrelativistic mechanics to enhance the stability of the sampling process. By\nincorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring\nthat sample velocities remain bounded within a constant limit. This constraint\nserves as a fundamental mechanism for stabilizing the generative dynamics,\nleading to a more robust and controlled sampling process. We provide a rigorous\ntheoretical analysis demonstrating that the velocity constraint is preserved\nthroughout the sampling procedure within the ForM framework. To validate the\neffectiveness of our approach, we conduct extensive empirical evaluations. On\nthe \\textit{half-moons} dataset, ForM significantly outperforms baseline\nmethods, achieving the lowest Euclidean distance loss of \\textbf{0.714}, in\ncontrast to vanilla first-order flow matching (5.853) and first- and\nsecond-order flow matching (5.793). Additionally, we perform an ablation study\nto further investigate the impact of our velocity constraint, reaffirming the\nsuperiority of ForM in stabilizing the generative process. The theoretical\nguarantees and empirical results underscore the potential of integrating\nspecial relativity principles into generative modeling. Our findings suggest\nthat ForM provides a promising pathway toward achieving stable, efficient, and\nflexible generative processes. This work lays the foundation for future\nadvancements in high-dimensional generative modeling, opening new avenues for\nthe application of physical principles in machine learning.\n","authors":["Yang Cao","Bo Chen","Xiaoyu Li","Yingyu Liang","Zhizhou Sha","Zhenmei Shi","Zhao Song","Mingda Wan"],"pdf_url":"https://arxiv.org/pdf/2502.08150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02252v4","updated":"2025-02-12T06:27:34Z","published":"2024-07-02T13:17:49Z","title":"GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion\n  Models and Large Language Models","summary":"  Posters play a crucial role in marketing and advertising by enhancing visual\ncommunication and brand visibility, making significant contributions to\nindustrial design. With the latest advancements in controllable T2I diffusion\nmodels, increasing research has focused on rendering text within synthesized\nimages. Despite improvements in text rendering accuracy, the field of automatic\nposter generation remains underexplored. In this paper, we propose an automatic\nposter generation framework with text rendering capabilities leveraging LLMs,\nutilizing a triple-cross attention mechanism based on alignment learning. This\nframework aims to create precise poster text within a detailed contextual\nbackground. Additionally, the framework supports controllable fonts, adjustable\nimage resolution, and the rendering of posters with descriptions and text in\nboth English and Chinese.Furthermore, we introduce a high-resolution font\ndataset and a poster dataset with resolutions exceeding 1024 pixels. Our\napproach leverages the SDXL architecture. Extensive experiments validate our\nmethod's capability in generating poster images with complex and contextually\nrich backgrounds.Codes is available at\nhttps://github.com/OPPO-Mente-Lab/GlyphDraw2.\n","authors":["Jian Ma","Yonglin Deng","Chen Chen","Nanyang Du","Haonan Lu","Zhenyu Yang"],"pdf_url":"https://arxiv.org/pdf/2407.02252v4.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2408.15239v2","updated":"2025-02-12T06:26:29Z","published":"2024-08-27T17:57:14Z","title":"Generative Inbetweening: Adapting Image-to-Video Models for Keyframe\n  Interpolation","summary":"  We present a method for generating video sequences with coherent motion\nbetween a pair of input key frames. We adapt a pretrained large-scale\nimage-to-video diffusion model (originally trained to generate videos moving\nforward in time from a single input image) for key frame interpolation, i.e.,\nto produce a video in between two input frames. We accomplish this adaptation\nthrough a lightweight fine-tuning technique that produces a version of the\nmodel that instead predicts videos moving backwards in time from a single input\nimage. This model (along with the original forward-moving model) is\nsubsequently used in a dual-directional diffusion sampling process that\ncombines the overlapping model estimates starting from each of the two\nkeyframes. Our experiments show that our method outperforms both existing\ndiffusion-based methods and traditional frame interpolation techniques.\n","authors":["Xiaojuan Wang","Boyang Zhou","Brian Curless","Ira Kemelmacher-Shlizerman","Aleksander Holynski","Steven M. Seitz"],"pdf_url":"https://arxiv.org/pdf/2408.15239v2.pdf","comment":"Published at ICLR 2025; Project page:\n  https://svd-keyframe-interpolation.github.io/"},{"id":"http://arxiv.org/abs/2502.08149v1","updated":"2025-02-12T06:26:05Z","published":"2025-02-12T06:26:05Z","title":"Generalized Class Discovery in Instance Segmentation","summary":"  This work addresses the task of generalized class discovery (GCD) in instance\nsegmentation. The goal is to discover novel classes and obtain a model capable\nof segmenting instances of both known and novel categories, given labeled and\nunlabeled data. Since the real world contains numerous objects with long-tailed\ndistributions, the instance distribution for each class is inherently\nimbalanced. To address the imbalanced distributions, we propose an\ninstance-wise temperature assignment (ITA) method for contrastive learning and\nclass-wise reliability criteria for pseudo-labels. The ITA method relaxes\ninstance discrimination for samples belonging to head classes to enhance GCD.\nThe reliability criteria are to avoid excluding most pseudo-labels for tail\nclasses when training an instance segmentation network using pseudo-labels from\nGCD. Additionally, we propose dynamically adjusting the criteria to leverage\ndiverse samples in the early stages while relying only on reliable\npseudo-labels in the later stages. We also introduce an efficient soft\nattention module to encode object-specific representations for GCD. Finally, we\nevaluate our proposed method by conducting experiments on two settings:\nCOCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results\ndemonstrate that the proposed method outperforms previous state-of-the-art\nmethods.\n","authors":["Cuong Manh Hoang","Yeejin Lee","Byeongkeun Kang"],"pdf_url":"https://arxiv.org/pdf/2502.08149v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2502.05206v2","updated":"2025-02-12T06:16:00Z","published":"2025-02-02T05:14:22Z","title":"Safety at Scale: A Comprehensive Survey of Large Model Safety","summary":"  The rapid advancement of large models, driven by their exceptional abilities\nin learning and generalization through large-scale pre-training, has reshaped\nthe landscape of Artificial Intelligence (AI). These models are now\nfoundational to a wide range of applications, including conversational AI,\nrecommendation systems, autonomous driving, content generation, medical\ndiagnostics, and scientific discovery. However, their widespread deployment\nalso exposes them to significant safety risks, raising concerns about\nrobustness, reliability, and ethical implications. This survey provides a\nsystematic review of current safety research on large models, covering Vision\nFoundation Models (VFMs), Large Language Models (LLMs), Vision-Language\nPre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models\n(DMs), and large-model-based Agents. Our contributions are summarized as\nfollows: (1) We present a comprehensive taxonomy of safety threats to these\nmodels, including adversarial attacks, data poisoning, backdoor attacks,\njailbreak and prompt injection attacks, energy-latency attacks, data and model\nextraction attacks, and emerging agent-specific threats. (2) We review defense\nstrategies proposed for each type of attacks if available and summarize the\ncommonly used datasets and benchmarks for safety research. (3) Building on\nthis, we identify and discuss the open challenges in large model safety,\nemphasizing the need for comprehensive safety evaluations, scalable and\neffective defense mechanisms, and sustainable data practices. More importantly,\nwe highlight the necessity of collective efforts from the research community\nand international collaboration. Our work can serve as a useful reference for\nresearchers and practitioners, fostering the ongoing development of\ncomprehensive defense systems and platforms to safeguard AI models.\n","authors":["Xingjun Ma","Yifeng Gao","Yixu Wang","Ruofan Wang","Xin Wang","Ye Sun","Yifan Ding","Hengyuan Xu","Yunhao Chen","Yunhan Zhao","Hanxun Huang","Yige Li","Jiaming Zhang","Xiang Zheng","Yang Bai","Zuxuan Wu","Xipeng Qiu","Jingfeng Zhang","Yiming Li","Jun Sun","Cong Wang","Jindong Gu","Baoyuan Wu","Siheng Chen","Tianwei Zhang","Yang Liu","Mingming Gong","Tongliang Liu","Shirui Pan","Cihang Xie","Tianyu Pang","Yinpeng Dong","Ruoxi Jia","Yang Zhang","Shiqing Ma","Xiangyu Zhang","Neil Gong","Chaowei Xiao","Sarah Erfani","Bo Li","Masashi Sugiyama","Dacheng Tao","James Bailey","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.05206v2.pdf","comment":"47 pages, 3 figures, 11 tables GitHub:\n  https://github.com/xingjunm/Awesome-Large-Model-Safety"},{"id":"http://arxiv.org/abs/2410.20971v2","updated":"2025-02-12T05:52:11Z","published":"2024-10-28T12:43:47Z","title":"BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against\n  Jailbreak Attacks","summary":"  In this paper, we focus on black-box defense for VLMs against jailbreak\nattacks. Existing black-box defense methods are either unimodal or bimodal.\nUnimodal methods enhance either the vision or language module of the VLM, while\nbimodal methods robustify the model through text-image representation\nrealignment. However, these methods suffer from two limitations: 1) they fail\nto fully exploit the cross-modal information, or 2) they degrade the model\nperformance on benign inputs. To address these limitations, we propose a novel\nblue-team method BlueSuffix that defends target VLMs against jailbreak attacks\nwithout compromising its performance under black-box setting. BlueSuffix\nincludes three key components: 1) a visual purifier against jailbreak images,\n2) a textual purifier against jailbreak texts, and 3) a blue-team suffix\ngenerator using reinforcement fine-tuning for enhancing cross-modal robustness.\nWe empirically show on four VLMs (LLaVA, MiniGPT-4, InstructionBLIP, and\nGemini) and four safety benchmarks (Harmful Instruction, AdvBench,\nMM-SafetyBench, and RedTeam-2K) that BlueSuffix outperforms the baseline\ndefenses by a significant margin. Our BlueSuffix opens up a promising direction\nfor defending VLMs against jailbreak attacks. Code is available at\nhttps://github.com/Vinsonzyh/BlueSuffix.\n","authors":["Yunhan Zhao","Xiang Zheng","Lin Luo","Yige Li","Xingjun Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.20971v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14415v2","updated":"2025-02-12T05:41:42Z","published":"2024-12-19T00:06:09Z","title":"DriveGPT: Scaling Autoregressive Behavior Models for Driving","summary":"  We present DriveGPT, a scalable behavior model for autonomous driving. We\nmodel driving as a sequential decision-making task, and learn a transformer\nmodel to predict future agent states as tokens in an autoregressive fashion. We\nscale up our model parameters and training data by multiple orders of\nmagnitude, enabling us to explore the scaling properties in terms of dataset\nsize, model parameters, and compute. We evaluate DriveGPT across different\nscales in a planning task, through both quantitative metrics and qualitative\nexamples, including closed-loop driving in complex real-world scenarios. In a\nseparate prediction task, DriveGPT outperforms state-of-the-art baselines and\nexhibits improved performance by pretraining on a large-scale dataset, further\nvalidating the benefits of data scaling.\n","authors":["Xin Huang","Eric M. Wolff","Paul Vernaza","Tung Phan-Minh","Hongge Chen","David S. Hayden","Mark Edmonds","Brian Pierce","Xinxin Chen","Pratik Elias Jacob","Xiaobai Chen","Chingiz Tairbekov","Pratik Agarwal","Tianshi Gao","Yuning Chai","Siddhartha Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2412.14415v2.pdf","comment":"13 pages, 16 figures, 8 tables, and 1 video link"},{"id":"http://arxiv.org/abs/2502.08137v1","updated":"2025-02-12T05:41:25Z","published":"2025-02-12T05:41:25Z","title":"Riemannian Complex Hermit Positive Definite Convolution Network for\n  Polarimetric SAR Image Classification","summary":"  Deep learning can learn high-level semantic features in Euclidean space\neffectively for PolSAR images, while they need to covert the complex covariance\nmatrix into a feature vector or complex-valued vector as the network input.\nHowever, the complex covariance matrices are essentially a complex Hermit\npositive definite (HPD) matrix endowed in Riemannian manifold rather than\nEuclidean space. The matrix's real and imagery parts are with the same\nsignificance, as the imagery part represents the phase information. The matrix\nvectorization will destroy the geometric structure and manifold characteristics\nof complex covariance matrices. To learn complex HPD matrices directly, we\npropose a Riemannian complex HPD convolution network(HPD\\_CNN) for PolSAR\nimages. This method consists of a complex HPD unfolding network(HPDnet) and a\nCV-3DCNN enhanced network. The proposed complex HPDnet defines the HPD mapping,\nrectifying and the logEig layers to learn geometric features of complex\nmatrices. In addition, a fast eigenvalue decomposition method is designed to\nreduce computation burden. Finally, a Riemannian-to-Euclidean enhanced network\nis defined to enhance contextual information for classification. Experimental\nresults on two real PolSSAR datasets demonstrate the proposed method can\nachieve superior performance than the state-of-the-art methods especially in\nheterogeneous regions.\n","authors":["Junfei Shi","Mengmeng Nie","Yuke Li","Haiyan Jin","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2502.08137v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.14891v3","updated":"2025-02-12T05:38:07Z","published":"2024-09-23T10:38:20Z","title":"Observe Then Act: Asynchronous Active Vision-Action Model for Robotic\n  Manipulation","summary":"  In real-world scenarios, many robotic manipulation tasks are hindered by\nocclusions and limited fields of view, posing significant challenges for\npassive observation-based models that rely on fixed or wrist-mounted cameras.\nIn this paper, we investigate the problem of robotic manipulation under limited\nvisual observation and propose a task-driven asynchronous active vision-action\nmodel.Our model serially connects a camera Next-Best-View (NBV) policy with a\ngripper Next-Best Pose (NBP) policy, and trains them in a sensor-motor\ncoordination framework using few-shot reinforcement learning. This approach\nallows the agent to adjust a third-person camera to actively observe the\nenvironment based on the task goal, and subsequently infer the appropriate\nmanipulation actions.We trained and evaluated our model on 8\nviewpoint-constrained tasks in RLBench. The results demonstrate that our model\nconsistently outperforms baseline algorithms, showcasing its effectiveness in\nhandling visual constraints in manipulation tasks.\n","authors":["Guokang Wang","Hang Li","Shuyuan Zhang","Di Guo","Yanhong Liu","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2409.14891v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08134v1","updated":"2025-02-12T05:34:48Z","published":"2025-02-12T05:34:48Z","title":"A Survey on Data Curation for Visual Contrastive Learning: Why Crafting\n  Effective Positive and Negative Pairs Matters","summary":"  Visual contrastive learning aims to learn representations by contrasting\nsimilar (positive) and dissimilar (negative) pairs of data samples. The design\nof these pairs significantly impacts representation quality, training\nefficiency, and computational cost. A well-curated set of pairs leads to\nstronger representations and faster convergence. As contrastive pre-training\nsees wider adoption for solving downstream tasks, data curation becomes\nessential for optimizing its effectiveness. In this survey, we attempt to\ncreate a taxonomy of existing techniques for positive and negative pair\ncuration in contrastive learning, and describe them in detail.\n","authors":["Shasvat Desai","Debasmita Ghose","Deep Chakraborty"],"pdf_url":"https://arxiv.org/pdf/2502.08134v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.08676v1","updated":"2025-02-12T05:28:10Z","published":"2025-02-12T05:28:10Z","title":"LIR-LIVO: A Lightweight,Robust LiDAR/Vision/Inertial Odometry with\n  Illumination-Resilient Deep Features","summary":"  In this paper, we propose LIR-LIVO, a lightweight and robust\nLiDAR-inertial-visual odometry system designed for challenging illumination and\ndegraded environments. The proposed method leverages deep learning-based\nillumination-resilient features and LiDAR-Inertial-Visual Odometry (LIVO). By\nincorporating advanced techniques such as uniform depth distribution of\nfeatures enabled by depth association with LiDAR point clouds and adaptive\nfeature matching utilizing Superpoint and LightGlue, LIR-LIVO achieves\nstate-of-the-art (SOTA) accuracy and robustness with low computational cost.\nExperiments are conducted on benchmark datasets, including NTU-VIRAL, Hilti'22,\nand R3LIVE-Dataset. The corresponding results demonstrate that our proposed\nmethod outperforms other SOTA methods on both standard and challenging\ndatasets. Particularly, the proposed method demonstrates robust pose estimation\nunder poor ambient lighting conditions in the Hilti'22 dataset. The code of\nthis work is publicly accessible on GitHub to facilitate advancements in the\nrobotics community.\n","authors":["Shujie Zhou","Zihao Wang","Xinye Dai","Weiwei Song","Shengfeng Gu"],"pdf_url":"https://arxiv.org/pdf/2502.08676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09591v3","updated":"2025-02-12T05:19:36Z","published":"2024-04-15T09:01:47Z","title":"3D Gaussian Splatting as Markov Chain Monte Carlo","summary":"  While 3D Gaussian Splatting has recently become popular for neural rendering,\ncurrent methods rely on carefully engineered cloning and splitting strategies\nfor placing Gaussians, which can lead to poor-quality renderings, and reliance\non a good initialization. In this work, we rethink the set of 3D Gaussians as a\nrandom sample drawn from an underlying probability distribution describing the\nphysical representation of the scene-in other words, Markov Chain Monte Carlo\n(MCMC) samples. Under this view, we show that the 3D Gaussian updates can be\nconverted as Stochastic Gradient Langevin Dynamics (SGLD) updates by simply\nintroducing noise. We then rewrite the densification and pruning strategies in\n3D Gaussian Splatting as simply a deterministic state transition of MCMC\nsamples, removing these heuristics from the framework. To do so, we revise the\n'cloning' of Gaussians into a relocalization scheme that approximately\npreserves sample probability. To encourage efficient use of Gaussians, we\nintroduce a regularizer that promotes the removal of unused Gaussians. On\nvarious standard evaluation scenes, we show that our method provides improved\nrendering quality, easy control over the number of Gaussians, and robustness to\ninitialization.\n","authors":["Shakiba Kheradmand","Daniel Rebain","Gopal Sharma","Weiwei Sun","Jeff Tseng","Hossam Isack","Abhishek Kar","Andrea Tagliasacchi","Kwang Moo Yi"],"pdf_url":"https://arxiv.org/pdf/2404.09591v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11811v3","updated":"2025-02-12T05:16:17Z","published":"2024-08-21T17:57:06Z","title":"EmbodiedSAM: Online Segment Any 3D Thing in Real Time","summary":"  Embodied tasks require the agent to fully understand 3D scenes simultaneously\nwith its exploration, so an online, real-time, fine-grained and\nhighly-generalized 3D perception model is desperately needed. Since\nhigh-quality 3D data is limited, directly training such a model in 3D is almost\ninfeasible. Meanwhile, vision foundation models (VFM) has revolutionized the\nfield of 2D computer vision with superior performance, which makes the use of\nVFM to assist embodied 3D perception a promising direction. However, most\nexisting VFM-assisted 3D perception methods are either offline or too slow that\ncannot be applied in practical embodied tasks. In this paper, we aim to\nleverage Segment Anything Model (SAM) for real-time 3D instance segmentation in\nan online setting. This is a challenging problem since future frames are not\navailable in the input streaming RGB-D video, and an instance may be observed\nin several frames so object matching between frames is required. To address\nthese challenges, we first propose a geometric-aware query lifting module to\nrepresent the 2D masks generated by SAM by 3D-aware queries, which is then\niteratively refined by a dual-level query decoder. In this way, the 2D masks\nare transferred to fine-grained shapes on 3D point clouds. Benefit from the\nquery representation for 3D masks, we can compute the similarity matrix between\nthe 3D masks from different views by efficient matrix operation, which enables\nreal-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan\nshow our method achieves leading performance even compared with offline\nmethods. Our method also demonstrates great generalization ability in several\nzero-shot dataset transferring experiments and show great potential in\nopen-vocabulary and data-efficient setting. Code and demo are available at\nhttps://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for\ntraining and evaluation.\n","authors":["Xiuwei Xu","Huangxing Chen","Linqing Zhao","Ziwei Wang","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2408.11811v3.pdf","comment":"ICLR25 Oral. Project page: https://xuxw98.github.io/ESAM/"},{"id":"http://arxiv.org/abs/2404.13016v3","updated":"2025-02-12T04:59:47Z","published":"2024-04-19T17:25:43Z","title":"Optimizing Calibration by Gaining Aware of Prediction Correctness","summary":"  Model calibration aims to align confidence with prediction correctness. The\nCross-Entropy (CE) loss is widely used for calibrator training, which enforces\nthe model to increase confidence on the ground truth class. However, we find\nthe CE loss has intrinsic limitations. For example, for a narrow\nmisclassification (e.g., a test sample is wrongly classified and its softmax\nscore on the ground truth class is 0.4), a calibrator trained by the CE loss\noften produces high confidence on the wrongly predicted class, which is\nundesirable. In this paper, we propose a new post-hoc calibration objective\nderived from the aim of calibration. Intuitively, the proposed objective\nfunction asks that the calibrator decrease model confidence on wrongly\npredicted samples and increase confidence on correctly predicted samples.\nBecause a sample itself has insufficient ability to indicate correctness, we\nuse its transformed versions (e.g., rotated, greyscaled, and color-jittered)\nduring calibrator training. Trained on an in-distribution validation set and\ntested with isolated, individual test samples, our method achieves competitive\ncalibration performance on both in-distribution and out-of-distribution test\nsets compared with the state of the art. Further, our analysis points out the\ndifference between our method and commonly used objectives such as CE loss and\nMean Square Error (MSE) loss, where the latters sometimes deviates from the\ncalibration aim.\n","authors":["Yuchi Liu","Lei Wang","Yuli Zou","James Zou","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.13016v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01162v2","updated":"2025-02-12T04:35:28Z","published":"2024-09-02T10:49:10Z","title":"Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic\n  Optimized Token Representation in Multimodal Large Language Models","summary":"  Recently, multimodal large language models (MM-LLMs) have achieved\nsignificant success in various tasks, but their high computational costs limit\nwidespread application. The main computational burden arises from processing\nconcatenated text and visual tokens in the LLM layer, where input token length\ndirectly affects efficiency. Our analysis of visual tokens reveals that their\nsimilarity to the CLS token follows a long-tail distribution, with only a few\nshowing high similarity. To address this, we propose a dynamic pruning\nalgorithm that identifies the inflection point in the visual CLS token\nsimilarity curve, enabling effective trimming of visual markers to accelerate\nmodel performance. Additionally, we perform a second round of pruning in the\nLLM layer, filtering out low-correlation tokens through the interaction between\nvisual and textual features. Experimental results demonstrate that our method\nachieves performance comparable to the original while utilizing only 22% of the\noriginal token quantity. Our source code will be made publicly available upon\nacceptance.\n","authors":["Gaotong Yu","Yi Chen","Jian Xu"],"pdf_url":"https://arxiv.org/pdf/2409.01162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06114v2","updated":"2025-02-12T04:33:28Z","published":"2025-02-10T02:48:56Z","title":"A Novel Multi-Teacher Knowledge Distillation for Real-Time Object\n  Detection using 4D Radar","summary":"  Accurate 3D object detection is crucial for safe autonomous navigation,\nrequiring reliable performance across diverse weather conditions. While LiDAR\nperformance deteriorates in challenging weather, Radar systems maintain their\nreliability. Traditional Radars have limitations due to their lack of elevation\ndata, but the recent 4D Radars overcome this by measuring elevation alongside\nrange, azimuth, and Doppler velocity, making them invaluable for autonomous\nvehicles. The primary challenge in utilizing 4D Radars is the sparsity of their\npoint clouds. Previous works address this by developing architectures that\nbetter capture semantics and context in sparse point cloud, largely drawing\nfrom LiDAR-based approaches. However, these methods often overlook a unique\nadvantage of 4D Radars: the dense Radar tensor, which encapsulates power\nmeasurements across three spatial dimensions and the Doppler dimension. Our\npaper leverages this tensor to tackle the sparsity issue. We introduce a novel\nknowledge distillation framework that enables a student model to densify its\nsparse input in the latent space by emulating an ensemble of teacher models.\nOur experiments demonstrate a 25% performance improvement over the\nstate-of-the-art RTNH model on the K-Radar dataset. Notably, this improvement\nis achieved while still maintaining a real-time inference speed.\n","authors":["Seung-Hyun Song","Dong-Hee Paek","Minh-Quan Dao","Ezio Malis","Seung-Hyun Kong"],"pdf_url":"https://arxiv.org/pdf/2502.06114v2.pdf","comment":"Arxiv preprint"},{"id":"http://arxiv.org/abs/2310.07916v3","updated":"2025-02-12T04:19:43Z","published":"2023-10-11T22:04:33Z","title":"Dynamic Appearance Particle Neural Radiance Field","summary":"  Neural Radiance Fields (NeRFs) have shown great potential in modeling 3D\nscenes. Dynamic NeRFs extend this model by capturing time-varying elements,\ntypically using deformation fields. The existing dynamic NeRFs employ a similar\nEulerian representation for both light radiance and deformation fields. This\nleads to a close coupling of appearance and motion and lacks a physical\ninterpretation. In this work, we propose Dynamic Appearance Particle Neural\nRadiance Field (DAP-NeRF), which introduces particle-based representation to\nmodel the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists\nof the superposition of a static field and a dynamic field. The dynamic field\nis quantized as a collection of appearance particles, which carries the visual\ninformation of a small dynamic element in the scene and is equipped with a\nmotion model. All components, including the static field, the visual features\nand the motion models of particles, are learned from monocular videos without\nany prior geometric knowledge of the scene. We develop an efficient\ncomputational framework for the particle-based model. We also construct a new\ndataset to evaluate motion modeling. Experimental results show that DAP-NeRF is\nan effective technique to capture not only the appearance but also the\nphysically meaningful motions in a 3D dynamic scene. Code is available at:\nhttps://github.com/Cenbylin/DAP-NeRF.\n","authors":["Ancheng Lin","Yusheng Xiang","Jun Li","Mukesh Prasad"],"pdf_url":"https://arxiv.org/pdf/2310.07916v3.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.19313v3","updated":"2025-02-12T23:37:50Z","published":"2024-10-25T05:59:30Z","title":"COAT: Compressing Optimizer states and Activation for Memory-Efficient\n  FP8 Training","summary":"  FP8 training has emerged as a promising method for improving training\nefficiency. Existing frameworks accelerate training by applying FP8 computation\nto linear layers while leaving optimizer states and activations in higher\nprecision, which fails to fully optimize memory usage. This paper introduces\nCOAT (Compressing Optimizer States and Activations for FP8 Training), a novel\nFP8 training framework designed to significantly reduce memory footprint when\ntraining large models. COAT addresses current limitations through two key\ninnovations: (1) Dynamic Range Expansion, which aligns optimizer state\ndistributions more closely with the FP8 representation range, thereby reducing\nquantization error, and (2) Mixed-Granularity Activation Quantization, which\noptimizes activation memory using a combination of per-tensor and per-group\nquantization strategies. Experiments demonstrate that COAT effectively reduces\nend-to-end training memory footprint by 1.54x compared to BF16 while achieving\nnearly lossless performance across various tasks, such as Large Language Model\npretraining and fine-tuning and Vision Language Model training. COAT also\nachieves a 1.43x end-to-end training speedup compared to BF16, performing on\npar with or surpassing TransformerEngine's speedup. COAT enables efficient\nfull-parameter training of large models on fewer GPUs, and facilitates doubling\nthe batch size in distributed training settings, providing a practical solution\nfor scaling large-scale model training. The code is available at\nhttps://github.com/NVlabs/COAT.\n","authors":["Haocheng Xi","Han Cai","Ligeng Zhu","Yao Lu","Kurt Keutzer","Jianfei Chen","Song Han"],"pdf_url":"https://arxiv.org/pdf/2410.19313v3.pdf","comment":"Accepted by ICLR 2025. 22 pages. 9 Figures. 13 Tables"},{"id":"http://arxiv.org/abs/2401.16754v3","updated":"2025-02-12T23:28:07Z","published":"2024-01-30T05:22:45Z","title":"AI Oversight and Human Mistakes: Evidence from Centre Court","summary":"  Powered by the increasing predictive capabilities of machine learning\nalgorithms, artificial intelligence (AI) systems have the potential to overrule\nhuman mistakes in many settings. We provide the first field evidence that the\nuse of AI oversight can impact human decision-making. We investigate one of the\nhighest visibility settings where AI oversight has occurred: Hawk-Eye review of\numpires in top tennis tournaments. We find that umpires lowered their overall\nmistake rate after the introduction of Hawk-Eye review, but also that umpires\nincreased the rate at which they called balls in, producing a shift from making\nType II errors (calling a ball out when in) to Type I errors (calling a ball in\nwhen out). We structurally estimate the psychological costs of being overruled\nby AI using a model of attention-constrained umpires, and our results suggest\nthat because of these costs, umpires cared 37% more about Type II errors under\nAI oversight.\n","authors":["David Almog","Romain Gauriot","Lionel Page","Daniel Martin"],"pdf_url":"https://arxiv.org/pdf/2401.16754v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06453v2","updated":"2025-02-12T23:16:27Z","published":"2025-02-10T13:31:46Z","title":"MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard\n  Perturbations","summary":"  Large language models have demonstrated impressive performance on challenging\nmathematical reasoning tasks, which has triggered the discussion of whether the\nperformance is achieved by true reasoning capability or memorization. To\ninvestigate this question, prior work has constructed mathematical benchmarks\nwhen questions undergo simple perturbations -- modifications that still\npreserve the underlying reasoning patterns of the solutions. However, no work\nhas explored hard perturbations, which fundamentally change the nature of the\nproblem so that the original solution steps do not apply. To bridge the gap, we\nconstruct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard\nperturbation, respectively. Each consists of 279 perturbed math problems\nderived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.\nal., 2021). We observe significant performance drops on MATH-P-Hard across\nvarious models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking\n(-12.9%). We also raise concerns about a novel form of memorization where\nmodels blindly apply learned problem-solving skills without assessing their\napplicability to modified contexts. This issue is amplified when using original\nproblems for in-context learning. We call for research efforts to address this\nchallenge, which is critical for developing more robust and reliable reasoning\nmodels.\n","authors":["Kaixuan Huang","Jiacheng Guo","Zihao Li","Xiang Ji","Jiawei Ge","Wenzhe Li","Yingqing Guo","Tianle Cai","Hui Yuan","Runzhe Wang","Yue Wu","Ming Yin","Shange Tang","Yangsibo Huang","Chi Jin","Xinyun Chen","Chiyuan Zhang","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06453v2.pdf","comment":"v2: fix bugs in Fig. 1"},{"id":"http://arxiv.org/abs/2406.06736v2","updated":"2025-02-12T23:14:07Z","published":"2024-06-10T18:57:06Z","title":"Long-Term Fairness Inquiries and Pursuits in Machine Learning: A Survey\n  of Notions, Methods, and Challenges","summary":"  The widespread integration of Machine Learning systems in daily life,\nparticularly in high-stakes domains, has raised concerns about the fairness\nimplications. While prior works have investigated static fairness measures,\nrecent studies reveal that automated decision-making has long-term implications\nand that off-the-shelf fairness approaches may not serve the purpose of\nachieving long-term fairness. Additionally, the existence of feedback loops and\nthe interaction between models and the environment introduces additional\ncomplexities that may deviate from the initial fairness goals. In this survey,\nwe review existing literature on long-term fairness from different perspectives\nand present a taxonomy for long-term fairness studies. We highlight key\nchallenges and consider future research directions, analyzing both current\nissues and potential further explorations.\n","authors":["Usman Gohar","Zeyu Tang","Jialu Wang","Kun Zhang","Peter L. Spirtes","Yang Liu","Lu Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.06736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08834v1","updated":"2025-02-12T22:51:54Z","published":"2025-02-12T22:51:54Z","title":"A Reversible Solver for Diffusion SDEs","summary":"  Diffusion models have quickly become the state-of-the-art for generation\ntasks across many different data modalities. An important ability of diffusion\nmodels is the ability to encode samples from the data distribution back into\nthe sampling prior distribution. This is useful for performing alterations to\nreal data samples along with guided generation via the continuous adjoint\nequations. We propose an algebraically reversible solver for diffusion SDEs\nthat can exactly invert real data samples into the prior distribution.\n","authors":["Zander W. Blasingame","Chen Liu"],"pdf_url":"https://arxiv.org/pdf/2502.08834v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.08829v1","updated":"2025-02-12T22:35:29Z","published":"2025-02-12T22:35:29Z","title":"PLayer-FL: A Principled Approach to Personalized Layer-wise Cross-Silo\n  Federated Learning","summary":"  Non-identically distributed data is a major challenge in Federated Learning\n(FL). Personalized FL tackles this by balancing local model adaptation with\nglobal model consistency. One variant, partial FL, leverages the observation\nthat early layers learn more transferable features by federating only early\nlayers. However, current partial FL approaches use predetermined,\narchitecture-specific rules to select layers, limiting their applicability. We\nintroduce Principled Layer-wise-FL (PLayer-FL), which uses a novel federation\nsensitivity metric to identify layers that benefit from federation. This\nmetric, inspired by model pruning, quantifies each layer's contribution to\ncross-client generalization after the first training epoch, identifying a\ntransition point in the network where the benefits of federation diminish. We\nfirst demonstrate that our federation sensitivity metric shows strong\ncorrelation with established generalization measures across diverse\narchitectures. Next, we show that PLayer-FL outperforms existing FL algorithms\non a range of tasks, also achieving more uniform performance improvements\nacross clients.\n","authors":["Ahmed Elhussein","Gamze G√ºrsoy"],"pdf_url":"https://arxiv.org/pdf/2502.08829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08828v1","updated":"2025-02-12T22:34:50Z","published":"2025-02-12T22:34:50Z","title":"A Survey on Data-Centric AI: Tabular Learning from Reinforcement\n  Learning and Generative AI Perspective","summary":"  Tabular data is one of the most widely used data formats across various\ndomains such as bioinformatics, healthcare, and marketing. As artificial\nintelligence moves towards a data-centric perspective, improving data quality\nis essential for enhancing model performance in tabular data-driven\napplications. This survey focuses on data-driven tabular data optimization,\nspecifically exploring reinforcement learning (RL) and generative approaches\nfor feature selection and feature generation as fundamental techniques for\nrefining data spaces. Feature selection aims to identify and retain the most\ninformative attributes, while feature generation constructs new features to\nbetter capture complex data patterns. We systematically review existing\ngenerative methods for tabular data engineering, analyzing their latest\nadvancements, real-world applications, and respective strengths and\nlimitations. This survey emphasizes how RL-based and generative techniques\ncontribute to the automation and intelligence of feature engineering. Finally,\nwe summarize the existing challenges and discuss future research directions,\naiming to provide insights that drive continued innovation in this field.\n","authors":["Wangyang Ying","Cong Wei","Nanxu Gong","Xinyuan Wang","Haoyue Bai","Arun Vignesh Malarkkan","Sixun Dong","Dongjie Wang","Denghui Zhang","Yanjie Fu"],"pdf_url":"https://arxiv.org/pdf/2502.08828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07072v2","updated":"2025-02-12T22:33:02Z","published":"2024-06-11T08:59:20Z","title":"On the relation between trainability and dequantization of variational\n  quantum learning models","summary":"  The quest for successful variational quantum machine learning (QML) relies on\nthe design of suitable parametrized quantum circuits (PQCs), as analogues to\nneural networks in classical machine learning. Successful QML models must\nfulfill the properties of trainability and non-dequantization, among others.\nRecent works have highlighted an intricate interplay between trainability and\ndequantization of such models, which is still unresolved. In this work we\ncontribute to this debate from the perspective of machine learning, proving a\nnumber of results identifying, among others when trainability and\nnon-dequantization are not mutually exclusive. We begin by providing a number\nof new somewhat broader definitions of the relevant concepts, compared to what\nis found in other literature, which are operationally motivated, and consistent\nwith prior art. With these precise definitions given and motivated, we then\nstudy the relation between trainability and dequantization of variational QML.\nNext, we also discuss the degrees of \"variationalness\" of QML models, where we\ndistinguish between models like the hardware efficient ansatz and quantum\nkernel methods. Finally, we introduce recipes for building PQC-based QML models\nwhich are both trainable and nondequantizable, and corresponding to different\ndegrees of variationalness. We do not address the practical utility for such\nmodels. Our work however does point toward a way forward for finding more\ngeneral constructions, for which finding applications may become feasible.\n","authors":["Elies Gil-Fuster","Casper Gyurik","Adri√°n P√©rez-Salinas","Vedran Dunjko"],"pdf_url":"https://arxiv.org/pdf/2406.07072v2.pdf","comment":"25 pages (14+11), 3 figures"},{"id":"http://arxiv.org/abs/2502.04397v2","updated":"2025-02-12T22:26:50Z","published":"2025-02-06T06:58:09Z","title":"Multimodal Medical Code Tokenizer","summary":"  Foundation models trained on patient electronic health records (EHRs) require\ntokenizing medical data into sequences of discrete vocabulary items. Existing\ntokenizers treat medical codes from EHRs as isolated textual tokens. However,\neach medical code is defined by its textual description, its position in\nontological hierarchies, and its relationships to other codes, such as disease\nco-occurrences and drug-treatment associations. Medical vocabularies contain\nmore than 600,000 codes with critical information for clinical reasoning. We\nintroduce MedTok, a multimodal medical code tokenizer that uses the text\ndescriptions and relational context of codes. MedTok processes text using a\nlanguage model encoder and encodes the relational structure with a graph\nencoder. It then quantizes both modalities into a unified token space,\npreserving modality-specific and cross-modality information. We integrate\nMedTok into five EHR models and evaluate it on operational and clinical tasks\nacross in-patient and out-patient datasets, including outcome prediction,\ndiagnosis classification, drug recommendation, and risk stratification.\nSwapping standard EHR tokenizers with MedTok improves AUPRC across all EHR\nmodels, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with\nthe largest gains in drug recommendation. Beyond EHR modeling, we demonstrate\nusing MedTok tokenizer with medical QA systems. Our results demonstrate the\npotential of MedTok as a unified tokenizer for medical codes, improving\ntokenization for medical foundation models.\n","authors":["Xiaorui Su","Shvat Messica","Yepeng Huang","Ruth Johnson","Lukas Fesser","Shanghua Gao","Faryad Sahneh","Marinka Zitnik"],"pdf_url":"https://arxiv.org/pdf/2502.04397v2.pdf","comment":"conference"},{"id":"http://arxiv.org/abs/2502.08821v1","updated":"2025-02-12T22:24:49Z","published":"2025-02-12T22:24:49Z","title":"DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with\n  Saliency Maps","summary":"  The recent surge in advanced generative models, such as diffusion models and\ngenerative adversarial networks (GANs), has led to an alarming rise in\nAI-generated images across various domains on the web. While such technologies\noffer benefits such as democratizing artistic creation, they also pose\nchallenges in misinformation, digital forgery, and authenticity verification.\nAdditionally, the uncredited use of AI-generated images in media and marketing\nhas sparked significant backlash from online communities. In response to this,\nwe introduce DejAIvu, a Chrome Web extension that combines real-time\nAI-generated image detection with saliency-based explainability while users\nbrowse the web. Using an ONNX-optimized deep learning model, DejAIvu\nautomatically analyzes images on websites such as Google Images, identifies\nAI-generated content using model inference, and overlays a saliency heatmap to\nhighlight AI-related artifacts. Our approach integrates efficient in-browser\ninference, gradient-based saliency analysis, and a seamless user experience,\nensuring that AI detection is both transparent and interpretable. We also\nevaluate DejAIvu across multiple pretrained architectures and benchmark\ndatasets, demonstrating high accuracy and low latency, making it a practical\nand deployable tool for enhancing AI image accountability. The code for this\nsystem can be found at https://github.com/Noodulz/dejAIvu.\n","authors":["Jocelyn Dzuong"],"pdf_url":"https://arxiv.org/pdf/2502.08821v1.pdf","comment":"5 pages, 3 figures, submitted to IJCAI 2025 demo track"},{"id":"http://arxiv.org/abs/2409.05975v3","updated":"2025-02-12T22:16:55Z","published":"2024-09-09T18:18:47Z","title":"CoDiCast: Conditional Diffusion Model for Global Weather Prediction with\n  Uncertainty Quantification","summary":"  Accurate weather forecasting is critical for science and society. Yet,\nexisting methods have not managed to simultaneously have the properties of high\naccuracy, low uncertainty, and high computational efficiency. On one hand, to\nquantify the uncertainty in weather predictions, the strategy of ensemble\nforecast (i.e., generating a set of diverse predictions) is often employed.\nHowever, traditional ensemble numerical weather prediction (NWP) is\ncomputationally intensive. On the other hand, most existing machine\nlearning-based weather prediction (MLWP) approaches are efficient and accurate.\nNevertheless, they are deterministic and cannot capture the uncertainty of\nweather forecasting. In this work, we propose CoDiCast, a conditional diffusion\nmodel to generate accurate global weather prediction, while achieving\nuncertainty quantification with ensemble forecasts and modest computational\ncost. The key idea is to simulate a conditional version of the reverse\ndenoising process in diffusion models, which starts from pure Gaussian noise to\ngenerate realistic weather scenarios for a future time point. Each denoising\nstep is conditioned on observations from the recent past. Ensemble forecasts\nare achieved by repeatedly sampling from stochastic Gaussian noise to represent\nuncertainty quantification. CoDiCast is trained on a decade of ERA5 reanalysis\ndata from the European Centre for Medium-Range Weather Forecasts (ECMWF).\nExperimental results demonstrate that our approach outperforms several existing\ndata-driven methods in accuracy. Our conditional diffusion model, CoDiCast, can\ngenerate 6-day global weather forecasts, at 6-hour steps and $5.625^\\circ$\nlatitude-longitude resolution, for over 5 variables, in about 12 minutes on a\ncommodity A100 GPU machine with 80GB memory. The open-souced code is provided\nat https://github.com/JimengShi/CoDiCast.\n","authors":["Jimeng Shi","Bowen Jin","Jiawei Han","Sundararaman Gopalakrishnan","Giri Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2409.05975v3.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2502.08808v1","updated":"2025-02-12T21:44:06Z","published":"2025-02-12T21:44:06Z","title":"A First-order Generative Bilevel Optimization Framework for Diffusion\n  Models","summary":"  Diffusion models, which iteratively denoise data samples to synthesize\nhigh-quality outputs, have achieved empirical success across domains. However,\noptimizing these models for downstream tasks often involves nested bilevel\nstructures, such as tuning hyperparameters for fine-tuning tasks or noise\nschedules in training dynamics, where traditional bilevel methods fail due to\nthe infinite-dimensional probability space and prohibitive sampling costs. We\nformalize this challenge as a generative bilevel optimization problem and\naddress two key scenarios: (1) fine-tuning pre-trained models via an\ninference-only lower-level solver paired with a sample-efficient gradient\nestimator for the upper level, and (2) training diffusion models from scratch\nwith noise schedule optimization by reparameterizing the lower-level problem\nand designing a computationally tractable gradient estimator. Our first-order\nbilevel framework overcomes the incompatibility of conventional bilevel methods\nwith diffusion processes, offering theoretical grounding and computational\npracticality. Experiments demonstrate that our method outperforms existing\nfine-tuning and hyperparameter search baselines.\n","authors":["Quan Xiao","Hui Yuan","A F M Saif","Gaowen Liu","Ramana Kompella","Mengdi Wang","Tianyi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.08808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08807v1","updated":"2025-02-12T21:43:51Z","published":"2025-02-12T21:43:51Z","title":"InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data\n  Volume Variation in DNNs","summary":"  The rise of deep neural networks (DNNs) has driven a boom in AI services,\nwhich results in an increased demand for computing power and memory. In modern\nDNNs, the data sizes produced and consumed are highly varied across operations\n(high data volume variation, HDV). Because existing design paradigms use fixed\nexecution patterns that lead to either low computational efficiency due to\npipeline stalls or frequent off-chip memory accesses to manage large\nintermediate data, HDV applications are challenging to accelerate on FPGAs. To\naddress these challenges, we introduce the Inter-Task Auto-Reconfigurable\nAccelerator (InTAR), a novel accelerator design for HDV applications on FPGAs.\nInTAR combines the high computational efficiency of sequential execution with\nthe reduced off-chip memory overhead of dataflow execution. It switches\nexecution patterns automatically with a static schedule determined before\ncircuit design based on resource constraints and model parameters. Unlike\nprevious reconfigurable accelerators, InTAR encodes reconfiguration schedules\nduring circuit design, allowing model-specific optimizations that allocate only\nthe necessary logic and interconnects. Thus, InTAR achieves a high clock\nfrequency with fewer resources and low reconfiguration time. Furthermore, InTAR\nsupports high-level tools such as HLS for fast design generation. We implement\na set of multi-task kernels in various HDV DNNs using InTAR. Compared with\ndataflow and sequential accelerators, InTAR exhibits $1.8\\times$ and $7.1\n\\times$ speedups correspondingly. We also implement InTAR for GPT-2 medium as a\nmore complex example, which achieves a speedup of $\\mathbf{3.65 \\sim\n39.14\\times}$ and a $\\mathbf{1.72 \\sim 10.44\\times}$ boost in DSP efficiency\ncompared to the corresponding SoTA accelerators on FPGAs.\n","authors":["Zifan He","Anderson Truong","Yingqi Cao","Jason Cong"],"pdf_url":"https://arxiv.org/pdf/2502.08807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08806v1","updated":"2025-02-12T21:42:56Z","published":"2025-02-12T21:42:56Z","title":"CLOVER: A Test Case Generation Benchmark with Coverage, Long-Context,\n  and Verification","summary":"  Software testing is a critical aspect of software development, yet generating\ntest cases remains a routine task for engineers. This paper presents a\nbenchmark, CLOVER, to evaluate models' capabilities in generating and\ncompleting test cases under specific conditions. Spanning from simple assertion\ncompletions to writing test cases that cover specific code blocks across\nmultiple files, these tasks are based on 12 python repositories, analyzing 845\nproblems with context lengths ranging from 4k to 128k tokens. Utilizing code\ntesting frameworks, we propose a method to construct retrieval contexts using\ncoverage information. While models exhibit comparable performance with short\ncontexts, notable differences emerge with 16k contexts. Notably, models like\nGPT-4o and Claude 3.5 can effectively leverage relevant snippets; however, all\nmodels score below 35\\% on the complex Task III, even with the oracle context\nprovided, underscoring the benchmark's significance and the potential for model\nimprovement. The benchmark is containerized for code execution across tasks,\nand we will release the code, data, and construction methodologies.\n","authors":["Jiacheng Xu","Bo Pang","Jin Qu","Hiroaki Hayashi","Caiming Xiong","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08806v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2410.18870v2","updated":"2025-02-12T21:37:54Z","published":"2024-10-24T15:57:17Z","title":"End-to-end Training for Recommendation with Language-based User Profiles","summary":"  There is a growing interest in natural language-based user profiles for\nrecommender systems, which aims to enhance transparency and scrutability\ncompared with embedding-based methods. Existing studies primarily generate\nthese profiles using zero-shot inference from large language models (LLMs), but\ntheir quality remains insufficient, leading to suboptimal recommendation\nperformance. In this paper, we introduce LangPTune, the first end-to-end\ntraining framework to optimize LLM-generated user profiles. Our method\nsignificantly outperforms zero-shot approaches by explicitly training the LLM\nfor the recommendation objective. Through extensive evaluations across diverse\ntraining configurations and benchmarks, we demonstrate that LangPTune not only\nsurpasses zero-shot baselines but can also matches the performance of\nstate-of-the-art embedding-based methods. Finally, we investigate whether the\ntraining procedure preserves the interpretability of these profiles compared to\nzero-shot inference through both GPT-4 simulations and crowdworker user\nstudies. Implementation of LangPTune can be found at\nhttps://github.com/ZhaolinGao/LangPTune.\n","authors":["Zhaolin Gao","Joyce Zhou","Yijia Dai","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2410.18870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08803v1","updated":"2025-02-12T21:32:51Z","published":"2025-02-12T21:32:51Z","title":"Deep EEG Super-Resolution: Upsampling EEG Spatial Resolution with\n  Generative Adversarial Networks","summary":"  Electroencephalography (EEG) activity contains a wealth of information about\nwhat is happening within the human brain. Recording more of this data has the\npotential to unlock endless future applications. However, the cost of EEG\nhardware is increasingly expensive based upon the number of EEG channels being\nrecorded simultaneously. We combat this problem in this paper by proposing a\nnovel deep EEG super-resolution (SR) approach based on Generative Adversarial\nNetworks (GANs). This approach can produce high spatial resolution EEG data\nfrom low resolution samples, by generating channel-wise upsampled data to\neffectively interpolate numerous missing channels, thus reducing the need for\nexpensive EEG equipment. We tested the performance using an EEG dataset from a\nmental imagery task. Our proposed GAN model provided 10^4 fold and 10^2 fold\nreduction in mean-squared error (MSE) and mean-absolute error (MAE),\nrespectively, over the baseline bicubic interpolation method. We further\nvalidate our method by training a classifier on the original classification\ntask, which displayed minimal loss in accuracy while using the super-resolved\ndata. The proposed SR EEG by GAN is a promising approach to improve the spatial\nresolution of low density EEG headsets.\n","authors":["Isaac Corley","Yufei Huang"],"pdf_url":"https://arxiv.org/pdf/2502.08803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06808v2","updated":"2025-02-12T21:24:00Z","published":"2025-02-04T03:04:04Z","title":"On the Benefits of Attribute-Driven Graph Domain Adaptation","summary":"  Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network\nlearning, particularly pertinent due to the absence of labeled data in\nreal-world graph datasets. Recent studies attempted to learn domain invariant\nrepresentations by eliminating structural shifts between graphs. In this work,\nwe show that existing methodologies have overlooked the significance of the\ngraph node attribute, a pivotal factor for graph domain alignment.\nSpecifically, we first reveal the impact of node attributes for GDA by\ntheoretically proving that in addition to the graph structural divergence\nbetween the domains, the node attribute discrepancy also plays a critical role\nin GDA. Moreover, we also empirically show that the attribute shift is more\nsubstantial than the topology shift, which further underscores the importance\nof node attribute alignment in GDA. Inspired by this finding, a novel\ncross-channel module is developed to fuse and align both views between the\nsource and target graphs for GDA. Experimental results on a variety of\nbenchmarks verify the effectiveness of our method.\n","authors":["Ruiyi Fang","Bingheng Li","Zhao Kang","Qiuhao Zeng","Ruizhi Pu","Nima Hosseini Dashtbayaz","Boyu Wang","Charles Ling"],"pdf_url":"https://arxiv.org/pdf/2502.06808v2.pdf","comment":"Accepted by the ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08795v1","updated":"2025-02-12T21:19:28Z","published":"2025-02-12T21:19:28Z","title":"Low-Resolution Neural Networks","summary":"  The expanding scale of large neural network models introduces significant\nchallenges, driving efforts to reduce memory usage and enhance computational\nefficiency. Such measures are crucial to ensure the practical implementation\nand effective application of these sophisticated models across a wide array of\nuse cases. This study examines the impact of parameter bit precision on model\nperformance compared to standard 32-bit models, with a focus on multiclass\nobject classification in images. The models analyzed include those with fully\nconnected layers, convolutional layers, and transformer blocks, with model\nweight resolution ranging from 1 bit to 4.08 bits. The findings indicate that\nmodels with lower parameter bit precision achieve results comparable to 32-bit\nmodels, showing promise for use in memory-constrained devices. While\nlow-resolution models with a small number of parameters require more training\nepochs to achieve accuracy comparable to 32-bit models, those with a large\nnumber of parameters achieve similar performance within the same number of\nepochs. Additionally, data augmentation can destabilize training in\nlow-resolution models, but including zero as a potential value in the weight\nparameters helps maintain stability and prevents performance degradation.\nOverall, 2.32-bit weights offer the optimal balance of memory reduction,\nperformance, and efficiency. However, further research should explore other\ndataset types and more complex and larger models. These findings suggest a\npotential new era for optimized neural network models with reduced memory\nrequirements and improved computational efficiency, though advancements in\ndedicated hardware are necessary to fully realize this potential.\n","authors":["Eduardo Lobo Lustosa Cabral","Larissa Driemeier"],"pdf_url":"https://arxiv.org/pdf/2502.08795v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.08794v1","updated":"2025-02-12T21:17:30Z","published":"2025-02-12T21:17:30Z","title":"Spectral Journey: How Transformers Predict the Shortest Path","summary":"  Decoder-only transformers lead to a step-change in capability of large\nlanguage models. However, opinions are mixed as to whether they are really\nplanning or reasoning. A path to making progress in this direction is to study\nthe model's behavior in a setting with carefully controlled data. Then\ninterpret the learned representations and reverse-engineer the computation\nperformed internally. We study decoder-only transformer language models trained\nfrom scratch to predict shortest paths on simple, connected and undirected\ngraphs. In this setting, the representations and the dynamics learned by the\nmodel are interpretable. We present three major results: (1) Two-layer\ndecoder-only language models can learn to predict shortest paths on simple,\nconnected graphs containing up to 10 nodes. (2) Models learn a graph embedding\nthat is correlated with the spectral decomposition of the line graph. (3)\nFollowing the insights, we discover a novel approximate path-finding algorithm\nSpectral Line Navigator (SLN) that finds shortest path by greedily selecting\nnodes in the space of spectral embedding of the line graph.\n","authors":["Andrew Cohen","Andrey Gromov","Kaiyu Yang","Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2502.08794v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2502.08788v1","updated":"2025-02-12T21:01:10Z","published":"2025-02-12T21:01:10Z","title":"If Multi-Agent Debate is the Answer, What is the Question?","summary":"  Multi-agent debate (MAD) has emerged as a promising approach to enhance the\nfactual accuracy and reasoning quality of large language models (LLMs) by\nengaging multiple agents in iterative discussions during inference. Despite its\npotential, we argue that current MAD research suffers from critical\nshortcomings in evaluation practices, including limited dataset overlap and\ninconsistent baselines, raising significant concerns about generalizability.\nCorrespondingly, this paper presents a systematic evaluation of five\nrepresentative MAD methods across nine benchmarks using four foundational\nmodels. Surprisingly, our findings reveal that MAD methods fail to reliably\noutperform simple single-agent baselines such as Chain-of-Thought and\nSelf-Consistency, even when consuming additional inference-time computation.\nFrom our analysis, we found that model heterogeneity can significantly improve\nMAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the\noutput from heterogeneous foundation models, which boosts the performance of\ncurrent MAD frameworks. Finally, we outline potential directions for advancing\nMAD, aiming to spark a broader conversation and inspire future work in this\narea.\n","authors":["Hangfan Zhang","Zhiyao Cui","Xinrun Wang","Qiaosheng Zhang","Zhen Wang","Dinghao Wu","Shuyue Hu"],"pdf_url":"https://arxiv.org/pdf/2502.08788v1.pdf","comment":"This position paper takes a critical view of the status quo of MAD\n  research, and outline multiple potential directions to improve MAD"},{"id":"http://arxiv.org/abs/2502.08785v1","updated":"2025-02-12T20:56:17Z","published":"2025-02-12T20:56:17Z","title":"Decision Tree Based Wrappers for Hearing Loss","summary":"  Audiology entities are using Machine Learning (ML) models to guide their\nscreening towards people at risk. Feature Engineering (FE) focuses on\noptimizing data for ML models, with evolutionary methods being effective in\nfeature selection and construction tasks. This work aims to benchmark an\nevolutionary FE wrapper, using models based on decision trees as proxies. The\nFEDORA framework is applied to a Hearing Loss (HL) dataset, being able to\nreduce data dimensionality and statistically maintain baseline performance.\nCompared to traditional methods, FEDORA demonstrates superior performance, with\na maximum balanced accuracy of 76.2%, using 57 features. The framework also\ngenerated an individual that achieved 72.8% balanced accuracy using a single\nfeature.\n","authors":["Miguel Rabuge","Nuno Louren√ßo"],"pdf_url":"https://arxiv.org/pdf/2502.08785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08783v1","updated":"2025-02-12T20:53:34Z","published":"2025-02-12T20:53:34Z","title":"Learning Discontinuous Galerkin Solutions to Elliptic Problems via Small\n  Linear Convolutional Neural Networks","summary":"  In recent years, there has been an increasing interest in using deep learning\nand neural networks to tackle scientific problems, particularly in solving\npartial differential equations (PDEs). However, many neural network-based\nmethods, such as physics-informed neural networks, depend on automatic\ndifferentiation and the sampling of collocation points, which can result in a\nlack of interpretability and lower accuracy compared to traditional numerical\nmethods. To address this issue, we propose two approaches for learning\ndiscontinuous Galerkin solutions to PDEs using small linear convolutional\nneural networks. Our first approach is supervised and depends on labeled data,\nwhile our second approach is unsupervised and does not rely on any training\ndata. In both cases, our methods use substantially fewer parameters than\nsimilar numerics-based neural networks while also demonstrating comparable\naccuracy to the true and DG solutions for elliptic problems.\n","authors":["Adrian Celaya","Yimo Wang","David Fuentes","Beatrice Riviere"],"pdf_url":"https://arxiv.org/pdf/2502.08783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02890v3","updated":"2025-02-12T20:39:01Z","published":"2024-10-03T18:28:10Z","title":"Theoretically Grounded Framework for LLM Watermarking: A\n  Distribution-Adaptive Approach","summary":"  Watermarking has emerged as a crucial method to distinguish AI-generated text\nfrom human-created text. In this paper, we present a novel theoretical\nframework for watermarking Large Language Models (LLMs) that jointly optimizes\nboth the watermarking scheme and the detection process. Our approach focuses on\nmaximizing detection performance while maintaining control over the worst-case\nType-I error and text distortion. We characterize \\emph{the universally minimum\nType-II error}, showing a fundamental trade-off between watermark detectability\nand text distortion. Importantly, we identify that the optimal watermarking\nschemes are adaptive to the LLM generative distribution. Building on our\ntheoretical insights, we propose an efficient, model-agnostic,\ndistribution-adaptive watermarking algorithm, utilizing a surrogate model\nalongside the Gumbel-max trick. Experiments conducted on Llama2-13B and\nMistral-8$\\times$7B models confirm the effectiveness of our approach.\nAdditionally, we examine incorporating robustness into our framework, paving a\nway to future watermarking systems that withstand adversarial attacks more\neffectively.\n","authors":["Haiyun He","Yepeng Liu","Ziqiao Wang","Yongyi Mao","Yuheng Bu"],"pdf_url":"https://arxiv.org/pdf/2410.02890v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00025v2","updated":"2025-02-12T20:37:55Z","published":"2025-01-21T15:41:20Z","title":"Leveraging Large Language Models to Enhance Machine Learning\n  Interpretability and Predictive Performance: A Case Study on Emergency\n  Department Returns for Mental Health Patients","summary":"  Objective: To evaluate whether integrating large language models (LLMs) with\ntraditional machine learning approaches improves both the predictive accuracy\nand clinical interpretability of ED mental health returns risk models. Methods:\nThis retrospective cohort study analyzed 42,464 ED visits for 27,904 unique\nmental health patients at an Academic Medical Center in the deep South of the\nUnited States between January 2018 and December 2022. Main Outcomes and\nMeasures: Two primary outcomes were evaluated: (1) 30 days ED return prediction\naccuracy and (2) model interpretability through a novel retrieval-augmented\ngeneration (RAG) framework integrating SHAP (SHapley Additive exPlanations)\nvalues with contextual clinical knowledge. Results: The proposed machine\nlearning interpretability framework, leveraging LLM, achieved 99% accuracy in\ntranslating complex model predictions into clinically relevant explanations.\nIntegration of LLM-extracted features enhanced predictive performance,\nimproving the XGBoost model area under the curve (AUC) from 0.73 to 0.76. The\nLLM-based feature extraction using 10-shot learning significantly outperformed\ntraditional approaches, achieving an accuracy of 0.882 and an F1 score of 0.86\nfor chief complaint classification (compared to conventional methods with an\naccuracy range of 0.59 to 0.63) and demonstrating accuracy values ranging from\n0.65 to 0.93 across multiple SDoH categories, underscoring its robust\nperformance in extracting features from clinical notes. Conclusions and\nRelevance: Integrating LLMs with traditional machine learning models yielded\nmodest but consistent improvements in ED return prediction accuracy while\nsubstantially enhancing model interpretability through automated, clinically\nrelevant explanations. This approach offers a framework for translating complex\npredictive analytics into actionable clinical insights.\n","authors":["Abdulaziz Ahmed","Mohammad Saleem","Mohammed Alzeen","Badari Birur","Rachel E Fargason","Bradley G Burk","Hannah Rose Harkins","Ahmed Alhassan","Mohammed Ali Al-Garadi"],"pdf_url":"https://arxiv.org/pdf/2502.00025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08776v1","updated":"2025-02-12T20:35:36Z","published":"2025-02-12T20:35:36Z","title":"Treatment response as a latent variable","summary":"  Scientists often need to analyze the samples in a study that responded to\ntreatment in order to refine their hypotheses and find potential causal drivers\nof response. Natural variation in outcomes makes teasing apart responders from\nnon-responders a statistical inference problem. To handle latent responses, we\nintroduce the causal two-groups (C2G) model, a causal extension of the\nclassical two-groups model. The C2G model posits that treated samples may or\nmay not experience an effect, according to some prior probability. We propose\ntwo empirical Bayes procedures for the causal two-groups model, one under\nsemi-parametric conditions and another under fully nonparametric conditions.\nThe semi-parametric model assumes additive treatment effects and is\nidentifiable from observed data. The nonparametric model is unidentifiable, but\nwe show it can still be used to test for response in each treated sample. We\nshow empirically and theoretically that both methods for selecting responders\ncontrol the false discovery rate at the target level with near-optimal power.\nWe also propose two novel estimands of interest and provide a strategy for\nderiving estimand intervals in the unidentifiable nonparametric model. On a\ncancer immunotherapy dataset, the nonparametric C2G model recovers\nclinically-validated predictive biomarkers of both positive and negative\noutcomes. Code is available at https://github.com/tansey-lab/causal2groups.\n","authors":["Christopher Tosh","Boyuan Zhang","Wesley Tansey"],"pdf_url":"https://arxiv.org/pdf/2502.08776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08774v1","updated":"2025-02-12T20:31:47Z","published":"2025-02-12T20:31:47Z","title":"Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal\n  Brain in 3D Ultrasound","summary":"  Monitoring the growth of subcortical regions of the fetal brain in ultrasound\n(US) images can help identify the presence of abnormal development. Manually\nsegmenting these regions is a challenging task, but recent work has shown that\nit can be automated using deep learning. However, applying pretrained models to\nunseen freehand US volumes often leads to a degradation of performance due to\nthe vast differences in acquisition and alignment. In this work, we first\ndemonstrate that test time adaptation (TTA) can be used to improve model\nperformance in the presence of both real and simulated domain shifts. We\nfurther propose a novel TTA method by incorporating a normative atlas as a\nprior for anatomy. In the presence of various types of domain shifts, we\nbenchmark the performance of different TTA methods and demonstrate the\nimprovements brought by our proposed approach, which may further facilitate\nautomated monitoring of fetal brain development. Our code is available at\nhttps://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation.\n","authors":["Joshua Omolegan","Pak Hei Yeung","Madeleine K. Wyburd","Linde Hesse","Monique Haak","Intergrowth-21st Consortium","Ana I. L. Namburete","Nicola K. Dinsdale"],"pdf_url":"https://arxiv.org/pdf/2502.08774v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.08773v1","updated":"2025-02-12T20:30:28Z","published":"2025-02-12T20:30:28Z","title":"Universal Model Routing for Efficient LLM Inference","summary":"  Large language models' significant advances in capabilities are accompanied\nby significant increases in inference costs. Model routing is a simple\ntechnique for reducing inference cost, wherein one maintains a pool of\ncandidate LLMs, and learns to route each prompt to the smallest feasible LLM.\nExisting works focus on learning a router for a fixed pool of LLMs. In this\npaper, we consider the problem of dynamic routing, where new, previously\nunobserved LLMs are available at test time. We propose a new approach to this\nproblem that relies on representing each LLM as a feature vector, derived based\non predictions on a set of representative prompts. Based on this, we detail two\neffective strategies, relying on cluster-based routing and a learned cluster\nmap respectively. We prove that these strategies are estimates of a\ntheoretically optimal routing rule, and provide an excess risk bound to\nquantify their errors. Experiments on a range of public benchmarks show the\neffectiveness of the proposed strategies in routing amongst more than 30 unseen\nLLMs.\n","authors":["Wittawat Jitkrittum","Harikrishna Narasimhan","Ankit Singh Rawat","Jeevesh Juneja","Zifeng Wang","Chen-Yu Lee","Pradeep Shenoy","Rina Panigrahy","Aditya Krishna Menon","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.08773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08766v1","updated":"2025-02-12T20:12:45Z","published":"2025-02-12T20:12:45Z","title":"Unlocking Mental Health: Exploring College Students' Well-being through\n  Smartphone Behaviors","summary":"  The global mental health crisis is a pressing concern, with college students\nparticularly vulnerable to rising mental health disorders. The widespread use\nof smartphones among young adults, while offering numerous benefits, has also\nbeen linked to negative outcomes such as addiction and regret, significantly\nimpacting well-being. Leveraging the longest longitudinal dataset collected\nover four college years through passive mobile sensing, this study is the first\nto examine the relationship between students' smartphone unlocking behaviors\nand their mental health at scale in real-world settings. We provide the first\nevidence demonstrating the predictability of phone unlocking behaviors for\nmental health outcomes based on a large dataset, highlighting the potential of\nthese novel features for future predictive models. Our findings reveal\nimportant variations in smartphone usage across genders and locations, offering\na deeper understanding of the interplay between digital behaviors and mental\nhealth. We highlight future research directions aimed at mitigating adverse\neffects and promoting digital well-being in this population.\n","authors":["Wei Xuan","Meghna Roy Chowdhury","Yi Ding","Yixue Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.08766v1.pdf","comment":"Published at International Conference on Mobile Software Engineering\n  and Systems (MOBILESoft 2025)"},{"id":"http://arxiv.org/abs/2502.08764v1","updated":"2025-02-12T20:10:51Z","published":"2025-02-12T20:10:51Z","title":"Demand Response Optimization MILP Framework for Microgrids with DERs","summary":"  The integration of renewable energy sources in microgrids introduces\nsignificant operational challenges due to their intermittent nature and the\nmismatch between generation and demand patterns. Effective demand response (DR)\nstrategies are crucial for maintaining system stability and economic\nefficiency, particularly in microgrids with high renewable penetration. This\npaper presents a comprehensive mixed-integer linear programming (MILP)\nframework for optimizing DR operations in a microgrid with solar generation and\nbattery storage systems. The framework incorporates load classification,\ndynamic price thresholding, and multi-period coordination for optimal DR event\nscheduling. Analysis across seven distinct operational scenarios demonstrates\nconsistent peak load reduction of 10\\% while achieving energy cost savings\nranging from 13.1\\% to 38.0\\%. The highest performance was observed in\nscenarios with high solar generation, where the framework achieved 38.0\\%\nenergy cost reduction through optimal coordination of renewable resources and\nDR actions. The results validate the framework's effectiveness in managing\ndiverse operational challenges while maintaining system stability and economic\nefficiency.\n","authors":["K. Victor Sam Moses Babu","Pratyush Chakraborty","Mayukha Pal"],"pdf_url":"https://arxiv.org/pdf/2502.08764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11456v3","updated":"2025-02-12T20:10:41Z","published":"2024-09-17T17:48:12Z","title":"Two Stage Segmentation of Cervical Tumors using PocketNet","summary":"  Cervical cancer remains the fourth most common malignancy amongst women\nworldwide.1 Concurrent chemoradiotherapy (CRT) serves as the mainstay\ndefinitive treatment regimen for locally advanced cervical cancers and includes\nexternal beam radiation followed by brachytherapy.2 Integral to radiotherapy\ntreatment planning is the routine contouring of both the target tumor at the\nlevel of the cervix, associated gynecologic anatomy and the adjacent organs at\nrisk (OARs). However, manual contouring of these structures is both time and\nlabor intensive and associated with known interobserver variability that can\nimpact treatment outcomes. While multiple tools have been developed to\nautomatically segment OARs and the high-risk clinical tumor volume (HR-CTV)\nusing computed tomography (CT) images,3,4,5,6 the development of deep\nlearning-based tumor segmentation tools using routine T2-weighted (T2w)\nmagnetic resonance imaging (MRI) addresses an unmet clinical need to improve\nthe routine contouring of both anatomical structures and cervical cancers,\nthereby increasing quality and consistency of radiotherapy planning. This work\napplied a novel deep-learning model (PocketNet) to segment the cervix, vagina,\nuterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecture\nwas evaluated, when trained on data via five-fold cross validation. PocketNet\nachieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% for\ntumor segmentation and 80% for organ segmentation. Validation on a publicly\navailable dataset from The Cancer Imaging Archive (TCIA) demonstrated the\nmodels robustness, achieving DSC scores of 67.3% for tumor segmentation and\n80.8% for organ segmentation. These results suggest that PocketNet is robust to\nvariations in contrast protocols, providing reliable segmentation of the\nregions of interest.\n","authors":["Awj Twam","Adrian E. Celaya","Megan C. Jacobsen","Rachel Glenn","Peng Wei","Jia Sun","Ann Klopp","Aradhana M. Venkatesan","David Fuentes"],"pdf_url":"https://arxiv.org/pdf/2409.11456v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08758v1","updated":"2025-02-12T20:03:32Z","published":"2025-02-12T20:03:32Z","title":"Compression of Site-Specific Deep Neural Networks for Massive MIMO\n  Precoding","summary":"  The deployment of deep learning (DL) models for precoding in massive\nmultiple-input multiple-output (mMIMO) systems is often constrained by high\ncomputational demands and energy consumption. In this paper, we investigate the\ncompute energy efficiency of mMIMO precoders using DL-based approaches,\ncomparing them to conventional methods such as zero forcing and weighted\nminimum mean square error (WMMSE). Our energy consumption model accounts for\nboth memory access and calculation energy within DL accelerators. We propose a\nframework that incorporates mixed-precision quantization-aware training and\nneural architecture search to reduce energy usage without compromising\naccuracy. Using a ray-tracing dataset covering various base station sites, we\nanalyze how site-specific conditions affect the energy efficiency of compressed\nmodels. Our results show that deep neural network compression generates\nprecoders with up to 35 times higher energy efficiency than WMMSE at equal\nperformance, depending on the scenario and the desired rate. These results\nestablish a foundation and a benchmark for the development of energy-efficient\nDL-based mMIMO precoders.\n","authors":["Ghazal Kasalaee","Ali Hasanzadeh Karkan","Jean-Fran√ßois Frigon","Fran√ßois Leduc-Primeau"],"pdf_url":"https://arxiv.org/pdf/2502.08758v1.pdf","comment":"This preprint comprises 6 pages and features 3 figures. It has been\n  accepted to the IEEE International Conference on Machine Learning and\n  Computer Networking (ICMLCN) 2025"},{"id":"http://arxiv.org/abs/2502.08757v1","updated":"2025-02-12T20:02:36Z","published":"2025-02-12T20:02:36Z","title":"A Low-Complexity Plug-and-Play Deep Learning Model for Massive MIMO\n  Precoding Across Sites","summary":"  Massive multiple-input multiple-output (mMIMO) technology has transformed\nwireless communication by enhancing spectral efficiency and network capacity.\nThis paper proposes a novel deep learning-based mMIMO precoder to tackle the\ncomplexity challenges of existing approaches, such as weighted minimum mean\nsquare error (WMMSE), while leveraging meta-learning domain generalization and\na teacher-student architecture to improve generalization across diverse\ncommunication environments. When deployed to a previously unseen site, the\nproposed model achieves excellent sum-rate performance while maintaining low\ncomputational complexity by avoiding matrix inversions and by using a simpler\nneural network structure. The model is trained and tested on a custom\nray-tracing dataset composed of several base station locations. The\nexperimental results indicate that our method effectively balances\ncomputational efficiency with high sum-rate performance while showcasing strong\ngeneralization performance in unseen environments. Furthermore, with\nfine-tuning, the proposed model outperforms WMMSE across all tested sites and\nSNR conditions while reducing complexity by at least 73$\\times$.\n","authors":["Ali Hasanzadeh Karkan","Ahmed Ibrahim","Jean-Fran√ßois Frigon","Fran√ßois Leduc-Primeau"],"pdf_url":"https://arxiv.org/pdf/2502.08757v1.pdf","comment":"This preprint comprises 6 pages and features 2 figures. It has been\n  accepted to the IEEE International Conference on Machine Learning and\n  Computer Networking (ICMLCN) 2025"},{"id":"http://arxiv.org/abs/2501.17392v2","updated":"2025-02-12T19:37:28Z","published":"2025-01-29T03:01:01Z","title":"Byzantine-Robust Federated Learning over Ring-All-Reduce Distributed\n  Computing","summary":"  Federated learning (FL) has gained attention as a distributed learning\nparadigm for its data privacy benefits and accelerated convergence through\nparallel computation. Traditional FL relies on a server-client (SC)\narchitecture, where a central server coordinates multiple clients to train a\nglobal model, but this approach faces scalability challenges due to server\ncommunication bottlenecks. To overcome this, the ring-all-reduce (RAR)\narchitecture has been introduced, eliminating the central server and achieving\nbandwidth optimality. However, the tightly coupled nature of RAR's ring\ntopology exposes it to unique Byzantine attack risks not present in SC-based\nFL. Despite its potential, designing Byzantine-robust RAR-based FL algorithms\nremains an open problem. To address this gap, we propose BRACE\n(Byzantine-robust ring-all-reduce), the first RAR-based FL algorithm to achieve\nboth Byzantine robustness and communication efficiency. We provide theoretical\nguarantees for the convergence of BRACE under Byzantine attacks, demonstrate\nits bandwidth efficiency, and validate its practical effectiveness through\nexperiments. Our work offers a foundational understanding of Byzantine-robust\nRAR-based FL design.\n","authors":["Minghong Fang","Zhuqing Liu","Xuecen Zhao","Jia Liu"],"pdf_url":"https://arxiv.org/pdf/2501.17392v2.pdf","comment":"To appear in The Web Conference 2025"},{"id":"http://arxiv.org/abs/2408.11081v2","updated":"2025-02-12T19:34:51Z","published":"2024-08-20T11:19:06Z","title":"What can Large Language Models Capture about Code Functional\n  Equivalence?","summary":"  Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress\nin learning rich representations of the structure and syntax of code,\nsuccessfully using it to generate or classify code fragments. At the same time,\nunderstanding if they are able to do so because they capture code semantics,\nand how well, is still an open question. In this paper, we tackle this problem\nby introducing SeqCoBench, a benchmark for systematically assessing how\nCode-LLMs can capture code functional equivalence. SeqCoBench contains over 20\ncode transformations that either preserve or alter the semantics of Python\nprograms. We conduct extensive evaluations in different settings, including\nzero-shot and parameter-efficient finetuning methods on state-of-the-art\n(Code)-LLMs to see if they can discern semantically equivalent or different\npairs of programs in SeqCoBench. We find that the performance gap between these\nLLMs and classical match-based retrieval scores is minimal, with both\napproaches showing a concerning lack of depth in understanding code semantics.\n","authors":["Nickil Maveli","Antonio Vergari","Shay B. Cohen"],"pdf_url":"https://arxiv.org/pdf/2408.11081v2.pdf","comment":"Accepted to Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2402.02596v3","updated":"2025-02-12T19:18:54Z","published":"2024-02-04T20:06:20Z","title":"Dual Interior Point Optimization Learning","summary":"  In many practical applications of constrained optimization, scale and solving\ntime limits make traditional optimization solvers prohibitively slow. Thus, the\nresearch question of how to design optimization proxies -- machine learning\nmodels that produce high-quality solutions -- has recently received significant\nattention. Orthogonal to this research thread which focuses on learning primal\nsolutions, this paper studies how to learn dual feasible solutions that\ncomplement primal approaches and provide quality guarantees. The paper makes\ntwo distinct contributions. First, to train dual linear optimization proxies,\nthe paper proposes a smoothed self-supervised loss function that augments the\nobjective function with a dual penalty term. Second, the paper proposes a novel\ndual completion strategy that guarantees dual feasibility by solving a convex\noptimization problem. Moreover, the paper derives closed-form solutions to this\ncompletion optimization for several classes of dual penalties, eliminating the\nneed for computationally-heavy implicit layers. Numerical results are presented\non large linear optimization problems and demonstrate the effectiveness of the\nproposed approach. The proposed dual completion outperforms methods for\nlearning optimization proxies which do not exploit the structure of the dual\nproblem. Compared to commercial optimization solvers, the learned dual proxies\nachieve optimality gaps below $1\\%$ and several orders of magnitude speedups.\n","authors":["Michael Klamkin","Mathieu Tanneau","Pascal Van Hentenryck"],"pdf_url":"https://arxiv.org/pdf/2402.02596v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08736v1","updated":"2025-02-12T19:18:50Z","published":"2025-02-12T19:18:50Z","title":"Recurrent Memory for Online Interdomain Gaussian Processes","summary":"  We propose a novel online Gaussian process (GP) model that is capable of\ncapturing long-term memory in sequential data in an online regression setting.\nOur model, Online HiPPO Sparse Variational Gaussian Process Regression\n(OHSGPR), leverages the HiPPO (High-order Polynomial Projection Operators)\nframework, which is popularized in the RNN domain due to its long-range memory\nmodeling capabilities. We interpret the HiPPO time-varying orthogonal\nprojections as inducing variables with time-dependent orthogonal polynomial\nbasis functions, which allows the SGPR inducing points to memorize the process\nhistory. We show that the HiPPO framework fits naturally into the interdomain\nGP framework and demonstrate that the kernel matrices can also be updated\nonline in a recurrence form based on the ODE evolution of HiPPO. We evaluate\nour method on time series regression tasks, showing that it outperforms the\nexisting online GP method in terms of predictive performance and computational\nefficiency\n","authors":["Wenlong Chen","Naoki Kiyohara","Harrison Bo Hua Zhu","Yingzhen Li"],"pdf_url":"https://arxiv.org/pdf/2502.08736v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.08730v1","updated":"2025-02-12T19:04:26Z","published":"2025-02-12T19:04:26Z","title":"New Bounds for Sparse Variational Gaussian Processes","summary":"  Sparse variational Gaussian processes (GPs) construct tractable posterior\napproximations to GP models. At the core of these methods is the assumption\nthat the true posterior distribution over training function values ${\\bf f}$\nand inducing variables ${\\bf u}$ is approximated by a variational distribution\nthat incorporates the conditional GP prior $p({\\bf f} | {\\bf u})$ in its\nfactorization. While this assumption is considered as fundamental, we show that\nfor model training we can relax it through the use of a more general\nvariational distribution $q({\\bf f} | {\\bf u})$ that depends on $N$ extra\nparameters, where $N$ is the number of training examples. In GP regression, we\ncan analytically optimize the evidence lower bound over the extra parameters\nand express a tractable collapsed bound that is tighter than the previous\nbound. The new bound is also amenable to stochastic optimization and its\nimplementation requires minor modifications to existing sparse GP code.\nFurther, we also describe extensions to non-Gaussian likelihoods. On several\ndatasets we demonstrate that our method can reduce bias when learning the\nhyperpaparameters and can lead to better predictive performance.\n","authors":["Michalis K. Titsias"],"pdf_url":"https://arxiv.org/pdf/2502.08730v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.08728v1","updated":"2025-02-12T19:03:09Z","published":"2025-02-12T19:03:09Z","title":"A Comparative Study of Machine Learning Algorithms for Stock Price\n  Prediction Using Insider Trading Data","summary":"  The research paper empirically investigates several machine learning\nalgorithms to forecast stock prices depending on insider trading information.\nInsider trading offers special insights into market sentiment, pointing to\nupcoming changes in stock prices. This study examines the effectiveness of\nalgorithms like decision trees, random forests, support vector machines (SVM)\nwith different kernels, and K-Means Clustering using a dataset of Tesla stock\ntransactions. Examining past data from April 2020 to March 2023, this study\nfocuses on how well these algorithms identify trends and forecast stock price\nfluctuations. The paper uses Recursive Feature Elimination (RFE) and feature\nimportance analysis to optimize the feature set and, hence, increase prediction\naccuracy. While it requires substantially greater processing time than other\nmodels, SVM with the Radial Basis Function (RBF) kernel displays the best\naccuracy. This paper highlights the trade-offs between accuracy and efficiency\nin machine learning models and proposes the possibility of pooling multiple\ndata sources to raise prediction performance. The results of this paper aim to\nhelp financial analysts and investors in choosing strong algorithms to optimize\ninvestment strategies.\n","authors":["Amitabh Chakravorty","Nelly Elsayed"],"pdf_url":"https://arxiv.org/pdf/2502.08728v1.pdf","comment":"5 pages, accepted to publish"},{"id":"http://arxiv.org/abs/2502.08696v1","updated":"2025-02-12T18:59:55Z","published":"2025-02-12T18:59:55Z","title":"Scalable Discrete Diffusion Samplers: Combinatorial Optimization and\n  Statistical Physics","summary":"  Learning to sample from complex unnormalized distributions over discrete\ndomains emerged as a promising research direction with applications in\nstatistical physics, variational inference, and combinatorial optimization.\nRecent work has demonstrated the potential of diffusion models in this domain.\nHowever, existing methods face limitations in memory scaling and thus the\nnumber of attainable diffusion steps since they require backpropagation through\nthe entire generative process. To overcome these limitations we introduce two\nnovel training methods for discrete diffusion samplers, one grounded in the\npolicy gradient theorem and the other one leveraging Self-Normalized Neural\nImportance Sampling (SN-NIS). These methods yield memory-efficient training and\nachieve state-of-the-art results in unsupervised combinatorial optimization.\nNumerous scientific applications additionally require the ability of unbiased\nsampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte\nCarlo that enable for the first time the application of discrete diffusion\nmodels to this problem. We validate our methods on Ising model benchmarks and\nfind that they outperform popular autoregressive approaches. Our work opens new\navenues for applying diffusion models to a wide range of scientific\napplications in discrete domains that were hitherto restricted to exact\nlikelihood models.\n","authors":["Sebastian Sanokowski","Wilhelm Berghammer","Martin Ennemoser","Haoyu Peter Wang","Sepp Hochreiter","Sebastian Lehner"],"pdf_url":"https://arxiv.org/pdf/2502.08696v1.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08640v1","updated":"2025-02-12T18:55:43Z","published":"2025-02-12T18:55:43Z","title":"Utility Engineering: Analyzing and Controlling Emergent Value Systems in\n  AIs","summary":"  As AIs rapidly advance and become more agentic, the risk they pose is\ngoverned not only by their capabilities but increasingly by their propensities,\nincluding goals and values. Tracking the emergence of goals and values has\nproven a longstanding problem, and despite much interest over the years it\nremains unclear whether current AIs have meaningful values. We propose a\nsolution to this problem, leveraging the framework of utility functions to\nstudy the internal coherence of AI preferences. Surprisingly, we find that\nindependently-sampled preferences in current LLMs exhibit high degrees of\nstructural coherence, and moreover that this emerges with scale. These findings\nsuggest that value systems emerge in LLMs in a meaningful sense, a finding with\nbroad implications. To study these emergent value systems, we propose utility\nengineering as a research agenda, comprising both the analysis and control of\nAI utilities. We uncover problematic and often shocking values in LLM\nassistants despite existing control measures. These include cases where AIs\nvalue themselves over humans and are anti-aligned with specific individuals. To\nconstrain these emergent value systems, we propose methods of utility control.\nAs a case study, we show how aligning utilities with a citizen assembly reduces\npolitical biases and generalizes to new scenarios. Whether we like it or not,\nvalue systems have already emerged in AIs, and much work remains to fully\nunderstand and control these emergent representations.\n","authors":["Mantas Mazeika","Xuwang Yin","Rishub Tamirisa","Jaehyuk Lim","Bruce W. Lee","Richard Ren","Long Phan","Norman Mu","Adam Khoja","Oliver Zhang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2502.08640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08637v1","updated":"2025-02-12T18:54:10Z","published":"2025-02-12T18:54:10Z","title":"Joint Transmit and Pinching Beamforming for PASS: Optimization-Based or\n  Learning-Based?","summary":"  A novel pinching antenna system (PASS)-enabled downlink multi-user\nmultiple-input single-output (MISO) framework is proposed. PASS consists of\nmultiple waveguides spanning over thousands of wavelength, which equip numerous\nlow-cost dielectric particles, named pinching antennas (PAs), to radiate\nsignals into free space. The positions of PAs can be reconfigured to change\nboth the large-scale path losses and phases of signals, thus facilitating the\nnovel pinching beamforming design. A sum rate maximization problem is\nformulated, which jointly optimizes the transmit and pinching beamforming to\nadaptively achieve constructive signal enhancement and destructive interference\nmitigation. To solve this highly coupled and nonconvex problem, both\noptimization-based and learning-based methods are proposed. 1) For the\noptimization-based method, a majorization-minimization and penalty dual\ndecomposition (MM-PDD) algorithm is developed, which handles the nonconvex\ncomplex exponential component using a Lipschitz surrogate function and then\ninvokes PDD for problem decoupling. 2) For the learning-based method, a novel\nKarush-Kuhn-Tucker (KKT)-guided dual learning (KDL) approach is proposed, which\nenables KKT solutions to be reconstructed in a data-driven manner by learning\ndual variables. Following this idea, a KDL-Tranformer algorithm is developed,\nwhich captures both inter-PA/inter-user dependencies and\nchannel-state-information (CSI)-beamforming dependencies by attention\nmechanisms. Simulation results demonstrate that: i) The proposed PASS framework\nsignificantly outperforms conventional massive multiple input multiple output\n(MIMO) system even with a few PAs. ii) The proposed KDL-Transformer can improve\nover 30% system performance than MM-PDD algorithm, while achieving a\nmillisecond-level response on modern GPUs.\n","authors":["Xiaoxia Xu","Xidong Mu","Yuanwei Liu","Arumugam Nallanathan"],"pdf_url":"https://arxiv.org/pdf/2502.08637v1.pdf","comment":"Submitted to IEEE"},{"id":"http://arxiv.org/abs/2502.08634v1","updated":"2025-02-12T18:48:12Z","published":"2025-02-12T18:48:12Z","title":"Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale\n  Implicit Neural Representation","summary":"  Purpose: To develop and validate a novel image reconstruction technique using\nimplicit neural representations (INR) for multi-view thick-slice acquisitions\nwhile reducing the scan time but maintaining high signal-to-noise ratio (SNR).\nMethods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised\nneural network-based algorithm designed to reconstruct MRI data from multi-view\nthick slices, effectively reducing scan time by 2-fold while maintaining fine\nanatomical details. We compare our method to both bicubic interpolation and the\ncurrent state-of-the-art regularized least-squares super-resolution\nreconstruction (LS-SRR) technique. Validation is performed using ground-truth\nex-vivo monkey brain data, and we demonstrate superior reconstruction quality\nacross several in-vivo human datasets. Notably, we achieve the reconstruction\nof a whole human brain in-vivo T2-weighted image with an unprecedented\n180{\\mu}m isotropic spatial resolution, accomplished in just 17 minutes of scan\ntime on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in\nterms of reconstruction quality with 22.4% lower relative error (RE) and 7.5%\nlower full-width half maximum (FWHM) indicating better preservation of fine\nstructural details in nearly half the scan time. Conclusion: ROVER-MRI offers\nan efficient and robust approach for mesoscale MR imaging, enabling rapid,\nhigh-resolution whole-brain scans. Its versatility holds great promise for\nresearch applications requiring anatomical details and time-efficient imaging.\n","authors":["Jun Lyu","Lipeng Ning","William Consagra","Qiang Liu","Richard J. Rushmore","Berkin Bilgic","Yogesh Rathi"],"pdf_url":"https://arxiv.org/pdf/2502.08634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08632v1","updated":"2025-02-12T18:47:13Z","published":"2025-02-12T18:47:13Z","title":"Necessary and Sufficient Oracles: Toward a Computational Taxonomy For\n  Reinforcement Learning","summary":"  Algorithms for reinforcement learning (RL) in large state spaces crucially\nrely on supervised learning subroutines to estimate objects such as value\nfunctions or transition probabilities. Since only the simplest supervised\nlearning problems can be solved provably and efficiently, practical performance\nof an RL algorithm depends on which of these supervised learning \"oracles\" it\nassumes access to (and how they are implemented). But which oracles are better\nor worse? Is there a minimal oracle?\n  In this work, we clarify the impact of the choice of supervised learning\noracle on the computational complexity of RL, as quantified by the oracle\nstrength. First, for the task of reward-free exploration in Block MDPs in the\nstandard episodic access model -- a ubiquitous setting for RL with function\napproximation -- we identify two-context regression as a minimal oracle, i.e.\nan oracle that is both necessary and sufficient (under a mild regularity\nassumption). Second, we identify one-context regression as a near-minimal\noracle in the stronger reset access model, establishing a provable\ncomputational benefit of resets in the process. Third, we broaden our focus to\nLow-Rank MDPs, where we give cryptographic evidence that the analogous oracle\nfrom the Block MDP setting is insufficient.\n","authors":["Dhruv Rohatgi","Dylan J. Foster"],"pdf_url":"https://arxiv.org/pdf/2502.08632v1.pdf","comment":"84 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.08695v1","updated":"2025-02-12T18:39:01Z","published":"2025-02-12T18:39:01Z","title":"A Bayesian Nonparametric Perspective on Mahalanobis Distance for Out of\n  Distribution Detection","summary":"  Bayesian nonparametric methods are naturally suited to the problem of\nout-of-distribution (OOD) detection. However, these techniques have largely\nbeen eschewed in favor of simpler methods based on distances between\npre-trained or learned embeddings of data points. Here we show a formal\nrelationship between Bayesian nonparametric models and the relative Mahalanobis\ndistance score (RMDS), a commonly used method for OOD detection. Building on\nthis connection, we propose Bayesian nonparametric mixture models with\nhierarchical priors that generalize the RMDS. We evaluate these models on the\nOpenOOD detection benchmark and show that Bayesian nonparametric methods can\nimprove upon existing OOD methods, especially in regimes where training classes\ndiffer in their covariance structure and where there are relatively few data\npoints per class.\n","authors":["Randolph W. Linderman","Yiran Chen","Scott W. Linderman"],"pdf_url":"https://arxiv.org/pdf/2502.08695v1.pdf","comment":"32 pages, 5 figures, code is available at\n  https://github.com/rwl93/bnp4ood"},{"id":"http://arxiv.org/abs/2502.04689v2","updated":"2025-02-12T18:36:24Z","published":"2025-02-07T06:30:33Z","title":"ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning","summary":"  Large language models (LLMs) achieve remarkable performance on challenging\nbenchmarks that are often structured as multiple-choice question-answering (QA)\ntasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs\nbut provides only vague and generic guidance (\"think step by step\"). This paper\nintroduces ARR, an intuitive and effective zero-shot prompting method that\nexplicitly incorporates three key steps in QA solving: analyzing the intent of\nthe question, retrieving relevant information, and reasoning step by step.\nComprehensive experiments across diverse and challenging QA tasks demonstrate\nthat ARR consistently improves the Baseline (without ARR prompting) and\noutperforms CoT. Ablation and case studies further validate the positive\ncontributions of each component: analyzing, retrieving, and reasoning. Notably,\nintent analysis plays a vital role in ARR. Additionally, extensive evaluations\nacross various model sizes, LLM series, and generation settings solidify the\neffectiveness, robustness, and generalizability of ARR.\n","authors":["Yuwei Yin","Giuseppe Carenini"],"pdf_url":"https://arxiv.org/pdf/2502.04689v2.pdf","comment":"20 pages. Code: https://github.com/YuweiYin/ARR"},{"id":"http://arxiv.org/abs/2501.18823v2","updated":"2025-02-12T18:35:15Z","published":"2025-01-31T00:36:30Z","title":"Transcoders Beat Sparse Autoencoders for Interpretability","summary":"  Sparse autoencoders (SAEs) extract human-interpretable features from deep\nneural networks by transforming their activations into a sparse, higher\ndimensional latent space, and then reconstructing the activations from these\nlatents. Transcoders are similar to SAEs, but they are trained to reconstruct\nthe output of a component of a deep network given its input. In this work, we\ncompare the features found by transcoders and SAEs trained on the same model\nand data, finding that transcoder features are significantly more\ninterpretable. We also propose skip transcoders, which add an affine skip\nconnection to the transcoder architecture, and show that these achieve lower\nreconstruction loss with no effect on interpretability.\n","authors":["Gon√ßalo Paulo","Stepan Shabalin","Nora Belrose"],"pdf_url":"https://arxiv.org/pdf/2501.18823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13734v3","updated":"2025-02-12T18:32:37Z","published":"2025-01-23T15:10:51Z","title":"Sample complexity of data-driven tuning of model hyperparameters in\n  neural networks with structured parameter-dependent dual function","summary":"  Modern machine learning algorithms, especially deep learning based\ntechniques, typically involve careful hyperparameter tuning to achieve the best\nperformance. Despite the surge of intense interest in practical techniques like\nBayesian optimization and random search based approaches to automating this\nlaborious and compute intensive task, the fundamental learning theoretic\ncomplexity of tuning hyperparameters for deep neural networks is poorly\nunderstood. Inspired by this glaring gap, we initiate the formal study of\nhyperparameter tuning complexity in deep learning through a recently introduced\ndata driven setting. We assume that we have a series of deep learning tasks,\nand we have to tune hyperparameters to do well on average over the distribution\nof tasks. A major difficulty is that the utility function as a function of the\nhyperparameter is very volatile and furthermore, it is given implicitly by an\noptimization problem over the model parameters. To tackle this challenge, we\nintroduce a new technique to characterize the discontinuities and oscillations\nof the utility function on any fixed problem instance as we vary the\nhyperparameter; our analysis relies on subtle concepts including tools from\ndifferential/algebraic geometry and constrained optimization. This can be used\nto show that the learning theoretic complexity of the corresponding family of\nutility functions is bounded. We instantiate our results and provide sample\ncomplexity bounds for concrete applications tuning a hyperparameter that\ninterpolates neural activation functions and setting the kernel parameter in\ngraph neural networks.\n","authors":["Maria-Florina Balcan","Anh Tuan Nguyen","Dravyansh Sharma"],"pdf_url":"https://arxiv.org/pdf/2501.13734v3.pdf","comment":"50 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.08628v1","updated":"2025-02-12T18:30:36Z","published":"2025-02-12T18:30:36Z","title":"Concentration Inequalities for the Stochastic Optimization of Unbounded\n  Objectives with Application to Denoising Score Matching","summary":"  We derive novel concentration inequalities that bound the statistical error\nfor a large class of stochastic optimization problems, focusing on the case of\nunbounded objective functions. Our derivations utilize the following tools: 1)\nA new form of McDiarmid's inequality that is based on sample dependent one\ncomponent difference bounds and which leads to a novel uniform law of large\nnumbers result for unbounded functions. 2) A Rademacher complexity bound for\nfamilies of functions that satisfy an appropriate local Lipschitz property. As\nan application of these results, we derive statistical error bounds for\ndenoising score matching (DSM), an application that inherently requires one to\nconsider unbounded objective functions, even when the data distribution has\nbounded support. In addition, our results establish the benefit of sample reuse\nin algorithms that employ easily sampled auxiliary random variables in addition\nto the training data, e.g., as in DSM, which uses auxiliary Gaussian random\nvariables.\n","authors":["Jeremiah Birrell"],"pdf_url":"https://arxiv.org/pdf/2502.08628v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2410.06976v2","updated":"2025-02-12T18:27:29Z","published":"2024-10-09T15:15:40Z","title":"Matcha: Mitigating Graph Structure Shifts with Test-Time Adaptation","summary":"  Powerful as they are, graph neural networks (GNNs) are known to be vulnerable\nto distribution shifts. Recently, test-time adaptation (TTA) has attracted\nattention due to its ability to adapt a pre-trained model to a target domain,\nwithout re-accessing the source domain. However, existing TTA algorithms are\nprimarily designed for attribute shifts in vision tasks, where samples are\nindependent. These methods perform poorly on graph data that experience\nstructure shifts, where node connectivity differs between source and target\ngraphs. We attribute this performance gap to the distinct impact of node\nattribute shifts versus graph structure shifts: the latter significantly\ndegrades the quality of node representations and blurs the boundaries between\ndifferent node categories. To address structure shifts in graphs, we propose\nMatcha, an innovative framework designed for effective and efficient adaptation\nto structure shifts by adjusting the htop-aggregation parameters in GNNs. To\nenhance the representation quality, we design a prediction-informed clustering\nloss to encourage the formation of distinct clusters for different node\ncategories. Additionally, Matcha seamlessly integrates with existing TTA\nalgorithms, allowing it to handle attribute shifts effectively while improving\noverall performance under combined structure and attribute shifts. We validate\nthe effectiveness of Matcha on both synthetic and real-world datasets,\ndemonstrating its robustness across various combinations of structure and\nattribute shifts. Our code is available at https://github.com/baowenxuan/Matcha .\n","authors":["Wenxuan Bao","Zhichen Zeng","Zhining Liu","Hanghang Tong","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2410.06976v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08625v1","updated":"2025-02-12T18:25:13Z","published":"2025-02-12T18:25:13Z","title":"Randomness of Low-Layer Parameters Determines Confusing Samples in Terms\n  of Interaction Representations of a DNN","summary":"  In this paper, we find that the complexity of interactions encoded by a deep\nneural network (DNN) can explain its generalization power. We also discover\nthat the confusing samples of a DNN, which are represented by non-generalizable\ninteractions, are determined by its low-layer parameters. In comparison, other\nfactors, such as high-layer parameters and network architecture, have much less\nimpact on the composition of confusing samples. Two DNNs with different\nlow-layer parameters usually have fully different sets of confusing samples,\neven though they have similar performance. This finding extends the\nunderstanding of the lottery ticket hypothesis, and well explains distinctive\nrepresentation power of different DNNs.\n","authors":["Junpeng Zhang","Lei Cheng","Qing Li","Liang Lin","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13312v2","updated":"2025-02-12T18:22:42Z","published":"2025-01-23T01:43:31Z","title":"Tensor-Var: Variational Data Assimilation in Tensor Product Feature\n  Space","summary":"  Variational data assimilation estimates the dynamical system states by\nminimizing a cost function that fits the numerical models with observational\ndata. The widely used method, four-dimensional variational assimilation\n(4D-Var), has two primary challenges: (1) computationally demanding for complex\nnonlinear systems and (2) relying on state-observation mappings, which are\noften not perfectly known. Deep learning (DL) has been used as a more\nexpressive class of efficient model approximators to address these challenges.\nHowever, integrating such models into 4D-Var remains challenging due to their\ninherent nonlinearities and the lack of theoretical guarantees for consistency\nin assimilation results. In this paper, we propose Tensor-Var to address these\nchallenges using kernel Conditional Mean Embedding (CME). Tensor-Var improves\noptimization efficiency by characterizing system dynamics and state-observation\nmappings as linear operators, leading to a convex cost function in the feature\nspace. Furthermore, our method provides a new perspective to incorporate CME\ninto 4D-Var, offering theoretical guarantees of consistent assimilation results\nbetween the original and feature spaces. To improve scalability, we propose a\nmethod to learn deep features (DFs) using neural networks within the Tensor-Var\nframework. Experiments on chaotic systems and global weather prediction with\nreal-time observations show that Tensor-Var outperforms conventional and DL\nhybrid 4D-Var baselines in accuracy while achieving efficiency comparable to\nthe static 3D-Var method.\n","authors":["Yiming Yang","Xiaoyuan Cheng","Daniel Giles","Sibo Cheng","Yi He","Xiao Xue","Boli Chen","Yukun Hu"],"pdf_url":"https://arxiv.org/pdf/2501.13312v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08622v1","updated":"2025-02-12T18:20:41Z","published":"2025-02-12T18:20:41Z","title":"Forecasting Drought Using Machine Learning in California","summary":"  Drought is a frequent and costly natural disaster in California, with major\nnegative impacts on agricultural production and water resource availability,\nparticularly groundwater. This study investigated the performance of applying\ndifferent machine learning approaches to predicting the U.S. Drought Monitor\nclassification in California. Four approaches were used: a convolutional neural\nnetwork (CNN), random forest, XGBoost, and long short term memory (LSTM)\nrecurrent neural network, and compared to a baseline persistence model. We\nevaluated the models' performance in predicting severe drought (USDM drought\ncategory D2 or higher) using a macro F1 binary classification metric. The LSTM\nmodel emerged as the top performer, followed by XGBoost, CNN, and random\nforest. Further evaluation of our results at the county level suggested that\nthe LSTM model would perform best in counties with more consistent drought\npatterns and where severe drought was more common, and the LSTM model would\nperform worse where drought scores increased rapidly. Utilizing 30 weeks of\nhistorical data, the LSTM model successfully forecasted drought scores for a\n12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less\nthan half a drought category on a scale of 0 to 5. Additionally, the LSTM\nachieved a macro F1 score of 0.9, indicating high accuracy in binary\nclassification for severe drought conditions. Evaluation of different window\nand future horizon sizes in weeks suggested that at least 24 weeks of data\nwould result in the best performance, with best performance for shorter horizon\nsizes, particularly less than eight weeks.\n","authors":["Nan K. Li","Angela Chang","David Sherman"],"pdf_url":"https://arxiv.org/pdf/2502.08622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08620v1","updated":"2025-02-12T18:15:35Z","published":"2025-02-12T18:15:35Z","title":"Mathematical Data Science","summary":"  Can machine learning help discover new mathematical structures? In this\narticle we discuss an approach to doing this which one can call \"mathematical\ndata science\". In this paradigm, one studies mathematical objects collectively\nrather than individually, by creating datasets and doing machine learning\nexperiments and interpretations. After an overview, we present two case\nstudies: murmurations in number theory and loadings of partitions related to\nKronecker coefficients in representation theory and combinatorics.\n","authors":["Michael R. Douglas","Kyu-Hwan Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01512v2","updated":"2025-02-12T18:11:46Z","published":"2025-02-03T16:46:46Z","title":"Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices","summary":"  Circular and non-flat data distributions are prevalent across diverse domains\nof data science, yet their specific geometric structures often remain\nunderutilized in machine learning frameworks. A principled approach to\naccounting for the underlying geometry of such data is pivotal, particularly\nwhen extending statistical models, like the pervasive Gaussian distribution. In\nthis work, we tackle those issue by focusing on the manifold of symmetric\npositive definite matrices, a key focus in information geometry. We introduced\na non-isotropic wrapped Gaussian by leveraging the exponential map, we derive\ntheoretical properties of this distribution and propose a maximum likelihood\nframework for parameter estimation. Furthermore, we reinterpret established\nclassifiers on SPD through a probabilistic lens and introduce new classifiers\nbased on the wrapped Gaussian model. Experiments on synthetic and real-world\ndatasets demonstrate the robustness and flexibility of this geometry-aware\ndistribution, underscoring its potential to advance manifold-based data\nanalysis. This work lays the groundwork for extending classical machine\nlearning and statistical methods to more complex and structured data.\n","authors":["Thibault de Surrel","Fabien Lotte","Sylvain Chevallier","Florian Yger"],"pdf_url":"https://arxiv.org/pdf/2502.01512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08612v1","updated":"2025-02-12T18:01:04Z","published":"2025-02-12T18:01:04Z","title":"Continuous Cardiac Arrest Prediction in ICU using PPG Foundation Model","summary":"  Non-invasive patient monitoring for tracking and predicting adverse acute\nhealth events is an emerging area of research. We pursue in-hospital cardiac\narrest (IHCA) prediction using only single-channel finger photoplethysmography\n(PPG) signals. Our proposed two-stage model Feature Extractor-Aggregator\nNetwork (FEAN) leverages powerful representations from pre-trained PPG\nfoundation models (PPG-GPT of size up to 1 Billion) stacked with sequential\nclassification models. We propose two FEAN variants (\"1H\", \"FH\") which use the\nlatest one-hour and (max) 24-hour history to make decisions respectively. Our\nstudy is the first to present IHCA prediction results in ICU patients using\nonly unimodal (continuous PPG signal) waveform deep representations. With our\nbest model, we obtain an average of 0.79 AUROC over 24~h prediction window\nbefore CA event onset with our model peaking performance at 0.82 one hour\nbefore CA. We also provide a comprehensive analysis of our model through\narchitectural tuning and PaCMAP visualization of patient health trajectory in\nlatent space.\n","authors":["Saurabh Kataria","Ran Xiao","Timothy Ruchti","Matthew Clark","Jiaying Lu","Randall J. Lee","Jocelyn Grunwell","Xiao Hu"],"pdf_url":"https://arxiv.org/pdf/2502.08612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08611v1","updated":"2025-02-12T17:59:21Z","published":"2025-02-12T17:59:21Z","title":"Robustly Learning Monotone Generalized Linear Models via Data\n  Augmentation","summary":"  We study the task of learning Generalized Linear models (GLMs) in the\nagnostic model under the Gaussian distribution. We give the first\npolynomial-time algorithm that achieves a constant-factor approximation for\n\\textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners\nsucceed for a substantially smaller class of activations. Our work resolves a\nwell-known open problem, by developing a robust counterpart to the classical\nGLMtron algorithm (Kakade et al., 2011). Our robust learner applies more\ngenerally, encompassing all monotone activations with bounded\n$(2+\\zeta)$-moments, for any fixed $\\zeta>0$ -- a condition that is essentially\nnecessary. To obtain our results, we leverage a novel data augmentation\ntechnique with decreasing Gaussian noise injection and prove a number of\nstructural results that may be useful in other settings.\n","authors":["Nikos Zarifis","Puqian Wang","Ilias Diakonikolas","Jelena Diakonikolas"],"pdf_url":"https://arxiv.org/pdf/2502.08611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15537v3","updated":"2025-02-12T17:59:14Z","published":"2024-02-23T04:52:08Z","title":"Evaluating the Performance of ChatGPT for Spam Email Detection","summary":"  Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction with (or without) a\nfew demonstrations. We also investigate how the number of demonstrations in the\nprompt affects the performance of ChatGPT. For comparison, we also implement\nfive popular benchmark methods, including naive Bayes, support vector machines\n(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and\nBERT classifiers. Through extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset. This study provides insights into the potential and limitations of\nChatGPT for spam identification, highlighting its potential as a viable\nsolution for resource-constrained language domains.\n","authors":["Shijing Si","Yuwei Wu","Le Tang","Yugui Zhang","Jedrek Wosik","Qinliang Su"],"pdf_url":"https://arxiv.org/pdf/2402.15537v3.pdf","comment":"12 pages, 4 figures; Accepted by Pacific Journal of Optimization\n  (PJO)"},{"id":"http://arxiv.org/abs/2409.05655v2","updated":"2025-02-12T17:55:53Z","published":"2024-09-09T14:22:19Z","title":"Interactive incremental learning of generalizable skills with local\n  trajectory modulation","summary":"  The problem of generalization in learning from demonstration (LfD) has\nreceived considerable attention over the years, particularly within the context\nof movement primitives, where a number of approaches have emerged. Recently,\ntwo important approaches have gained recognition. While one leverages\nvia-points to adapt skills locally by modulating demonstrated trajectories,\nanother relies on so-called task-parameterized models that encode movements\nwith respect to different coordinate systems, using a product of probabilities\nfor generalization. While the former are well-suited to precise, local\nmodulations, the latter aim at generalizing over large regions of the workspace\nand often involve multiple objects. Addressing the quality of generalization by\nleveraging both approaches simultaneously has received little attention. In\nthis work, we propose an interactive imitation learning framework that\nsimultaneously leverages local and global modulations of trajectory\ndistributions. Building on the kernelized movement primitives (KMP) framework,\nwe introduce novel mechanisms for skill modulation from direct human corrective\nfeedback. Our approach particularly exploits the concept of via-points to\nincrementally and interactively 1) improve the model accuracy locally, 2) add\nnew objects to the task during execution and 3) extend the skill into regions\nwhere demonstrations were not provided. We evaluate our method on a bearing\nring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.\n","authors":["Markus Knauer","Alin Albu-Sch√§ffer","Freek Stulp","Jo√£o Silv√©rio"],"pdf_url":"https://arxiv.org/pdf/2409.05655v2.pdf","comment":"Accepted at IEEE Robotics and Automation Letters (RA-L), 16 pages, 19\n  figures, 6 tables. See\n  https://github.com/DLR-RM/interactive-incremental-learning for further\n  information and video"},{"id":"http://arxiv.org/abs/2502.08606v1","updated":"2025-02-12T17:52:47Z","published":"2025-02-12T17:52:47Z","title":"Distillation Scaling Laws","summary":"  We provide a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings reduce the risks associated with using distillation\nat scale; compute allocation for both the teacher and student models can now be\ndone to maximize student performance. We provide compute optimal distillation\nrecipes for when 1) a teacher exists, or 2) a teacher needs training. If many\nstudents are to be distilled, or a teacher already exists, distillation\noutperforms supervised pretraining until a compute level which grows\npredictably with student size. If one student is to be distilled and a teacher\nalso needs training, supervised learning should be done instead. Additionally,\nwe provide insights across our large scale study of distillation, which\nincrease our understanding of distillation and inform experimental design.\n","authors":["Dan Busbridge","Amitis Shidani","Floris Weers","Jason Ramapuram","Etai Littwin","Russ Webb"],"pdf_url":"https://arxiv.org/pdf/2502.08606v1.pdf","comment":"67 pages, 54 figures, 13 tables"},{"id":"http://arxiv.org/abs/2502.08605v1","updated":"2025-02-12T17:49:46Z","published":"2025-02-12T17:49:46Z","title":"CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection","summary":"  Does the intrinsic curvature of complex networks hold the key to unveiling\ngraph anomalies that conventional approaches overlook? Reconstruction-based\ngraph anomaly detection (GAD) methods overlook such geometric outliers,\nfocusing only on structural and attribute-level anomalies. To this end, we\npropose CurvGAD - a mixed-curvature graph autoencoder that introduces the\nnotion of curvature-based geometric anomalies. CurvGAD introduces two parallel\npipelines for enhanced anomaly interpretability: (1) Curvature-equivariant\ngeometry reconstruction, which focuses exclusively on reconstructing the edge\ncurvatures using a mixed-curvature, Riemannian encoder and Gaussian\nkernel-based decoder; and (2) Curvature-invariant structure and attribute\nreconstruction, which decouples structural and attribute anomalies from\ngeometric irregularities by regularizing graph curvature under discrete\nOllivier-Ricci flow, thereby isolating the non-geometric anomalies. By\nleveraging curvature, CurvGAD refines the existing anomaly classifications and\nidentifies new curvature-driven anomalies. Extensive experimentation over 10\nreal-world datasets (both homophilic and heterophilic) demonstrates an\nimprovement of up to 6.5% over state-of-the-art GAD methods.\n","authors":["Karish Grover","Geoffrey J. Gordon","Christos Faloutsos"],"pdf_url":"https://arxiv.org/pdf/2502.08605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08603v1","updated":"2025-02-12T17:44:40Z","published":"2025-02-12T17:44:40Z","title":"Scalable Thermodynamic Second-order Optimization","summary":"  Many hardware proposals have aimed to accelerate inference in AI workloads.\nLess attention has been paid to hardware acceleration of training, despite the\nenormous societal impact of rapid training of AI models. Physics-based\ncomputers, such as thermodynamic computers, offer an efficient means to solve\nkey primitives in AI training algorithms. Optimizers that normally would be\ncomputationally out-of-reach (e.g., due to expensive matrix inversions) on\ndigital hardware could be unlocked with physics-based hardware. In this work,\nwe propose a scalable algorithm for employing thermodynamic computers to\naccelerate a popular second-order optimizer called Kronecker-factored\napproximate curvature (K-FAC). Our asymptotic complexity analysis predicts\nincreasing advantage with our algorithm as $n$, the number of neurons per\nlayer, increases. Numerical experiments show that even under significant\nquantization noise, the benefits of second-order optimization can be preserved.\nFinally, we predict substantial speedups for large-scale vision and graph\nproblems based on realistic hardware characteristics.\n","authors":["Kaelan Donatella","Samuel Duffield","Denis Melanson","Maxwell Aifer","Phoebe Klett","Rajath Salegame","Zach Belateche","Gavin Crooks","Antonio J. Martinez","Patrick J. Coles"],"pdf_url":"https://arxiv.org/pdf/2502.08603v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2411.05771v3","updated":"2025-02-12T17:43:56Z","published":"2024-11-08T18:33:03Z","title":"Sketched Equivariant Imaging Regularization and Deep Internal Learning\n  for Inverse Problems","summary":"  Equivariant Imaging (EI) regularization has become the de-facto technique for\nunsupervised training of deep imaging networks, without any need of\nground-truth data. Observing that the EI-based unsupervised training paradigm\ncurrently has significant computational redundancy leading to inefficiency in\nhigh-dimensional applications, we propose a sketched EI regularization which\nleverages the randomized sketching techniques for acceleration. We then extend\nour sketched EI regularization to develop an accelerated deep internal learning\nframework, Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be\nefficiently applied for single-image and task-adapted reconstruction.\nAdditionally, for network adaptation tasks, we propose a parameter-efficient\napproach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the\nnormalization layers. Our numerical study on X-ray CT and multi-coil MRI image\nreconstruction tasks demonstrate that our approach can achieve significant\ncomputational acceleration over standard EI-based counterpart in single-input\nsetting and network adaptation at test time.\n","authors":["Guixian Xu","Jinglai Li","Junqi Tang"],"pdf_url":"https://arxiv.org/pdf/2411.05771v3.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2501.18715v2","updated":"2025-02-12T17:41:57Z","published":"2025-01-30T19:26:05Z","title":"chebgreen: Learning and Interpolating Continuous Empirical Green's\n  Functions from Data","summary":"  In this work, we present a mesh-independent, data-driven library, chebgreen,\nto mathematically model one-dimensional systems, possessing an associated\ncontrol parameter, and whose governing partial differential equation is\nunknown. The proposed method learns an Empirical Green's Function for the\nassociated, but hidden, boundary value problem, in the form of a Rational\nNeural Network from which we subsequently construct a bivariate representation\nin a Chebyshev basis. We uncover the Green's function, at an unseen control\nparameter value, by interpolating the left and right singular functions within\na suitable library, expressed as points on a manifold of Quasimatrices, while\nthe associated singular values are interpolated with Lagrange polynomials.\n","authors":["Harshwardhan Praveen","Jacob Brown","Christopher Earls"],"pdf_url":"https://arxiv.org/pdf/2501.18715v2.pdf","comment":"Code is available at https://github.com/hsharsh/chebgreen"},{"id":"http://arxiv.org/abs/2501.07602v2","updated":"2025-02-12T17:41:23Z","published":"2025-01-10T23:33:15Z","title":"An Explainable Pipeline for Machine Learning with Functional Data","summary":"  Machine learning (ML) models have shown success in applications with an\nobjective of prediction, but the algorithmic complexity of some models makes\nthem difficult to interpret. Methods have been proposed to provide insight into\nthese \"black-box\" models, but there is little research that focuses on\nsupervised ML when the model inputs are functional data. In this work, we\nconsider two applications from high-consequence spaces with objectives of\nmaking predictions using functional data inputs. One application aims to\nclassify material types to identify explosive materials given hyperspectral\ncomputed tomography scans of the materials. The other application considers the\nforensics science task of connecting an inkjet printed document to the source\nprinter using color signatures extracted by Raman spectroscopy. An instinctive\nroute to consider for analyzing these data is a data driven ML model for\nclassification, but due to the high consequence nature of the applications, we\nargue it is important to appropriately account for the nature of the data in\nthe analysis to not obscure or misrepresent patterns. As such, we propose the\nVariable importance Explainable Elastic Shape Analysis (VEESA) pipeline for\ntraining ML models with functional data that (1) accounts for the vertical and\nhorizontal variability in the functional data and (2) provides an explanation\nin the original data space of how the model uses variability in the functional\ndata for prediction. The pipeline makes use of elastic functional principal\ncomponents analysis (efPCA) to generate uncorrelated model inputs and\npermutation feature importance (PFI) to identify the principal components\nimportant for prediction. The variability captured by the important principal\ncomponents in visualized the original data space. We ultimately discuss ideas\nfor natural extensions of the VEESA pipeline and challenges for future\nresearch.\n","authors":["Katherine Goode","J. Derek Tucker","Daniel Ries","Heike Hofmann"],"pdf_url":"https://arxiv.org/pdf/2501.07602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08600v1","updated":"2025-02-12T17:39:02Z","published":"2025-02-12T17:39:02Z","title":"Two-stage hybrid models for enhancing forecasting accuracy on\n  heterogeneous time series","summary":"  Compared to local models built in a series-by-series manner, global models\nleverage relevant information across time series, resulting in improved\nforecasting performance and generalization capacity. Constructing global models\non a set of time series is becoming mainstream in the field of time series\nforecasting. However, the advantages of global models may not always be\nrealized when dealing with heterogeneous data. While they can adapt to\nheterogeneous datasets by increasing the model complexity, the model cannot be\ninfinitely complex due to the finite sample size, which poses challenges for\nthe application of global models. Additionally, determining whether the time\nseries data is homogeneous or heterogeneous can be ambiguous in practice. To\naddress these research gaps, this paper argues that the heterogeneity of the\ndata should be defined by the global model used, and for each series, the\nportion not modelled by the global model represents heterogeneity. It further\nproposes two-stage hybrid models, which include a second stage to identify and\nmodel heterogeneous patterns. In this second stage, we can estimate either all\nlocal models or sub-global models across different domains divided based on\nheterogeneity. Experiments on four open datasets reveal that the proposed\nmethods significantly outperform five existing models, indicating they\ncontribute to fully unleash the potential of global models on heterogeneous\ndatasets.\n","authors":["Junru Ren","Shaomin Wu"],"pdf_url":"https://arxiv.org/pdf/2502.08600v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.08598v1","updated":"2025-02-12T17:35:43Z","published":"2025-02-12T17:35:43Z","title":"Enhancing Diffusion Models Efficiency by Disentangling Total-Variance\n  and Signal-to-Noise Ratio","summary":"  The long sampling time of diffusion models remains a significant bottleneck,\nwhich can be mitigated by reducing the number of diffusion time steps. However,\nthe quality of samples with fewer steps is highly dependent on the noise\nschedule, i.e., the specific manner in which noise is introduced and the signal\nis reduced at each step. Although prior work has improved upon the original\nvariance-preserving and variance-exploding schedules, these approaches\n$\\textit{passively}$ adjust the total variance, without direct control over it.\nIn this work, we propose a novel total-variance/signal-to-noise-ratio\ndisentangled (TV/SNR) framework, where TV and SNR can be controlled\nindependently. Our approach reveals that different existing schedules, where\nthe TV explodes exponentially, can be $\\textit{improved}$ by setting a constant\nTV schedule while preserving the same SNR schedule. Furthermore, generalizing\nthe SNR schedule of the optimal transport flow matching significantly improves\nthe performance in molecular structure generation, achieving few step\ngeneration of stable molecules. A similar tendency is observed in image\ngeneration, where our approach with a uniform diffusion time grid performs\ncomparably to the highly tailored EDM sampler.\n","authors":["Khaled Kahouli","Winfried Ripken","Stefan Gugler","Oliver T. Unke","Klaus-Robert M√ºller","Shinichi Nakajima"],"pdf_url":"https://arxiv.org/pdf/2502.08598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03943v2","updated":"2025-02-12T17:29:54Z","published":"2024-10-04T22:00:13Z","title":"Oscillatory State-Space Models","summary":"  We propose Linear Oscillatory State-Space models (LinOSS) for efficiently\nlearning on long sequences. Inspired by cortical dynamics of biological neural\nnetworks, we base our proposed LinOSS model on a system of forced harmonic\noscillators. A stable discretization, integrated over time using fast\nassociative parallel scans, yields the proposed state-space model. We prove\nthat LinOSS produces stable dynamics only requiring nonnegative diagonal state\nmatrix. This is in stark contrast to many previous state-space models relying\nheavily on restrictive parameterizations. Moreover, we rigorously show that\nLinOSS is universal, i.e., it can approximate any continuous and causal\noperator mapping between time-varying functions, to desired accuracy. In\naddition, we show that an implicit-explicit discretization of LinOSS perfectly\nconserves the symmetry of time reversibility of the underlying dynamics.\nTogether, these properties enable efficient modeling of long-range\ninteractions, while ensuring stable and accurate long-horizon forecasting.\nFinally, our empirical results, spanning a wide range of time-series tasks from\nmid-range to very long-range classification and regression, as well as\nlong-horizon forecasting, demonstrate that our proposed LinOSS model\nconsistently outperforms state-of-the-art sequence models. Notably, LinOSS\noutperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with\nsequences of length 50k.\n","authors":["T. Konstantin Rusch","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2410.03943v2.pdf","comment":"ICLR (Oral)"},{"id":"http://arxiv.org/abs/2408.05486v2","updated":"2025-02-12T17:29:28Z","published":"2024-08-10T08:27:58Z","title":"Topological Blindspots: Understanding and Extending Topological Deep\n  Learning Through the Lens of Expressivity","summary":"  Topological deep learning (TDL) is a rapidly growing field that seeks to\nleverage topological structure in data and facilitate learning from data\nsupported on topological objects, ranging from molecules to 3D shapes. Most TDL\narchitectures can be unified under the framework of higher-order\nmessage-passing (HOMP), which generalizes graph message-passing to higher-order\ndomains. In the first part of the paper, we explore HOMP's expressive power\nfrom a topological perspective, demonstrating the framework's inability to\ncapture fundamental topological and metric invariants such as diameter,\norientability, planarity, and homology. In addition, we demonstrate HOMP's\nlimitations in fully leveraging lifting and pooling methods on graphs. To the\nbest of our knowledge, this is the first work to study the expressivity of TDL\nfrom a \\emph{topological} perspective. In the second part of the paper, we\ndevelop two new classes of architectures -- multi-cellular networks (MCN) and\nscalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can\nreach full expressivity, but scaling it to large data objects can be\ncomputationally expansive. Designed as a more scalable alternative, SMCN still\nmitigates many of HOMP's expressivity limitations. Finally, we create new\nbenchmarks for evaluating models based on their ability to learn topological\nproperties of complexes. We then evaluate SMCN on these benchmarks and on\nreal-world graph datasets, demonstrating improvements over both HOMP baselines\nand expressive graph methods, highlighting the value of expressively leveraging\ntopological information. Code and data are available at\nhttps://github.com/yoavgelberg/SMCN.\n","authors":["Yam Eitan","Yoav Gelberg","Guy Bar-Shalom","Fabrizio Frasca","Michael Bronstein","Haggai Maron"],"pdf_url":"https://arxiv.org/pdf/2408.05486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11316v3","updated":"2025-02-12T17:28:17Z","published":"2024-06-17T08:26:51Z","title":"Improved Algorithms for Contextual Dynamic Pricing","summary":"  In contextual dynamic pricing, a seller sequentially prices goods based on\ncontextual information. Buyers will purchase products only if the prices are\nbelow their valuations. The goal of the seller is to design a pricing strategy\nthat collects as much revenue as possible. We focus on two different valuation\nmodels. The first assumes that valuations linearly depend on the context and\nare further distorted by noise. Under minor regularity assumptions, our\nalgorithm achieves an optimal regret bound of $\\tilde{\\mathcal{O}}(T^{2/3})$,\nimproving the existing results. The second model removes the linearity\nassumption, requiring only that the expected buyer valuation is\n$\\beta$-H\\\"older in the context. For this model, our algorithm obtains a regret\n$\\tilde{\\mathcal{O}}(T^{d+2\\beta/d+3\\beta})$, where $d$ is the dimension of the\ncontext space.\n","authors":["Matilde Tullii","Solenne Gaucher","Nadav Merlis","Vianney Perchet"],"pdf_url":"https://arxiv.org/pdf/2406.11316v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08586v1","updated":"2025-02-12T17:19:36Z","published":"2025-02-12T17:19:36Z","title":"Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous\n  Attacks","summary":"  A high volume of recent ML security literature focuses on attacks against\naligned large language models (LLMs). These attacks may extract private\ninformation or coerce the model into producing harmful outputs. In real-world\ndeployments, LLMs are often part of a larger agentic pipeline including memory\nsystems, retrieval, web access, and API calling. Such additional components\nintroduce vulnerabilities that make these LLM-powered agents much easier to\nattack than isolated LLMs, yet relatively little work focuses on the security\nof LLM agents. In this paper, we analyze security and privacy vulnerabilities\nthat are unique to LLM agents. We first provide a taxonomy of attacks\ncategorized by threat actors, objectives, entry points, attacker observability,\nattack strategies, and inherent vulnerabilities of agent pipelines. We then\nconduct a series of illustrative attacks on popular open-source and commercial\nagents, demonstrating the immediate practical implications of their\nvulnerabilities. Notably, our attacks are trivial to implement and require no\nunderstanding of machine learning.\n","authors":["Ang Li","Yin Zhou","Vethavikashini Chithrra Raghuram","Tom Goldstein","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2502.08586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08585v1","updated":"2025-02-12T17:18:14Z","published":"2025-02-12T17:18:14Z","title":"Scalable Bilevel Loss Balancing for Multi-Task Learning","summary":"  Multi-task learning (MTL) has been widely adopted for its ability to\nsimultaneously learn multiple tasks. While existing gradient manipulation\nmethods often yield more balanced solutions than simple scalarization-based\napproaches, they typically incur a significant computational overhead of\n$\\mathcal{O}(K)$ in both time and memory, where $K$ is the number of tasks. In\nthis paper, we propose BiLB4MTL, a simple and scalable loss balancing approach\nfor MTL, formulated from a novel bilevel optimization perspective. Our method\nincorporates three key components: (i) an initial loss normalization, (ii) a\nbilevel loss-balancing formulation, and (iii) a scalable first-order algorithm\nthat requires only $\\mathcal{O}(1)$ time and memory. Theoretically, we prove\nthat BiLB4MTL guarantees convergence not only to a stationary point of the\nbilevel loss balancing problem but also to an $\\epsilon$-accurate Pareto\nstationary point for all $K$ loss functions under mild conditions. Extensive\nexperiments on diverse multi-task datasets demonstrate that BiLB4MTL achieves\nstate-of-the-art performance in both accuracy and efficiency. Code is available\nat https://github.com/OptMN-Lab/-BiLB4MTL.\n","authors":["Peiyao Xiao","Chaosheng Dong","Shaofeng Zou","Kaiyi Ji"],"pdf_url":"https://arxiv.org/pdf/2502.08585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.13800v3","updated":"2025-02-12T17:18:07Z","published":"2022-02-28T13:50:47Z","title":"Differential equation and probability inspired graph neural networks for\n  latent variable learning","summary":"  Probabilistic theory and differential equation are powerful tools for the\ninterpretability and guidance of the design of machine learning models,\nespecially for illuminating the mathematical motivation of learning latent\nvariable from observation. Subspace learning maps high-dimensional features on\nlow-dimensional subspace to capture efficient representation. Graphs are widely\napplied for modeling latent variable learning problems, and graph neural\nnetworks implement deep learning architectures on graphs. Inspired by\nprobabilistic theory and differential equations, this paper conducts notes and\nproposals about graph neural networks to solve subspace learning problems by\nvariational inference and differential equation.\n","authors":["Zhuangwei Shi"],"pdf_url":"https://arxiv.org/pdf/2202.13800v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08582v1","updated":"2025-02-12T17:14:07Z","published":"2025-02-12T17:14:07Z","title":"A method for classification of data with uncertainty using hypothesis\n  testing","summary":"  Binary classification is a task that involves the classification of data into\none of two distinct classes. It is widely utilized in various fields. However,\nconventional classifiers tend to make overconfident predictions for data that\nbelong to overlapping regions of the two class distributions or for data\noutside the distributions (out-of-distribution data). Therefore, conventional\nclassifiers should not be applied in high-risk fields where classification\nresults can have significant consequences. In order to address this issue, it\nis necessary to quantify uncertainty and adopt decision-making approaches that\ntake it into account. Many methods have been proposed for this purpose;\nhowever, implementing these methods often requires performing resampling,\nimproving the structure or performance of models, and optimizing the thresholds\nof classifiers. We propose a new decision-making approach using two types of\nhypothesis testing. This method is capable of detecting ambiguous data that\nbelong to the overlapping regions of two class distributions, as well as\nout-of-distribution data that are not included in the training data\ndistribution. In addition, we quantify uncertainty using the empirical\ndistribution of feature values derived from the training data obtained through\nthe trained model. The classification threshold is determined by the\n$\\alpha$-quantile and ($1-\\alpha$)-quantile, where the significance level\n$\\alpha$ is set according to each specific situation.\n","authors":["Shoma Yokura","Akihisa Ichiki"],"pdf_url":"https://arxiv.org/pdf/2502.08582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08577v1","updated":"2025-02-12T17:10:53Z","published":"2025-02-12T17:10:53Z","title":"FBFL: A Field-Based Coordination Approach for Data Heterogeneity in\n  Federated Learning","summary":"  In the last years, Federated learning (FL) has become a popular solution to\ntrain machine learning models in domains with high privacy concerns. However,\nFL scalability and performance face significant challenges in real-world\ndeployments where data across devices are non-independently and identically\ndistributed (non-IID). The heterogeneity in data distribution frequently arises\nfrom spatial distribution of devices, leading to degraded model performance in\nthe absence of proper handling. Additionally, FL typical reliance on\ncentralized architectures introduces bottlenecks and single-point-of-failure\nrisks, particularly problematic at scale or in dynamic environments. To close\nthis gap, we propose Field-Based Federated Learning (FBFL), a novel approach\nleveraging macroprogramming and field coordination to address these limitations\nthrough: (i) distributed spatial-based leader election for personalization to\nmitigate non-IID data challenges; and (ii) construction of a self-organizing,\nhierarchical architecture using advanced macroprogramming patterns. Moreover,\nFBFL not only overcomes the aforementioned limitations, but also enables the\ndevelopment of more specialized models tailored to the specific data\ndistribution in each subregion. This paper formalizes FBFL and evaluates it\nextensively using MNIST, FashionMNIST, and Extended MNIST datasets. We\ndemonstrate that, when operating under IID data conditions, FBFL performs\ncomparably to the widely-used FedAvg algorithm. Furthermore, in challenging\nnon-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other\nstate-of-the-art methods, namely FedProx and Scaffold, which have been\nspecifically designed to address non-IID data distributions. Additionally, we\nshowcase the resilience of FBFL's self-organizing hierarchical architecture\nagainst server failures.\n","authors":["Davide Domini","Gianluca Aguzzi","Lukas Esterle","Mirko Viroli"],"pdf_url":"https://arxiv.org/pdf/2502.08577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08576v1","updated":"2025-02-12T17:10:34Z","published":"2025-02-12T17:10:34Z","title":"Mapping the Landscape of Generative AI in Network Monitoring and\n  Management","summary":"  Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management.\n","authors":["Giampaolo Bovenzi","Francesco Cerasuolo","Domenico Ciuonzo","Davide Di Monda","Idio Guarino","Antonio Montieri","Valerio Persico","Antonio Pescap√®"],"pdf_url":"https://arxiv.org/pdf/2502.08576v1.pdf","comment":"32 pages, 9 figure, 10 tables"},{"id":"http://arxiv.org/abs/2502.08574v1","updated":"2025-02-12T17:09:13Z","published":"2025-02-12T17:09:13Z","title":"COAST: Intelligent Time-Adaptive Neural Operators","summary":"  We introduce Causal Operator with Adaptive Solver Transformer (COAST), a\nnovel neural operator learning method that leverages a causal language model\n(CLM) framework to dynamically adapt time steps. Our method predicts both the\nevolution of a system and its optimal time step, intelligently balancing\ncomputational efficiency and accuracy. We find that COAST generates variable\nstep sizes that correlate with the underlying system intrinsicities, both\nwithin and across dynamical systems. Within a single trajectory, smaller steps\nare taken in regions of high complexity, while larger steps are employed in\nsimpler regions. Across different systems, more complex dynamics receive more\ngranular time steps. Benchmarked on diverse systems with varied dynamics, COAST\nconsistently outperforms state-of-the-art methods, achieving superior\nperformance in both efficiency and accuracy. This work underscores the\npotential of CLM-based intelligent adaptive solvers for scalable operator\nlearning of dynamical systems.\n","authors":["Zhikai Wu","Shiyang Zhang","Sizhuang He","Sifan Wang","Min Zhu","Anran Jiao","Lu Lu","David van Dijk"],"pdf_url":"https://arxiv.org/pdf/2502.08574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18304v4","updated":"2025-02-12T17:02:06Z","published":"2023-10-27T17:53:53Z","title":"A Stability Principle for Learning under Non-Stationarity","summary":"  We develop a versatile framework for statistical learning in non-stationary\nenvironments. In each time period, our approach applies a stability principle\nto select a look-back window that maximizes the utilization of historical data\nwhile keeping the cumulative bias within an acceptable range relative to the\nstochastic error. Our theory and numerical experiments showcase the adaptivity\nof this approach to unknown non-stationarity. We prove regret bounds that are\nminimax optimal up to logarithmic factors when the population losses are\nstrongly convex, or Lipschitz only. At the heart of our analysis lie two novel\ncomponents: a measure of similarity between functions and a segmentation\ntechnique for dividing the non-stationary data sequence into quasi-stationary\npieces.\n","authors":["Chengpiao Huang","Kaizheng Wang"],"pdf_url":"https://arxiv.org/pdf/2310.18304v4.pdf","comment":"65 pages, 7 figures"},{"id":"http://arxiv.org/abs/2501.04686v3","updated":"2025-02-12T16:49:50Z","published":"2025-01-08T18:49:41Z","title":"URSA: Understanding and Verifying Chain-of-thought Reasoning in\n  Multimodal Mathematics","summary":"  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.\n","authors":["Ruilin Luo","Zhuofan Zheng","Yifan Wang","Yiyao Yu","Xinzhe Ni","Zicheng Lin","Jin Zeng","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2501.04686v3.pdf","comment":"Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io"},{"id":"http://arxiv.org/abs/2502.06914v2","updated":"2025-02-12T16:47:32Z","published":"2025-02-10T09:46:26Z","title":"UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme\n  Active-Site Knowledge","summary":"  Enzyme-catalyzed protein cleavage is essential for many biological functions.\nAccurate prediction of cleavage sites can facilitate various applications such\nas drug development, enzyme design, and a deeper understanding of biological\nmechanisms. However, most existing models are restricted to an individual\nenzyme, which neglects shared knowledge of enzymes and fails generalize to\nnovel enzymes. Thus, we introduce a unified protein cleavage site predictor\nnamed UniZyme, which can generalize across diverse enzymes. To enhance the\nenzyme encoding for the protein cleavage site prediction, UniZyme employs a\nnovel biochemically-informed model architecture along with active-site\nknowledge of proteolytic enzymes. Extensive experiments demonstrate that\nUniZyme achieves high accuracy in predicting cleavage sites across a range of\nproteolytic enzymes, including unseen enzymes. The code is available in\nhttps://anonymous.4open.science/r/UniZyme-4A67.\n","authors":["Chenao Li","Shuo Yan","Enyan Dai"],"pdf_url":"https://arxiv.org/pdf/2502.06914v2.pdf","comment":"18 pages,8 figures"},{"id":"http://arxiv.org/abs/2110.06257v3","updated":"2025-02-12T16:47:01Z","published":"2021-10-12T18:12:57Z","title":"Causal Discovery from Conditionally Stationary Time Series","summary":"  Causal discovery, i.e., inferring underlying causal relationships from\nobservational data, is highly challenging for AI systems. In a time series\nmodeling context, traditional causal discovery methods mainly consider\nconstrained scenarios with fully observed variables and/or data from stationary\ntime-series. We develop a causal discovery approach to handle a wide class of\nnonstationary time series that are conditionally stationary, where the\nnonstationary behaviour is modeled as stationarity conditioned on a set of\nlatent state variables. Named State-Dependent Causal Inference (SDCI), our\napproach is able to recover the underlying causal dependencies, with provable\nidentifiablity for the state-dependent causal structures. Empirical experiments\non nonlinear particle interaction data and gene regulatory networks demonstrate\nSDCI's superior performance over baseline causal discovery methods. Improved\nresults over non-causal RNNs on modeling NBA player movements demonstrate the\npotential of our method and motivate the use of causality-driven methods for\nforecasting.\n","authors":["Carles Balsells-Rodas","Xavier Sumba","Tanmayee Narendra","Ruibo Tu","Gabriele Schweikert","Hedvig Kjellstrom","Yingzhen Li"],"pdf_url":"https://arxiv.org/pdf/2110.06257v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08557v1","updated":"2025-02-12T16:39:06Z","published":"2025-02-12T16:39:06Z","title":"QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion\n  in Information Retrieval","summary":"  Query expansion is widely used in Information Retrieval (IR) to improve\nsearch outcomes by enriching queries with additional contextual information.\nAlthough recent Large Language Model (LLM) based methods generate\npseudo-relevant content and expanded terms via multiple prompts, they often\nyield repetitive, narrow expansions that lack the diverse context needed to\nretrieve all relevant information. In this paper, we introduce QA-Expand, a\nnovel and effective framework for query expansion. It first generates multiple\nrelevant questions from the initial query and subsequently produces\ncorresponding pseudo-answers as surrogate documents. A feedback model further\nrewrites and filters these answers to ensure only the most informative\naugmentations are incorporated. Extensive experiments on benchmarks such as\nBEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up\nto 13% over state-of-the-art methods, offering a robust solution for modern\nretrieval challenges.\n","authors":["Wonduk Seo","Seunghyun Lee"],"pdf_url":"https://arxiv.org/pdf/2502.08557v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2502.08556v1","updated":"2025-02-12T16:38:40Z","published":"2025-02-12T16:38:40Z","title":"Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling","summary":"  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n","authors":["Shixiang Tang","Yizhou Wang","Lu Chen","Yuan Wang","Sida Peng","Dan Xu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2502.08556v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08555v1","updated":"2025-02-12T16:35:46Z","published":"2025-02-12T16:35:46Z","title":"A Machine Learning-Ready Data Processing Tool for Near Real-Time\n  Forecasting","summary":"  Space weather forecasting is critical for mitigating radiation risks in space\nexploration and protecting Earth-based technologies from geomagnetic\ndisturbances. This paper presents the development of a Machine Learning (ML)-\nready data processing tool for Near Real-Time (NRT) space weather forecasting.\nBy merging data from diverse NRT sources such as solar imagery, magnetic field\nmeasurements, and energetic particle fluxes, the tool addresses key gaps in\ncurrent space weather prediction capabilities. The tool processes and\nstructures the data for machine learning models, focusing on time-series\nforecasting and event detection for extreme solar events. It provides users\nwith a framework to download, process, and label data for ML applications,\nstreamlining the workflow for improved NRT space weather forecasting and\nscientific research.\n","authors":["Maher A Dayeh","Michael J Starkey","Subhamoy Chatterjee","Heather Elliott","Samuel Hart","Kimberly Moreland"],"pdf_url":"https://arxiv.org/pdf/2502.08555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08549v1","updated":"2025-02-12T16:30:39Z","published":"2025-02-12T16:30:39Z","title":"Copula-based mixture model identification for subgroup clustering with\n  imaging applications","summary":"  Model-based clustering techniques have been widely applied to various\napplication areas, while most studies focus on canonical mixtures with unique\ncomponent distribution form. However, this strict assumption is often hard to\nsatisfy. In this paper, we consider the more flexible Copula-Based Mixture\nModels (CBMMs) for clustering, which allow heterogeneous component\ndistributions composed by flexible choices of marginal and copula forms. More\nspecifically, we propose an adaptation of the Generalized Iterative Conditional\nEstimation (GICE) algorithm to identify the CBMMs in an unsupervised manner,\nwhere the marginal and copula forms and their parameters are estimated\niteratively. GICE is adapted from its original version developed for switching\nMarkov model identification with the choice of realization time. Our CBMM-GICE\nclustering method is then tested on synthetic two-cluster data (N=2000 samples)\nwith discussion of the factors impacting its convergence. Finally, it is\ncompared to the Expectation Maximization identified mixture models with unique\ncomponent form on the entire MNIST database (N=70000), and on real cardiac\nmagnetic resonance data (N=276) to illustrate its value for imaging\napplications.\n","authors":["Fei Zheng","Nicolas Duchateau"],"pdf_url":"https://arxiv.org/pdf/2502.08549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08542v1","updated":"2025-02-12T16:27:40Z","published":"2025-02-12T16:27:40Z","title":"Beyond Predictions: A Participatory Framework for Multi-Stakeholder\n  Decision-Making","summary":"  Conventional decision-support systems, primarily based on supervised\nlearning, focus on outcome prediction models to recommend actions. However,\nthey often fail to account for the complexities of multi-actor environments,\nwhere diverse and potentially conflicting stakeholder preferences must be\nbalanced. In this paper, we propose a novel participatory framework that\nredefines decision-making as a multi-stakeholder optimization problem,\ncapturing each actor's preferences through context-dependent reward functions.\nOur framework leverages $k$-fold cross-validation to fine-tune user-provided\noutcome prediction models and evaluate decision strategies, including\ncompromise functions mediating stakeholder trade-offs. We introduce a synthetic\nscoring mechanism that exploits user-defined preferences across multiple\nmetrics to rank decision-making strategies and identify the optimal\ndecision-maker. The selected decision-maker can then be used to generate\nactionable recommendations for new data. We validate our framework using two\nreal-world use cases, demonstrating its ability to deliver recommendations that\neffectively balance multiple metrics, achieving results that are often beyond\nthe scope of purely prediction-based methods. Ablation studies demonstrate that\nour framework, with its modular, model-agnostic, and inherently transparent\ndesign, integrates seamlessly with various predictive models, reward\nstructures, evaluation metrics, and sample sizes, making it particularly suited\nfor complex, high-stakes decision-making contexts.\n","authors":["Vittoria Vineis","Giuseppe Perelli","Gabriele Tolomei"],"pdf_url":"https://arxiv.org/pdf/2502.08542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07577v2","updated":"2025-02-12T16:25:44Z","published":"2025-02-11T14:23:13Z","title":"Automated Capability Discovery via Model Self-Exploration","summary":"  Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.\n","authors":["Cong Lu","Shengran Hu","Jeff Clune"],"pdf_url":"https://arxiv.org/pdf/2502.07577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08536v1","updated":"2025-02-12T16:21:01Z","published":"2025-02-12T16:21:01Z","title":"Matrix Completion with Graph Information: A Provable Nonconvex\n  Optimization Approach","summary":"  We consider the problem of matrix completion with graphs as side information\ndepicting the interrelations between variables. The key challenge lies in\nleveraging the similarity structure of the graph to enhance matrix recovery.\nExisting approaches, primarily based on graph Laplacian regularization, suffer\nfrom several limitations: (1) they focus only on the similarity between\nneighboring variables, while overlooking long-range correlations; (2) they are\nhighly sensitive to false edges in the graphs and (3) they lack theoretical\nguarantees regarding statistical and computational complexities. To address\nthese issues, we propose in this paper a novel graph regularized matrix\ncompletion algorithm called GSGD, based on preconditioned projected gradient\ndescent approach. We demonstrate that GSGD effectively captures the\nhigher-order correlation information behind the graphs, and achieves superior\nrobustness and stability against the false edges. Theoretically, we prove that\nGSGD achieves linear convergence to the global optimum with near-optimal sample\ncomplexity, providing the first theoretical guarantees for both recovery\naccuracy and efficacy in the perspective of nonconvex optimization. Our\nnumerical experiments on both synthetic and real-world data further validate\nthat GSGD achieves superior recovery accuracy and scalability compared with\nseveral popular alternatives.\n","authors":["Yao Wang","Yiyang Yang","Kaidong Wang","Shanxing Gao","Xiuwu Liao"],"pdf_url":"https://arxiv.org/pdf/2502.08536v1.pdf","comment":"41 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.08531v1","updated":"2025-02-12T16:08:48Z","published":"2025-02-12T16:08:48Z","title":"On Different Notions of Redundancy in Conditional-Independence-Based\n  Discovery of Graphical Models","summary":"  The goal of conditional-independence-based discovery of graphical models is\nto find a graph that represents the independence structure of variables in a\ngiven dataset. To learn such a representation, conditional-independence-based\napproaches conduct a set of statistical tests that suffices to identify the\ngraphical representation under some assumptions on the underlying distribution\nof the data. In this work, we highlight that due to the conciseness of the\ngraphical representation, there are often many tests that are not used in the\nconstruction of the graph. These redundant tests have the potential to detect\nor sometimes correct errors in the learned model. We show that not all tests\ncontain this additional information and that such redundant tests have to be\napplied with care. Precisely, we argue that particularly those conditional\n(in)dependence statements are interesting that follow only from graphical\nassumptions but do not hold for every probability distribution.\n","authors":["Philipp M. Faller","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2502.08531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09262v2","updated":"2025-02-12T16:07:40Z","published":"2025-01-16T03:11:50Z","title":"On the convergence rate of noisy Bayesian Optimization with Expected\n  Improvement","summary":"  Expected improvement (EI) is one of the most widely used acquisition\nfunctions in Bayesian optimization (BO). Despite its proven success in\napplications for decades, important open questions remain on the theoretical\nconvergence behaviors and rates for EI. In this paper, we contribute to the\nconvergence theory of EI in three novel and critical areas. First, we consider\nobjective functions that fit under the Gaussian process (GP) prior assumption,\nwhereas existing works mostly focus on functions in the reproducing kernel\nHilbert space (RKHS). Second, we establish for the first time the asymptotic\nerror bound and its corresponding rate for GP-EI with noisy observations under\nthe GP prior assumption. Third, by investigating the exploration and\nexploitation properties of the non-convex EI function, we establish improved\nerror bounds of GP-EI for both the noise-free and noisy cases.\n","authors":["Jingyi Wang","Haowei Wang","Nai-Yuan Chiang","Cosmin G. Petra"],"pdf_url":"https://arxiv.org/pdf/2501.09262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08524v1","updated":"2025-02-12T16:00:11Z","published":"2025-02-12T16:00:11Z","title":"LLM Pretraining with Continuous Concepts","summary":"  Next token prediction has been the standard training objective used in large\nlanguage model pretraining. Representations are learned as a result of\noptimizing for token-level perplexity. We propose Continuous Concept Mixing\n(CoCoMix), a novel pretraining framework that combines discrete next token\nprediction with continuous concepts. Specifically, CoCoMix predicts continuous\nconcepts learned from a pretrained sparse autoencoder and mixes them into the\nmodel's hidden state by interleaving with token hidden representations. Through\nexperiments on multiple benchmarks, including language modeling and downstream\nreasoning tasks, we show that CoCoMix is more sample efficient and consistently\noutperforms standard next token prediction, knowledge distillation and\ninserting pause tokens. We find that combining both concept learning and\ninterleaving in an end-to-end framework is critical to performance gains.\nFurthermore, CoCoMix enhances interpretability and steerability by allowing\ndirect inspection and modification of the predicted concept, offering a\ntransparent way to guide the model's internal reasoning process.\n","authors":["Jihoon Tack","Jack Lanchantin","Jane Yu","Andrew Cohen","Ilia Kulikov","Janice Lan","Shibo Hao","Yuandong Tian","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2502.08524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08518v1","updated":"2025-02-12T15:54:56Z","published":"2025-02-12T15:54:56Z","title":"FedMHO: Heterogeneous One-Shot Federated Learning Towards\n  Resource-Constrained Edge Devices","summary":"  Federated Learning (FL) is increasingly adopted in edge computing scenarios,\nwhere a large number of heterogeneous clients operate under constrained or\nsufficient resources. The iterative training process in conventional FL\nintroduces significant computation and communication overhead, which is\nunfriendly for resource-constrained edge devices. One-shot FL has emerged as a\npromising approach to mitigate communication overhead, and model-heterogeneous\nFL solves the problem of diverse computing resources across clients. However,\nexisting methods face challenges in effectively managing model-heterogeneous\none-shot FL, often leading to unsatisfactory global model performance or\nreliance on auxiliary datasets. To address these challenges, we propose a novel\nFL framework named FedMHO, which leverages deep classification models on\nresource-sufficient clients and lightweight generative models on\nresource-constrained devices. On the server side, FedMHO involves a two-stage\nprocess that includes data generation and knowledge fusion. Furthermore, we\nintroduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem\nduring the knowledge fusion stage, and an unsupervised data optimization\nsolution to improve the quality of synthetic samples. Comprehensive experiments\ndemonstrate the effectiveness of our methods, as they outperform\nstate-of-the-art baselines in various experimental setups.\n","authors":["Dezhong Yao","Yuexin Shi","Tongtong Liu","Zhiqiang Xu"],"pdf_url":"https://arxiv.org/pdf/2502.08518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08692v1","updated":"2025-02-12T15:51:39Z","published":"2025-02-12T15:51:39Z","title":"Efficient Split Learning LSTM Models for FPGA-based Edge IoT Devices","summary":"  Split Learning (SL) recently emerged as an efficient paradigm for distributed\nMachine Learning (ML) suitable for the Internet Of Things (IoT)-Cloud systems.\nHowever, deploying SL on resource-constrained edge IoT platforms poses a\nsignificant challenge in terms of balancing the model performance against the\nprocessing, memory, and energy resources. In this work, we present a practical\nstudy of deploying SL framework on a real-world Field-Programmable Gate Array\n(FPGA)-based edge IoT platform. We address the SL framework applied to a\ntime-series processing model based on Recurrent Neural Networks (RNNs). Set in\nthe context of river water quality monitoring and using real-world data, we\ntrain, optimize, and deploy a Long Short-Term Memory (LSTM) model on a given\nedge IoT FPGA platform in different SL configurations. Our results demonstrate\nthe importance of aligning design choices with specific application\nrequirements, whether it is maximizing speed, minimizing power, or optimizing\nfor resource constraints.\n","authors":["Romina Soledad Molina","Vukan Ninkovic","Dejan Vukobratovic","Maria Liz Crespo","Marco Zennaro"],"pdf_url":"https://arxiv.org/pdf/2502.08692v1.pdf","comment":"Accepted for publication at IEEE ICMLCN 2025"},{"id":"http://arxiv.org/abs/2502.08515v1","updated":"2025-02-12T15:47:48Z","published":"2025-02-12T15:47:48Z","title":"The Paradox of Stochasticity: Limited Creativity and Computational\n  Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data","summary":"  This study examines how temperature settings and model architectures affect\nthe generation of structured fictional data (names, birthdates) across three\nlarge language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest.\nBy systematically testing temperature values from 0.0 to 1.0 in increments of\n0.1, we conducted 330 trials yielding 889 structured entities, validated for\nsyntactic consistency. Key findings reveal that model architecture\nsignificantly influences computational efficiency, with mistral:latest and\nllama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to\nexpectations, temperature showed no correlation with processing time,\nchallenging assumptions about stochastic sampling costs. Output diversity\nremained limited, as models consistently defaulted to common name archetypes\n(e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names\nclustered at intermediate values (0.3-0.7). These results demonstrate that\narchitectural optimizations, rather than temperature adjustments, dominate\nperformance in structured generation tasks. The findings emphasize prioritizing\nmodel selection over hyperparameter tuning for efficiency and suggest explicit\ndiversity constraints are necessary to mitigate default output biases in\nsynthetic data pipelines.\n","authors":["Evgenii Evstafev"],"pdf_url":"https://arxiv.org/pdf/2502.08515v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.05980v3","updated":"2025-02-12T15:40:35Z","published":"2024-02-08T06:48:01Z","title":"Do Large Code Models Understand Programming Concepts? Counterfactual\n  Analysis for Code Predicates","summary":"  Large Language Models' success on text generation has also made them better\nat code generation and coding tasks. While a lot of work has demonstrated their\nremarkable performance on tasks such as code completion and editing, it is\nstill unclear as to why. We help bridge this gap by exploring to what degree\nauto-regressive models understand the logical constructs of the underlying\nprograms. We propose Counterfactual Analysis for Programming Concept Predicates\n(CACP) as a counterfactual testing framework to evaluate whether Large Code\nModels understand programming concepts. With only black-box access to the\nmodel, we use CACP to evaluate ten popular Large Code Models for four different\nprogramming concepts. Our findings suggest that current models lack\nunderstanding of concepts such as data flow and control flow.\n","authors":["Ashish Hooda","Mihai Christodorescu","Miltiadis Allamanis","Aaron Wilson","Kassem Fawaz","Somesh Jha"],"pdf_url":"https://arxiv.org/pdf/2402.05980v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08505v1","updated":"2025-02-12T15:36:38Z","published":"2025-02-12T15:36:38Z","title":"Bridging Domain Adaptation and Graph Neural Networks: A Tensor-Based\n  Framework for Effective Label Propagation","summary":"  Graph Neural Networks (GNNs) have recently become the predominant tools for\nstudying graph data. Despite state-of-the-art performance on graph\nclassification tasks, GNNs are overwhelmingly trained in a single domain under\nsupervision, thus necessitating a prohibitively high demand for labels and\nresulting in poorly transferable representations. To address this challenge, we\npropose the Label-Propagation Tensor Graph Neural Network (LP-TGNN) framework\nto bridge the gap between graph data and traditional domain adaptation methods.\nIt extracts graph topological information holistically with a tensor\narchitecture and then reduces domain discrepancy through label propagation. It\nis readily compatible with general GNNs and domain adaptation techniques with\nminimal adjustment through pseudo-labeling. Experiments on various real-world\nbenchmarks show that our LP-TGNN outperforms baselines by a notable margin. We\nalso validate and analyze each component of the proposed framework in the\nablation study.\n","authors":["Tao Wen","Elynn Chen","Yuzhou Chen","Qi Lei"],"pdf_url":"https://arxiv.org/pdf/2502.08505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10229v2","updated":"2025-02-12T15:34:01Z","published":"2024-05-16T16:28:11Z","title":"Random ReLU Neural Networks as Non-Gaussian Processes","summary":"  We consider a large class of shallow neural networks with randomly\ninitialized parameters and rectified linear unit activation functions. We prove\nthat these random neural networks are well-defined non-Gaussian processes. As a\nby-product, we demonstrate that these networks are solutions to stochastic\ndifferential equations driven by impulsive white noise (combinations of random\nDirac measures). These processes are parameterized by the law of the weights\nand biases as well as the density of activation thresholds in each bounded\nregion of the input domain. We prove that these processes are isotropic and\nwide-sense self-similar with Hurst exponent 3/2. We also derive a remarkably\nsimple closed-form expression for their autocovariance function. Our results\nare fundamentally different from prior work in that we consider a\nnon-asymptotic viewpoint: The number of neurons in each bounded region of the\ninput domain (i.e., the width) is itself a random variable with a Poisson law\nwith mean proportional to the density parameter. Finally, we show that, under\nsuitable hypotheses, as the expected width tends to infinity, these processes\ncan converge in law not only to Gaussian processes, but also to non-Gaussian\nprocesses depending on the law of the weights. Our asymptotic results provide a\nnew take on several classical results (wide networks converge to Gaussian\nprocesses) as well as some new ones (wide networks can converge to non-Gaussian\nprocesses).\n","authors":["Rahul Parhi","Pakshal Bohra","Ayoub El Biari","Mehrsa Pourya","Michael Unser"],"pdf_url":"https://arxiv.org/pdf/2405.10229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08496v1","updated":"2025-02-12T15:31:16Z","published":"2025-02-12T15:31:16Z","title":"Fine-Tuning Topics through Weighting Aspect Keywords","summary":"  Topic modeling often requires examining topics from multiple perspectives to\nuncover hidden patterns, especially in less explored areas. This paper presents\nan approach to address this need, utilizing weighted keywords from various\naspects derived from a domain knowledge. The research method starts with\nstandard topic modeling. Then, it adds a process consisting of four key steps.\nFirst, it defines keywords for each aspect. Second, it gives weights to these\nkeywords based on their relevance. Third, it calculates relevance scores for\naspect-weighted keywords and topic keywords to create aspect-topic models.\nFourth, it uses these scores to tune relevant new documents. Finally, the\ngenerated topic models are interpreted and validated. The findings show that\ntop-scoring documents are more likely to be about the same aspect of a topic.\nThis highlights the model's effectiveness in finding the related documents to\nthe aspects.\n","authors":["Ali Nazari","Michael Weiss"],"pdf_url":"https://arxiv.org/pdf/2502.08496v1.pdf","comment":"17 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.08488v1","updated":"2025-02-12T15:23:29Z","published":"2025-02-12T15:23:29Z","title":"One-Shot Federated Learning with Classifier-Free Diffusion Models","summary":"  Federated learning (FL) enables collaborative learning without data\ncentralization but introduces significant communication costs due to multiple\ncommunication rounds between clients and the server. One-shot federated\nlearning (OSFL) addresses this by forming a global model with a single\ncommunication round, often relying on the server's model distillation or\nauxiliary dataset generation - often through pre-trained diffusion models\n(DMs). Existing DM-assisted OSFL methods, however, typically employ\nclassifier-guided DMs, which require training auxiliary classifier models at\neach client, introducing additional computation overhead. This work introduces\nOSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a\nnovel OSFL approach that eliminates the need for auxiliary models. OSCAR uses\nfoundation models to devise category-specific data representations at each\nclient, seamlessly integrated into a classifier-free diffusion model pipeline\nfor server-side data generation. OSCAR is a simple yet cost-effective OSFL\napproach that outperforms the state-of-the-art on four benchmarking datasets\nwhile reducing the communication load by at least 99%.\n","authors":["Obaidullah Zaland","Shutong Jin","Florian T. Pokorny","Monowar Bhuyan"],"pdf_url":"https://arxiv.org/pdf/2502.08488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07746v2","updated":"2025-02-12T15:18:45Z","published":"2024-10-10T09:23:33Z","title":"Benign Overfitting in Single-Head Attention","summary":"  The phenomenon of benign overfitting, where a trained neural network\nperfectly fits noisy training data but still achieves near-optimal test\nperformance, has been extensively studied in recent years for linear models and\nfully-connected/convolutional networks. In this work, we study benign\noverfitting in a single-head softmax attention model, which is the fundamental\nbuilding block of Transformers. We prove that under appropriate conditions, the\nmodel exhibits benign overfitting in a classification setting already after two\nsteps of gradient descent. Moreover, we show conditions where a\nminimum-norm/maximum-margin interpolator exhibits benign overfitting. We study\nhow the overfitting behavior depends on the signal-to-noise ratio (SNR) of the\ndata distribution, namely, the ratio between norms of signal and noise tokens,\nand prove that a sufficiently large SNR is both necessary and sufficient for\nbenign overfitting.\n","authors":["Roey Magen","Shuning Shang","Zhiwei Xu","Spencer Frei","Wei Hu","Gal Vardi"],"pdf_url":"https://arxiv.org/pdf/2410.07746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10949v2","updated":"2025-02-12T15:18:32Z","published":"2024-07-15T17:45:53Z","title":"Representing Rule-based Chatbots with Transformers","summary":"  What kind of internal mechanisms might Transformers use to conduct fluid,\nnatural-sounding conversations? Prior work has illustrated by construction how\nTransformers can solve various synthetic tasks, such as sorting a list or\nrecognizing formal languages, but it remains unclear how to extend this\napproach to a conversational setting. In this work, we propose using ELIZA, a\nclassic rule-based chatbot, as a setting for formal, mechanistic analysis of\nTransformer-based chatbots. ELIZA allows us to formally model key aspects of\nconversation, including local pattern matching and long-term dialogue state\ntracking. We first present a theoretical construction of a Transformer that\nimplements the ELIZA chatbot. Building on prior constructions, particularly\nthose for simulating finite-state automata, we show how simpler mechanisms can\nbe composed and extended to produce more sophisticated behavior. Next, we\nconduct a set of empirical analyses of Transformers trained on synthetically\ngenerated ELIZA conversations. Our analysis illustrates the kinds of mechanisms\nthese models tend to prefer--for example, models favor an induction head\nmechanism over a more precise, position-based copying mechanism; and using\nintermediate generations to simulate recurrent data structures, akin to an\nimplicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit\nconnection between neural chatbots and interpretable, symbolic mechanisms, our\nresults provide a new framework for the mechanistic analysis of conversational\nagents.\n","authors":["Dan Friedman","Abhishek Panigrahi","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10949v2.pdf","comment":"NAACL 2025. Code and data are available at\n  https://github.com/princeton-nlp/ELIZA-Transformer"},{"id":"http://arxiv.org/abs/2502.08482v1","updated":"2025-02-12T15:17:04Z","published":"2025-02-12T15:17:04Z","title":"Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned\n  Reasoning","summary":"  Chain-of-Thought (CoT) prompting has emerged as a powerful technique for\nenhancing language model's reasoning capabilities. However, generating long and\ncorrect CoT trajectories is challenging. Recent studies have demonstrated that\nLooped Transformers possess remarkable length generalization capabilities, but\ntheir limited generality and adaptability prevent them from serving as an\nalternative to auto-regressive solutions. To better leverage the strengths of\nLooped Transformers, we propose RELAY (REasoning through Loop Alignment\niterativelY). Specifically, we align the steps of Chain-of-Thought (CoT)\nreasoning with loop iterations and apply intermediate supervision during the\ntraining of Looped Transformers. This additional iteration-wise supervision not\nonly preserves the Looped Transformer's ability for length generalization but\nalso enables it to predict CoT reasoning steps for unseen data. Therefore, we\nleverage this Looped Transformer to generate accurate reasoning chains for\ncomplex problems that exceed the training length, which will then be used to\nfine-tune an auto-regressive model. We conduct extensive experiments, and the\nresults demonstrate the effectiveness of our approach, with significant\nimprovements in the performance of the auto-regressive model. Code will be\nreleased at https://github.com/qifanyu/RELAY.\n","authors":["Qifan Yu","Zhenyu He","Sijie Li","Xun Zhou","Jun Zhang","Jingjing Xu","Di He"],"pdf_url":"https://arxiv.org/pdf/2502.08482v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2411.02540v3","updated":"2025-02-12T15:14:01Z","published":"2024-11-04T19:21:06Z","title":"GraphXAIN: Narratives to Explain Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose challenges in interpretability. Existing\nGNN explanation methods usually yield technical outputs, such as subgraphs and\nfeature importance scores, that are difficult for non-data scientists to\nunderstand and thereby violate the purpose of explanations. Motivated by recent\nExplainable AI (XAI) research, we propose GraphXAIN, a method that generates\nnatural language narratives explaining GNN predictions. GraphXAIN is a model-\nand explainer-agnostic method that uses Large Language Models (LLMs) to\ntranslate explanatory subgraphs and feature importance scores into coherent,\nstory-like explanations of GNN decision-making processes. Evaluations on\nreal-world datasets demonstrate GraphXAIN's ability to improve graph\nexplanations. A survey of machine learning researchers and practitioners\nreveals that GraphXAIN enhances four explainability dimensions:\nunderstandability, satisfaction, convincingness, and suitability for\ncommunicating model predictions. When combined with another graph explainer\nmethod, GraphXAIN further improves trustworthiness, insightfulness, confidence,\nand usability. Notably, 95% of participants found GraphXAIN to be a valuable\naddition to the GNN explanation method. By incorporating natural language\nnarratives, our approach serves both graph practitioners and non-expert users\nby providing clearer and more effective explanations.\n","authors":["Mateusz Cedro","David Martens"],"pdf_url":"https://arxiv.org/pdf/2411.02540v3.pdf","comment":"19 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.11759v4","updated":"2025-02-12T15:07:01Z","published":"2024-10-15T16:28:55Z","title":"LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and\n  General Noise for Global Causal Discovery","summary":"  Inferring causal relationships from observational data is crucial when\nexperiments are costly or infeasible. Additive noise models (ANMs) enable\nunique directed acyclic graph (DAG) identification, but existing ANM methods\noften rely on restrictive assumptions on the data generating process, limiting\ntheir applicability to real-world settings. We propose local search in additive\nnoise models, LoSAM, a topological ordering method for learning a unique DAG in\nANMs with mixed causal mechanisms and general noise distributions. We introduce\nnew causal substructures and criteria for identifying roots and leaves,\nenabling efficient top-down learning. We prove asymptotic consistency and\npolynomial runtime, ensuring scalability and sample efficiency. We test LoSAM\non synthetic and real-world data, demonstrating state-of-the-art performance\nacross all mixed mechanism settings.\n","authors":["Sujai Hiremath","Promit Ghosal","Kyra Gan"],"pdf_url":"https://arxiv.org/pdf/2410.11759v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11140v4","updated":"2025-02-12T15:06:14Z","published":"2023-04-21T17:22:08Z","title":"Convergence of Message Passing Graph Neural Networks with Generic\n  Aggregation On Large Random Graphs","summary":"  We study the convergence of message passing graph neural networks on random\ngraph models to their continuous counterpart as the number of nodes tends to\ninfinity. Until now, this convergence was only known for architectures with\naggregation functions in the form of normalized means, or, equivalently, of an\napplication of classical operators like the adjacency matrix or the graph\nLaplacian. We extend such results to a large class of aggregation functions,\nthat encompasses all classically used message passing graph neural networks,\nsuch as attention-based message passing, max convolutional message passing,\n(degree-normalized) convolutional message passing, or moment-based aggregation\nmessage passing. Under mild assumptions, we give non-asymptotic bounds with\nhigh probability to quantify this convergence. Our main result is based on the\nMcDiarmid inequality. Interestingly, this result does not apply to the case\nwhere the aggregation is a coordinate-wise maximum. We treat this case\nseparately and obtain a different convergence rate.\n","authors":["Matthieu Cordonnier","Nicolas Keriven","Nicolas Tremblay","Samuel Vaiter"],"pdf_url":"https://arxiv.org/pdf/2304.11140v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08470v1","updated":"2025-02-12T15:04:23Z","published":"2025-02-12T15:04:23Z","title":"Numerical Schemes for Signature Kernels","summary":"  Signature kernels have emerged as a powerful tool within kernel methods for\nsequential data. In the paper \"The Signature Kernel is the solution of a\nGoursat PDE\", the authors identify a kernel trick that demonstrates that, for\ncontinuously differentiable paths, the signature kernel satisfies a Goursat\nproblem for a hyperbolic partial differential equation (PDE) in two independent\ntime variables. While finite difference methods have been explored for this\nPDE, they face limitations in accuracy and stability when handling highly\noscillatory inputs. In this work, we introduce two advanced numerical schemes\nthat leverage polynomial representations of boundary conditions through either\napproximation or interpolation techniques, and rigorously establish the\ntheoretical convergence of the polynomial approximation scheme. Experimental\nevaluations reveal that our approaches yield improvements of several orders of\nmagnitude in mean absolute percentage error (MAPE) compared to traditional\nfinite difference schemes, without increasing computational complexity.\nFurthermore, like finite difference methods, our algorithms can be\nGPU-parallelized to reduce computational complexity from quadratic to linear in\nthe length of the input sequences, thereby improving scalability for\nhigh-frequency data. We have implemented these algorithms in a dedicated Python\nlibrary, which is publicly available at:\nhttps://github.com/FrancescoPiatti/polysigkernel.\n","authors":["Thomas Cass","Francesco Piatti","Jeffrey Pei"],"pdf_url":"https://arxiv.org/pdf/2502.08470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08690v1","updated":"2025-02-12T15:03:26Z","published":"2025-02-12T15:03:26Z","title":"Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient\n  Text-to-Image Generation","summary":"  Large-scale text encoders in text-to-image (T2I) diffusion models have\ndemonstrated exceptional performance in generating high-quality images from\ntextual prompts. Unlike denoising modules that rely on multiple iterative\nsteps, text encoders require only a single forward pass to produce text\nembeddings. However, despite their minimal contribution to total inference time\nand floating-point operations (FLOPs), text encoders demand significantly\nhigher memory usage, up to eight times more than denoising modules. To address\nthis inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet\neffective pruning strategy specifically designed for text encoders in T2I\ndiffusion models. Skrr exploits the inherent redundancy in transformer blocks\nby selectively skipping or reusing certain layers in a manner tailored for T2I\ntasks, thereby reducing memory consumption without compromising performance.\nExtensive experiments demonstrate that Skrr maintains image quality comparable\nto the original model even under high sparsity levels, outperforming existing\nblockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory\nefficiency while preserving performance across multiple evaluation metrics,\nincluding the FID, CLIP, DreamSim, and GenEval scores.\n","authors":["Hoigi Seo","Wongi Jeong","Jae-sun Seo","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2502.08690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03667v2","updated":"2025-02-12T14:56:35Z","published":"2024-05-06T17:43:39Z","title":"Fault Detection and Monitoring using a Data-Driven Information-Based\n  Strategy: Method, Theory, and Application","summary":"  The ability to detect when a system undergoes an incipient fault is of\nparamount importance in preventing a critical failure. Classic methods for\nfault detection (including model-based and data-driven approaches) rely on\nthresholding error statistics or simple input-residual dependencies but face\ndifficulties with non-linear or non-Gaussian systems. Behavioral methods (e.g.,\nthose relying on digital twins) address these difficulties but still face\nchallenges when faulty data is scarce, decision guarantees are required, or\nworking with already-deployed models is required. In this work, we propose an\ninformation-driven fault detection method based on a novel concept drift\ndetector, addressing these challenges. The method is tailored to identifying\ndrifts in input-output relationships of additive noise models (i.e., model\ndrifts) and is based on a distribution-free mutual information (MI) estimator.\nOur scheme does not require prior faulty examples and can be applied\ndistribution-free over a large class of system models. Our core contributions\nare twofold. First, we demonstrate the connection between fault detection,\nmodel drift detection, and testing independence between two random variables.\nSecond, we prove several theoretical properties of the proposed MI-based fault\ndetection scheme: (i) strong consistency, (ii) exponentially fast detection of\nthe non-faulty case, and (iii) control of both significance levels and power of\nthe test. To conclude, we validate our theory with synthetic data and the\nbenchmark dataset N-CMAPSS of aircraft turbofan engines. These empirical\nresults support the usefulness of our methodology in many practical and\nrealistic settings, and the theoretical results show performance guarantees\nthat other methods cannot offer.\n","authors":["Camilo Ram√≠rez","Jorge F. Silva","Ferhat Tamssaouet","Tom√°s Rojas","Marcos E. Orchard"],"pdf_url":"https://arxiv.org/pdf/2405.03667v2.pdf","comment":"31 pages, 15 figures. This is the accepted manuscript for publication\n  in Mechanical Systems and Signal Processing. The arXiv version has been\n  updated accordingly"},{"id":"http://arxiv.org/abs/2408.09966v2","updated":"2025-02-12T14:55:29Z","published":"2024-08-19T13:14:02Z","title":"Mask in the Mirror: Implicit Sparsification","summary":"  Continuous sparsification strategies are among the most effective methods for\nreducing the inference costs and memory demands of large-scale neural networks.\nA key factor in their success is the implicit $L_1$ regularization induced by\njointly learning both mask and weight variables, which has been shown\nexperimentally to outperform explicit $L_1$ regularization. We provide a\ntheoretical explanation for this observation by analyzing the learning\ndynamics, revealing that early continuous sparsification is governed by an\nimplicit $L_2$ regularization that gradually transitions to an $L_1$ penalty\nover time. Leveraging this insight, we propose a method to dynamically control\nthe strength of this implicit bias. Through an extension of the mirror flow\nframework, we establish convergence and optimality guarantees in the context of\nunderdetermined linear regression. Our theoretical findings may be of\nindependent interest, as we demonstrate how to enter the rich regime and show\nthat the implicit bias can be controlled via a time-dependent Bregman\npotential. To validate these insights, we introduce PILoT, a continuous\nsparsification approach with novel initialization and dynamic regularization,\nwhich consistently outperforms baselines in standard experiments.\n","authors":["Tom Jacobs","Rebekka Burkholz"],"pdf_url":"https://arxiv.org/pdf/2408.09966v2.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.08457v1","updated":"2025-02-12T14:52:04Z","published":"2025-02-12T14:52:04Z","title":"Learning Theory for Kernel Bilevel Optimization","summary":"  Bilevel optimization has emerged as a technique for addressing a wide range\nof machine learning problems that involve an outer objective implicitly\ndetermined by the minimizer of an inner problem. In this paper, we investigate\nthe generalization properties for kernel bilevel optimization problems where\nthe inner objective is optimized over a Reproducing Kernel Hilbert Space. This\nsetting enables rich function approximation while providing a foundation for\nrigorous theoretical analysis. In this context, we establish novel\ngeneralization error bounds for the bilevel problem under finite-sample\napproximation. Our approach adopts a functional perspective, inspired by\n(Petrulionyte et al., 2024), and leverages tools from empirical process theory\nand maximal inequalities for degenerate $U$-processes to derive uniform error\nbounds. These generalization error estimates allow to characterize the\nstatistical accuracy of gradient-based methods applied to the empirical\ndiscretization of the bilevel problem.\n","authors":["Fares El Khoury","Edouard Pauwels","Samuel Vaiter","Michael Arbel"],"pdf_url":"https://arxiv.org/pdf/2502.08457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08649v2","updated":"2025-02-12T14:49:16Z","published":"2024-07-11T16:28:31Z","title":"Confidence-based Estimators for Predictive Performance in Model\n  Monitoring","summary":"  After a machine learning model has been deployed into production, its\npredictive performance needs to be monitored. Ideally, such monitoring can be\ncarried out by comparing the model's predictions against ground truth labels.\nFor this to be possible, the ground truth labels must be available relatively\nsoon after inference. However, there are many use cases where ground truth\nlabels are available only after a significant delay, or in the worst case, not\nat all. In such cases, directly monitoring the model's predictive performance\nis impossible.\n  Recently, novel methods for estimating the predictive performance of a model\nwhen ground truth is unavailable have been developed. Many of these methods\nleverage model confidence or other uncertainty estimates and are experimentally\ncompared against a naive baseline method, namely Average Confidence (AC), which\nestimates model accuracy as the average of confidence scores for a given set of\npredictions. However, until now the theoretical properties of the AC method\nhave not been properly explored. In this paper, we try to fill this gap by\nreviewing the AC method and show that under certain general assumptions, it is\nan unbiased and consistent estimator of model accuracy with many desirable\nproperties. We also compare this baseline estimator against some more complex\nestimators empirically and show that in many cases the AC method is able to\nbeat the others, although the comparative quality of the different estimators\nis heavily case-dependent.\n","authors":["Juhani Kivim√§ki","Jakub Bia≈Çek","Jukka K. Nurminen","Wojtek Kuberski"],"pdf_url":"https://arxiv.org/pdf/2407.08649v2.pdf","comment":"This version corresponds to the final published version in JAIR. The\n  published article is available at [https://doi.org/10.1613/jair.1.16709]"},{"id":"http://arxiv.org/abs/2311.00055v2","updated":"2025-02-12T14:43:07Z","published":"2023-10-31T18:03:54Z","title":"Rethinking Pre-Training in Tabular Data: A Neighborhood Embedding\n  Perspective","summary":"  Pre-training is prevalent in deep learning for vision and text data,\nleveraging knowledge from other datasets to enhance downstream tasks. However,\nfor tabular data, the inherent heterogeneity in attribute and label spaces\nacross datasets complicates the learning of shareable knowledge. We propose\nTabular data Pre-Training via Meta-representation (TabPTM), aiming to pre-train\na general tabular model over diverse datasets. The core idea is to embed data\ninstances into a shared feature space, where each instance is represented by\nits distance to a fixed number of nearest neighbors and their labels. This\n''meta-representation'' transforms heterogeneous tasks into homogeneous local\nprediction problems, enabling the model to infer labels (or scores for each\nlabel) based on neighborhood information. As a result, the pre-trained TabPTM\ncan be applied directly to new datasets, regardless of their diverse attributes\nand labels, without further fine-tuning. Extensive experiments on 101 datasets\nconfirm TabPTM's effectiveness in both classification and regression tasks,\nwith and without fine-tuning.\n","authors":["Han-Jia Ye","Qi-Le Zhou","Huai-Hong Yin","De-Chuan Zhan","Wei-Lun Chao"],"pdf_url":"https://arxiv.org/pdf/2311.00055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08448v1","updated":"2025-02-12T14:40:19Z","published":"2025-02-12T14:40:19Z","title":"Monge SAM: Robust Reparameterization-Invariant Sharpness-Aware\n  Minimization Based on Loss Geometry","summary":"  Recent studies on deep neural networks show that flat minima of the loss\nlandscape correlate with improved generalization. Sharpness-aware minimization\n(SAM) efficiently finds flat regions by updating the parameters according to\nthe gradient at an adversarial perturbation. The perturbation depends on the\nEuclidean metric, making SAM non-invariant under reparametrizations, which\nblurs sharpness and generalization. We propose Monge SAM (M-SAM), a\nreparametrization invariant version of SAM by considering a Riemannian metric\nin the parameter space induced naturally by the loss surface. Compared to\nprevious approaches, M-SAM works under any modeling choice, relies only on mild\nassumptions while being as computationally efficient as SAM. We theoretically\nargue that M-SAM varies between SAM and gradient descent (GD), which increases\nrobustness to hyperparameter selection and reduces attraction to suboptimal\nequilibria like saddle points. We demonstrate this behavior both theoretically\nand empirically on a multi-modal representation alignment task.\n","authors":["Albert Kj√∏ller Jacobsen","Georgios Arvanitidis"],"pdf_url":"https://arxiv.org/pdf/2502.08448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08445v1","updated":"2025-02-12T14:36:25Z","published":"2025-02-12T14:36:25Z","title":"$\\texttt{LucidAtlas}$: Learning Uncertainty-Aware,\n  Covariate-Disentangled, Individualized Atlas Representations","summary":"  The goal of this work is to develop principled techniques to extract\ninformation from high dimensional data sets with complex dependencies in areas\nsuch as medicine that can provide insight into individual as well as population\nlevel variation. We develop $\\texttt{LucidAtlas}$, an approach that can\nrepresent spatially varying information, and can capture the influence of\ncovariates as well as population uncertainty. As a versatile atlas\nrepresentation, $\\texttt{LucidAtlas}$ offers robust capabilities for covariate\ninterpretation, individualized prediction, population trend analysis, and\nuncertainty estimation, with the flexibility to incorporate prior knowledge.\nAdditionally, we discuss the trustworthiness and potential risks of neural\nadditive models for analyzing dependent covariates and then introduce a\nmarginalization approach to explain the dependence of an individual predictor\non the models' response (the atlas). To validate our method, we demonstrate its\ngeneralizability on two medical datasets. Our findings underscore the critical\nrole of by-construction interpretable models in advancing scientific discovery.\nOur code will be publicly available upon acceptance.\n","authors":["Yining Jiao","Sreekalyani Bhamidi","Huaizhi Qu","Carlton Zdanski","Julia Kimbell","Andrew Prince","Cameron Worden","Samuel Kirse","Christopher Rutter","Benjamin Shields","William Dunn","Jisan Mahmud","Tianlong Chen","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2502.08445v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2404.02690v2","updated":"2025-02-12T14:32:46Z","published":"2024-04-03T12:37:34Z","title":"How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse","summary":"  Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.\n","authors":["Yichuan Deng","Zhao Song","Jing Xiong","Chiwun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.02690v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16737v2","updated":"2025-02-12T14:25:30Z","published":"2024-11-23T13:16:06Z","title":"Federated Learning in Chemical Engineering: A Tutorial on a Framework\n  for Privacy-Preserving Collaboration Across Distributed Data Sources","summary":"  Federated Learning (FL) is a decentralized machine learning approach that has\ngained attention for its potential to enable collaborative model training\nacross clients while protecting data privacy, making it an attractive solution\nfor the chemical industry. This work aims to provide the chemical engineering\ncommunity with an accessible introduction to the discipline. Supported by a\nhands-on tutorial and a comprehensive collection of examples, it explores the\napplication of FL in tasks such as manufacturing optimization, multimodal data\nintegration, and drug discovery while addressing the unique challenges of\nprotecting proprietary information and managing distributed datasets. The\ntutorial was built using key frameworks such as $\\texttt{Flower}$ and\n$\\texttt{TensorFlow Federated}$ and was designed to provide chemical engineers\nwith the right tools to adopt FL in their specific needs. We compare the\nperformance of FL against centralized learning across three different datasets\nrelevant to chemical engineering applications, demonstrating that FL will often\nmaintain or improve classification performance, particularly for complex and\nheterogeneous data. We conclude with an outlook on the open challenges in\nfederated learning to be tackled and current approaches designed to remediate\nand improve this framework.\n","authors":["Siddhant Dutta","Iago Leal de Freitas","Pedro Maciel Xavier","Claudio Miceli de Farias","David Esteban Bernal Neira"],"pdf_url":"https://arxiv.org/pdf/2411.16737v2.pdf","comment":"53 Pages, 8 figures, Under review in ACS Industrial & Engineering\n  Chemistry Research Journal"},{"id":"http://arxiv.org/abs/2502.08436v1","updated":"2025-02-12T14:20:36Z","published":"2025-02-12T14:20:36Z","title":"From Haystack to Needle: Label Space Reduction for Zero-shot\n  Classification","summary":"  We present Label Space Reduction (LSR), a novel method for improving\nzero-shot classification performance of Large Language Models (LLMs). LSR\niteratively refines the classification label space by systematically ranking\nand reducing candidate classes, enabling the model to concentrate on the most\nrelevant options. By leveraging unlabeled data with the statistical learning\ncapabilities of data-driven models, LSR dynamically optimizes the label space\nrepresentation at test time. Our experiments across seven benchmarks\ndemonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to\n14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet\ncompared to standard zero-shot classification baselines. To reduce the\ncomputational overhead of LSR, which requires an additional LLM call at each\niteration, we propose distilling the model into a probabilistic classifier,\nallowing for efficient inference.\n","authors":["Nathan Vandemoortele","Bram Steenwinckel","Femke Ongenae","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2502.08436v1.pdf","comment":"Under review at ICML 2025"},{"id":"http://arxiv.org/abs/2502.08432v1","updated":"2025-02-12T14:16:45Z","published":"2025-02-12T14:16:45Z","title":"Closer through commonality: Enhancing hypergraph contrastive learning\n  with shared groups","summary":"  Hypergraphs provide a superior modeling framework for representing complex\nmultidimensional relationships in the context of real-world interactions that\noften occur in groups, overcoming the limitations of traditional homogeneous\ngraphs. However, there have been few studies on hypergraphbased contrastive\nlearning, and existing graph-based contrastive learning methods have not been\nable to fully exploit the highorder correlation information in hypergraphs.\nHere, we propose a Hypergraph Fine-grained contrastive learning (HyFi) method\ndesigned to exploit the complex high-dimensional information inherent in\nhypergraphs. While avoiding traditional graph augmentation methods that corrupt\nthe hypergraph topology, the proposed method provides a simple and efficient\nlearning augmentation function by adding noise to node features. Furthermore,\nwe expands beyond the traditional dichotomous relationship between positive and\nnegative samples in contrastive learning by introducing a new relationship of\nweak positives. It demonstrates the importance of fine-graining positive\nsamples in contrastive learning. Therefore, HyFi is able to produce highquality\nembeddings, and outperforms both supervised and unsupervised baselines in\naverage rank on node classification across 10 datasets. Our approach\neffectively exploits high-dimensional hypergraph information, shows significant\nimprovement over existing graph-based contrastive learning methods, and is\nefficient in terms of training speed and GPU memory cost. The source code is\navailable at https://github.com/Noverse0/HyFi.git.\n","authors":["Daeyoung Roh","Donghee Han","Daehee Kim","Keejun Han","Mun Yi"],"pdf_url":"https://arxiv.org/pdf/2502.08432v1.pdf","comment":"11page, 5 figures, 6 tables, 2024 IEEE International Conference on\n  Big Data"},{"id":"http://arxiv.org/abs/2502.08426v1","updated":"2025-02-12T14:09:05Z","published":"2025-02-12T14:09:05Z","title":"Semantic Learning for Molecular Communication in Internet of Bio-Nano\n  Things","summary":"  Molecular communication (MC) provides a foundational framework for\ninformation transmission in the Internet of Bio-Nano Things (IoBNT), where\nefficiency and reliability are crucial. However, the inherent limitations of\nmolecular channels, such as low transmission rates, noise, and inter-symbol\ninterference (ISI), limit their ability to support complex data transmission.\nThis paper proposes an end-to-end semantic learning framework designed to\noptimize task-oriented molecular communication, with a focus on biomedical\ndiagnostic tasks under resource-constrained conditions. The proposed framework\nemploys a deep encoder-decoder architecture to efficiently extract, quantize,\nand decode semantic features, prioritizing task-relevant semantic information\nto enhance diagnostic classification performance. Additionally, a probabilistic\nchannel network is introduced to approximate molecular propagation dynamics,\nenabling gradient-based optimization for end-to-end learning. Experimental\nresults demonstrate that the proposed semantic framework improves diagnostic\naccuracy by at least 25% compared to conventional JPEG compression with LDPC\ncoding methods under resource-constrained communication scenarios.\n","authors":["Hanlin Cai","Ozgur B. Akan"],"pdf_url":"https://arxiv.org/pdf/2502.08426v1.pdf","comment":"4 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2502.08416v1","updated":"2025-02-12T13:59:22Z","published":"2025-02-12T13:59:22Z","title":"Multifidelity Simulation-based Inference for Computationally Expensive\n  Simulators","summary":"  Across many domains of science, stochastic models are an essential tool to\nunderstand the mechanisms underlying empirically observed data. Models can be\nof different levels of detail and accuracy, with models of high-fidelity (i.e.,\nhigh accuracy) to the phenomena under study being often preferable. However,\ninferring parameters of high-fidelity models via simulation-based inference is\nchallenging, especially when the simulator is computationally expensive. We\nintroduce MF-NPE, a multifidelity approach to neural posterior estimation that\nleverages inexpensive low-fidelity simulations to infer parameters of\nhigh-fidelity simulators within a limited simulation budget. MF-NPE performs\nneural posterior estimation with limited high-fidelity resources by virtue of\ntransfer learning, with the ability to prioritize individual observations using\nactive learning. On one statistical task with analytical ground-truth and two\nreal-world tasks, MF-NPE shows comparable performance to current approaches\nwhile requiring up to two orders of magnitude fewer high-fidelity simulations.\nOverall, MF-NPE opens new opportunities to perform efficient Bayesian inference\non computationally expensive simulators.\n","authors":["Anastasia N. Krouglova","Hayden R. Johnson","Basile Confavreux","Michael Deistler","Pedro J. Gon√ßalves"],"pdf_url":"https://arxiv.org/pdf/2502.08416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08414v1","updated":"2025-02-12T13:57:09Z","published":"2025-02-12T13:57:09Z","title":"Sparse Estimation of Inverse Covariance and Partial Correlation Matrices\n  via Joint Partial Regression","summary":"  We present a new method for estimating high-dimensional sparse partial\ncorrelation and inverse covariance matrices, which exploits the connection\nbetween the inverse covariance matrix and linear regression. The method is a\ntwo-stage estimation method wherein each individual feature is regressed on all\nother features while positive semi-definiteness is enforced simultaneously. We\nprovide statistical rates of convergence for the proposed method which match,\nand improve upon, the state-of-the-art for inverse covariance and partial\ncorrelation matrix estimation, respectively. We also propose an efficient\nproximal splitting algorithm for numerically computing the estimate. The\neffectiveness of the proposed method is demonstrated on both synthetic and\nreal-world data.\n","authors":["Samuel Erickson","Tobias Ryd√©n"],"pdf_url":"https://arxiv.org/pdf/2502.08414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05431v2","updated":"2025-02-12T13:54:01Z","published":"2025-02-08T03:41:16Z","title":"APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding","summary":"  Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.\n","authors":["Xinyu Yang","Tianqi Chen","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2502.05431v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08397v1","updated":"2025-02-12T13:40:00Z","published":"2025-02-12T13:40:00Z","title":"Strong bounds for large-scale Minimum Sum-of-Squares Clustering","summary":"  Clustering is a fundamental technique in data analysis and machine learning,\nused to group similar data points together. Among various clustering methods,\nthe Minimum Sum-of-Squares Clustering (MSSC) is one of the most widely used.\nMSSC aims to minimize the total squared Euclidean distance between data points\nand their corresponding cluster centroids. Due to the unsupervised nature of\nclustering, achieving global optimality is crucial, yet computationally\nchallenging. The complexity of finding the global solution increases\nexponentially with the number of data points, making exact methods impractical\nfor large-scale datasets. Even obtaining strong lower bounds on the optimal\nMSSC objective value is computationally prohibitive, making it difficult to\nassess the quality of heuristic solutions. We address this challenge by\nintroducing a novel method to validate heuristic MSSC solutions through\noptimality gaps. Our approach employs a divide-and-conquer strategy,\ndecomposing the problem into smaller instances that can be handled by an exact\nsolver. The decomposition is guided by an auxiliary optimization problem, the\n\"anticlustering problem\", for which we design an efficient heuristic.\nComputational experiments demonstrate the effectiveness of the method for\nlarge-scale instances, achieving optimality gaps below 3% in most cases while\nmaintaining reasonable computational times. These results highlight the\npracticality of our approach in assessing feasible clustering solutions for\nlarge datasets, bridging a critical gap in MSSC evaluation.\n","authors":["Anna Livia Croella","Veronica Piccialli","Antonio M. Sudoso"],"pdf_url":"https://arxiv.org/pdf/2502.08397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08707v2","updated":"2025-02-12T13:29:11Z","published":"2024-08-16T12:40:01Z","title":"Beam Prediction based on Large Language Models","summary":"  In this letter, we use large language models (LLMs) to develop a\nhigh-performing and robust beam prediction method. We formulate the millimeter\nwave (mmWave) beam prediction problem as a time series forecasting task, where\nthe historical observations are aggregated through cross-variable attention and\nthen transformed into text-based representations using a trainable tokenizer.\nBy leveraging the prompt-as-prefix (PaP) technique for contextual enrichment,\nour method harnesses the power of LLMs to predict future optimal beams.\nSimulation results demonstrate that our LLM-based approach outperforms\ntraditional learning-based models in prediction accuracy as well as robustness,\nhighlighting the significant potential of LLMs in enhancing wireless\ncommunication systems.\n","authors":["Yucheng Sheng","Kai Huang","Le Liang","Peng Liu","Shi Jin","Geoffrey Ye Li"],"pdf_url":"https://arxiv.org/pdf/2408.08707v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06581v2","updated":"2025-02-12T13:25:22Z","published":"2025-02-10T15:48:11Z","title":"A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems","summary":"  The explosive growth of video data has driven the development of distributed\nvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enabling\nefficient video processing, real-time inference, and privacy-preserving\nanalysis. Among multiple advantages, CETC systems can distribute video\nprocessing tasks and enable adaptive analytics across cloud, edge, and terminal\ndevices, leading to breakthroughs in video surveillance, autonomous driving,\nand smart cities. In this survey, we first analyze fundamental architectural\ncomponents, including hierarchical, distributed, and hybrid frameworks,\nalongside edge computing platforms and resource management mechanisms. Building\nupon these foundations, edge-centric approaches emphasize on-device processing,\nedge-assisted offloading, and edge intelligence, while cloud-centric methods\nleverage powerful computational capabilities for complex video understanding\nand model training. Our investigation also covers hybrid video analytics\nincorporating adaptive task offloading and resource-aware scheduling techniques\nthat optimize performance across the entire system. Beyond conventional\napproaches, recent advances in large language models and multimodal integration\nreveal both opportunities and challenges in platform scalability, data\nprotection, and system reliability. Future directions also encompass\nexplainable systems, efficient processing mechanisms, and advanced video\nanalytics, offering valuable insights for researchers and practitioners in this\ndynamic field.\n","authors":["Linxiao Gong","Hao Yang","Gaoyun Fang","Bobo Ju","Juncen Guo","Xiaoguang Zhu","Yan Wang","Xiping Hu","Peng Sun","Azzedine Boukerche"],"pdf_url":"https://arxiv.org/pdf/2502.06581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11959v2","updated":"2025-02-12T13:25:10Z","published":"2024-12-16T16:41:51Z","title":"Gramian Multimodal Representation Learning and Alignment","summary":"  Human perception integrates multiple modalities, such as vision, hearing, and\nlanguage, into a unified understanding of the surrounding reality. While recent\nmultimodal models have achieved significant progress by aligning pairs of\nmodalities via contrastive learning, their solutions are unsuitable when\nscaling to multiple modalities. These models typically align each modality to a\ndesignated anchor without ensuring the alignment of all modalities with each\nother, leading to suboptimal performance in tasks requiring a joint\nunderstanding of multiple modalities. In this paper, we structurally rethink\nthe pairwise conventional approach to multimodal learning and we present the\nnovel Gramian Representation Alignment Measure (GRAM), which overcomes the\nabove-mentioned limitations. GRAM learns and then aligns $n$ modalities\ndirectly in the higher-dimensional space in which modality embeddings lie by\nminimizing the Gramian volume of the $k$-dimensional parallelotope spanned by\nthe modality vectors, ensuring the geometric alignment of all modalities\nsimultaneously. GRAM can replace cosine similarity in any downstream method,\nholding for 2 to $n$ modalities and providing more meaningful alignment with\nrespect to previous similarity measures. The novel GRAM-based contrastive loss\nfunction enhances the alignment of multimodal models in the higher-dimensional\nembedding space, leading to new state-of-the-art performance in downstream\ntasks such as video-audio-text retrieval and audio-video classification. The\nproject page, the code, and the pretrained models are available at\nhttps://ispamm.github.io/GRAM/.\n","authors":["Giordano Cicchetti","Eleonora Grassucci","Luigi Sigillo","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.11959v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.15981v2","updated":"2025-02-12T13:22:54Z","published":"2024-10-21T13:06:38Z","title":"Robust Visual Representation Learning with Multi-modal Prior Knowledge\n  for Image Classification Under Distribution Shift","summary":"  Despite the remarkable success of deep neural networks (DNNs) in computer\nvision, they fail to remain high-performing when facing distribution shifts\nbetween training and testing data. In this paper, we propose Knowledge-Guided\nVisual representation learning (KGV) - a distribution-based learning approach\nleveraging multi-modal prior knowledge - to improve generalization under\ndistribution shift. It integrates knowledge from two distinct modalities: 1) a\nknowledge graph (KG) with hierarchical and association relationships; and 2)\ngenerated synthetic images of visual elements semantically represented in the\nKG. The respective embeddings are generated from the given modalities in a\ncommon latent space, i.e., visual embeddings from original and synthetic images\nas well as knowledge graph embeddings (KGEs). These embeddings are aligned via\na novel variant of translation-based KGE methods, where the node and relation\nembeddings of the KG are modeled as Gaussian distributions and translations,\nrespectively. We claim that incorporating multi-model prior knowledge enables\nmore regularized learning of image representations. Thus, the models are able\nto better generalize across different data distributions. We evaluate KGV on\ndifferent image classification tasks with major or minor distribution shifts,\nnamely road sign classification across datasets from Germany, China, and\nRussia, image classification with the mini-ImageNet dataset and its variants,\nas well as the DVM-CAR dataset. The results demonstrate that KGV consistently\nexhibits higher accuracy and data efficiency across all experiments.\n","authors":["Hongkuan Zhou","Lavdim Halilaj","Sebastian Monka","Stefan Schmid","Yuqicheng Zhu","Bo Xiong","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2410.15981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06862v2","updated":"2025-02-12T13:21:32Z","published":"2025-02-08T00:43:57Z","title":"Poincar√© Inequality for Local Log-Polyak-Lojasiewicz Measures :\n  Non-asymptotic Analysis in Low-temperature Regime","summary":"  Potential functions in highly pertinent applications, such as deep learning\nin over-parameterized regime, are empirically observed to admit non-isolated\nminima. To understand the convergence behavior of stochastic dynamics in such\nlandscapes, we propose to study the class of \\logPLmeasure\\ measures\n$\\mu_\\epsilon \\propto \\exp(-V/\\epsilon)$, where the potential $V$ satisfies a\nlocal Polyak-{\\L}ojasiewicz (P\\L) inequality, and its set of local minima is\nprovably \\emph{connected}. Notably, potentials in this class can exhibit local\nmaxima and we characterize its optimal set S to be a compact $\\mathcal{C}^2$\n\\emph{embedding submanifold} of $\\mathbb{R}^d$ without boundary. The\n\\emph{non-contractibility} of S distinguishes our function class from the\nclassical convex setting topologically. Moreover, the embedding structure\ninduces a naturally defined Laplacian-Beltrami operator on S, and we show that\nits first non-trivial eigenvalue provides an \\emph{$\\epsilon$-independent}\nlower bound for the \\Poincare\\ constant in the \\Poincare\\ inequality of\n$\\mu_\\epsilon$. As a direct consequence, Langevin dynamics with such non-convex\npotential $V$ and diffusion coefficient $\\epsilon$ converges to its equilibrium\n$\\mu_\\epsilon$ at a rate of $\\tilde{\\mathcal{O}}(1/\\epsilon)$, provided\n$\\epsilon$ is sufficiently small. Here $\\tilde{\\mathcal{O}}$ hides logarithmic\nterms.\n","authors":["Yun Gong","Zebang Shen","Niao He"],"pdf_url":"https://arxiv.org/pdf/2502.06862v2.pdf","comment":"This is a replacement version of arXiv:2501.00429"},{"id":"http://arxiv.org/abs/2402.04059v2","updated":"2025-02-12T13:16:29Z","published":"2024-02-06T15:03:53Z","title":"Deep Learning for Multivariate Time Series Imputation: A Survey","summary":"  Missing values are ubiquitous in multivariate time series (MTS) data, posing\nsignificant challenges for accurate analysis and downstream applications. In\nrecent years, deep learning-based methods have successfully handled missing\ndata by leveraging complex temporal dependencies and learned data\ndistributions. In this survey, we provide a comprehensive summary of deep\nlearning approaches for multivariate time series imputation (MTSI) tasks. We\npropose a novel taxonomy that categorizes existing methods based on two key\nperspectives: imputation uncertainty and neural network architecture.\nFurthermore, we summarize existing MTSI toolkits with a particular emphasis on\nthe PyPOTS Ecosystem, which provides an integrated and standardized foundation\nfor MTSI research. Finally, we discuss key challenges and future research\ndirections, which give insight for further MTSI research. This survey aims to\nserve as a valuable resource for researchers and practitioners in the field of\ntime series analysis and missing data imputation tasks.\n","authors":["Jun Wang","Wenjie Du","Yiyuan Yang","Linglong Qian","Wei Cao","Keli Zhang","Wenjia Wang","Yuxuan Liang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2402.04059v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.13306v2","updated":"2025-02-12T13:14:53Z","published":"2024-09-20T08:04:12Z","title":"Predicting DNA fragmentation: A non-destructive analogue to chemical\n  assays using machine learning","summary":"  Globally, infertility rates are increasing, with 2.5\\% of all births being\nassisted by in vitro fertilisation (IVF) in 2022. Male infertility is the cause\nfor approximately half of these cases. The quality of sperm DNA has substantial\nimpact on the success of IVF. The assessment of sperm DNA is traditionally done\nthrough chemical assays which render sperm cells ineligible for IVF. Many\ncompounding factors lead to the population crisis, with fertility rates\ndropping globally in recent history. As such assisted reproductive technologies\n(ART) have been the focus of recent research efforts. Simultaneously,\nartificial intelligence has grown ubiquitous and is permeating more aspects of\nmodern life. With the advent of state-of-the-art machine learning and its\nexceptional performance in many sectors, this work builds on these successes\nand proposes a novel framework for the prediction of sperm cell DNA\nfragmentation from images of unstained sperm. Rendering a predictive model\nwhich preserves sperm integrity and allows for optimal selection of sperm for\nIVF.\n","authors":["Byron A Jacobs","Ifthakaar Shaik","Frando Lin"],"pdf_url":"https://arxiv.org/pdf/2409.13306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08378v1","updated":"2025-02-12T13:10:09Z","published":"2025-02-12T13:10:09Z","title":"Learning Humanoid Standing-up Control across Diverse Postures","summary":"  Standing-up control is crucial for humanoid robots, with the potential for\nintegration into current locomotion and loco-manipulation systems, such as fall\nrecovery. Existing approaches are either limited to simulations that overlook\nhardware constraints or rely on predefined ground-specific motion trajectories,\nfailing to enable standing up across postures in real-world scenes. To bridge\nthis gap, we present HoST (Humanoid Standing-up Control), a reinforcement\nlearning framework that learns standing-up control from scratch, enabling\nrobust sim-to-real transfer across diverse postures. HoST effectively learns\nposture-adaptive motions by leveraging a multi-critic architecture and\ncurriculum-based training on diverse simulated terrains. To ensure successful\nreal-world deployment, we constrain the motion with smoothness regularization\nand implicit motion speed bound to alleviate oscillatory and violent motions on\nphysical hardware, respectively. After simulation-based training, the learned\ncontrol policies are directly deployed on the Unitree G1 humanoid robot. Our\nexperimental results demonstrate that the controllers achieve smooth, stable,\nand robust standing-up motions across a wide range of laboratory and outdoor\nenvironments. Videos are available at\nhttps://taohuang13.github.io/humanoid-standingup.github.io/.\n","authors":["Tao Huang","Junli Ren","Huayi Wang","Zirui Wang","Qingwei Ben","Muning Wen","Xiao Chen","Jianan Li","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2502.08378v1.pdf","comment":"Humanoid Standing-up Control, 12 pages"},{"id":"http://arxiv.org/abs/2502.08376v1","updated":"2025-02-12T13:07:18Z","published":"2025-02-12T13:07:18Z","title":"Enhanced Load Forecasting with GAT-LSTM: Leveraging Grid and Temporal\n  Features","summary":"  Accurate power load forecasting is essential for the efficient operation and\nplanning of electrical grids, particularly given the increased variability and\ncomplexity introduced by renewable energy sources. This paper introduces\nGAT-LSTM, a hybrid model that combines Graph Attention Networks (GAT) and Long\nShort-Term Memory (LSTM) networks. A key innovation of the model is the\nincorporation of edge attributes, such as line capacities and efficiencies,\ninto the attention mechanism, enabling it to dynamically capture spatial\nrelationships grounded in grid-specific physical and operational constraints.\nAdditionally, by employing an early fusion of spatial graph embeddings and\ntemporal sequence features, the model effectively learns and predicts complex\ninteractions between spatial dependencies and temporal patterns, providing a\nrealistic representation of the dynamics of power grids. Experimental\nevaluations on the Brazilian Electricity System dataset demonstrate that the\nGAT-LSTM model significantly outperforms state-of-the-art models, achieving\nreductions of 21. 8% in MAE, 15. 9% in RMSE and 20. 2% in MAPE. These results\nunderscore the robustness and adaptability of the GAT-LSTM model, establishing\nit as a powerful tool for applications in grid management and energy planning.\n","authors":["Ugochukwu Orji","√ái√ßek G√ºven","Dan Stowell"],"pdf_url":"https://arxiv.org/pdf/2502.08376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16332v2","updated":"2025-02-12T13:07:07Z","published":"2023-05-22T01:14:46Z","title":"Continual Learning through Human-Robot Interaction: Human Perceptions of\n  a Continual Learning Robot in Repeated Interactions","summary":"  For long-term deployment in dynamic real-world environments, assistive robots\nmust continue to learn and adapt to their environments. Researchers have\ndeveloped various computational models for continual learning (CL) that can\nallow robots to continually learn from limited training data, and avoid\nforgetting previous knowledge. While these CL models can mitigate forgetting on\nstatic, systematically collected datasets, it is unclear how human users might\nperceive a robot that continually learns over multiple interactions with them.\nIn this paper, we developed a system that integrates CL models for object\nrecognition with a Fetch mobile manipulator robot and allows human participants\nto directly teach and test the robot over multiple sessions. We conducted an\nin-person study with 60 participants that interacted with our system in 300\nsessions (5 sessions per participant). We conducted a between-subject study\nwith three different CL models to understand human perceptions of continual\nlearning robots over multiple sessions. Our results suggest that participants'\nperceptions of trust, competence, and usability of a continual learning robot\nsignificantly decrease over multiple sessions if the robot forgets previously\nlearned objects. However, the perceived task load on participants for teaching\nand testing the robot remains the same over multiple sessions even if the robot\nforgets previously learned objects. Our results also indicate that\nstate-of-the-art CL models might perform unreliably when applied on robots\ninteracting with human participants. Further, continual learning robots are not\nperceived as very trustworthy or competent by human participants, regardless of\nthe underlying continual learning model or the session number.\n","authors":["Ali Ayub","Zachary De Francesco","Patrick Holthaus","Chrystopher L. Nehaniv","Kerstin Dautenhahn"],"pdf_url":"https://arxiv.org/pdf/2305.16332v2.pdf","comment":"Accepted at the International Journal of Social Robotics (SoRo), 2025"},{"id":"http://arxiv.org/abs/2502.08365v1","updated":"2025-02-12T12:51:36Z","published":"2025-02-12T12:51:36Z","title":"Towards Principled Multi-Agent Task Agnostic Exploration","summary":"  In reinforcement learning, we typically refer to task-agnostic exploration\nwhen we aim to explore the environment without access to the task specification\na priori. In a single-agent setting the problem has been extensively studied\nand mostly understood. A popular approach cast the task-agnostic objective as\nmaximizing the entropy of the state distribution induced by the agent's policy,\nfrom which principles and methods follows. In contrast, little is known about\ntask-agnostic exploration in multi-agent settings, which are ubiquitous in the\nreal world. How should different agents explore in the presence of others? In\nthis paper, we address this question through a generalization to multiple\nagents of the problem of maximizing the state distribution entropy. First, we\ninvestigate alternative formulations, highlighting respective positives and\nnegatives. Then, we present a scalable, decentralized, trust-region policy\nsearch algorithm to address the problem in practical settings. Finally, we\nprovide proof of concept experiments to both corroborate the theoretical\nfindings and pave the way for task-agnostic exploration in challenging\nmulti-agent settings.\n","authors":["Riccardo Zamboni","Mirco Mutti","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2502.08365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08364v1","updated":"2025-02-12T12:50:24Z","published":"2025-02-12T12:50:24Z","title":"A Survey on Pre-Trained Diffusion Model Distillations","summary":"  Diffusion Models~(DMs) have emerged as the dominant approach in Generative\nArtificial Intelligence (GenAI), owing to their remarkable performance in tasks\nsuch as text-to-image synthesis. However, practical DMs, such as stable\ndiffusion, are typically trained on massive datasets and thus usually require\nlarge storage. At the same time, many steps may be required, i.e., recursively\nevaluating the trained neural network, to generate a high-quality image, which\nresults in significant computational costs during sample generation. As a\nresult, distillation methods on pre-trained DM have become widely adopted\npractices to develop smaller, more efficient models capable of rapid, few-step\ngeneration in low-resource environment. When these distillation methods are\ndeveloped from different perspectives, there is an urgent need for a systematic\nsurvey, particularly from a methodological perspective. In this survey, we\nreview distillation methods through three aspects: output loss distillation,\ntrajectory distillation and adversarial distillation. We also discuss current\nchallenges and outline future research directions in the conclusion.\n","authors":["Xuhui Fan","Zhangkai Wu","Hongyu Wu"],"pdf_url":"https://arxiv.org/pdf/2502.08364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18652v7","updated":"2025-02-12T12:49:36Z","published":"2024-10-24T11:32:00Z","title":"$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation","summary":"  Generating high-quality charts with Large Language Models (LLMs) presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. $\\langle \\text{instruction}, \\text{data}, \\text{code} \\rangle$\ntriplets are scarce and expensive to manually curate as their creation demands\ntechnical expertise. To address this scalability challenge, we introduce a\nreference-free automatic feedback generator, which eliminates the need for\ncostly human intervention. Our novel framework, C$^2$, consists of (1) an\nautomatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset\n(ChartUIE-8K). The results are compelling: in our first experiment, 74% of\nrespondents strongly preferred, and 10% preferred, the results after feedback.\nThe second post-feedback experiment demonstrates that ChartAF outperform nine\nbaselines. Moreover, ChartUIE-8K significantly improves data diversity by\nincreasing queries, datasets, and chart types by 5982%, 1936%, and 91%,\nrespectively, over benchmarks. Finally, a study of LLM users revealed that 94%\nof participants preferred ChartUIE-8K's queries, with 93% deeming them aligned\nwith real-world use cases. Core contributions are available as open-source at\nchartsquared.github.io, with ample qualitative examples.\n","authors":["Woosung Koh","Jang Han Yoon","MinHyung Lee","Youngjin Song","Jaegwan Cho","Jaehyun Kang","Taehyeon Kim","Se-Young Yun","Youngjae Yu","Bongshin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.18652v7.pdf","comment":"NAACL 2025 Main (Long)"},{"id":"http://arxiv.org/abs/2502.08689v1","updated":"2025-02-12T12:41:13Z","published":"2025-02-12T12:41:13Z","title":"Advancing machine fault diagnosis: A detailed examination of\n  convolutional neural networks","summary":"  The growing complexity of machinery and the increasing demand for operational\nefficiency and safety have driven the development of advanced fault diagnosis\ntechniques. Among these, convolutional neural networks (CNNs) have emerged as a\npowerful tool, offering robust and accurate fault detection and classification\ncapabilities. This comprehensive review delves into the application of CNNs in\nmachine fault diagnosis, covering its theoretical foundation, architectural\nvariations, and practical implementations. The strengths and limitations of\nCNNs are analyzed in this domain, discussing their effectiveness in handling\nvarious fault types, data complexities, and operational environments.\nFurthermore, we explore the evolving landscape of CNN-based fault diagnosis,\nexamining recent advancements in data augmentation, transfer learning, and\nhybrid architectures. Finally, we highlight future research directions and\npotential challenges to further enhance the application of CNNs for reliable\nand proactive machine fault diagnosis.\n","authors":["Govind Vashishtha","Sumika Chauhan","Mert Sehri","Justyna Hebda-Sobkowicz","Radoslaw Zimroz","Patrick Dumond","Rajesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.08689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07071v2","updated":"2025-02-12T12:38:13Z","published":"2025-01-31T19:43:13Z","title":"TRADES: Generating Realistic Market Simulations with Diffusion Models","summary":"  Financial markets are complex systems characterized by high statistical\nnoise, nonlinearity, and constant evolution. Thus, modeling them is extremely\nhard. We address the task of generating realistic and responsive Limit Order\nBook (LOB) market simulations, which are fundamental for calibrating and\ntesting trading strategies, performing market impact experiments, and\ngenerating synthetic market data. Previous works lack realism, usefulness, and\nresponsiveness of the generated simulations. To bridge this gap, we propose a\nnovel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB\nSimulations (TRADES). TRADES generates realistic order flows conditioned on the\nstate of the market, leveraging a transformer-based architecture that captures\nthe temporal and spatial characteristics of high-frequency market data. There\nis a notable absence of quantitative metrics for evaluating generative market\nsimulation models in the literature. To tackle this problem, we adapt the\npredictive score, a metric measured as an MAE, by training a stock price\npredictive model on synthetic data and testing it on real data. We compare\nTRADES with previous works on two stocks, reporting an x3.27 and x3.47\nimprovement over SoTA according to the predictive score, demonstrating that we\ngenerate useful synthetic market data for financial downstream tasks. We assess\nTRADES's market simulation realism and responsiveness, showing that it\neffectively learns the conditional data distribution and successfully reacts to\nan experimental agent, giving sprout to possible calibrations and evaluations\nof trading strategies and market impact experiments. We developed DeepMarket,\nthe first open-source Python framework for market simulation with deep\nlearning. Our repository includes a synthetic LOB dataset composed of TRADES's\ngenerates simulations. We release the code at\ngithub.com/LeonardoBerti00/DeepMarket.\n","authors":["Leonardo Berti","Bardh Prenkaj","Paola Velardi"],"pdf_url":"https://arxiv.org/pdf/2502.07071v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.08355v1","updated":"2025-02-12T12:30:49Z","published":"2025-02-12T12:30:49Z","title":"Loss Landscape Analysis for Reliable Quantized ML Models for Scientific\n  Sensing","summary":"  In this paper, we propose a method to perform empirical analysis of the loss\nlandscape of machine learning (ML) models. The method is applied to two ML\nmodels for scientific sensing, which necessitates quantization to be deployed\nand are subject to noise and perturbations due to experimental conditions. Our\nmethod allows assessing the robustness of ML models to such effects as a\nfunction of quantization precision and under different regularization\ntechniques -- two crucial concerns that remained underexplored so far. By\ninvestigating the interplay between performance, efficiency, and robustness by\nmeans of loss landscape analysis, we both established a strong correlation\nbetween gently-shaped landscapes and robustness to input and weight\nperturbations and observed other intriguing and non-obvious phenomena. Our\nmethod allows a systematic exploration of such trade-offs a priori, i.e.,\nwithout training and testing multiple models, leading to more efficient\ndevelopment workflows. This work also highlights the importance of\nincorporating robustness into the Pareto optimization of ML models, enabling\nmore reliable and adaptive scientific sensing systems.\n","authors":["Tommaso Baldi","Javier Campos","Olivia Weng","Caleb Geniesse","Nhan Tran","Ryan Kastner","Alessandro Biondi"],"pdf_url":"https://arxiv.org/pdf/2502.08355v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2502.08353v1","updated":"2025-02-12T12:28:39Z","published":"2025-02-12T12:28:39Z","title":"Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy","summary":"  With the extensive application of Graph Neural Networks (GNNs) across various\ndomains, their trustworthiness has emerged as a focal point of research. Some\nexisting studies have shown that the integration of large language models\n(LLMs) can improve the semantic understanding and generation capabilities of\nGNNs, which in turn improves the trustworthiness of GNNs from various aspects.\nOur review introduces a taxonomy that offers researchers a clear framework for\ncomprehending the principles and applications of different methods and helps\nclarify the connections and differences among various approaches. Then we\nsystematically survey representative approaches along the four categories of\nour taxonomy. Through our taxonomy, researchers can understand the applicable\nscenarios, potential advantages, and limitations of each approach for the the\ntrusted integration of GNNs with LLMs. Finally, we present some promising\ndirections of work and future trends for the integration of LLMs and GNNs to\nimprove model trustworthiness.\n","authors":["Ruizhan Xue","Huimin Deng","Fang He","Maojun Wang","Zeyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08353v1.pdf","comment":"Submitted to IJCAI 2025"},{"id":"http://arxiv.org/abs/2501.16937v3","updated":"2025-02-12T12:25:56Z","published":"2025-01-28T13:31:18Z","title":"TAID: Temporally Adaptive Interpolated Distillation for Efficient\n  Knowledge Transfer in Language Models","summary":"  Causal language models have demonstrated remarkable capabilities, but their\nsize poses significant challenges for deployment in resource-constrained\nenvironments. Knowledge distillation, a widely-used technique for transferring\nknowledge from a large teacher model to a small student model, presents a\npromising approach for model compression. A significant remaining issue lies in\nthe major differences between teacher and student models, namely the\nsubstantial capacity gap, mode averaging, and mode collapse, which pose\nbarriers during distillation. To address these issues, we introduce\n$\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel\nknowledge distillation approach that dynamically interpolates student and\nteacher distributions through an adaptive intermediate distribution, gradually\nshifting from the student's initial distribution towards the teacher's\ndistribution. We provide a theoretical analysis demonstrating TAID's ability to\nprevent mode collapse and empirically show its effectiveness in addressing the\ncapacity gap while balancing mode averaging and mode collapse. Our\ncomprehensive experiments demonstrate TAID's superior performance across\nvarious model sizes and architectures in both instruction tuning and\npre-training scenarios. Furthermore, we showcase TAID's practical impact by\ndeveloping two state-of-the-art compact foundation models:\n$\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for\nvision-language tasks. These results demonstrate TAID's effectiveness in\ncreating high-performing and efficient models, advancing the development of\nmore accessible AI technologies.\n","authors":["Makoto Shing","Kou Misaki","Han Bao","Sho Yokoi","Takuya Akiba"],"pdf_url":"https://arxiv.org/pdf/2501.16937v3.pdf","comment":"To appear at the 13th International Conference on Learning\n  Representations (ICLR 2025) as a Spotlight presentation"},{"id":"http://arxiv.org/abs/2502.08346v1","updated":"2025-02-12T12:13:51Z","published":"2025-02-12T12:13:51Z","title":"Graph Foundation Models for Recommendation: A Comprehensive Survey","summary":"  Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.\n","authors":["Bin Wu","Yihang Wang","Yuanhao Zeng","Jiawei Liu","Jiashu Zhao","Cheng Yang","Yawen Li","Long Xia","Dawei Yin","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.08346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08340v1","updated":"2025-02-12T12:07:09Z","published":"2025-02-12T12:07:09Z","title":"Hierarchical Learning-based Graph Partition for Large-scale Vehicle\n  Routing Problems","summary":"  Neural solvers based on the divide-and-conquer approach for Vehicle Routing\nProblems (VRPs) in general, and capacitated VRP (CVRP) in particular,\nintegrates the global partition of an instance with local constructions for\neach subproblem to enhance generalization. However, during the global partition\nphase, misclusterings within subgraphs have a tendency to progressively\ncompound throughout the multi-step decoding process of the learning-based\npartition policy. This suboptimal behavior in the global partition phase, in\nturn, may lead to a dramatic deterioration in the performance of the overall\ndecomposition-based system, despite using optimal local constructions. To\naddress these challenges, we propose a versatile Hierarchical Learning-based\nGraph Partition (HLGP) framework, which is tailored to benefit the partition of\nCVRP instances by synergistically integrating global and local partition\npolicies. Specifically, the global partition policy is tasked with creating the\ncoarse multi-way partition to generate the sequence of simpler two-way\npartition subtasks. These subtasks mark the initiation of the subsequent K\nlocal partition levels. At each local partition level, subtasks exclusive for\nthis level are assigned to the local partition policy which benefits from the\ninsensitive local topological features to incrementally alleviate the\ncompounded errors. This framework is versatile in the sense that it optimizes\nthe involved partition policies towards a unified objective harmoniously\ncompatible with both reinforcement learning (RL) and supervised learning (SL).\n(*Due to the notification of arXiv \"The Abstract field cannot be longer than\n1,920 characters\", the appeared Abstract is shortened. For the full Abstract,\nplease download the Article.)\n","authors":["Yuxin Pan","Ruohong Liu","Yize Chen","Zhiguang Cao","Fangzhen Lin"],"pdf_url":"https://arxiv.org/pdf/2502.08340v1.pdf","comment":"Accepted as a Full Paper at AAMAS 2025 (24th International Conference\n  on Autonomous Agents and Multiagent Systems)"},{"id":"http://arxiv.org/abs/2502.08686v1","updated":"2025-02-12T12:06:36Z","published":"2025-02-12T12:06:36Z","title":"EEG Artifact Detection and Correction with Deep Autoencoders","summary":"  EEG signals convey important information about brain activity both in healthy\nand pathological conditions. However, they are inherently noisy, which poses\nsignificant challenges for accurate analysis and interpretation. Traditional\nEEG artifact removal methods, while effective, often require extensive expert\nintervention. This study presents LSTEEG, a novel LSTM-based autoencoder\ndesigned for the detection and correction of artifacts in EEG signals.\nLeveraging deep learning, particularly LSTM layers, LSTEEG captures non-linear\ndependencies in sequential EEG data. LSTEEG demonstrates superior performance\nin both artifact detection and correction tasks compared to other\nstate-of-the-art convolutional autoencoders. Our methodology enhances the\ninterpretability and utility of the autoencoder's latent space, enabling\ndata-driven automated artefact removal in EEG its application in downstream\ntasks. This research advances the field of efficient and accurate multi-channel\nEEG preprocessing, and promotes the implementation and usage of automated EEG\nanalysis pipelines for brain health applications.\n","authors":["David Aquilu√©-Llorens","Aureli Soria-Frisch"],"pdf_url":"https://arxiv.org/pdf/2502.08686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08685v1","updated":"2025-02-12T12:01:08Z","published":"2025-02-12T12:01:08Z","title":"Beyond Models! Explainable Data Valuation and Metric Adaption for\n  Recommendation","summary":"  User behavior records serve as the foundation for recommender systems. While\nthe behavior data exhibits ease of acquisition, it often suffers from varying\nquality. Current methods employ data valuation to discern high-quality data\nfrom low-quality data. However, they tend to employ black-box design, lacking\ntransparency and interpretability. Besides, they are typically tailored to\nspecific evaluation metrics, leading to limited generality across various\ntasks. To overcome these issues, we propose an explainable and versatile\nframework DVR which can enhance the efficiency of data utilization tailored to\nany requirements of the model architectures and evaluation metrics. For\nexplainable data valuation, a data valuator is presented to evaluate the data\nquality via calculating its Shapley value from the game-theoretic perspective,\nensuring robust mathematical properties and reliability. In order to\naccommodate various evaluation metrics, including differentiable and\nnon-differentiable ones, a metric adapter is devised based on reinforcement\nlearning, where a metric is treated as the reinforcement reward that guides\nmodel optimization. Extensive experiments conducted on various benchmarks\nverify that our framework can improve the performance of current recommendation\nalgorithms on various metrics including ranking accuracy, diversity, and\nfairness. Specifically, our framework achieves up to 34.7\\% improvements over\nexisting methods in terms of representative NDCG metric. The code is available\nat https://github.com/renqii/DVR.\n","authors":["Renqi Jia","Xiaokun Zhang","Bowei He","Qiannan Zhu","Weitao Xu","Jiehao Chen","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2502.08685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08337v1","updated":"2025-02-12T12:00:58Z","published":"2025-02-12T12:00:58Z","title":"Hierarchical Multi-Agent Framework for Carbon-Efficient Liquid-Cooled\n  Data Center Clusters","summary":"  Reducing the environmental impact of cloud computing requires efficient\nworkload distribution across geographically dispersed Data Center Clusters\n(DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time\nshift of workloads within individual data centers (DC). This paper introduces\nGreen-DCC, which proposes a Reinforcement Learning (RL) based hierarchical\ncontroller to optimize both workload and liquid cooling dynamically in a DCC.\nBy incorporating factors such as weather, carbon intensity, and resource\navailability, Green-DCC addresses realistic constraints and interdependencies.\nWe demonstrate how the system optimizes multiple data centers synchronously,\nenabling the scope of digital twins, and compare the performance of various RL\napproaches based on carbon emissions and sustainability metrics while also\noffering a framework and benchmark simulation for broader ML research in\nsustainability.\n","authors":["Soumyendu Sarkar","Avisek Naug","Antonio Guillen","Vineet Gundecha","Ricardo Luna Gutierrez","Sahand Ghorbanpour","Sajad Mousavi","Ashwin Ramesh Babu","Desik Rengarajan","Cullen Bash"],"pdf_url":"https://arxiv.org/pdf/2502.08337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04237v3","updated":"2025-02-12T11:50:45Z","published":"2023-11-06T15:45:33Z","title":"Online Learning Quantum States with the Logarithmic Loss via VB-FTRL","summary":"  Online learning of quantum states with the logarithmic loss (LL-OLQS) is a\nquantum generalization of online portfolio selection (OPS), a classic open\nproblem in online learning for over three decades. This problem also emerges in\ndesigning stochastic optimization algorithms for maximum-likelihood quantum\nstate tomography. Recently, Jezequel et al. (arXiv:2209.13932) proposed the\nVB-FTRL algorithm, the first regret-optimal algorithm for OPS with moderate\ncomputational complexity. In this paper, we generalize VB-FTRL for LL-OLQS. Let\n$d$ denote the dimension and $T$ the number of rounds. The generalized\nalgorithm achieves a regret rate of $O ( d^2 \\log ( d + T ) )$ for LL-OLQS.\nEach iteration of the algorithm consists of solving a semidefinite program that\ncan be implemented in polynomial time by, for example, cutting-plane methods.\nFor comparison, the best-known regret rate for LL-OLQS is currently $O ( d^2\n\\log T )$, achieved by an exponential weight method. However, no explicit\nimplementation is available for the exponential weight method for LL-OLQS. To\nfacilitate the generalization, we introduce the notion of VB-convexity.\nVB-convexity is a sufficient condition for the volumetric barrier associated\nwith any function to be convex and is of independent interest.\n","authors":["Wei-Fu Tseng","Kai-Chun Chen","Zi-Hong Xiao","Yen-Huan Li"],"pdf_url":"https://arxiv.org/pdf/2311.04237v3.pdf","comment":"ALT 2025"},{"id":"http://arxiv.org/abs/2502.08326v1","updated":"2025-02-12T11:48:15Z","published":"2025-02-12T11:48:15Z","title":"Model-Free Counterfactual Subset Selection at Scale","summary":"  Ensuring transparency in AI decision-making requires interpretable\nexplanations, particularly at the instance level. Counterfactual explanations\nare a powerful tool for this purpose, but existing techniques frequently depend\non synthetic examples, introducing biases from unrealistic assumptions, flawed\nmodels, or skewed data. Many methods also assume full dataset availability, an\nimpractical constraint in real-time environments where data flows continuously.\nIn contrast, streaming explanations offer adaptive, real-time insights without\nrequiring persistent storage of the entire dataset. This work introduces a\nscalable, model-free approach to selecting diverse and relevant counterfactual\nexamples directly from observed data. Our algorithm operates efficiently in\nstreaming settings, maintaining $O(\\log k)$ update complexity per item while\nensuring high-quality counterfactual selection. Empirical evaluations on both\nreal-world and synthetic datasets demonstrate superior performance over\nbaseline methods, with robust behavior even under adversarial conditions.\n","authors":["Minh Hieu Nguyen","Viet Hung Doan","Anh Tuan Nguyen","Jun Jo","Quoc Viet Hung Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.08326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00003v4","updated":"2025-02-12T11:47:03Z","published":"2024-10-15T06:53:30Z","title":"Unsupervised Training of Diffusion Models for Feasible Solution\n  Generation in Neural Combinatorial Optimization","summary":"  Recent advancements in neural combinatorial optimization (NCO) methods have\nshown promising results in generating near-optimal solutions without the need\nfor expert-crafted heuristics. However, high performance of these approaches\noften rely on problem-specific human-expertise-based search after generating\ncandidate solutions, limiting their applicability to commonly solved CO\nproblems such as Traveling Salesman Problem (TSP). In this paper, we present\nIC/DC, an unsupervised CO framework that directly trains a diffusion model from\nscratch. We train our model in a self-supervised way to minimize the cost of\nthe solution while adhering to the problem-specific constraints. IC/DC is\nspecialized in addressing CO problems involving two distinct sets of items, and\nit does not need problem-specific search processes to generate valid solutions.\nIC/DC employs a novel architecture capable of capturing the intricate\nrelationships between items, and thereby enabling effective optimization in\nchallenging CO scenarios. IC/DC achieves state-of-the-art performance relative\nto existing NCO methods on the Parallel Machine Scheduling Problem (PMSP) and\nAsymmetric Traveling Salesman Problem (ATSP).\n","authors":["Seong-Hyun Hong","Hyun-Sung Kim","Zian Jang","Deunsol Yoon","Hyungseok Song","Byung-Jun Lee"],"pdf_url":"https://arxiv.org/pdf/2411.00003v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18071v2","updated":"2025-02-12T11:31:48Z","published":"2025-01-30T00:42:43Z","title":"Towards Transparent and Accurate Diabetes Prediction Using Machine\n  Learning and Explainable Artificial Intelligence","summary":"  Diabetes mellitus (DM) is a global health issue of significance that must be\ndiagnosed as early as possible and managed well. This study presents a\nframework for diabetes prediction using Machine Learning (ML) models,\ncomplemented with eXplainable Artificial Intelligence (XAI) tools, to\ninvestigate both the predictive accuracy and interpretability of the\npredictions from ML models. Data Preprocessing is based on the Synthetic\nMinority Oversampling Technique (SMOTE) and feature scaling used on the\nDiabetes Binary Health Indicators dataset to deal with class imbalance and\nvariability of clinical features. The ensemble model provided high accuracy,\nwith a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General\nHealth, Income, and Physical Activity were the most influential predictors\nobtained from the model explanations. The results of this study suggest that ML\ncombined with XAI is a promising means of developing accurate and\ncomputationally transparent tools for use in healthcare systems.\n","authors":["Pir Bakhsh Khokhar","Viviana Pentangelo","Fabio Palomba","Carmine Gravino"],"pdf_url":"https://arxiv.org/pdf/2501.18071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08684v1","updated":"2025-02-12T11:22:33Z","published":"2025-02-12T11:22:33Z","title":"Self-Evaluation for Job-Shop Scheduling","summary":"  Combinatorial optimization problems, such as scheduling and route planning,\nare crucial in various industries but are computationally intractable due to\ntheir NP-hard nature. Neural Combinatorial Optimization methods leverage\nmachine learning to address these challenges but often depend on sequential\ndecision-making, which is prone to error accumulation as small mistakes\npropagate throughout the process. Inspired by self-evaluation techniques in\nLarge Language Models, we propose a novel framework that generates and\nevaluates subsets of assignments, moving beyond traditional stepwise\napproaches. Applied to the Job-Shop Scheduling Problem, our method integrates a\nheterogeneous graph neural network with a Transformer to build a policy model\nand a self-evaluation function. Experimental validation on challenging,\nwell-known benchmarks demonstrates the effectiveness of our approach,\nsurpassing state-of-the-art methods.\n","authors":["Imanol Echeverria","Maialen Murua","Roberto Santana"],"pdf_url":"https://arxiv.org/pdf/2502.08684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08683v1","updated":"2025-02-12T11:16:15Z","published":"2025-02-12T11:16:15Z","title":"A Deep Learning approach for parametrized and time dependent Partial\n  Differential Equations using Dimensionality Reduction and Neural ODEs","summary":"  Partial Differential Equations (PDEs) are central to science and engineering.\nSince solving them is computationally expensive, a lot of effort has been put\ninto approximating their solution operator via both traditional and recently\nincreasingly Deep Learning (DL) techniques. A conclusive methodology capable of\naccounting both for (continuous) time and parameter dependency in such DL\nmodels however is still lacking. In this paper, we propose an autoregressive\nand data-driven method using the analogy with classical numerical solvers for\ntime-dependent, parametric and (typically) nonlinear PDEs. We present how\nDimensionality Reduction (DR) can be coupled with Neural Ordinary Differential\nEquations (NODEs) in order to learn the solution operator of arbitrary PDEs.\nThe idea of our work is that it is possible to map the high-fidelity (i.e.,\nhigh-dimensional) PDE solution space into a reduced (low-dimensional) space,\nwhich subsequently exhibits dynamics governed by a (latent) Ordinary\nDifferential Equation (ODE). Solving this (easier) ODE in the reduced space\nallows avoiding solving the PDE in the high-dimensional solution space, thus\ndecreasing the computational burden for repeated calculations for e.g.,\nuncertainty quantification or design optimization purposes. The main outcome of\nthis work is the importance of exploiting DR as opposed to the recent trend of\nbuilding large and complex architectures: we show that by leveraging DR we can\ndeliver not only more accurate predictions, but also a considerably lighter and\nfaster DL model compared to existing methodologies.\n","authors":["Alessandro Longhi","Danny Lathouwers","Zolt√°n Perk√≥"],"pdf_url":"https://arxiv.org/pdf/2502.08683v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08302v1","updated":"2025-02-12T11:03:51Z","published":"2025-02-12T11:03:51Z","title":"HDT: Hierarchical Discrete Transformer for Multivariate Time Series\n  Forecasting","summary":"  Generative models have gained significant attention in multivariate time\nseries forecasting (MTS), particularly due to their ability to generate\nhigh-fidelity samples. Forecasting the probability distribution of multivariate\ntime series is a challenging yet practical task. Although some recent attempts\nhave been made to handle this task, two major challenges persist: 1) some\nexisting generative methods underperform in high-dimensional multivariate time\nseries forecasting, which is hard to scale to higher dimensions; 2) the\ninherent high-dimensional multivariate attributes constrain the forecasting\nlengths of existing generative models. In this paper, we point out that\ndiscrete token representations can model high-dimensional MTS with faster\ninference time, and forecasting the target with long-term trends of itself can\nextend the forecasting length with high accuracy. Motivated by this, we propose\na vector quantized framework called Hierarchical Discrete Transformer (HDT)\nthat models time series into discrete token representations with l2\nnormalization enhanced vector quantized strategy, in which we transform the MTS\nforecasting into discrete tokens generation. To address the limitations of\ngenerative models in long-term forecasting, we propose a hierarchical discrete\nTransformer. This model captures the discrete long-term trend of the target at\nthe low level and leverages this trend as a condition to generate the discrete\nrepresentation of the target at the high level that introduces the features of\nthe target itself to extend the forecasting length in high-dimensional MTS.\nExtensive experiments on five popular MTS datasets verify the effectiveness of\nour proposed method.\n","authors":["Shibo Feng","Peilin Zhao","Liu Liu","Pengcheng Wu","Zhiqi Shen"],"pdf_url":"https://arxiv.org/pdf/2502.08302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08298v1","updated":"2025-02-12T10:58:57Z","published":"2025-02-12T10:58:57Z","title":"Improving Existing Optimization Algorithms with LLMs","summary":"  The integration of Large Language Models (LLMs) into optimization has created\na powerful synergy, opening exciting research opportunities. This paper\ninvestigates how LLMs can enhance existing optimization algorithms. Using their\npre-trained knowledge, we demonstrate their ability to propose innovative\nheuristic variations and implementation strategies. To evaluate this, we\napplied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt\n(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that\nincorporates a heuristic in the solution construction phase. Our results show\nthat an alternative heuristic proposed by GPT-4o outperforms the\nexpert-designed heuristic of CMSA, with the performance gap widening on larger\nand denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/\n","authors":["Camilo Chac√≥n Sartori","Christian Blum"],"pdf_url":"https://arxiv.org/pdf/2502.08298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08682v1","updated":"2025-02-12T10:50:46Z","published":"2025-02-12T10:50:46Z","title":"On the Role of Pre-trained Embeddings in Binary Code Analysis","summary":"  Deep learning has enabled remarkable progress in binary code analysis. In\nparticular, pre-trained embeddings of assembly code have become a gold standard\nfor solving analysis tasks, such as measuring code similarity or recognizing\nfunctions. These embeddings are capable of learning a vector representation\nfrom unlabeled code. In contrast to natural language processing, however, label\ninformation is not scarce for many tasks in binary code analysis. For example,\nlabeled training data for function boundaries, optimization levels, and\nargument types can be easily derived from debug information provided by a\ncompiler. Consequently, the main motivation of embeddings does not transfer\ndirectly to binary code analysis.\n  In this paper, we explore the role of pre-trained embeddings from a critical\nperspective. To this end, we systematically evaluate recent embeddings for\nassembly code on five downstream tasks using a corpus of 1.2 million functions\nfrom the Debian distribution. We observe that several embeddings perform\nsimilarly when sufficient labeled data is available, and that differences\nreported in prior work are hardly noticeable. Surprisingly, we find that\nend-to-end learning without pre-training performs best on average, which calls\ninto question the need for specialized embeddings. By varying the amount of\nlabeled data, we eventually derive guidelines for when embeddings offer\nadvantages and when end-to-end learning is preferable for binary code analysis.\n","authors":["Alwin Maier","Felix Weissberg","Konrad Rieck"],"pdf_url":"https://arxiv.org/pdf/2502.08682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04304v2","updated":"2025-02-12T10:49:40Z","published":"2025-01-08T06:30:31Z","title":"DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion\n  Models","summary":"  Despite the widespread use of text-to-image diffusion models across various\ntasks, their computational and memory demands limit practical applications. To\nmitigate this issue, quantization of diffusion models has been explored. It\nreduces memory usage and computational costs by compressing weights and\nactivations into lower-bit formats. However, existing methods often struggle to\npreserve both image quality and text-image alignment, particularly in\nlower-bit($<$ 8bits) quantization. In this paper, we analyze the challenges\nassociated with quantizing text-to-image diffusion models from a distributional\nperspective. Our analysis reveals that activation outliers play a crucial role\nin determining image quality. Additionally, we identify distinctive patterns in\ncross-attention scores, which significantly affects text-image alignment. To\naddress these challenges, we propose Distribution-aware Group Quantization\n(DGQ), a method that identifies and adaptively handles pixel-wise and\nchannel-wise outliers to preserve image quality. Furthermore, DGQ applies\nprompt-specific logarithmic quantization scales to maintain text-image\nalignment. Our method demonstrates remarkable performance on datasets such as\nMS-COCO and PartiPrompts. We are the first to successfully achieve low-bit\nquantization of text-to-image diffusion models without requiring additional\nfine-tuning of weight quantization parameters. Code is available at\nhttps://github.com/ugonfor/DGQ.\n","authors":["Hyogon Ryu","NaHyeon Park","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2501.04304v2.pdf","comment":"Accepted ICLR 2025. Project page: https://ugonfor.kr/DGQ"},{"id":"http://arxiv.org/abs/2502.08284v1","updated":"2025-02-12T10:42:04Z","published":"2025-02-12T10:42:04Z","title":"Data Pricing for Graph Neural Networks without Pre-purchased Inspection","summary":"  Machine learning (ML) models have become essential tools in various\nscenarios. Their effectiveness, however, hinges on a substantial volume of data\nfor satisfactory performance. Model marketplaces have thus emerged as crucial\nplatforms bridging model consumers seeking ML solutions and data owners\npossessing valuable data. These marketplaces leverage model trading mechanisms\nto properly incentive data owners to contribute their data, and return a well\nperforming ML model to the model consumers. However, existing model trading\nmechanisms often assume the data owners are willing to share their data before\nbeing paid, which is not reasonable in real world. Given that, we propose a\nnovel mechanism, named Structural Importance based Model Trading (SIMT)\nmechanism, that assesses the data importance and compensates data owners\naccordingly without disclosing the data. Specifically, SIMT procures feature\nand label data from data owners according to their structural importance, and\nthen trains a graph neural network for model consumers. Theoretically, SIMT\nensures incentive compatible, individual rational and budget feasible. The\nexperiments on five popular datasets validate that SIMT consistently\noutperforms vanilla baselines by up to $40\\%$ in both MacroF1 and MicroF1.\n","authors":["Yiping Liu","Mengxiao Zhang","Jiamou Liu","Song Yang"],"pdf_url":"https://arxiv.org/pdf/2502.08284v1.pdf","comment":"Accepted by AAMAS-2025"},{"id":"http://arxiv.org/abs/2502.08282v1","updated":"2025-02-12T10:41:21Z","published":"2025-02-12T10:41:21Z","title":"Individualised Treatment Effects Estimation with Composite Treatments\n  and Composite Outcomes","summary":"  Estimating individualised treatment effect (ITE) -- that is the causal effect\nof a set of variables (also called exposures, treatments, actions, policies, or\ninterventions), referred to as \\textit{composite treatments}, on a set of\noutcome variables of interest, referred to as \\textit{composite outcomes}, for\na unit from observational data -- remains a fundamental problem in causal\ninference with applications across disciplines, such as healthcare, economics,\neducation, social science, marketing, and computer science. Previous work in\ncausal machine learning for ITE estimation is limited to simple settings, like\nsingle treatments and single outcomes. This hinders their use in complex\nreal-world scenarios; for example, consider studying the effect of different\nICU interventions, such as beta-blockers and statins for a patient admitted for\nheart surgery, on different outcomes of interest such as atrial fibrillation\nand in-hospital mortality. The limited research into composite treatments and\noutcomes is primarily due to data scarcity for all treatments and outcomes. To\naddress the above challenges, we propose a novel and innovative\nhypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation\nunder composite treatments and composite outcomes, which tackles the data\nscarcity issue by dynamically sharing information across treatments and\noutcomes. Our empirical analysis with binary and arbitrary composite treatments\nand outcomes demonstrates the effectiveness of the proposed approach compared\nto existing methods.\n","authors":["Vinod Kumar Chauhan","Lei Clifton","Gaurav Nigam","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2502.08282v1.pdf","comment":"6 pages (double column), 4 figures"},{"id":"http://arxiv.org/abs/2305.19270v2","updated":"2025-02-12T10:37:04Z","published":"2023-05-30T17:59:32Z","title":"Learning without Forgetting for Vision-Language Models","summary":"  Class-Incremental Learning (CIL) or continual learning is a desired\ncapability in the real world, which requires a learning system to adapt to new\ntasks without forgetting former ones. While traditional CIL methods focus on\nvisual information to grasp core features, recent advances in Vision-Language\nModels (VLM) have shown promising capabilities in learning generalizable\nrepresentations with the aid of textual information. However, when continually\ntrained with new classes, VLMs often suffer from catastrophic forgetting of\nformer knowledge. Applying VLMs to CIL poses two major challenges: 1) how to\nadapt the model without forgetting; and 2) how to make full use of the\nmulti-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that\nenables VLMs to learn without forgetting. To handle the first challenge, we\npropose training task-specific projections based on the frozen image/text\nencoders. When facing new tasks, new projections are expanded and former\nprojections are fixed, alleviating the forgetting of old concepts. For the\nsecond challenge, we propose the fusion module to better utilize the\ncross-modality information. By jointly adjusting visual and textual features,\nthe model can capture semantic information with stronger representation\nability. Extensive experiments on nine benchmark datasets validate PROOF\nachieves state-of-the-art performance. Code is available at\nhttps://github.com/zhoudw-zdw/PROOF\n","authors":["Da-Wei Zhou","Yuanhan Zhang","Yan Wang","Jingyi Ning","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2305.19270v2.pdf","comment":"Accepted to TPAMI. Code is available at\n  https://github.com/zhoudw-zdw/PROOF"},{"id":"http://arxiv.org/abs/2502.08266v1","updated":"2025-02-12T10:19:50Z","published":"2025-02-12T10:19:50Z","title":"Dealing with Annotator Disagreement in Hate Speech Classification","summary":"  Hate speech detection is a crucial task, especially on social media, where\nharmful content can spread quickly. Implementing machine learning models to\nautomatically identify and address hate speech is essential for mitigating its\nimpact and preventing its proliferation. The first step in developing an\neffective hate speech detection model is to acquire a high-quality dataset for\ntraining. Labeled data is foundational for most natural language processing\ntasks, but categorizing hate speech is difficult due to the diverse and often\nsubjective nature of hate speech, which can lead to varying interpretations and\ndisagreements among annotators. This paper examines strategies for addressing\nannotator disagreement, an issue that has been largely overlooked. In\nparticular, we evaluate different approaches to deal with annotator\ndisagreement regarding hate speech classification in Turkish tweets, based on a\nfine-tuned BERT model. Our work highlights the importance of the problem and\nprovides state-of-art benchmark results for detection and understanding of hate\nspeech in online discourse.\n","authors":["Somaiyeh Dehghan","Mehmet Umut Sen","Berrin Yanikoglu"],"pdf_url":"https://arxiv.org/pdf/2502.08266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01382v2","updated":"2025-02-12T10:18:06Z","published":"2024-08-02T16:40:58Z","title":"Explaining a probabilistic prediction on the simplex with Shapley\n  compositions","summary":"  Originating in game theory, Shapley values are widely used for explaining a\nmachine learning model's prediction by quantifying the contribution of each\nfeature's value to the prediction. This requires a scalar prediction as in\nbinary classification, whereas a multiclass probabilistic prediction is a\ndiscrete probability distribution, living on a multidimensional simplex. In\nsuch a multiclass setting the Shapley values are typically computed separately\non each class in a one-vs-rest manner, ignoring the compositional nature of the\noutput distribution. In this paper, we introduce Shapley compositions as a\nwell-founded way to properly explain a multiclass probabilistic prediction,\nusing the Aitchison geometry from compositional data analysis. We prove that\nthe Shapley composition is the unique quantity satisfying linearity, symmetry\nand efficiency on the Aitchison simplex, extending the corresponding axiomatic\nproperties of the standard Shapley value. We demonstrate this proper multiclass\ntreatment in a range of scenarios.\n","authors":["Paul-Gauthier No√©","Miquel Perell√≥-Nieto","Jean-Fran√ßois Bonastre","Peter Flach"],"pdf_url":"https://arxiv.org/pdf/2408.01382v2.pdf","comment":"Published in ECAI2024's proceedings"},{"id":"http://arxiv.org/abs/2502.08681v1","updated":"2025-02-12T10:16:06Z","published":"2025-02-12T10:16:06Z","title":"Centrally Coordinated Multi-Agent Reinforcement Learning for Power Grid\n  Topology Control","summary":"  Power grid operation is becoming more complex due to the increase in\ngeneration of renewable energy. The recent series of Learning To Run a Power\nNetwork (L2RPN) competitions have encouraged the use of artificial agents to\nassist human dispatchers in operating power grids. However, the combinatorial\nnature of the action space poses a challenge to both conventional optimizers\nand learned controllers. Action space factorization, which breaks down\ndecision-making into smaller sub-tasks, is one approach to tackle the curse of\ndimensionality. In this study, we propose a centrally coordinated multi-agent\n(CCMA) architecture for action space factorization. In this approach, regional\nagents propose actions and subsequently a coordinating agent selects the final\naction. We investigate several implementations of the CCMA architecture, and\nbenchmark in different experimental settings against various L2RPN baseline\napproaches. The CCMA architecture exhibits higher sample efficiency and\nsuperior final performance than the baseline approaches. The results suggest\nhigh potential of the CCMA approach for further application in\nhigher-dimensional L2RPN as well as real-world power grid settings.\n","authors":["Barbera de Mol","Davide Barbieri","Jan Viebahn","Davide Grossi"],"pdf_url":"https://arxiv.org/pdf/2502.08681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08262v1","updated":"2025-02-12T10:10:04Z","published":"2025-02-12T10:10:04Z","title":"GenIAS: Generator for Instantiating Anomalies in time Series","summary":"  A recent and promising approach for building time series anomaly detection\n(TSAD) models is to inject synthetic samples of anomalies within real data\nsets. The existing injection mechanisms have significant limitations - most of\nthem rely on ad hoc, hand-crafted strategies which fail to capture the natural\ndiversity of anomalous patterns, or are restricted to univariate time series\nsettings. To address these challenges, we design a generative model for TSAD\nusing a variational autoencoder, which is referred to as a Generator for\nInstantiating Anomalies in Time Series (GenIAS). GenIAS is designed to produce\ndiverse and realistic synthetic anomalies for TSAD tasks. By employing a novel\nlearned perturbation mechanism in the latent space and injecting the perturbed\npatterns in different segments of time series, GenIAS can generate anomalies\nwith greater diversity and varying scales. Further, guided by a new triplet\nloss function, which uses a min-max margin and a new variance-scaling approach\nto further enforce the learning of compact normal patterns, GenIAS ensures that\nanomalies are distinct from normal samples while remaining realistic. The\napproach is effective for both univariate and multivariate time series. We\ndemonstrate the diversity and realism of the generated anomalies. Our extensive\nexperiments demonstrate that GenIAS - when integrated into a TSAD task -\nconsistently outperforms seventeen traditional and deep anomaly detection\nmodels, thereby highlighting the potential of generative models for time series\nanomaly generation.\n","authors":["Zahra Zamanzadeh Darban","Qizhou Wang","Geoffrey I. Webb","Shirui Pan","Charu C. Aggarwal","Mahsa Salehi"],"pdf_url":"https://arxiv.org/pdf/2502.08262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08259v1","updated":"2025-02-12T10:05:25Z","published":"2025-02-12T10:05:25Z","title":"Balancing optimism and pessimism in offline-to-online learning","summary":"  We consider what we call the offline-to-online learning setting, focusing on\nstochastic finite-armed bandit problems. In offline-to-online learning, a\nlearner starts with offline data collected from interactions with an unknown\nenvironment in a way that is not under the learner's control. Given this data,\nthe learner begins interacting with the environment, gradually improving its\ninitial strategy as it collects more data to maximize its total reward. The\nlearner in this setting faces a fundamental dilemma: if the policy is deployed\nfor only a short period, a suitable strategy (in a number of senses) is the\nLower Confidence Bound (LCB) algorithm, which is based on pessimism. LCB can\neffectively compete with any policy that is sufficiently \"covered\" by the\noffline data. However, for longer time horizons, a preferred strategy is the\nUpper Confidence Bound (UCB) algorithm, which is based on optimism. Over time,\nUCB converges to the performance of the optimal policy at a rate that is nearly\nthe best possible among all online algorithms. In offline-to-online learning,\nhowever, UCB initially explores excessively, leading to worse short-term\nperformance compared to LCB. This suggests that a learner not in control of how\nlong its policy will be in use should start with LCB for short horizons and\ngradually transition to a UCB-like strategy as more rounds are played. This\narticle explores how and why this transition should occur. Our main result\nshows that our new algorithm performs nearly as well as the better of LCB and\nUCB at any point in time. The core idea behind our algorithm is broadly\napplicable, and we anticipate that our results will extend beyond the\nmulti-armed bandit setting.\n","authors":["Sentenac Flore","Lee Albin","Szepesvari Csaba"],"pdf_url":"https://arxiv.org/pdf/2502.08259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08680v1","updated":"2025-02-12T09:53:10Z","published":"2025-02-12T09:53:10Z","title":"Mathematical Reasoning in Large Language Models: Assessing Logical and\n  Arithmetic Errors across Wide Numerical Ranges","summary":"  Mathematical reasoning in Large Language Models (LLMs) is often evaluated\nusing benchmarks with limited numerical ranges, failing to reflect real-world\nproblem-solving across diverse scales. Furthermore, most existing evaluation\nmethods only compare model outputs to ground-truth answers, obscuring insights\ninto reasoning processes. To address these limitations, we introduce\nGSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs\nnumerical values in math problems to assess model robustness across varying\nnumerical scales. Additionally, we propose a novel grading methodology that\ndistinguishes between logical and non-logical errors, offering a more precise\nevaluation of reasoning processes beyond computational accuracy. Our\nexperiments with various models reveal a significant increase in logical error\nrates-up to 14 percentage points-as numerical complexity rises, demonstrating a\ngeneral weakness in reasoning with out-of-distribution numerical values.\nMoreover, while models demonstrate high accuracy on standalone arithmetic\ntasks, their performance deteriorates substantially when computations are\nembedded within word problems. These findings provide a comprehensive\nevaluation of LLMs' mathematical reasoning capabilities and inform future\nresearch directions for improving numerical generalization in language models.\n","authors":["Safal Shrestha","Minwu Kim","Keith Ross"],"pdf_url":"https://arxiv.org/pdf/2502.08680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08578v2","updated":"2025-02-12T09:50:05Z","published":"2024-10-11T07:07:12Z","title":"Logarithmic Regret for Unconstrained Submodular Maximization Stochastic\n  Bandit","summary":"  We address the online unconstrained submodular maximization problem (Online\nUSM), in a setting with stochastic bandit feedback. In this framework, a\ndecision-maker receives noisy rewards from a non monotone submodular function\ntaking values in a known bounded interval. This paper proposes Double-Greedy -\nExplore-then-Commit (DG-ETC), adapting the Double-Greedy approach from the\noffline and online full-information settings. DG-ETC satisfies a $O(d\\log(dT))$\nproblem-dependent upper bound for the $1/2$-approximate pseudo-regret, as well\nas a $O(dT^{2/3}\\log(dT)^{1/3})$ problem-free one at the same time,\noutperforming existing approaches. In particular, we introduce a\nproblem-dependent notion of hardness characterizing the transition between\nlogarithmic and polynomial regime for the upper bounds.\n","authors":["Julien Zhou","Pierre Gaillard","Thibaud Rahier","Julyan Arbel"],"pdf_url":"https://arxiv.org/pdf/2410.08578v2.pdf","comment":"Camera-ready version for ALT 2025"},{"id":"http://arxiv.org/abs/2502.08253v1","updated":"2025-02-12T09:49:25Z","published":"2025-02-12T09:49:25Z","title":"Multi-View Oriented GPLVM: Expressiveness and Efficiency","summary":"  The multi-view Gaussian process latent variable model (MV-GPLVM) aims to\nlearn a unified representation from multi-view data but is hindered by\nchallenges such as limited kernel expressiveness and low computational\nefficiency. To overcome these issues, we first introduce a new duality between\nthe spectral density and the kernel function. By modeling the spectral density\nwith a bivariate Gaussian mixture, we then derive a generic and expressive\nkernel termed Next-Gen Spectral Mixture (NG-SM) for MV-GPLVMs. To address the\ninherent computational inefficiency of the NG-SM kernel, we propose a random\nFourier feature approximation. Combined with a tailored reparameterization\ntrick, this approximation enables scalable variational inference for both the\nmodel and the unified latent representations. Numerical evaluations across a\ndiverse range of multi-view datasets demonstrate that our proposed method\nconsistently outperforms state-of-the-art models in learning meaningful latent\nrepresentations.\n","authors":["Zi Yang","Ying Li","Zhidi Lin","Michael Minyi Zhang","Pablo M. Olmos"],"pdf_url":"https://arxiv.org/pdf/2502.08253v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2412.16205v2","updated":"2025-02-12T09:48:29Z","published":"2024-12-17T10:53:12Z","title":"Machine Learning-Based Estimation Of Wave Direction For Unmanned Surface\n  Vehicles","summary":"  Unmanned Surface Vehicles (USVs) have become critical tools for marine\nexploration, environmental monitoring, and autonomous navigation. Accurate\nestimation of wave direction is essential for improving USV navigation and\nensuring operational safety, but traditional methods often suffer from high\ncosts and limited spatial resolution. This paper proposes a machine\nlearning-based approach leveraging LSTM (Long Short-Term Memory) networks to\npredict wave direction using sensor data collected from USVs. Experimental\nresults show the capability of the LSTM model to learn temporal dependencies\nand provide accurate predictions, outperforming simpler baselines.\n","authors":["Manele Ait Habouche","Micka√´l Kerboeuf","Goulven Guillou","Jean-Philippe Babau"],"pdf_url":"https://arxiv.org/pdf/2412.16205v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12676v3","updated":"2025-02-12T09:47:58Z","published":"2023-12-20T00:31:43Z","title":"Bayesian Analysis of Combinatorial Gaussian Process Bandits","summary":"  We consider the combinatorial volatile Gaussian process (GP) semi-bandit\nproblem. Each round, an agent is provided a set of available base arms and must\nselect a subset of them to maximize the long-term cumulative reward. We study\nthe Bayesian setting and provide novel Bayesian cumulative regret bounds for\nthree GP-based algorithms: GP-UCB, GP-BayesUCB and GP-TS. Our bounds extend\nprevious results for GP-UCB and GP-TS to the infinite, volatile and\ncombinatorial setting, and to the best of our knowledge, we provide the first\nregret bound for GP-BayesUCB. Volatile arms encompass other widely considered\nbandit problems such as contextual bandits. Furthermore, we employ our\nframework to address the challenging real-world problem of online\nenergy-efficient navigation, where we demonstrate its effectiveness compared to\nthe alternatives.\n","authors":["Jack Sandberg","Niklas √Ökerblom","Morteza Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2312.12676v3.pdf","comment":"34 pages, 9 figures. Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2412.09119v2","updated":"2025-02-12T09:38:31Z","published":"2024-12-12T09:54:38Z","title":"The Utility and Complexity of in- and out-of-Distribution Machine\n  Unlearning","summary":"  Machine unlearning, the process of selectively removing data from trained\nmodels, is increasingly crucial for addressing privacy concerns and knowledge\ngaps post-deployment. Despite this importance, existing approaches are often\nheuristic and lack formal guarantees. In this paper, we analyze the fundamental\nutility, time, and space complexity trade-offs of approximate unlearning,\nproviding rigorous certification analogous to differential privacy. For\nin-distribution forget data -- data similar to the retain set -- we show that a\nsurprisingly simple and general procedure, empirical risk minimization with\noutput perturbation, achieves tight unlearning-utility-complexity trade-offs,\naddressing a previous theoretical gap on the separation from unlearning \"for\nfree\" via differential privacy, which inherently facilitates the removal of\nsuch data. However, such techniques fail with out-of-distribution forget data\n-- data significantly different from the retain set -- where unlearning time\ncomplexity can exceed that of retraining, even for a single sample. To address\nthis, we propose a new robust and noisy gradient descent variant that provably\namortizes unlearning time complexity without compromising utility.\n","authors":["Youssef Allouah","Joshua Kazdan","Rachid Guerraoui","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2412.09119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12706v4","updated":"2025-02-12T09:33:52Z","published":"2022-05-25T12:02:59Z","title":"Maximum Mean Discrepancy on Exponential Windows for Online Change\n  Detection","summary":"  Detecting changes is of fundamental importance when analyzing data streams\nand has many applications, e.g., in predictive maintenance, fraud detection, or\nmedicine. A principled approach to detect changes is to compare the\ndistributions of observations within the stream to each other via hypothesis\ntesting. Maximum mean discrepancy (MMD), a (semi-)metric on the space of\nprobability distributions, provides powerful non-parametric two-sample tests on\nkernel-enriched domains. In particular, MMD is able to detect any disparity\nbetween distributions under mild conditions. However, classical MMD estimators\nsuffer from a quadratic runtime complexity, which renders their direct use for\nchange detection in data streams impractical. In this article, we propose a new\nchange detection algorithm, called Maximum Mean Discrepancy on Exponential\nWindows (MMDEW), that combines the benefits of MMD with an efficient\ncomputation based on exponential windows. We prove that MMDEW enjoys\npolylogarithmic runtime and logarithmic memory complexity and show empirically\nthat it outperforms the state of the art on benchmark data streams.\n","authors":["Florian Kalinke","Marco Heyden","Georg Gntuni","Edouard Fouch√©","Klemens B√∂hm"],"pdf_url":"https://arxiv.org/pdf/2205.12706v4.pdf","comment":"Published in TMLR 02/25"},{"id":"http://arxiv.org/abs/2209.06735v3","updated":"2025-02-12T09:32:17Z","published":"2022-09-14T15:52:19Z","title":"Falsification of Cyber-Physical Systems using Bayesian Optimization","summary":"  Cyber-physical systems (CPSs) are often complex and safety-critical, making\nit both challenging and crucial to ensure that the system's specifications are\nmet. Simulation-based falsification is a practical testing technique for\nincreasing confidence in a CPS's correctness, as it only requires that the\nsystem be simulated. Reducing the number of computationally intensive\nsimulations needed for falsification is a key concern. In this study, we\ninvestigate Bayesian optimization (BO), a sample-efficient approach that learns\na surrogate model to capture the relationship between input signal\nparameterization and specification evaluation. We propose two enhancements to\nthe basic BO for improving falsification: (1) leveraging local surrogate\nmodels, and (2) utilizing the user's prior knowledge. Additionally, we address\nthe formulation of acquisition functions for falsification by proposing and\nevaluating various alternatives. Our benchmark evaluation demonstrates\nsignificant improvements when using local surrogate models in BO for falsifying\nchallenging benchmark examples. Incorporating prior knowledge is found to be\nespecially beneficial when the simulation budget is constrained. For some\nbenchmark problems, the choice of acquisition function noticeably impacts the\nnumber of simulations required for successful falsification.\n","authors":["Zahra Ramezani","Kenan ≈†ehiƒá","Luigi Nardi","Knut √Ökesson"],"pdf_url":"https://arxiv.org/pdf/2209.06735v3.pdf","comment":"Accepted in ACM Transactions on Embedded Computing Systems"},{"id":"http://arxiv.org/abs/2402.09780v3","updated":"2025-02-12T09:25:25Z","published":"2024-02-15T08:09:17Z","title":"TinyCL: An Efficient Hardware Architecture for Continual Learning on\n  Autonomous Systems","summary":"  The Continuous Learning (CL) paradigm consists of continuously evolving the\nparameters of the Deep Neural Network (DNN) model to progressively learn to\nperform new tasks without reducing the performance on previous tasks, i.e.,\navoiding the so-called catastrophic forgetting. However, the DNN parameter\nupdate in CL-based autonomous systems is extremely resource-hungry. The\nexisting DNN accelerators cannot be directly employed in CL because they only\nsupport the execution of the forward propagation. Only a few prior\narchitectures execute the backpropagation and weight update, but they lack the\ncontrol and management for CL. Towards this, we design a hardware architecture,\nTinyCL, to perform CL on resource-constrained autonomous systems. It consists\nof a processing unit that executes both forward and backward propagation, and a\ncontrol unit that manages memory-based CL workload. To minimize the memory\naccesses, the sliding window of the convolutional layer moves in a snake-like\nfashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at\nruntime to execute different operations. As per our knowledge, our proposed\nTinyCL represents the first hardware accelerator that executes CL on autonomous\nsystems. We synthesize the complete TinyCL architecture in a 65 nm CMOS\ntechnology node with the conventional ASIC design flow. It executes 1 epoch of\ntraining on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while\n1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,\nthus achieving a 58x speedup, consuming 86 mW in a 4.74 mm2 die.\n","authors":["Eugenio Ressa","Alberto Marchisio","Maurizio Martina","Guido Masera","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2402.09780v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08231v1","updated":"2025-02-12T09:20:08Z","published":"2025-02-12T09:20:08Z","title":"Keep your distance: learning dispersed embeddings on $\\mathbb{S}_d$","summary":"  Learning well-separated features in high-dimensional spaces, such as text or\nimage embeddings, is crucial for many machine learning applications. Achieving\nsuch separation can be effectively accomplished through the dispersion of\nembeddings, where unrelated vectors are pushed apart as much as possible. By\nconstraining features to be on a hypersphere, we can connect dispersion to\nwell-studied problems in mathematics and physics, where optimal solutions are\nknown for limited low-dimensional cases. However, in representation learning we\ntypically deal with a large number of features in high-dimensional space, and\nmoreover, dispersion is usually traded off with some other task-oriented\ntraining objective, making existing theoretical and numerical solutions\ninapplicable. Therefore, it is common to rely on gradient-based methods to\nencourage dispersion, usually by minimizing some function of the pairwise\ndistances. In this work, we first give an overview of existing methods from\ndisconnected literature, making new connections and highlighting similarities.\nNext, we introduce some new angles. We propose to reinterpret pairwise\ndispersion using a maximum mean discrepancy (MMD) motivation. We then propose\nan online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an\neffective alternative regularizer for dispersion on generic domains. Finally,\nwe derive a novel dispersion method that directly exploits properties of the\nhypersphere. Our experiments show the importance of dispersion in image\nclassification and natural language processing tasks, and how algorithms\nexhibit different trade-offs in different regimes.\n","authors":["Evgeniia Tokarchuk","Hua Chang Bakker","Vlad Niculae"],"pdf_url":"https://arxiv.org/pdf/2502.08231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08227v1","updated":"2025-02-12T09:12:45Z","published":"2025-02-12T09:12:45Z","title":"Enhancing Sample Selection by Cutting Mislabeled Easy Examples","summary":"  Sample selection is a prevalent approach in learning with noisy labels,\naiming to identify confident samples for training. Although existing sample\nselection methods have achieved decent results by reducing the noise rate of\nthe selected subset, they often overlook that not all mislabeled examples harm\nthe model's performance equally. In this paper, we demonstrate that mislabeled\nexamples correctly predicted by the model early in the training process are\nparticularly harmful to model performance. We refer to these examples as\nMislabeled Easy Examples (MEEs). To address this, we propose Early Cutting,\nwhich introduces a recalibration step that employs the model's later training\nstate to re-select the confident subset identified early in training, thereby\navoiding misleading confidence from early learning and effectively filtering\nout MEEs. Experiments on the CIFAR, WebVision, and full ImageNet-1k datasets\ndemonstrate that our method effectively improves sample selection and model\nperformance by reducing MEEs.\n","authors":["Suqin Yuan","Lei Feng","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.08227v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08226v1","updated":"2025-02-12T09:12:30Z","published":"2025-02-12T09:12:30Z","title":"TRISHUL: Towards Region Identification and Screen Hierarchy\n  Understanding for Large VLM based GUI Agents","summary":"  Recent advancements in Large Vision Language Models (LVLMs) have enabled the\ndevelopment of LVLM-based Graphical User Interface (GUI) agents under various\nparadigms. Training-based approaches, such as CogAgent and SeeClick, struggle\nwith cross-dataset and cross-platform generalization due to their reliance on\ndataset-specific training. Generalist LVLMs, such as GPT-4V, employ\nSet-of-Marks (SoM) for action grounding, but obtaining SoM labels requires\nmetadata like HTML source, which is not consistently available across\nplatforms. Moreover, existing methods often specialize in singular GUI tasks\nrather than achieving comprehensive GUI understanding. To address these\nlimitations, we introduce TRISHUL, a novel, training-free agentic framework\nthat enhances generalist LVLMs for holistic GUI comprehension. Unlike prior\nworks that focus on either action grounding (mapping instructions to GUI\nelements) or GUI referring (describing GUI elements given a location), TRISHUL\nseamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen\nParsing (HSP) and the Spatially Enhanced Element Description (SEED) module,\nwhich work synergistically to provide multi-granular, spatially, and\nsemantically enriched representations of GUI elements. Our results demonstrate\nTRISHUL's superior performance in action grounding across the ScreenSpot,\nVisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring,\nTRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new\nstandard for robust and adaptable GUI comprehension.\n","authors":["Kunal Singh","Shreyas Singh","Mukund Khanna"],"pdf_url":"https://arxiv.org/pdf/2502.08226v1.pdf","comment":"Under review at ICML 2025, 8 pages 5 figures"},{"id":"http://arxiv.org/abs/2409.01428v2","updated":"2025-02-12T09:10:44Z","published":"2024-09-02T19:13:26Z","title":"Self-Directed Learning of Convex Labelings on Graphs","summary":"  We study the problem of classifying the nodes of a given graph in the\nself-directed learning setup. This learning setting is a variant of online\nlearning, where rather than an adversary determining the sequence in which\nnodes are presented, the learner autonomously and adaptively selects them.\nWhile self-directed learning of Euclidean halfspaces, linear functions, and\ngeneral multiclass hypothesis classes was recently considered, no results\npreviously existed specifically for self-directed node classification on\ngraphs. In this paper, we address this problem developing efficient algorithms\nfor it. More specifically, we focus on the case of (geodesically) convex\nclusters, i.e., for every two nodes sharing the same label, all nodes on every\nshortest path between them also share the same label. In particular, we devise\nan algorithm with runtime polynomial in $n$ that makes only $3(h(G)+1)^4 \\ln n$\nmistakes on graphs with two convex clusters, where $n$ is the total number of\nnodes and $h(G)$ is the Hadwiger number, i.e., the size of the largest clique\nminor of the graph $G$. We also show that our algorithm is robust to the case\nthat clusters are slightly non-convex, still achieving a mistake bound\nlogarithmic in $n$. Finally, we devise a simple and efficient algorithm for\nhomophilic clusters, where strongly connected nodes tend to belong to the same\nclass.\n","authors":["Georgy Sokolov","Maximilian Thiessen","Margarita Akhmejanova","Fabio Vitale","Francesco Orabona"],"pdf_url":"https://arxiv.org/pdf/2409.01428v2.pdf","comment":"ALT 2025"},{"id":"http://arxiv.org/abs/2502.05416v2","updated":"2025-02-12T08:58:06Z","published":"2025-02-08T02:53:32Z","title":"Deep Generative Models with Hard Linear Equality Constraints","summary":"  While deep generative models~(DGMs) have demonstrated remarkable success in\ncapturing complex data distributions, they consistently fail to learn\nconstraints that encode domain knowledge and thus require constraint\nintegration. Existing solutions to this challenge have primarily relied on\nheuristic methods and often ignore the underlying data distribution, harming\nthe generative performance. In this work, we propose a probabilistically sound\napproach for enforcing the hard constraints into DGMs to generate\nconstraint-compliant and realistic data. This is achieved by our proposed\ngradient estimators that allow the constrained distribution, the data\ndistribution conditioned on constraints, to be differentiably learned. We carry\nout extensive experiments with various DGM model architectures over five image\ndatasets and three scientific applications in which domain knowledge is\ngoverned by linear equality constraints. We validate that the standard DGMs\nalmost surely generate data violating the constraints. Among all the constraint\nintegration strategies, ours not only guarantees the satisfaction of\nconstraints in generation but also archives superior generative performance\nthan the other methods across every benchmark.\n","authors":["Ruoyan Li","Dipti Ranjan Sahu","Guy Van den Broeck","Zhe Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.05416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08679v1","updated":"2025-02-12T08:56:35Z","published":"2025-02-12T08:56:35Z","title":"Deep Learning-Driven Malware Classification with API Call Sequence\n  Analysis and Concept Drift Handling","summary":"  Malware classification in dynamic environments presents a significant\nchallenge due to concept drift, where the statistical properties of malware\ndata evolve over time, complicating detection efforts. To address this issue,\nwe propose a deep learning framework enhanced with a genetic algorithm to\nimprove malware classification accuracy and adaptability. Our approach\nincorporates mutation operations and fitness score evaluations within genetic\nalgorithms to continuously refine the deep learning model, ensuring robustness\nagainst evolving malware threats. Experimental results demonstrate that this\nhybrid method significantly enhances classification performance and\nadaptability, outperforming traditional static models. Our proposed approach\noffers a promising solution for real-time malware classification in\never-changing cybersecurity landscapes.\n","authors":["Bishwajit Prasad Gond","Durga Prasad Mohapatra"],"pdf_url":"https://arxiv.org/pdf/2502.08679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08213v1","updated":"2025-02-12T08:48:55Z","published":"2025-02-12T08:48:55Z","title":"LLM Modules: Knowledge Transfer from a Large to a Small Model using\n  Enhanced Cross-Attention","summary":"  In this work, we propose an architecture of LLM Modules that enables the\ntransfer of knowledge from a large pre-trained model to a smaller model using\nan Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B\nmodel is frozen and its representations are passed through specially designed\nattention layers to the GPT-Neo-125M model, which is trained on limited\ncomputational resources. Experimental results on the Bespoke-Stratos-17k\ndataset demonstrate that after 15 epochs of training, the combined model\ngenerates responses comparable in quality to those obtained by distillation. We\ndiscuss the advantages of the modular approach, provide examples of input\nqueries and comparative analysis, and outline prospects for further extension\nof the method.\n","authors":["Konstantin Kolomeitsev"],"pdf_url":"https://arxiv.org/pdf/2502.08213v1.pdf","comment":"Code and pre-trained weights available at\n  https://huggingface.co/kkolomeitsev/llm-modules"},{"id":"http://arxiv.org/abs/2502.08211v1","updated":"2025-02-12T08:40:57Z","published":"2025-02-12T08:40:57Z","title":"Quality over Quantity: Boosting Data Efficiency Through Ensembled\n  Multimodal Data Curation","summary":"  In an era overwhelmed by vast amounts of data, the effective curation of\nweb-crawl datasets is essential for optimizing model performance. This paper\ntackles the challenges associated with the unstructured and heterogeneous\nnature of such datasets. Traditional heuristic curation methods often\ninadequately capture complex features, resulting in biases and the exclusion of\nrelevant data. We introduce an advanced, learning-driven approach, Ensemble\nCuration Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel\nquality-guided deduplication method to ensure balanced feature distributions.\nEcoDatum strategically integrates various unimodal and multimodal data curation\noperators within a weak supervision ensemble framework, utilizing automated\noptimization to score each data point effectively. EcoDatum, which\nsignificantly improves the data curation quality and efficiency, outperforms\nexisting state-of-the-art (SOTA) techniques, ranked 1st on the DataComp\nleaderboard, with an average performance score of 0.182 across 38 diverse\nevaluation datasets. This represents a 28% improvement over the DataComp\nbaseline method, demonstrating its effectiveness in improving dataset curation\nand model training efficiency.\n","authors":["Jinda Xu","Yuhao Song","Daming Wang","Weiwei Zhao","Minghua Chen","Kangliang Chen","Qinya Li"],"pdf_url":"https://arxiv.org/pdf/2502.08211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08209v1","updated":"2025-02-12T08:39:26Z","published":"2025-02-12T08:39:26Z","title":"Equivariant Masked Position Prediction for Efficient Molecular\n  Representation","summary":"  Graph neural networks (GNNs) have shown considerable promise in computational\nchemistry. However, the limited availability of molecular data raises concerns\nregarding GNNs' ability to effectively capture the fundamental principles of\nphysics and chemistry, which constrains their generalization capabilities. To\naddress this challenge, we introduce a novel self-supervised approach termed\nEquivariant Masked Position Prediction (EMPP), grounded in intramolecular\npotential and force theory. Unlike conventional attribute masking techniques,\nEMPP formulates a nuanced position prediction task that is more well-defined\nand enhances the learning of quantum mechanical features. EMPP also bypasses\nthe approximation of the Gaussian mixture distribution commonly used in\ndenoising methods, allowing for more accurate acquisition of physical\nproperties. Experimental results indicate that EMPP significantly enhances\nperformance of advanced molecular architectures, surpassing state-of-the-art\nself-supervised approaches. Our code is released in\nhttps://github.com/ajy112/EMPP.\n","authors":["Junyi An","Chao Qu","Yun-Fei Shi","XinHao Liu","Qianwei Tang","Fenglei Cao","Yuan Qi"],"pdf_url":"https://arxiv.org/pdf/2502.08209v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.08208v1","updated":"2025-02-12T08:38:37Z","published":"2025-02-12T08:38:37Z","title":"Exploring Exploration in Bayesian Optimization","summary":"  A well-balanced exploration-exploitation trade-off is crucial for successful\nacquisition functions in Bayesian optimization. However, there is a lack of\nquantitative measures for exploration, making it difficult to analyze and\ncompare different acquisition functions. This work introduces two novel\napproaches - observation traveling salesman distance and observation entropy -\nto quantify the exploration characteristics of acquisition functions based on\ntheir selected observations. Using these measures, we examine the explorative\nnature of several well-known acquisition functions across a diverse set of\nblack-box problems, uncover links between exploration and empirical\nperformance, and reveal new relationships among existing acquisition functions.\nBeyond enabling a deeper understanding of acquisition functions, these measures\nalso provide a foundation for guiding their design in a more principled and\nsystematic manner.\n","authors":["Leonard Papenmeier","Nuojin Cheng","Stephen Becker","Luigi Nardi"],"pdf_url":"https://arxiv.org/pdf/2502.08208v1.pdf","comment":"28 pages, 34 figures"},{"id":"http://arxiv.org/abs/2502.08206v1","updated":"2025-02-12T08:38:13Z","published":"2025-02-12T08:38:13Z","title":"Optimizing Asynchronous Federated Learning: A Delicate Trade-Off Between\n  Model-Parameter Staleness and Update Frequency","summary":"  Synchronous federated learning (FL) scales poorly with the number of clients\ndue to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync\naddress this limitation by enabling asynchronous communication between clients\nand the central server. In this work, we rely on stochastic modeling to better\nunderstand the impact of design choices in asynchronous FL algorithms, such as\nthe concurrency level and routing probabilities, and we leverage this knowledge\nto optimize loss. We characterize in particular a fundamental trade-off for\noptimizing asynchronous FL: minimizing gradient estimation errors by avoiding\nmodel parameter staleness, while also speeding up the system by increasing the\nthroughput of model updates. Our two main contributions can be summarized as\nfollows. First, we prove a discrete variant of Little's law to derive a\nclosed-form expression for relative delay, a metric that quantifies staleness.\nThis allows us to efficiently minimize the average loss per model update, which\nhas been the gold standard in literature to date. Second, we observe that\nnaively optimizing this metric leads us to slow down the system drastically by\noveremphazing staleness at the detriment of throughput. This motivates us to\nintroduce an alternative metric that also takes system speed into account, for\nwhich we derive a tractable upper-bound that can be minimized numerically.\nExtensive numerical results show that these optimizations enhance accuracy by\n10% to 30%.\n","authors":["Abdelkrim Alahyane","C√©line Comte","Matthieu Jonckheere","√âric Moulines"],"pdf_url":"https://arxiv.org/pdf/2502.08206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08205v1","updated":"2025-02-12T08:35:10Z","published":"2025-02-12T08:35:10Z","title":"Wisdom of the Crowds in Forecasting: Forecast Summarization for\n  Supporting Future Event Prediction","summary":"  Future Event Prediction (FEP) is an essential activity whose demand and\napplication range across multiple domains. While traditional methods like\nsimulations, predictive and time-series forecasting have demonstrated promising\noutcomes, their application in forecasting complex events is not entirely\nreliable due to the inability of numerical data to accurately capture the\nsemantic information related to events. One forecasting way is to gather and\naggregate collective opinions on the future to make predictions as cumulative\nperspectives carry the potential to help estimating the likelihood of upcoming\nevents. In this work, we organize the existing research and frameworks that aim\nto support future event prediction based on crowd wisdom through aggregating\nindividual forecasts. We discuss the challenges involved, available datasets,\nas well as the scope of improvement and future research directions for this\ntask. We also introduce a novel data model to represent individual forecast\nstatements.\n","authors":["Anisha Saha","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.08205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08202v1","updated":"2025-02-12T08:32:10Z","published":"2025-02-12T08:32:10Z","title":"Privacy amplification by random allocation","summary":"  We consider the privacy guarantees of an algorithm in which a user's data is\nused in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$\ndifferentially private steps. We demonstrate that the privacy guarantees of\nthis sampling scheme can be upper bound by the privacy guarantees of the\nwell-studied independent (or Poisson) subsampling in which each step uses the\nuser's data with probability $(1+ o(1))k/t $. Further, we provide two\nadditional analysis techniques that lead to numerical improvements in some\nparameter regimes. The case of $k=1$ has been previously studied in the context\nof DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024).\nPrivacy analysis of Balle et al. (2020) relies on privacy amplification by\nshuffling which leads to overly conservative bounds. Privacy analysis of Chua\net al. (2024a) relies on Monte Carlo simulations that are computationally\nprohibitive in many practical scenarios and have additional inherent\nlimitations.\n","authors":["Vitaly Feldman","Moshe Shenfeld"],"pdf_url":"https://arxiv.org/pdf/2502.08202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11647v3","updated":"2025-02-12T08:14:17Z","published":"2023-09-20T21:23:52Z","title":"Potential and limitations of random Fourier features for dequantizing\n  quantum machine learning","summary":"  Quantum machine learning is arguably one of the most explored applications of\nnear-term quantum devices. Much focus has been put on notions of variational\nquantum machine learning where parameterized quantum circuits (PQCs) are used\nas learning models. These PQC models have a rich structure which suggests that\nthey might be amenable to efficient dequantization via random Fourier features\n(RFF). In this work, we establish necessary and sufficient conditions under\nwhich RFF does indeed provide an efficient dequantization of variational\nquantum machine learning for regression. We build on these insights to make\nconcrete suggestions for PQC architecture design, and to identify structures\nwhich are necessary for a regression problem to admit a potential quantum\nadvantage via PQC based optimization.\n","authors":["Ryan Sweke","Erik Recio-Armengol","Sofiene Jerbi","Elies Gil-Fuster","Bryce Fuller","Jens Eisert","Johannes Jakob Meyer"],"pdf_url":"https://arxiv.org/pdf/2309.11647v3.pdf","comment":"44 pages (33+11). 6 Figures, with many clarifying figures added to\n  this version from original version. Comments and feedback welcome. Now\n  accepted in Quantum - this is the final version"},{"id":"http://arxiv.org/abs/2407.18170v3","updated":"2025-02-12T08:02:01Z","published":"2024-07-25T16:33:35Z","title":"RIDA: A Robust Attack Framework on Incomplete Graphs","summary":"  Graph Neural Networks (GNNs) are vital in data science but are increasingly\nsusceptible to adversarial attacks. To help researchers develop more robust GNN\nmodels, it's essential to focus on designing strong attack models as\nfoundational benchmarks and guiding references. Among adversarial attacks,\ngray-box poisoning attacks are noteworthy due to their effectiveness and fewer\nconstraints. These attacks exploit GNNs' need for retraining on updated data,\nthereby impacting their performance by perturbing these datasets. However,\ncurrent research overlooks the real-world scenario of incomplete graphs. To\naddress this gap, we introduce the Robust Incomplete Deep Attack Framework\n(RIDA). It is the first algorithm for robust gray-box poisoning attacks on\nincomplete graphs. The approach innovatively aggregates distant vertex\ninformation and ensures powerful data utilization. Extensive tests against 9\nSOTA baselines on 3 real-world datasets demonstrate that RIDA's superiority in\nhandling incompleteness and high attack performance on the incomplete graph.\n","authors":["Jianke Yu","Hanchen Wang","Chen Chen","Xiaoyang Wang","Lu Qin","Wenjie Zhang","Ying Zhang","Xijuan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.18170v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05434v2","updated":"2025-02-12T07:46:40Z","published":"2025-02-08T03:47:00Z","title":"Sample-Efficient Reinforcement Learning from Human Feedback via\n  Information-Directed Sampling","summary":"  We study the problem of reinforcement learning from human feedback (RLHF), a\ncritical problem in training large language models, from a theoretical\nperspective. Our main contribution is the design of novel sample-efficient RLHF\nalgorithms based on information-directed sampling (IDS), an online\ndecision-making principle inspired by information theory. Our algorithms\nmaximize the sum of the value function and a mutual information term that\nencourages exploration of the unknown environment (which quantifies the\ninformation gained about the environment through observed human feedback data).\nTo tackle the challenge of large state spaces and improve sample efficiency, we\nconstruct a simplified \\emph{surrogate environment} and introduce a novel\ndistance measure (named the \\emph{$\\ell_g$-distance}), enabling our IDS-based\nalgorithm to achieve a Bayesian regret upper bound of order\n$O(H^{\\frac{3}{2}}\\sqrt{\\log(K(\\epsilon)) T})$, where $H$ is the episode\nlength, $T$ is the number of episode and $K(\\epsilon)$ is related to the\ncovering number of the environment. Specializing to the tabular settings, this\nregret bound is of order $\\tilde{O}(H^2\\sqrt{SAT})$, where $S$ and $A$ are the\nnumbers of states and actions. Finally, we propose an Approximate-IDS algorithm\nthat is computationally more efficient while maintaining nearly the same sample\nefficiency. The design principle of this approximate algorithm is not only\neffective in RLHF settings but also applicable to the standard RL framework.\nMoreover, our work showcases the value of information theory in reinforcement\nlearning and in the training of large language models.\n","authors":["Han Qi","Haochen Yang","Qiaosheng Zhang","Zhuoran Yang"],"pdf_url":"https://arxiv.org/pdf/2502.05434v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08181v1","updated":"2025-02-12T07:39:44Z","published":"2025-02-12T07:39:44Z","title":"Latest Advancements Towards Catastrophic Forgetting under Data Scarcity:\n  A Comprehensive Survey on Few-Shot Class Incremental Learning","summary":"  Data scarcity significantly complicates the continual learning problem, i.e.,\nhow a deep neural network learns in dynamic environments with very few samples.\nHowever, the latest progress of few-shot class incremental learning (FSCIL)\nmethods and related studies show insightful knowledge on how to tackle the\nproblem. This paper presents a comprehensive survey on FSCIL that highlights\nseveral important aspects i.e. comprehensive and formal objectives of FSCIL\napproaches, the importance of prototype rectifications, the new learning\nparadigms based on pre-trained model and language-guided mechanism, the deeper\nanalysis of FSCIL performance metrics and evaluation, and the practical\ncontexts of FSCIL in various areas. Our extensive discussion presents the open\nchallenges, potential solutions, and future directions of FSCIL.\n","authors":["M. Anwar Ma'sum","Mahardhika Pratama","Igor Skrjanc"],"pdf_url":"https://arxiv.org/pdf/2502.08181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05806v2","updated":"2025-02-12T07:38:08Z","published":"2024-10-08T08:39:15Z","title":"A Parameter Update Balancing Algorithm for Multi-task Ranking Models in\n  Recommendation Systems","summary":"  Multi-task ranking models have become essential for modern real-world\nrecommendation systems. While most recommendation researches focus on designing\nsophisticated models for specific scenarios, achieving performance improvement\nfor multi-task ranking models across various scenarios still remains a\nsignificant challenge. Training all tasks naively can result in inconsistent\nlearning, highlighting the need for the development of multi-task optimization\n(MTO) methods to tackle this challenge. Conventional methods assume that the\noptimal joint gradient on shared parameters leads to optimal parameter updates.\nHowever, the actual update on model parameters may deviates significantly from\ngradients when using momentum based optimizers such as Adam, and we design and\nexecute statistical experiments to support the observation. In this paper, we\npropose a novel Parameter Update Balancing algorithm for multi-task\noptimization, denoted as PUB. In contrast to traditional MTO method which are\nbased on gradient level tasks fusion or loss level tasks fusion, PUB is the\nfirst work to optimize multiple tasks through parameter update balancing.\nComprehensive experiments on benchmark multi-task ranking datasets demonstrate\nthat PUB consistently improves several multi-task backbones and achieves\nstate-of-the-art performance. Additionally, experiments on benchmark computer\nvision datasets show the great potential of PUB in various multi-task learning\nscenarios. Furthermore, we deployed our method for an industrial evaluation on\nthe real-world commercial platform, HUAWEI AppGallery, where PUB significantly\nenhances the online multi-task ranking model, efficiently managing the primary\ntraffic of a crucial channel.\n","authors":["Jun Yuan","Guohao Cai","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2410.05806v2.pdf","comment":"Accepted by ICDM'24"},{"id":"http://arxiv.org/abs/2502.07531v2","updated":"2025-02-12T07:35:56Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03333v4","updated":"2025-02-12T07:19:15Z","published":"2024-03-05T21:34:23Z","title":"Federated Learning over Connected Modes","summary":"  Statistical heterogeneity in federated learning poses two major challenges:\nslow global training due to conflicting gradient signals, and the need of\npersonalization for local distributions. In this work, we tackle both\nchallenges by leveraging recent advances in \\emph{linear mode connectivity} --\nidentifying a linearly connected low-loss region in the parameter space of\nneural networks, which we call solution simplex. We propose federated learning\nover connected modes (\\textsc{Floco}), where clients are assigned local\nsubregions in this simplex based on their gradient signals, and together learn\nthe shared global solution simplex. This allows personalization of the client\nmodels to fit their local distributions within the degrees of freedom in the\nsolution simplex and homogenizes the update signals for the global simplex\ntraining. Our experiments show that \\textsc{Floco} accelerates the global\ntraining process, and significantly improves the local accuracy with minimal\ncomputational overhead in cross-silo federated learning settings.\n","authors":["Dennis Grinwald","Philipp Wiesner","Shinichi Nakajima"],"pdf_url":"https://arxiv.org/pdf/2403.03333v4.pdf","comment":"10 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)"},{"id":"http://arxiv.org/abs/2502.08167v1","updated":"2025-02-12T07:14:54Z","published":"2025-02-12T07:14:54Z","title":"DNNs May Determine Major Properties of Their Outputs Early, with Timing\n  Possibly Driven by Bias","summary":"  This paper argues that deep neural networks (DNNs) mostly determine their\noutputs during the early stages of inference, where biases inherent in the\nmodel play a crucial role in shaping this process. We draw a parallel between\nthis phenomenon and human decision-making, which often relies on fast,\nintuitive heuristics. Using diffusion models (DMs) as a case study, we\ndemonstrate that DNNs often make early-stage decision-making influenced by the\ntype and extent of bias in their design and training. Our findings offer a new\nperspective on bias mitigation, efficient inference, and the interpretation of\nmachine learning systems. By identifying the temporal dynamics of\ndecision-making in DNNs, this paper aims to inspire further discussion and\nresearch within the machine learning community.\n","authors":["Song Park","Sanghyuk Chun","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2502.08167v1.pdf","comment":"First two authors contributed equally"},{"id":"http://arxiv.org/abs/2502.08166v1","updated":"2025-02-12T07:11:33Z","published":"2025-02-12T07:11:33Z","title":"From Individual Experience to Collective Evidence: A Reporting-Based\n  Framework for Identifying Systemic Harms","summary":"  When an individual reports a negative interaction with some system, how can\ntheir personal experience be contextualized within broader patterns of system\nbehavior? We study the incident database problem, where individual reports of\nadverse events arrive sequentially, and are aggregated over time. In this work,\nour goal is to identify whether there are subgroups--defined by any combination\nof relevant features--that are disproportionately likely to experience harmful\ninteractions with the system. We formalize this problem as a sequential\nhypothesis test, and identify conditions on reporting behavior that are\nsufficient for making inferences about disparities in true rates of harm across\nsubgroups. We show that algorithms for sequential hypothesis tests can be\napplied to this problem with a standard multiple testing correction. We then\ndemonstrate our method on real-world datasets, including mortgage decisions and\nvaccine side effects; on each, our method (re-)identifies subgroups known to\nexperience disproportionate harm using only a fraction of the data that was\ninitially used to discover them.\n","authors":["Jessica Dai","Paula Gradu","Inioluwa Deborah Raji","Benjamin Recht"],"pdf_url":"https://arxiv.org/pdf/2502.08166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08160v1","updated":"2025-02-12T07:03:32Z","published":"2025-02-12T07:03:32Z","title":"Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly","summary":"  Vertical Federated Learning (VFL) is a privacy-preserving collaborative\nlearning paradigm that enables multiple parties with distinct feature sets to\njointly train machine learning models without sharing their raw data. Despite\nits potential to facilitate cross-organizational collaborations, the deployment\nof VFL systems in real-world applications remains limited. To investigate the\ngap between existing VFL research and practical deployment, this survey\nanalyzes the real-world data distributions in potential VFL applications and\nidentifies four key findings that highlight this gap. We propose a novel\ndata-oriented taxonomy of VFL algorithms based on real VFL data distributions.\nOur comprehensive review of existing VFL algorithms reveals that some common\npractical VFL scenarios have few or no viable solutions. Based on these\nobservations, we outline key research directions aimed at bridging the gap\nbetween current VFL research and real-world applications.\n","authors":["Zhaomin Wu","Zhen Qin","Junyi Hou","Haodong Zhao","Qinbin Li","Bingsheng He","Lixin Fan"],"pdf_url":"https://arxiv.org/pdf/2502.08160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14054v2","updated":"2025-02-12T07:00:51Z","published":"2023-09-25T11:36:20Z","title":"Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning\n  in Generative Adversarial Networks","summary":"  Owing to the growing concerns about privacy and regulatory compliance, it is\ndesirable to regulate the output of generative models. To that end, the\nobjective of this work is to prevent the generation of outputs containing\nundesired features from a pre-trained Generative Adversarial Network (GAN)\nwhere the underlying training data set is inaccessible. Our approach is\ninspired by the observation that the parameter space of GANs exhibits\nmeaningful directions that can be leveraged to suppress specific undesired\nfeatures. However, such directions usually result in the degradation of the\nquality of generated samples. Our proposed two-stage method, known as\n'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also\nmaintaining the quality of generated samples. In the initial stage, we adapt a\npre-trained GAN on a set of negative samples (containing undesired features)\nprovided by the user. Subsequently, we train the original pre-trained GAN using\npositive samples, along with a repulsion regularizer. This regularizer\nencourages the learned model parameters to move away from the parameters of the\nadapted model (first stage) while not degrading the generation quality. We\nprovide theoretical insights into the proposed method. To the best of our\nknowledge, our approach stands as the first method addressing unlearning within\nthe realm of high-fidelity GANs (such as StyleGAN). We validate the\neffectiveness of our method through comprehensive experiments, encompassing\nboth class-level unlearning on the MNIST and AFHQ dataset and feature-level\nunlearning tasks on the CelebA-HQ dataset. Our code and implementation is\navailable at: https://github.com/atriguha/Adapt_Unlearn.\n","authors":["Piyush Tiwary","Atri Guha","Subhodip Panda","Prathosh A. P"],"pdf_url":"https://arxiv.org/pdf/2309.14054v2.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2411.16821v2","updated":"2025-02-12T06:54:53Z","published":"2024-11-25T17:15:41Z","title":"KL-geodesics flow matching with a novel sampling scheme","summary":"  Non-autoregressive language models generate all tokens simultaneously,\noffering potential speed advantages over traditional autoregressive models, but\nthey face challenges in modeling the complex dependencies inherent in text\ndata. In this work, we investigate a conditional flow matching approach for\ntext generation. We represent tokens as one-hot vectors in a \\(V\\)-dimensional\nsimplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which\ncorrespond to linear interpolation in logit space. We provide a theoretical\njustification that maximizing the conditional likelihood \\(P_{\\theta}(x_1 \\mid\nx_t, t)\\) yields the exact flow matching velocity under logit interpolation. To\naddress the suboptimal performance of basic inference, we propose a novel\nempirical sampling scheme that iteratively samples from the conditional\ndistribution and introduces additional noise, significantly improving results\ndespite lacking full theoretical underpinnings. Furthermore, we propose a\nhybrid inference method that combines the basic approach with the sampling\nscheme. This method demonstrates superior performance on both conditional and\nunconditional text generation experiments compared to previous SOTA method for\ndiscrete flow matching.\n","authors":["Egor Sevriugov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2411.16821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05722v2","updated":"2025-02-12T06:48:01Z","published":"2025-02-08T23:35:37Z","title":"Explainable and Class-Revealing Signal Feature Extraction via Scattering\n  Transform and Constrained Zeroth-Order Optimization","summary":"  We propose a new method to extract discriminant and explainable features from\na particular machine learning model, i.e., a combination of the scattering\ntransform and the multiclass logistic regression. Although this model is\nwell-known for its ability to learn various signal classes with high\nclassification rate, it remains elusive to understand why it can generate such\nsuccessful classification, mainly due to the nonlinearity of the scattering\ntransform. In order to uncover the meaning of the scattering transform\ncoefficients selected by the multiclass logistic regression (with the Lasso\npenalty), we adopt zeroth-order optimization algorithms to search an input\npattern that maximizes the class probability of a class of interest given the\nlearned model. In order to do so, it turns out that imposing sparsity and\nsmoothness of input patterns is important. We demonstrate the effectiveness of\nour proposed method using a couple of synthetic time-series classification\nproblems.\n","authors":["Naoki Saito","David Weber"],"pdf_url":"https://arxiv.org/pdf/2502.05722v2.pdf","comment":"5 pages; 6 figures; submitted to 2025 IEEE Statistical Signal\n  Processing Workshop"},{"id":"http://arxiv.org/abs/2405.18253v2","updated":"2025-02-12T06:47:33Z","published":"2024-05-28T15:04:17Z","title":"Proper Dataset Valuation by Pointwise Mutual Information","summary":"  Data plays a central role in the development of modern artificial\nintelligence, with high-quality data emerging as a key driver of model\nperformance. This has prompted the development of various data curation methods\nin recent years. However, measuring the effectiveness of these data curation\ntechniques remains a major challenge. Traditional evaluation methods, which\nassess a trained model's performance on specific benchmarks, risk promoting\npractices that merely make the data more similar to the test data. This issue\nexemplifies Goodhart's law: when a measure becomes a target, it ceases to be a\ngood measure. To address this, we propose an information-theoretic framework\nfor evaluating data curation methods, where dataset quality is measured by its\ninformativeness about the true model parameters using the Blackwell ordering.\nWe compare informativeness by the Shannon mutual information of the evaluated\ndata and the test data, and we propose a novel method for estimating the mutual\ninformation of datasets by training Bayesian models on embedded data and\ncomputing the mutual information from the model's parameter posteriors.\nExperiments on real-world data demonstrate that our mutual information-based\nevaluation assigns appropriately lower scores to data curation strategies that\nreduce dataset informativeness, while traditional test score-based evaluation\nmethods may favor data curation strategies that overfit to the test set but\ncompromise the training data's informativeness.\n","authors":["Shuran Zheng","Xuan Qi","Rui Ray Chen","Yongchan Kwon","James Zou"],"pdf_url":"https://arxiv.org/pdf/2405.18253v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08155v1","updated":"2025-02-12T06:47:25Z","published":"2025-02-12T06:47:25Z","title":"DGSense: A Domain Generalization Framework for Wireless Sensing","summary":"  Wireless sensing is of great benefits to our daily lives. However, wireless\nsignals are sensitive to the surroundings. Various factors, e.g. environments,\nlocations, and individuals, may induce extra impact on wireless propagation.\nSuch a change can be regarded as a domain, in which the data distribution\nshifts. A vast majority of the sensing schemes are learning-based. They are\ndependent on the training domains, resulting in performance degradation in\nunseen domains. Researchers have proposed various solutions to address this\nissue. But these solutions leverage either semi-supervised or unsupervised\ndomain adaptation techniques. They still require some data in the target\ndomains and do not perform well in unseen domains. In this paper, we propose a\ndomain generalization framework DGSense, to eliminate the domain dependence\nproblem in wireless sensing. The framework is a general solution working across\ndiverse sensing tasks and wireless technologies. Once the sensing model is\nbuilt, it can generalize to unseen domains without any data from the target\ndomain. To achieve the goal, we first increase the diversity of the training\nset by a virtual data generator, and then extract the domain independent\nfeatures via episodic training between the main feature extractor and the\ndomain feature extractors. The feature extractors employ a pre-trained Residual\nNetwork (ResNet) with an attention mechanism for spatial features, and a 1D\nConvolutional Neural Network (1DCNN) for temporal features. To demonstrate the\neffectiveness and generality of DGSense, we evaluated on WiFi gesture\nrecognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall\ndetection. All the systems exhibited high generalization capability to unseen\ndomains, including new users, locations, and environments, free of new data and\nretraining.\n","authors":["Rui Zhou","Yu Cheng","Songlin Li","Hongwang Zhang","Chenxu Liu"],"pdf_url":"https://arxiv.org/pdf/2502.08155v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2411.16769v2","updated":"2025-02-12T06:39:07Z","published":"2024-11-25T04:17:24Z","title":"In-Context Experience Replay Facilitates Safety Red-Teaming of\n  Text-to-Image Diffusion Models","summary":"  Text-to-image (T2I) models have shown remarkable progress, but their\npotential to generate harmful content remains a critical concern in the ML\ncommunity. While various safety mechanisms have been developed, the field lacks\nsystematic tools for evaluating their effectiveness against real-world misuse\nscenarios. In this work, we propose ICER, a novel red-teaming framework that\nleverages Large Language Models (LLMs) and a bandit optimization-based\nalgorithm to generate interpretable and semantic meaningful problematic prompts\nby learning from past successful red-teaming attempts. Our ICER efficiently\nprobes safety mechanisms across different T2I models without requiring internal\naccess or additional training, making it broadly applicable to deployed\nsystems. Through extensive experiments, we demonstrate that ICER significantly\noutperforms existing prompt attack methods in identifying model vulnerabilities\nwhile maintaining high semantic similarity with intended content. By uncovering\nthat successful jailbreaking instances can systematically facilitate the\ndiscovery of new vulnerabilities, our work provides crucial insights for\ndeveloping more robust safety mechanisms in T2I systems.\n","authors":["Zhi-Yi Chin","Mario Fritz","Pin-Yu Chen","Wei-Chen Chiu"],"pdf_url":"https://arxiv.org/pdf/2411.16769v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08151v1","updated":"2025-02-12T06:37:26Z","published":"2025-02-12T06:37:26Z","title":"Local Differential Privacy is Not Enough: A Sample Reconstruction Attack\n  against Federated Learning with Local Differential Privacy","summary":"  Reconstruction attacks against federated learning (FL) aim to reconstruct\nusers' samples through users' uploaded gradients. Local differential privacy\n(LDP) is regarded as an effective defense against various attacks, including\nsample reconstruction in FL, where gradients are clipped and perturbed.\nExisting attacks are ineffective in FL with LDP since clipped and perturbed\ngradients obliterate most sample information for reconstruction. Besides,\nexisting attacks embed additional sample information into gradients to improve\nthe attack effect and cause gradient expansion, leading to a more severe\ngradient clipping in FL with LDP. In this paper, we propose a sample\nreconstruction attack against LDP-based FL with any target models to\nreconstruct victims' sensitive samples to illustrate that FL with LDP is not\nflawless. Considering gradient expansion in reconstruction attacks and noise in\nLDP, the core of the proposed attack is gradient compression and reconstructed\nsample denoising. For gradient compression, an inference structure based on\nsample characteristics is presented to reduce redundant gradients against LDP.\nFor reconstructed sample denoising, we artificially introduce zero gradients to\nobserve noise distribution and scale confidence interval to filter the noise.\nTheoretical proof guarantees the effectiveness of the proposed attack.\nEvaluations show that the proposed attack is the only attack that reconstructs\nvictims' training samples in LDP-based FL and has little impact on the target\nmodel's accuracy. We conclude that LDP-based FL needs further improvements to\ndefend against sample reconstruction attacks effectively.\n","authors":["Zhichao You","Xuewen Dong","Shujun Li","Ximeng Liu","Siqi Ma","Yulong Shen"],"pdf_url":"https://arxiv.org/pdf/2502.08151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08150v1","updated":"2025-02-12T06:30:01Z","published":"2025-02-12T06:30:01Z","title":"Force Matching with Relativistic Constraints: A Physics-Inspired\n  Approach to Stable and Efficient Generative Modeling","summary":"  This paper introduces Force Matching (ForM), a novel framework for generative\nmodeling that represents an initial exploration into leveraging special\nrelativistic mechanics to enhance the stability of the sampling process. By\nincorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring\nthat sample velocities remain bounded within a constant limit. This constraint\nserves as a fundamental mechanism for stabilizing the generative dynamics,\nleading to a more robust and controlled sampling process. We provide a rigorous\ntheoretical analysis demonstrating that the velocity constraint is preserved\nthroughout the sampling procedure within the ForM framework. To validate the\neffectiveness of our approach, we conduct extensive empirical evaluations. On\nthe \\textit{half-moons} dataset, ForM significantly outperforms baseline\nmethods, achieving the lowest Euclidean distance loss of \\textbf{0.714}, in\ncontrast to vanilla first-order flow matching (5.853) and first- and\nsecond-order flow matching (5.793). Additionally, we perform an ablation study\nto further investigate the impact of our velocity constraint, reaffirming the\nsuperiority of ForM in stabilizing the generative process. The theoretical\nguarantees and empirical results underscore the potential of integrating\nspecial relativity principles into generative modeling. Our findings suggest\nthat ForM provides a promising pathway toward achieving stable, efficient, and\nflexible generative processes. This work lays the foundation for future\nadvancements in high-dimensional generative modeling, opening new avenues for\nthe application of physical principles in machine learning.\n","authors":["Yang Cao","Bo Chen","Xiaoyu Li","Yingyu Liang","Zhizhou Sha","Zhenmei Shi","Zhao Song","Mingda Wan"],"pdf_url":"https://arxiv.org/pdf/2502.08150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13155v2","updated":"2025-02-12T06:15:48Z","published":"2024-09-20T01:45:10Z","title":"Convergence of Distributed Adaptive Optimization with Local Updates","summary":"  We study distributed adaptive algorithms with local updates (intermittent\ncommunication). Despite the great empirical success of adaptive methods in\ndistributed training of modern machine learning models, the theoretical\nbenefits of local updates within adaptive methods, particularly in terms of\nreducing communication complexity, have not been fully understood yet. In this\npaper, for the first time, we prove that \\em Local SGD \\em with momentum (\\em\nLocal \\em SGDM) and \\em Local \\em Adam can outperform their minibatch\ncounterparts in convex and weakly convex settings in certain regimes,\nrespectively. Our analysis relies on a novel technique to prove contraction\nduring local iterations, which is a crucial yet challenging step to show the\nadvantages of local updates, under generalized smoothness assumption and\ngradient clipping strategy.\n","authors":["Ziheng Cheng","Margalit Glasgow"],"pdf_url":"https://arxiv.org/pdf/2409.13155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00361v2","updated":"2025-02-12T06:10:33Z","published":"2025-02-01T07:55:06Z","title":"Soft Diffusion Actor-Critic: Efficient Online Reinforcement Learning for\n  Diffusion Policy","summary":"  Diffusion policies have achieved superior performance in imitation learning\nand offline reinforcement learning (RL) due to their rich expressiveness.\nHowever, the vanilla diffusion training procedure requires samples from target\ndistribution, which is impossible in online RL since we cannot sample from the\noptimal policy, making training diffusion policies highly non-trivial in online\nRL. Backpropagating policy gradient through the diffusion process incurs huge\ncomputational costs and instability, thus being expensive and impractical. To\nenable efficient diffusion policy training for online RL, we propose Soft\nDiffusion Actor-Critic (SDAC), exploiting the viewpoint of diffusion models as\nnoise-perturbed energy-based models. The proposed SDAC relies solely on the\nstate-action value function as the energy functions to train diffusion\npolicies, bypassing sampling from the optimal policy while maintaining\nlightweight computations. We conducted comprehensive comparisons on MuJoCo\nbenchmarks. The empirical results show that SDAC outperforms all recent\ndiffusion-policy online RLs on most tasks, and improves more than 120% over\nsoft actor-critic on complex locomotion tasks such as Humanoid and Ant.\n","authors":["Haitong Ma","Tianyi Chen","Kai Wang","Na Li","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2502.00361v2.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.08146v1","updated":"2025-02-12T06:09:27Z","published":"2025-02-12T06:09:27Z","title":"Knowledge-Guided Wasserstein Distributionally Robust Optimization","summary":"  Transfer learning is a popular strategy to leverage external knowledge and\nimprove statistical efficiency, particularly with a limited target sample. We\npropose a novel knowledge-guided Wasserstein Distributionally Robust\nOptimization (KG-WDRO) framework that adaptively incorporates multiple sources\nof external knowledge to overcome the conservativeness of vanilla WDRO, which\noften results in overly pessimistic shrinkage toward zero. Our method\nconstructs smaller Wasserstein ambiguity sets by controlling the transportation\nalong directions informed by the source knowledge. This strategy can alleviate\nperturbations on the predictive projection of the covariates and protect\nagainst information loss. Theoretically, we establish the equivalence between\nour WDRO formulation and the knowledge-guided shrinkage estimation based on\ncollinear similarity, ensuring tractability and geometrizing the feasible set.\nThis also reveals a novel and general interpretation for recent shrinkage-based\ntransfer learning approaches from the perspective of distributional robustness.\nIn addition, our framework can adjust for scaling differences in the regression\nmodels between the source and target and accommodates general types of\nregularization such as lasso and ridge. Extensive simulations demonstrate the\nsuperior performance and adaptivity of KG-WDRO in enhancing small-sample\ntransfer learning.\n","authors":["Zitao Wang","Ziyuan Wang","Molei Liu","Nian Si"],"pdf_url":"https://arxiv.org/pdf/2502.08146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13135v8","updated":"2025-02-12T06:07:40Z","published":"2023-09-22T18:43:41Z","title":"Global Deep Forecasting with Patient-Specific Pharmacokinetics","summary":"  Forecasting healthcare time series data is vital for early detection of\nadverse outcomes and patient monitoring. However, it can be challenging in\npractice due to variable medication administration and unique pharmacokinetic\n(PK) properties of each patient. To address these challenges, we propose a\nnovel hybrid global-local architecture and a PK encoder that informs deep\nlearning models of patient-specific treatment effects. We showcase the efficacy\nof our approach in achieving significant accuracy gains in a blood glucose\nforecasting task using both realistically simulated and real-world data. Our PK\nencoder surpasses baselines by up to 16.4% on simulated data and 4.9% on\nreal-world data for individual patients during critical events of severely high\nand low glucose levels. Furthermore, our proposed hybrid global-local\narchitecture outperforms patient-specific PK models by 15.8%, on average.\n","authors":["Willa Potosnak","Cristian Challu","Kin G. Olivares","Keith A. Dufendach","Artur Dubrawski"],"pdf_url":"https://arxiv.org/pdf/2309.13135v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08145v1","updated":"2025-02-12T06:05:52Z","published":"2025-02-12T06:05:52Z","title":"Democratizing AI: Open-source Scalable LLM Training on GPU-based\n  Supercomputers","summary":"  Training and fine-tuning large language models (LLMs) with hundreds of\nbillions to trillions of parameters requires tens of thousands of GPUs, and a\nhighly scalable software stack. In this work, we present a novel\nfour-dimensional hybrid parallel algorithm implemented in a highly scalable,\nportable, open-source framework called AxoNN. We describe several performance\noptimizations in AxoNN to improve matrix multiply kernel performance, overlap\nnon-blocking collectives with computation, and performance modeling to choose\nperformance optimal configurations. These have resulted in unprecedented\nscaling and peak flop/s (bf16) for training of GPT-style transformer models on\nPerlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423\nExaflop/s).\n  While the abilities of LLMs improve with the number of trainable parameters,\nso do privacy and copyright risks caused by memorization of training data,\nwhich can cause disclosure of sensitive or private information at inference\ntime. We highlight this side effect of scale through experiments that explore\n\"catastrophic memorization\", where models are sufficiently large to memorize\ntraining data in a single pass, and present an approach to prevent it. As part\nof this study, we demonstrate fine-tuning of a 405-billion parameter LLM using\nAxoNN on Frontier.\n","authors":["Siddharth Singh","Prajwal Singhania","Aditya Ranjan","John Kirchenbauer","Jonas Geiping","Yuxin Wen","Neel Jain","Abhimanyu Hans","Manli Shu","Aditya Tomar","Tom Goldstein","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2502.08145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20971v2","updated":"2025-02-12T05:52:11Z","published":"2024-10-28T12:43:47Z","title":"BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against\n  Jailbreak Attacks","summary":"  In this paper, we focus on black-box defense for VLMs against jailbreak\nattacks. Existing black-box defense methods are either unimodal or bimodal.\nUnimodal methods enhance either the vision or language module of the VLM, while\nbimodal methods robustify the model through text-image representation\nrealignment. However, these methods suffer from two limitations: 1) they fail\nto fully exploit the cross-modal information, or 2) they degrade the model\nperformance on benign inputs. To address these limitations, we propose a novel\nblue-team method BlueSuffix that defends target VLMs against jailbreak attacks\nwithout compromising its performance under black-box setting. BlueSuffix\nincludes three key components: 1) a visual purifier against jailbreak images,\n2) a textual purifier against jailbreak texts, and 3) a blue-team suffix\ngenerator using reinforcement fine-tuning for enhancing cross-modal robustness.\nWe empirically show on four VLMs (LLaVA, MiniGPT-4, InstructionBLIP, and\nGemini) and four safety benchmarks (Harmful Instruction, AdvBench,\nMM-SafetyBench, and RedTeam-2K) that BlueSuffix outperforms the baseline\ndefenses by a significant margin. Our BlueSuffix opens up a promising direction\nfor defending VLMs against jailbreak attacks. Code is available at\nhttps://github.com/Vinsonzyh/BlueSuffix.\n","authors":["Yunhan Zhao","Xiang Zheng","Lin Luo","Yige Li","Xingjun Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.20971v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08143v1","updated":"2025-02-12T05:48:57Z","published":"2025-02-12T05:48:57Z","title":"Data-dependent Bounds with $T$-Optimal Best-of-Both-Worlds Guarantees in\n  Multi-Armed Bandits using Stability-Penalty Matching","summary":"  Existing data-dependent and best-of-both-worlds regret bounds for multi-armed\nbandits problems have limited adaptivity as they are either data-dependent but\nnot best-of-both-worlds (BOBW), BOBW but not data-dependent or have sub-optimal\n$O(\\sqrt{T\\ln{T}})$ worst-case guarantee in the adversarial regime. To overcome\nthese limitations, we propose real-time stability-penalty matching (SPM), a new\nmethod for obtaining regret bounds that are simultaneously data-dependent,\nbest-of-both-worlds and $T$-optimal for multi-armed bandits problems. In\nparticular, we show that real-time SPM obtains bounds with worst-case\nguarantees of order $O(\\sqrt{T})$ in the adversarial regime and $O(\\ln{T})$ in\nthe stochastic regime while simultaneously being adaptive to data-dependent\nquantities such as sparsity, variations, and small losses. Our results are\nobtained by extending the SPM technique for tuning the learning rates in the\nfollow-the-regularized-leader (FTRL) framework, which further indicates that\nthe combination of SPM and FTRL is a promising approach for proving new\nadaptive bounds in online learning problems.\n","authors":["Quan Nguyen","Shinji Ito","Junpei Komiyama","Nishant A. Mehta"],"pdf_url":"https://arxiv.org/pdf/2502.08143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08141v1","updated":"2025-02-12T05:48:26Z","published":"2025-02-12T05:48:26Z","title":"LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits","summary":"  Fine-tuning large language models (LLMs) is increasingly costly as models\nscale to hundreds of billions of parameters, and even parameter-efficient\nfine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce\nLowRA, the first framework to enable LoRA fine-tuning below 2 bits per\nparameter with minimal performance loss. LowRA optimizes fine-grained\nquantization - mapping, threshold selection, and precision assignment - while\nleveraging efficient CUDA kernels for scalable deployment. Extensive\nevaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior\nperformance-precision trade-off above 2 bits and remains accurate down to 1.15\nbits, reducing memory usage by up to 50%. Our results highlight the potential\nof ultra-low-bit LoRA fine-tuning for resource-constrained environments.\n","authors":["Zikai Zhou","Qizheng Zhang","Hermann Kumbong","Kunle Olukotun"],"pdf_url":"https://arxiv.org/pdf/2502.08141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14415v2","updated":"2025-02-12T05:41:42Z","published":"2024-12-19T00:06:09Z","title":"DriveGPT: Scaling Autoregressive Behavior Models for Driving","summary":"  We present DriveGPT, a scalable behavior model for autonomous driving. We\nmodel driving as a sequential decision-making task, and learn a transformer\nmodel to predict future agent states as tokens in an autoregressive fashion. We\nscale up our model parameters and training data by multiple orders of\nmagnitude, enabling us to explore the scaling properties in terms of dataset\nsize, model parameters, and compute. We evaluate DriveGPT across different\nscales in a planning task, through both quantitative metrics and qualitative\nexamples, including closed-loop driving in complex real-world scenarios. In a\nseparate prediction task, DriveGPT outperforms state-of-the-art baselines and\nexhibits improved performance by pretraining on a large-scale dataset, further\nvalidating the benefits of data scaling.\n","authors":["Xin Huang","Eric M. Wolff","Paul Vernaza","Tung Phan-Minh","Hongge Chen","David S. Hayden","Mark Edmonds","Brian Pierce","Xinxin Chen","Pratik Elias Jacob","Xiaobai Chen","Chingiz Tairbekov","Pratik Agarwal","Tianshi Gao","Yuning Chai","Siddhartha Srinivasa"],"pdf_url":"https://arxiv.org/pdf/2412.14415v2.pdf","comment":"13 pages, 16 figures, 8 tables, and 1 video link"},{"id":"http://arxiv.org/abs/2502.08136v1","updated":"2025-02-12T05:40:11Z","published":"2025-02-12T05:40:11Z","title":"In-Context Learning of Linear Dynamical Systems with Transformers: Error\n  Bounds and Depth-Separation","summary":"  This paper investigates approximation-theoretic aspects of the in-context\nlearning capability of the transformers in representing a family of noisy\nlinear dynamical systems. Our first theoretical result establishes an upper\nbound on the approximation error of multi-layer transformers with respect to an\n$L^2$-testing loss uniformly defined across tasks. This result demonstrates\nthat transformers with logarithmic depth can achieve error bounds comparable\nwith those of the least-squares estimator. In contrast, our second result\nestablishes a non-diminishing lower bound on the approximation error for a\nclass of single-layer linear transformers, which suggests a depth-separation\nphenomenon for transformers in the in-context learning of dynamical systems.\nMoreover, this second result uncovers a critical distinction in the\napproximation power of single-layer linear transformers when learning from IID\nversus non-IID data.\n","authors":["Frank Cole","Yulong Lu","Tianhao Zhang","Yuxuan Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.08136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14249v3","updated":"2025-02-12T05:39:05Z","published":"2025-01-24T05:27:46Z","title":"Humanity's Last Exam","summary":"  Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 3,000\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.\n","authors":["Long Phan","Alice Gatti","Ziwen Han","Nathaniel Li","Josephina Hu","Hugh Zhang","Chen Bo Calvin Zhang","Mohamed Shaaban","John Ling","Sean Shi","Michael Choi","Anish Agrawal","Arnav Chopra","Adam Khoja","Ryan Kim","Richard Ren","Jason Hausenloy","Oliver Zhang","Mantas Mazeika","Tung Nguyen","Daron Anderson","Imad Ali Shah","Mikhail Doroshenko","Alun Cennyth Stokes","Mobeen Mahmood","Jaeho Lee","Oleksandr Pokutnyi","Oleg Iskra","Jessica P. Wang","Robert Gerbicz","John-Clark Levin","Serguei Popov","Fiona Feng","Steven Y. Feng","Haoran Zhao","Michael Yu","Varun Gangal","Chelsea Zou","Zihan Wang","Mstyslav Kazakov","Geoff Galgon","Johannes Schmitt","Alvaro Sanchez","Yongki Lee","Will Yeadon","Scott Sauers","Marc Roth","Chidozie Agu","S√∏ren Riis","Fabian Giska","Saiteja Utpala","Antrell Cheatom","Zachary Giboney","Gashaw M. Goshu","Sarah-Jane Crowson","Mohinder Maheshbhai Naiya","Noah Burns","Lennart Finke","Zerui Cheng","Hyunwoo Park","Francesco Fournier-Facio","Jennifer Zampese","John Wydallis","John B. Wydallis","Ryan G. Hoerr","Mark Nandor","Tim Gehrunger","Jiaqi Cai","Ben McCarty","Jungbae Nam","Edwin Taylor","Jun Jin","Gautier Abou Loume","Hangrui Cao","Alexis C Garretson","Damien Sileo","Qiuyu Ren","Doru Cojoc","Pavel Arkhipov","Usman Qazi","Aras Bacho","Lianghui Li","Sumeet Motwani","Christian Schroeder de Witt","Alexei Kopylov","Johannes Veith","Eric Singer","Paolo Rissone","Jaehyeok Jin","Jack Wei Lun Shi","Chris G. Willcocks","Ameya Prabhu","Longke Tang","Kevin Zhou","Emily de Oliveira Santos","Andrey Pupasov Maksimov","Edward Vendrow","Kengo Zenitani","Joshua Robinson","Aleksandar Mikov","Julien Guillod","Yuqi Li","Ben Pageler","Joshua Vendrow","Vladyslav Kuchkin","Pierre Marion","Denis Efremov","Jayson Lynch","Kaiqu Liang","Andrew Gritsevskiy","Dakotah Martinez","Nick Crispino","Dimitri Zvonkine","Natanael Wildner Fraga","Saeed Soori","Ori Press","Henry Tang","Julian Salazar","Sean R. Green","Lina Br√ºssel","Moon Twayana","Aymeric Dieuleveut","T. Ryan Rogers","Wenjin Zhang","Ross Finocchio","Bikun Li","Jinzhou Yang","Arun Rao","Gabriel Loiseau","Mikhail Kalinin","Marco Lukas","Ciprian Manolescu","Nate Stambaugh","Subrata Mishra","Ariel Ghislain Kemogne Kamdoum","Tad Hogg","Alvin Jin","Carlo Bosio","Gongbo Sun","Brian P Coppola","Haline Heidinger","Rafael Sayous","Stefan Ivanov","Joseph M Cavanagh","Jiawei Shen","Joseph Marvin Imperial","Philippe Schwaller","Shaipranesh Senthilkuma","Andres M Bran","Andres Algaba","Brecht Verbeken","Kelsey Van den Houte","Lynn Van Der Sypt","David Noever","Lisa Schut","Ilia Sucholutsky","Evgenii Zheltonozhskii","Qiaochu Yuan","Derek Lim","Richard Stanley","Shankar Sivarajan","Tong Yang","John Maar","Julian Wykowski","Mart√≠ Oller","Jennifer Sandlin","Anmol Sahu","Cesare Giulio Ardito","Yuzheng Hu","Felipe Meneguitti Dias","Tobias Kreiman","Kaivalya Rawal","Tobias Garcia Vilchis","Yuexuan Zu","Martin Lackner","James Koppel","Jeremy Nguyen","Daniil S. Antonenko","Steffi Chern","Bingchen Zhao","Pierrot Arsene","Sergey Ivanov","Rafa≈Ç Po≈õwiata","Chenguang Wang","Daofeng Li","Donato Crisostomi","Ali Dehghan","Andrea Achilleos","John Arnold Ambay","Benjamin Myklebust","Archan Sen","David Perrella","Nurdin Kaparov","Mark H Inlow","Allen Zang","Kalyan Ramakrishnan","Daniil Orel","Vladislav Poritski","Shalev Ben-David","Zachary Berger","Parker Whitfill","Michael Foster","Daniel Munro","Linh Ho","Dan Bar Hava","Aleksey Kuchkin","Robert Lauff","David Holmes","Frank Sommerhage","Anji Zhang","Richard Moat","Keith Schneider","Daniel Pyda","Zakayo Kazibwe","Mukhwinder Singh","Don Clarke","Dae Hyun Kim","Sara Fish","Veit Elser","Victor Efren Guadarrama Vilchis","Immo Klose","Christoph Demian","Ujjwala Anantheswaran","Adam Zweiger","Guglielmo Albani","Jeffery Li","Nicolas Daans","Maksim Radionov","V√°clav Rozho≈à","Vincent Ginis","Ziqiao Ma","Christian Stump","Jacob Platnick","Volodymyr Nevirkovets","Luke Basler","Marco Piccardo","Niv Cohen","Virendra Singh","Josef Tkadlec","Paul Rosu","Alan Goldfarb","Piotr Padlewski","Stanislaw Barzowski","Kyle Montgomery","Aline Menezes","Arkil Patel","Zixuan Wang","Jamie Tucker-Foltz","Jack Stade","Declan Grabb","Tom Goertzen","Fereshteh Kazemi","Jeremiah Milbauer","Abhishek Shukla","Hossam Elgnainy","Yan Carlos Leyva Labrador","Hao He","Ling Zhang","Alan Givr√©","Hew Wolff","G√∂zdenur Demir","Muhammad Fayez Aziz","Younesse Kaddar","Ivar √Ñngquist","Yanxu Chen","Elliott Thornley","Robin Zhang","Jiayi Pan","Antonio Terpin","Niklas Muennighoff","Hailey Schoelkopf","Eric Zheng","Avishy Carmi","Jainam Shah","Ethan D. L. Brown","Kelin Zhu","Max Bartolo","Richard Wheeler","Andrew Ho","Shaul Barkan","Jiaqi Wang","Martin Stehberger","Egor Kretov","Peter Bradshaw","JP Heimonen","Kaustubh Sridhar","Zaki Hossain","Ido Akov","Yury Makarychev","Joanna Tam","Hieu Hoang","David M. Cunningham","Vladimir Goryachev","Demosthenes Patramanis","Michael Krause","Andrew Redenti","David Aldous","Jesyin Lai","Shannon Coleman","Jiangnan Xu","Sangwon Lee","Ilias Magoulas","Sandy Zhao","Ning Tang","Michael K. Cohen","Micah Carroll","Orr Paradise","Jan Hendrik Kirchner","Stefan Steinerberger","Maksym Ovchynnikov","Jason O. Matos","Adithya Shenoy","Michael Wang","Yuzhou Nie","Paolo Giordano","Philipp Petersen","Anna Sztyber-Betley","Paolo Faraboschi","Robin Riblet","Jonathan Crozier","Shiv Halasyamani","Antonella Pinto","Shreyas Verma","Prashant Joshi","Eli Meril","Zheng-Xin Yong","Allison Tee","J√©r√©my Andr√©oletti","Orion Weller","Raghav Singhal","Gang Zhang","Alexander Ivanov","Seri Khoury","Nils Gustafsson","Hamid Mostaghimi","Kunvar Thaman","Qijia Chen","Tran Quoc Kh√°nh","Jacob Loader","Stefano Cavalleri","Hannah Szlyk","Zachary Brown","Himanshu Narayan","Jonathan Roberts","William Alley","Kunyang Sun","Ryan Stendall","Max Lamparth","Anka Reuel","Ting Wang","Hanmeng Xu","Pablo Hern√°ndez-C√°mara","Freddie Martin","Thomas Preu","Tomek Korbak","Marcus Abramovitch","Dominic Williamson","Ida Bosio","Ziye Chen","Bir√≥ B√°lint","Eve J. Y. Lo","Maria In√™s S. Nunes","Yibo Jiang","M Saiful Bari","Peyman Kassani","Zihao Wang","Behzad Ansarinejad","Yewen Sun","Stephane Durand","Guillaume Douville","Daniel Tordera","George Balabanian","Earth Anderson","Lynna Kvistad","Alejandro Jos√© Moyano","Hsiaoyun Milliron","Ahmad Sakor","Murat Eron","Isaac C. McAlister","Andrew Favre D. O.","Shailesh Shah","Xiaoxiang Zhou","Firuz Kamalov","Ronald Clark","Sherwin Abdoli","Tim Santens","Harrison K Wang","Evan Chen","Alessandro Tomasiello","G. Bruno De Luca","Shi-Zhuo Looi","Vinh-Kha Le","Noam Kolt","Niels M√ºndler","Avi Semler","Emma Rodman","Jacob Drori","Carl J Fossum","Luk Gloor","Milind Jagota","Ronak Pradeep","Honglu Fan","Tej Shah","Jonathan Eicher","Michael Chen","Kushal Thaman","William Merrill","Moritz Firsching","Carter Harris","Stefan Ciob√¢cƒÉ","Jason Gross","Rohan Pandey","Ilya Gusev","Adam Jones","Shashank Agnihotri","Pavel Zhelnov","Siranut Usawasutsakorn","Mohammadreza Mofayezi","Alexander Piperski","Marc Carauleanu","David K. Zhang","Kostiantyn Dobarskyi","Dylan Ler","Roman Leventov","Ignat Soroko","Thorben Jansen","Scott Creighton","Pascal Lauer","Joshua Duersch","Vage Taamazyan","Dario Bezzi","Wiktor Morak","Wenjie Ma","William Held","Tran ƒêuc Huy","Ruicheng Xian","Armel Randy Zebaze","Mohanad Mohamed","Julian Noah Leser","Michelle X Yuan","Laila Yacar","Johannes Lengler","Katarzyna Olszewska","Hossein Shahrtash","Edson Oliveira","Joseph W. Jackson","Daniel Espinosa Gonzalez","Andy Zou","Muthu Chidambaram","Timothy Manik","Hector Haffenden","Dashiell Stander","Ali Dasouqi","Alexander Shen","Emilien Duc","Bita Golshani","David Stap","Mikalai Uzhou","Alina Borisovna Zhidkovskaya","Lukas Lewark","Miguel Orbegozo Rodriguez","M√°ty√°s Vincze","Dustin Wehr","Colin Tang","Shaun Phillips","Fortuna Samuele","Jiang Muzhen","Fredrik Ekstr√∂m","Angela Hammon","Oam Patel","Faraz Farhidi","George Medley","Forough Mohammadzadeh","Madellene Pe√±aflor","Haile Kassahun","Alena Friedrich","Claire Sparrow","Rayner Hernandez Perez","Taom Sakal","Omkar Dhamane","Ali Khajegili Mirabadi","Eric Hallman","Kenchi Okutsu","Mike Battaglia","Mohammad Maghsoudimehrabani","Alon Amit","Dave Hulbert","Roberto Pereira","Simon Weber"," Handoko","Anton Peristyy","Stephen Malina","Samuel Albanie","Will Cai","Mustafa Mehkary","Rami Aly","Frank Reidegeld","Anna-Katharina Dick","Cary Friday","Jasdeep Sidhu","Hassan Shapourian","Wanyoung Kim","Mariana Costa","Hubeyb Gurdogan","Brian Weber","Harsh Kumar","Tong Jiang","Arunim Agarwal","Chiara Ceconello","Warren S. Vaz","Chao Zhuang","Haon Park","Andrew R. Tawfeek","Daattavya Aggarwal","Michael Kirchhof","Linjie Dai","Evan Kim","Johan Ferret","Yuzhou Wang","Minghao Yan","Krzysztof Burdzy","Lixin Zhang","Antonio Franca","Diana T. Pham","Kang Yong Loh","Joshua Robinson","Abram Jackson","Shreen Gul","Gunjan Chhablani","Zhehang Du","Adrian Cosma","Jesus Colino","Colin White","Jacob Votava","Vladimir Vinnikov","Ethan Delaney","Petr Spelda","Vit Stritecky","Syed M. Shahid","Jean-Christophe Mourrat","Lavr Vetoshkin","Koen Sponselee","Renas Bacho","Florencia de la Rosa","Xiuyu Li","Guillaume Malod","Leon Lang","Julien Laurendeau","Dmitry Kazakov","Fatimah Adesanya","Julien Portier","Lawrence Hollom","Victor Souza","Yuchen Anna Zhou","Julien Degorre","Yiƒüit Yalƒ±n","Gbenga Daniel Obikoya","Luca Arnaboldi"," Rai","Filippo Bigi","M. C. Bosc√°","Oleg Shumar","Kaniuar Bacho","Pierre Clavier","Gabriel Recchia","Mara Popescu","Nikita Shulga","Ngefor Mildred Tanwie","Denis Peskoff","Thomas C. H. Lux","Ben Rank","Colin Ni","Matthew Brooks","Alesia Yakimchyk"," Huanxu"," Liu","Olle H√§ggstr√∂m","Emil Verkama","Hans Gundlach","Leonor Brito-Santana","Brian Amaro","Vivek Vajipey","Rynaa Grover","Yiyang Fan","Gabriel Poesia Reis e Silva","Linwei Xin","Yosi Kratish","Jakub ≈Åucki","Wen-Ding Li","Sivakanth Gopi","Andrea Caciolai","Justin Xu","Kevin Joseph Scaria","Freddie Vargus","Farzad Habibi"," Long"," Lian","Emanuele Rodol√†","Jules Robins","Vincent Cheng","Tony Fruhauff","Brad Raynor","Hao Qi","Xi Jiang","Ben Segev","Jingxuan Fan","Sarah Martinson","Erik Y. Wang","Kaylie Hausknecht","Michael P. Brenner","Mao Mao","Xinyu Zhang","David Avagian","Eshawn Jessica Scipio","Alon Ragoler","Justin Tan","Blake Sims","Rebeka Plecnik","Aaron Kirtland","Omer Faruk Bodur","D. P. Shinde","Zahra Adoul","Mohamed Zekry","Ali Karakoc","Tania C. B. Santos","Samir Shamseldeen","Loukmane Karim","Anna Liakhovitskaia","Nate Resman","Nicholas Farina","Juan Carlos Gonzalez","Gabe Maayan","Sarah Hoback","Rodrigo De Oliveira Pena","Glen Sherman","Elizabeth Kelley","Hodjat Mariji","Rasoul Pouriamanesh","Wentao Wu","Sandra Mendoza","Ismail Alarab","Joshua Cole","Danyelle Ferreira","Bryan Johnson","Mohammad Safdari","Liangti Dai","Siriphan Arthornthurasuk","Alexey Pronin","Jing Fan","Angel Ramirez-Trinidad","Ashley Cartwright","Daphiny Pottmaier","Omid Taheri","David Outevsky","Stanley Stepanic","Samuel Perry","Luke Askew","Ra√∫l Adri√°n Huerta Rodr√≠guez","Ali M. R. Minissi","Sam Ali","Ricardo Lorena","Krishnamurthy Iyer","Arshad Anil Fasiludeen","Sk Md Salauddin","Murat Islam","Juan Gonzalez","Josh Ducey","Maja Somrak","Vasilios Mavroudis","Eric Vergo","Juehang Qin","Benj√°min Borb√°s","Eric Chu","Jack Lindsey","Anil Radhakrishnan","Antoine Jallon","I. M. J. McInnis","Pawan Kumar","Laxman Prasad Goswami","Daniel Bugas","Nasser Heydari","Ferenc Jeanplong","Archimedes Apronti","Abdallah Galal","Ng Ze-An","Ankit Singh","Joan of Arc Xavier","Kanu Priya Agarwal","Mohammed Berkani","Benedito Alves de Oliveira Junior","Dmitry Malishev","Nicolas Remy","Taylor D. Hartman","Tim Tarver","Stephen Mensah","Javier Gimenez","Roselynn Grace Montecillo","Russell Campbell","Asankhaya Sharma","Khalida Meer","Xavier Alapont","Deepakkumar Patil","Rajat Maheshwari","Abdelkader Dendane","Priti Shukla","Sergei Bogdanov","S√∂ren M√∂ller","Muhammad Rehan Siddiqi","Prajvi Saxena","Himanshu Gupta","Innocent Enyekwe","Ragavendran P V","Zienab EL-Wasif","Aleksandr Maksapetyan","Vivien Rossbach","Chris Harjadi","Mohsen Bahaloohoreh","Song Bian","John Lai","Justine Leon Uro","Greg Bateman","Mohamed Sayed","Ahmed Menshawy","Darling Duclosel","Yashaswini Jain","Ashley Aaron","Murat Tiryakioglu","Sheeshram Siddh","Keith Krenek","Alex Hoover","Joseph McGowan","Tejal Patwardhan","Summer Yue","Alexandr Wang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2501.14249v3.pdf","comment":"26 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.14654v2","updated":"2025-02-12T05:32:07Z","published":"2025-01-24T17:21:01Z","title":"MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical\n  LLM Agents","summary":"  Recent large language models (LLMs) have demonstrated significant\nadvancements, particularly in their ability to serve as agents thereby\nsurpassing their traditional role as chatbots. These agents can leverage their\nplanning and tool utilization capabilities to address tasks specified at a high\nlevel. However, a standardized dataset to benchmark the agent capabilities of\nLLMs in medical applications is currently lacking, making the evaluation of\nLLMs on complex tasks in interactive healthcare environments challenging. To\naddress this gap, we introduce MedAgentBench, a broad evaluation suite designed\nto assess the agent capabilities of large language models within medical\nrecords contexts. MedAgentBench encompasses 300 patient-specific\nclinically-derived tasks from 10 categories written by human physicians,\nrealistic profiles of 100 patients with over 700,000 data elements, a\nFHIR-compliant interactive environment, and an accompanying codebase. The\nenvironment uses the standard APIs and communication infrastructure used in\nmodern EMR systems, so it can be easily migrated into live EMR systems.\nMedAgentBench presents an unsaturated agent-oriented benchmark that current\nstate-of-the-art LLMs exhibit some ability to succeed at. The best model\n(Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is\nstill substantial space for improvement which gives the community a next\ndirection to optimize. Furthermore, there is significant variation in\nperformance across task categories. MedAgentBench establishes this and is\npublicly available at https://github.com/stanfordmlgroup/MedAgentBench ,\noffering a valuable framework for model developers to track progress and drive\ncontinuous improvements in the agent capabilities of large language models\nwithin the medical domain.\n","authors":["Yixing Jiang","Kameron C. Black","Gloria Geng","Danny Park","James Zou","Andrew Y. Ng","Jonathan H. Chen"],"pdf_url":"https://arxiv.org/pdf/2501.14654v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06832v2","updated":"2025-02-12T05:30:33Z","published":"2025-02-05T20:45:52Z","title":"Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model\n  Approach","summary":"  Mixture of Experts (MoE) have shown remarkable success in leveraging\nspecialized expert networks for complex machine learning tasks. However, their\nsusceptibility to adversarial attacks presents a critical challenge for\ndeployment in robust applications. This paper addresses the critical question\nof how to incorporate robustness into MoEs while maintaining high natural\naccuracy. We begin by analyzing the vulnerability of MoE components, finding\nthat expert networks are notably more susceptible to adversarial attacks than\nthe router. Based on this insight, we propose a targeted robust training\ntechnique that integrates a novel loss function to enhance the adversarial\nrobustness of MoE, requiring only the robustification of one additional expert\nwithout compromising training or inference efficiency. Building on this, we\nintroduce a dual-model strategy that linearly combines a standard MoE model\nwith our robustified MoE model using a smoothing parameter. This approach\nallows for flexible control over the robustness-accuracy trade-off. We further\nprovide theoretical foundations by deriving certified robustness bounds for\nboth the single MoE and the dual-model. To push the boundaries of robustness\nand accuracy, we propose a novel joint training strategy JTDMoE for the\ndual-model. This joint training enhances both robustness and accuracy beyond\nwhat is achievable with separate models. Experimental results on CIFAR-10 and\nTinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures\ndemonstrate the effectiveness of our proposed methods.\n","authors":["Xu Zhang","Kaidi Xu","Ziqing Hu","Ren Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06832v2.pdf","comment":"10 pages, 3 figures, submitted to ICML 2025 (under review)"},{"id":"http://arxiv.org/abs/2502.08132v1","updated":"2025-02-12T05:28:08Z","published":"2025-02-12T05:28:08Z","title":"SS4Rec: Continuous-Time Sequential Recommendation with State Space\n  Models","summary":"  Sequential recommendation is a key area in the field of recommendation\nsystems aiming to model user interest based on historical interaction sequences\nwith irregular intervals. While previous recurrent neural network-based and\nattention-based approaches have achieved significant results, they have\nlimitations in capturing system continuity due to the discrete characteristics.\nIn the context of continuous-time modeling, state space model (SSM) offers a\npotential solution, as it can effectively capture the dynamic evolution of user\ninterest over time. However, existing SSM-based approaches ignore the impact of\nirregular time intervals within historical user interactions, making it\ndifficult to model complexed user-item transitions in sequences. To address\nthis issue, we propose a hybrid SSM-based model called SS4Rec for\ncontinuous-time sequential recommendation. SS4Rec integrates a time-aware SSM\nto handle irregular time intervals and a relation-aware SSM to model contextual\ndependencies, enabling it to infer user interest from both temporal and\nsequential perspectives. In the training process, the time-aware SSM and the\nrelation-aware SSM are discretized by variable stepsizes according to user\ninteraction time intervals and input data, respectively. This helps capture the\ncontinuous dependency from irregular time intervals and provides time-specific\npersonalized recommendations. Experimental studies on five benchmark datasets\ndemonstrate the superiority and effectiveness of SS4Rec.\n","authors":["Wei Xiao","Huiying Wang","Qifeng Zhou","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08125v1","updated":"2025-02-12T05:06:23Z","published":"2025-02-12T05:06:23Z","title":"Incremental Approximate Single-Source Shortest Paths with Predictions","summary":"  The algorithms-with-predictions framework has been used extensively to\ndevelop online algorithms with improved beyond-worst-case competitive ratios.\nRecently, there is growing interest in leveraging predictions for designing\ndata structures with improved beyond-worst-case running times. In this paper,\nwe study the fundamental data structure problem of maintaining approximate\nshortest paths in incremental graphs in the algorithms-with-predictions model.\nGiven a sequence $\\sigma$ of edges that are inserted one at a time, the goal is\nto maintain approximate shortest paths from the source to each vertex in the\ngraph at each time step. Before any edges arrive, the data structure is given a\nprediction of the online edge sequence $\\hat{\\sigma}$ which is used to ``warm\nstart'' its state.\n  As our main result, we design a learned algorithm that maintains\n$(1+\\epsilon)$-approximate single-source shortest paths, which runs in\n$\\tilde{O}(m \\eta \\log W/\\epsilon)$ time, where $W$ is the weight of the\nheaviest edge and $\\eta$ is the prediction error. We show these techniques\nimmediately extend to the all-pairs shortest-path setting as well. Our\nalgorithms are consistent (performing nearly as fast as the offline algorithm)\nwhen predictions are nearly perfect, have a smooth degradation in performance\nwith respect to the prediction error and, in the worst case, match the best\noffline algorithm up to logarithmic factors.\n  As a building block, we study the offline incremental approximate\nsingle-source shortest-paths problem. In this problem, the edge sequence\n$\\sigma$ is known a priori and the goal is to efficiently return the length of\nthe shortest paths in the intermediate graph $G_t$ consisting of the first $t$\nedges, for all $t$. Note that the offline incremental problem is defined in the\nworst-case setting (without predictions) and is of independent interest.\n","authors":["Samuel McCauley","Benjamin Moseley","Aidin Niaparast","Helia Niaparast","Shikha Singh"],"pdf_url":"https://arxiv.org/pdf/2502.08125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08123v1","updated":"2025-02-12T05:05:40Z","published":"2025-02-12T05:05:40Z","title":"Provably Robust Federated Reinforcement Learning","summary":"  Federated reinforcement learning (FRL) allows agents to jointly learn a\nglobal decision-making policy under the guidance of a central server. While FRL\nhas advantages, its decentralized design makes it prone to poisoning attacks.\nTo mitigate this, Byzantine-robust aggregation techniques tailored for FRL have\nbeen introduced. Yet, in our work, we reveal that these current\nByzantine-robust techniques are not immune to our newly introduced Normalized\nattack. Distinct from previous attacks that targeted enlarging the distance of\npolicy updates before and after an attack, our Normalized attack emphasizes on\nmaximizing the angle of deviation between these updates. To counter these\nthreats, we develop an ensemble FRL approach that is provably secure against\nboth known and our newly proposed attacks. Our ensemble method involves\ntraining multiple global policies, where each is learnt by a group of agents\nusing any foundational aggregation rule. These well-trained global policies\nthen individually predict the action for a specific test state. The ultimate\naction is chosen based on a majority vote for discrete action systems or the\ngeometric median for continuous ones. Our experimental results across different\nsettings show that the Normalized attack can greatly disrupt non-ensemble\nByzantine-robust methods, and our ensemble approach offers substantial\nresistance against poisoning attacks.\n","authors":["Minghong Fang","Xilong Wang","Neil Zhenqiang Gong"],"pdf_url":"https://arxiv.org/pdf/2502.08123v1.pdf","comment":"To appear in The Web Conference 2025"},{"id":"http://arxiv.org/abs/2502.08122v1","updated":"2025-02-12T05:03:49Z","published":"2025-02-12T05:03:49Z","title":"Hookpad Aria: A Copilot for Songwriters","summary":"  We present Hookpad Aria, a generative AI system designed to assist musicians\nin writing Western pop songs. Our system is seamlessly integrated into Hookpad,\na web-based editor designed for the composition of lead sheets: symbolic music\nscores that describe melody and harmony. Hookpad Aria has numerous generation\ncapabilities designed to assist users in non-sequential composition workflows,\nincluding: (1) generating left-to-right continuations of existing material, (2)\nfilling in missing spans in the middle of existing material, and (3) generating\nharmony from melody and vice versa. Hookpad Aria is also a scalable data\nflywheel for music co-creation -- since its release in March 2024, Aria has\ngenerated 318k suggestions for 3k users who have accepted 74k into their songs.\n  More information about Hookpad Aria is available at\nhttps://www.hooktheory.com/hookpad/aria\n","authors":["Chris Donahue","Shih-Lun Wu","Yewon Kim","Dave Carlton","Ryan Miyakawa","John Thickstun"],"pdf_url":"https://arxiv.org/pdf/2502.08122v1.pdf","comment":"Extended abstract presented in the Late-Breaking Demo Session at\n  ISMIR 2024 (ISMIR LBD 2024)"},{"id":"http://arxiv.org/abs/2407.10784v4","updated":"2025-02-12T05:02:00Z","published":"2024-07-15T15:02:53Z","title":"AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware\n  Uncertainty Calibrator and Label Distribution Handler","summary":"  In real-world scenarios, tabular data often suffer from distribution shifts\nthat threaten the performance of machine learning models. Despite its\nprevalence and importance, handling distribution shifts in the tabular domain\nremains underexplored due to the inherent challenges within the tabular data\nitself. In this sense, test-time adaptation (TTA) offers a promising solution\nby adapting models to target data without accessing source data, crucial for\nprivacy-sensitive tabular domains. However, existing TTA methods either 1)\noverlook the nature of tabular distribution shifts, often involving label\ndistribution shifts, or 2) impose architectural constraints on the model,\nleading to a lack of applicability. To this end, we propose AdapTable, a novel\nTTA framework for tabular data. AdapTable operates in two stages: 1)\ncalibrating model predictions using a shift-aware uncertainty calibrator, and\n2) adjusting these predictions to match the target label distribution with a\nlabel distribution handler. We validate the effectiveness of AdapTable through\ntheoretical analysis and extensive experiments on various distribution shift\nscenarios. Our results demonstrate AdapTable's ability to handle various\nreal-world distribution shifts, achieving up to a 16% improvement on the HELOC\ndataset.\n","authors":["Changhun Kim","Taewon Kim","Seungyeon Woo","June Yong Yang","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2407.10784v4.pdf","comment":"NeurIPS Workshop on Table Representation Learning (NeurIPSW-TRL),\n  2024"},{"id":"http://arxiv.org/abs/2404.13016v3","updated":"2025-02-12T04:59:47Z","published":"2024-04-19T17:25:43Z","title":"Optimizing Calibration by Gaining Aware of Prediction Correctness","summary":"  Model calibration aims to align confidence with prediction correctness. The\nCross-Entropy (CE) loss is widely used for calibrator training, which enforces\nthe model to increase confidence on the ground truth class. However, we find\nthe CE loss has intrinsic limitations. For example, for a narrow\nmisclassification (e.g., a test sample is wrongly classified and its softmax\nscore on the ground truth class is 0.4), a calibrator trained by the CE loss\noften produces high confidence on the wrongly predicted class, which is\nundesirable. In this paper, we propose a new post-hoc calibration objective\nderived from the aim of calibration. Intuitively, the proposed objective\nfunction asks that the calibrator decrease model confidence on wrongly\npredicted samples and increase confidence on correctly predicted samples.\nBecause a sample itself has insufficient ability to indicate correctness, we\nuse its transformed versions (e.g., rotated, greyscaled, and color-jittered)\nduring calibrator training. Trained on an in-distribution validation set and\ntested with isolated, individual test samples, our method achieves competitive\ncalibration performance on both in-distribution and out-of-distribution test\nsets compared with the state of the art. Further, our analysis points out the\ndifference between our method and commonly used objectives such as CE loss and\nMean Square Error (MSE) loss, where the latters sometimes deviates from the\ncalibration aim.\n","authors":["Yuchi Liu","Lei Wang","Yuli Zou","James Zou","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2404.13016v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02913v4","updated":"2025-02-12T04:59:16Z","published":"2025-02-05T06:20:20Z","title":"Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient\n  Leakage","summary":"  The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.\n","authors":["Jiayang Meng","Tao Huang","Hong Chen","Xin Shi","Qingyu Huang","Chen Hou"],"pdf_url":"https://arxiv.org/pdf/2502.02913v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04961v2","updated":"2025-02-12T04:52:08Z","published":"2025-01-09T04:26:15Z","title":"Demystifying Domain-adaptive Post-training for Financial LLMs","summary":"  Domain-adaptive post-training of large language models (LLMs) has emerged as\na promising approach for specialized domains such as medicine and finance.\nHowever, significant challenges remain in identifying optimal adaptation\ncriteria and training strategies across varying data and model configurations.\nTo address these challenges, we introduce FINDAP, a systematic and fine-grained\ninvestigation into domain adaptive post-training of LLMs for the finance\ndomain. Our approach consists of four key components: FinCap, which defines the\ncore capabilities required for the target domain; FinRec, an effective training\nrecipe that jointly optimizes continual pre-training and instruction-following,\nalong with a novel preference data distillation method leveraging process\nsignals from a generative reward model; FinTrain, a curated set of training\ndatasets supporting FinRec; and FinEval, a comprehensive evaluation suite\naligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art\nperformance across a wide range of financial tasks. Our analysis also\nhighlights how each post-training stage contributes to distinct capabilities,\nuncovering specific challenges and effective solutions, providing valuable\ninsights for domain adaptation of LLMs.\n","authors":["Zixuan Ke","Yifei Ming","Xuan-Phi Nguyen","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2501.04961v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15005v3","updated":"2025-02-12T04:50:07Z","published":"2024-12-19T16:20:42Z","title":"DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start\n  Cross-Domain Recommendation","summary":"  Recommender systems are widely used in various real-world applications, but\nthey often encounter the persistent challenge of the user cold-start problem.\nCross-domain recommendation (CDR), which leverages user interactions from one\ndomain to improve prediction performance in another, has emerged as a promising\nsolution. However, users with similar preferences in the source domain may\nexhibit different interests in the target domain. Therefore, directly\ntransferring embeddings may introduce irrelevant source-domain collaborative\ninformation. In this paper, we propose a novel graph-based disentangled\ncontrastive learning framework to capture fine-grained user intent and filter\nout irrelevant collaborative information, thereby avoiding negative transfer.\nSpecifically, for each domain, we use a multi-channel graph encoder to capture\ndiverse user intents. We then construct the affinity graph in the embedding\nspace and perform multi-step random walks to capture high-order user similarity\nrelationships. Treating one domain as the target, we propose a disentangled\nintent-wise contrastive learning approach, guided by user similarity, to refine\nthe bridging of user intents across domains. Extensive experiments on four\nbenchmark CDR datasets demonstrate that DisCo consistently outperforms existing\nstate-of-the-art baselines, thereby validating the effectiveness of both DisCo\nand its components.\n","authors":["Hourun Li","Yifan Wang","Zhiping Xiao","Jia Yang","Changling Zhou","Ming Zhang","Wei Ju"],"pdf_url":"https://arxiv.org/pdf/2412.15005v3.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2405.06975v2","updated":"2025-02-12T04:48:53Z","published":"2024-05-11T10:05:55Z","title":"Input Snapshots Fusion for Scalable Discrete-Time Dynamic Graph Neural\n  Networks","summary":"  In recent years, there has been a surge in research on dynamic graph\nrepresentation learning, primarily focusing on modeling the evolution of\ntemporal-spatial patterns in real-world applications. However, within the\ndomain of discrete-time dynamic graphs, the exploration of temporal edges\nremains underexplored. Existing approaches often rely on additional sequential\nmodels to capture dynamics, leading to high computational and memory costs,\nparticularly for large-scale graphs. To address this limitation, we propose the\nInput {\\bf S}napshots {\\bf F}usion based {\\bf Dy}namic {\\bf G}raph Neural\nNetwork (SFDyG), which combines Hawkes processes with graph neural networks to\ncapture temporal and structural patterns in dynamic graphs effectively. By\nfusing multiple snapshots into a single temporal graph, SFDyG decouples\ncomputational complexity from the number of snapshots, enabling efficient\nfull-batch and mini-batch training. Experimental evaluations on eight diverse\ndynamic graph datasets for future link prediction tasks demonstrate that SFDyG\nconsistently outperforms existing methods.\n","authors":["QingGuo Qi","Hongyang Chen","Minhao Cheng","Han Liu"],"pdf_url":"https://arxiv.org/pdf/2405.06975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02936v4","updated":"2025-02-12T04:41:34Z","published":"2024-04-03T04:25:01Z","title":"Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large\n  Language Models","summary":"  The problem of pre-training data detection for large language models (LLMs)\nhas received growing attention due to its implications in critical issues like\ncopyright violation and test data contamination. Despite improved performance,\nexisting methods (including the state-of-the-art, Min-K%) are mostly developed\nupon simple heuristics and lack solid, reasonable foundations. In this work, we\npropose a novel and theoretically motivated methodology for pre-training data\ndetection, named Min-K%++. Specifically, we present a key insight that training\nsamples tend to be local maxima of the modeled distribution along each input\ndimension through maximum likelihood training, which in turn allow us to\ninsightfully translate the problem into identification of local maxima. Then,\nwe design our method accordingly that works under the discrete distribution\nmodeled by LLMs, whose core idea is to determine whether the input forms a mode\nor has relatively high probability under the conditional categorical\ndistribution. Empirically, the proposed method achieves new SOTA performance\nacross multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the\nrunner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the\nmore challenging MIMIR benchmark, it consistently improves upon reference-free\nmethods while performing on par with reference-based method that requires an\nextra reference model.\n","authors":["Jingyang Zhang","Jingwei Sun","Eric Yeats","Yang Ouyang","Martin Kuo","Jianyi Zhang","Hao Frank Yang","Hai Li"],"pdf_url":"https://arxiv.org/pdf/2404.02936v4.pdf","comment":"ICLR'25 Spotlight. Project page and code is available at\n  https://zjysteven.github.io/mink-plus-plus/"},{"id":"http://arxiv.org/abs/2502.08106v1","updated":"2025-02-12T04:07:14Z","published":"2025-02-12T04:07:14Z","title":"PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced\n  Text-to-Image Generation","summary":"  Diffusion models have made significant advancements in recent years. However,\ntheir performance often deteriorates when trained or fine-tuned on imbalanced\ndatasets. This degradation is largely due to the disproportionate\nrepresentation of majority and minority data in image-text pairs. In this\npaper, we propose a general fine-tuning approach, dubbed PoGDiff, to address\nthis challenge. Rather than directly minimizing the KL divergence between the\npredicted and ground-truth distributions, PoGDiff replaces the ground-truth\ndistribution with a Product of Gaussians (PoG), which is constructed by\ncombining the original ground-truth targets with the predicted distribution\nconditioned on a neighboring text embedding. Experiments on real-world datasets\ndemonstrate that our method effectively addresses the imbalance problem in\ndiffusion models, improving both generation accuracy and quality.\n","authors":["Ziyan Wang","Sizhe Wei","Xiaoming Huo","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08105v1","updated":"2025-02-12T04:07:12Z","published":"2025-02-12T04:07:12Z","title":"Out-of-Distribution Detection on Graphs: A Survey","summary":"  Graph machine learning has witnessed rapid growth, driving advancements\nacross diverse domains. However, the in-distribution assumption, where training\nand testing data share the same distribution, often breaks in real-world\nscenarios, leading to degraded model performance under distribution shifts.\nThis challenge has catalyzed interest in graph out-of-distribution (GOOD)\ndetection, which focuses on identifying graph data that deviates from the\ndistribution seen during training, thereby enhancing model robustness. In this\npaper, we provide a rigorous definition of GOOD detection and systematically\ncategorize existing methods into four types: enhancement-based,\nreconstruction-based, information propagation-based, and classification-based\napproaches. We analyze the principles and mechanisms of each approach and\nclarify the distinctions between GOOD detection and related fields, such as\ngraph anomaly detection, outlier detection, and GOOD generalization. Beyond\nmethodology, we discuss practical applications and theoretical foundations,\nhighlighting the unique challenges posed by graph data. Finally, we discuss the\nprimary challenges and propose future directions to advance this emerging\nfield. The repository of this survey is available at\nhttps://github.com/ca1man-2022/Awesome-GOOD-Detection.\n","authors":["Tingyi Cai","Yunliang Jiang","Yixin Liu","Ming Li","Changqin Huang","Shirui Pan"],"pdf_url":"https://arxiv.org/pdf/2502.08105v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.01138v5","updated":"2025-02-12T04:04:34Z","published":"2024-02-02T04:30:58Z","title":"Graph Neural Networks in EEG-based Emotion Recognition: A Survey","summary":"  Compared to other modalities, EEG-based emotion recognition can intuitively\nrespond to the emotional patterns in the human brain and, therefore, has become\none of the most concerning tasks in the brain-computer interfaces field. Since\ndependencies within brain regions are closely related to emotion, a significant\ntrend is to develop Graph Neural Networks (GNNs) for EEG-based emotion\nrecognition. However, brain region dependencies in emotional EEG have\nphysiological bases that distinguish GNNs in this field from those in other\ntime series fields. Besides, there is neither a comprehensive review nor\nguidance for constructing GNNs in EEG-based emotion recognition. In the survey,\nour categorization reveals the commonalities and differences of existing\napproaches under a unified framework of graph construction. We analyze and\ncategorize methods from three stages in the framework to provide clear guidance\non constructing GNNs in EEG-based emotion recognition. In addition, we discuss\nseveral open challenges and future directions, such as Temporal full-connected\ngraph and Graph condensation.\n","authors":["Chenyu Liu","Xinliang Zhou","Yihao Wu","Ruizhi Yang","Zhongruo Wang","Liming Zhai","Ziyu Jia","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2402.01138v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07328v2","updated":"2025-02-12T04:00:14Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Exploring Multicultural Representations in Music\n  Generation Models","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v2.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2502.08101v1","updated":"2025-02-12T03:56:35Z","published":"2025-02-12T03:56:35Z","title":"Rethinking Tokenized Graph Transformers for Node Classification","summary":"  Node tokenized graph Transformers (GTs) have shown promising performance in\nnode classification. The generation of token sequences is the key module in\nexisting tokenized GTs which transforms the input graph into token sequences,\nfacilitating the node representation learning via Transformer. In this paper,\nwe observe that the generations of token sequences in existing GTs only focus\non the first-order neighbors on the constructed similarity graphs, which leads\nto the limited usage of nodes to generate diverse token sequences, further\nrestricting the potential of tokenized GTs for node classification. To this\nend, we propose a new method termed SwapGT. SwapGT first introduces a novel\ntoken swapping operation based on the characteristics of token sequences that\nfully leverages the semantic relevance of nodes to generate more informative\ntoken sequences. Then, SwapGT leverages a Transformer-based backbone to learn\nnode representations from the generated token sequences. Moreover, SwapGT\ndevelops a center alignment loss to constrain the representation learning from\nmultiple token sequences, further enhancing the model performance. Extensive\nempirical results on various datasets showcase the superiority of SwapGT for\nnode classification.\n","authors":["Jinsong Chen","Chenyang Li","GaiChao Li","John E. Hopcroft","Kun He"],"pdf_url":"https://arxiv.org/pdf/2502.08101v1.pdf","comment":"Preprint version"},{"id":"http://arxiv.org/abs/2410.03159v3","updated":"2025-02-12T03:55:17Z","published":"2024-10-04T05:45:50Z","title":"WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting","summary":"  We propose a Weighted Autoregressive Varying gatE (WAVE) attention mechanism\nequipped with both Autoregressive (AR) and Moving-average (MA) components. It\ncan adapt to various attention mechanisms, enhancing and decoupling their\nability to capture long-range and local temporal patterns in time series data.\nIn this paper, we first demonstrate that, for the time series forecasting (TSF)\ntask, the previously overlooked decoder-only autoregressive Transformer model\ncan achieve results comparable to the best baselines when appropriate\ntokenization and training methods are applied. Moreover, inspired by the ARMA\nmodel from statistics and recent advances in linear attention, we introduce the\nfull ARMA structure into existing autoregressive attention mechanisms. By using\nan indirect MA weight generation method, we incorporate the MA term while\nmaintaining the time complexity and parameter size of the underlying efficient\nattention models. We further explore how indirect parameter generation can\nproduce implicit MA weights that align with the modeling requirements for local\ntemporal impacts. Experimental results show that WAVE attention that\nincorporates the ARMA structure consistently improves the performance of\nvarious AR attentions on TSF tasks, achieving state-of-the-art results.\n","authors":["Jiecheng Lu","Xu Han","Yan Sun","Shihao Yang"],"pdf_url":"https://arxiv.org/pdf/2410.03159v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08098v1","updated":"2025-02-12T03:52:47Z","published":"2025-02-12T03:52:47Z","title":"Unsupervised categorization of similarity measures","summary":"  In general, objects can be distinguished on the basis of their features, such\nas color or shape. In particular, it is assumed that similarity judgments about\nsuch features can be processed independently in different metric spaces.\nHowever, the unsupervised categorization mechanism of metric spaces\ncorresponding to object features remains unknown. Here, we show that the\nartificial neural network system can autonomously categorize metric spaces\nthrough representation learning to satisfy the algebraic independence between\nneural networks, and project sensory information onto multiple high-dimensional\nmetric spaces to independently evaluate the differences and similarities\nbetween features. Conventional methods often constrain the axes of the latent\nspace to be mutually independent or orthogonal. However, the independent axes\nare not suitable for categorizing metric spaces. High-dimensional metric spaces\nthat are independent of each other are not uniquely determined by the mutually\nindependent axes, because any combination of independent axes can form mutually\nindependent spaces. In other words, the mutually independent axes cannot be\nused to naturally categorize different feature spaces, such as color space and\nshape space. Therefore, constraining the axes to be mutually independent makes\nit difficult to categorize high-dimensional metric spaces. To overcome this\nproblem, we developed a method to constrain only the spaces to be mutually\nindependent and not the composed axes to be independent. Our theory provides\ngeneral conditions for the unsupervised categorization of independent metric\nspaces, thus advancing the mathematical theory of functional differentiation of\nneural networks.\n","authors":["Yoshiyuki Ohmura","Wataru Shimaya","Yasuo Kuniyoshi"],"pdf_url":"https://arxiv.org/pdf/2502.08098v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2306.00239"},{"id":"http://arxiv.org/abs/2410.00847v2","updated":"2025-02-12T03:34:29Z","published":"2024-10-01T16:29:59Z","title":"Uncertainty-aware Reward Model: Teaching Reward Models to Know What is\n  Unknown","summary":"  Reward models (RMs) are essential for aligning large language models (LLM)\nwith human expectations. However, existing RMs struggle to capture the\nstochastic and uncertain nature of human preferences and fail to assess the\nreliability of reward predictions. To address these challenges, we introduce\nthe Uncertainty-aware Reward Model (URM) and its ensemble variant, URME. URM\nemploys a probabilistic value head to capture aleatoric uncertainty by modeling\nthe distribution of disentangled human preference attributes. URME further\nquantifies epistemic uncertainty by examining discrepancies among individual\nURMs within the ensemble, enabling identification of unreliable evaluations.\nOur empirical evaluations demonstrate that URM achieves strong performance on\nRewardBench, outperforming competitive large-scale models. Additionally,\nextensive experiments, including best-of-n sampling (BoN), iterative direct\npreference optimization (iterative DPO), and proximal policy optimization\n(PPO), demonstrate that URM and URME significantly enhance LLMs' generation\nquality. Notably, reward predictions with lower uncertainty are far more\nreliable, demonstrate significantly higher quality, and result in substantially\nimproved alignment.\n","authors":["Xingzhou Lou","Dong Yan","Wei Shen","Yuzi Yan","Jian Xie","Junge Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.00847v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08193v2","updated":"2025-02-12T03:22:38Z","published":"2024-12-11T08:35:13Z","title":"Mixture of Experts Meets Decoupled Message Passing: Towards General and\n  Adaptive Node Classification","summary":"  Graph neural networks excel at graph representation learning but struggle\nwith heterophilous data and long-range dependencies. And graph transformers\naddress these issues through self-attention, yet face scalability and noise\nchallenges on large-scale graphs. To overcome these limitations, we propose\nGNNMoE, a universal model architecture for node classification. This\narchitecture flexibly combines fine-grained message-passing operations with a\nmixture-of-experts mechanism to build feature encoding blocks. Furthermore, by\nincorporating soft and hard gating layers to assign the most suitable expert\nnetworks to each node, we enhance the model's expressive power and adaptability\nto different graph types. In addition, we introduce adaptive residual\nconnections and an enhanced FFN module into GNNMoE, further improving the\nexpressiveness of node representation. Extensive experimental results\ndemonstrate that GNNMoE performs exceptionally well across various types of\ngraph data, effectively alleviating the over-smoothing issue and global noise,\nenhancing model robustness and adaptability, while also ensuring computational\nefficiency on large-scale graphs.\n","authors":["Xuanze Chen","Jiajun Zhou","Shanqing Yu","Qi Xuan"],"pdf_url":"https://arxiv.org/pdf/2412.08193v2.pdf","comment":"Accepted by ACM Web Conference 2025 as a short paper"},{"id":"http://arxiv.org/abs/2502.08673v1","updated":"2025-02-12T03:20:45Z","published":"2025-02-12T03:20:45Z","title":"High-Throughput SAT Sampling","summary":"  In this work, we present a novel technique for GPU-accelerated Boolean\nsatisfiability (SAT) sampling. Unlike conventional sampling algorithms that\ndirectly operate on conjunctive normal form (CNF), our method transforms the\nlogical constraints of SAT problems by factoring their CNF representations into\nsimplified multi-level, multi-output Boolean functions. It then leverages\ngradient-based optimization to guide the search for a diverse set of valid\nsolutions. Our method operates directly on the circuit structure of refactored\nSAT instances, reinterpreting the SAT problem as a supervised multi-output\nregression task. This differentiable technique enables independent bit-wise\noperations on each tensor element, allowing parallel execution of learning\nprocesses. As a result, we achieve GPU-accelerated sampling with significant\nruntime improvements ranging from $33.6\\times$ to $523.6\\times$ over\nstate-of-the-art heuristic samplers. We demonstrate the superior performance of\nour sampling method through an extensive evaluation on $60$ instances from a\npublic domain benchmark suite utilized in previous studies.\n","authors":["Arash Ardakani","Minwoo Kang","Kevin He","Qijing Huang","John Wawrzynek"],"pdf_url":"https://arxiv.org/pdf/2502.08673v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2502.06200v2","updated":"2025-02-12T03:10:55Z","published":"2025-02-10T06:54:16Z","title":"On the query complexity of sampling from non-log-concave distributions","summary":"  We study the problem of sampling from a $d$-dimensional distribution with\ndensity $p(x)\\propto e^{-f(x)}$, which does not necessarily satisfy good\nisoperimetric conditions.\n  Specifically, we show that for any $L,M$ satisfying $LM\\ge d\\ge 5$,\n$\\epsilon\\in \\left(0,\\frac{1}{32}\\right)$, and any algorithm with query\naccesses to the value of $f(x)$ and $\\nabla f(x)$, there exists an\n$L$-log-smooth distribution with second moment at most $M$ such that the\nalgorithm requires $\\left(\\frac{LM}{d\\epsilon}\\right)^{\\Omega(d)}$ queries to\ncompute a sample whose distribution is within $\\epsilon$ in total variation\ndistance to the target distribution. We complement the lower bound with an\nalgorithm requiring $\\left(\\frac{LM}{d\\epsilon}\\right)^{\\mathcal O(d)}$\nqueries, thereby characterizing the tight (up to the constant in the exponent)\nquery complexity for sampling from the family of non-log-concave distributions.\n  Our results are in sharp contrast with the recent work of Huang et al.\n(COLT'24), where an algorithm with quasi-polynomial query complexity was\nproposed for sampling from a non-log-concave distribution when\n$M=\\mathtt{poly}(d)$. Their algorithm works under the stronger condition that\nall distributions along the trajectory of the Ornstein-Uhlenbeck process,\nstarting from the target distribution, are $\\mathcal O(1)$-log-smooth. We\ninvestigate this condition and prove that it is strictly stronger than\nrequiring the target distribution to be $\\mathcal O(1)$-log-smooth.\nAdditionally, we study this condition in the context of mixtures of Gaussians.\n  Finally, we place our results within the broader theme of ``sampling versus\noptimization'', as studied in Ma et al. (PNAS'19). We show that for a wide\nrange of parameters, sampling is strictly easier than optimization by a\nsuper-exponential factor in the dimension $d$.\n","authors":["Yuchen He","Chihao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08083v1","updated":"2025-02-12T03:10:26Z","published":"2025-02-12T03:10:26Z","title":"Mixture of Decoupled Message Passing Experts with Entropy Constraint for\n  General Node Classification","summary":"  The varying degrees of homophily and heterophily in real-world graphs\npersistently constrain the universality of graph neural networks (GNNs) for\nnode classification. Adopting a data-centric perspective, this work reveals an\ninherent preference of different graphs towards distinct message encoding\nschemes: homophilous graphs favor local propagation, while heterophilous graphs\nexhibit preference for flexible combinations of propagation and transformation.\nTo address this, we propose GNNMoE, a universal node classification framework\nbased on the Mixture-of-Experts (MoE) mechanism. The framework first constructs\ndiverse message-passing experts through recombination of fine-grained encoding\noperators, then designs soft and hard gating layers to allocate the most\nsuitable expert networks for each node's representation learning, thereby\nenhancing both model expressiveness and adaptability to diverse graphs.\nFurthermore, considering that soft gating might introduce encoding noise in\nhomophilous scenarios, we introduce an entropy constraint to guide sharpening\nof soft gates, achieving organic integration of weighted combination and Top-K\nselection. Extensive experiments demonstrate that GNNMoE significantly\noutperforms mainstream GNNs, heterophilous GNNs, and graph transformers in both\nnode classification performance and universality across diverse graph datasets.\n","authors":["Xuanze Chen","Jiajun Zhou","Jinsong Chen","Shanqing Yu","Qi Xuan"],"pdf_url":"https://arxiv.org/pdf/2502.08083v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.08193"},{"id":"http://arxiv.org/abs/2406.02213v2","updated":"2025-02-12T02:53:56Z","published":"2024-06-04T11:11:53Z","title":"Random Policy Evaluation Uncovers Policies of Generative Flow Networks","summary":"  The Generative Flow Network (GFlowNet) is a probabilistic framework in which\nan agent learns a stochastic policy and flow functions to sample objects with\nprobability proportional to an unnormalized reward function. GFlowNets share a\nstrong connection with reinforcement learning (RL) that typically aims to\nmaximize reward. A number of recent works explored connections between\nGFlowNets and maximum entropy (MaxEnt) RL, which incorporates entropy\nregularization into the standard RL objective. However, the relationship\nbetween GFlowNets and standard RL remains largely unexplored, despite the\ninherent similarities in their sequential decision-making nature. While\nGFlowNets can discover diverse solutions through specialized flow-matching\nobjectives, connecting them to standard RL can simplify their implementation\nthrough well-established RL principles and also improve RL's capabilities in\ndiverse solution discovery (a critical requirement in many real-world\napplications), and bridging this gap can further unlock the potential of both\nfields. In this paper, we bridge this gap by revealing a fundamental connection\nbetween GFlowNets and one of the most basic components of RL -- policy\nevaluation. Surprisingly, we find that the value function obtained from\nevaluating a uniform policy is closely associated with the flow functions in\nGFlowNets. Building upon these insights, we introduce a rectified random policy\nevaluation (RPE) algorithm, which achieves the same reward-matching effect as\nGFlowNets based on simply evaluating a fixed random policy, offering a new\nperspective. Empirical results across extensive benchmarks demonstrate that RPE\nachieves competitive results compared to previous approaches, shedding light on\nthe previously overlooked connection between (non-MaxEnt) RL and GFlowNets.\n","authors":["Haoran He","Emmanuel Bengio","Qingpeng Cai","Ling Pan"],"pdf_url":"https://arxiv.org/pdf/2406.02213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07608v2","updated":"2025-02-12T02:52:28Z","published":"2025-02-11T14:58:54Z","title":"Beyond Prompting: Time2Lang -- Bridging Time-Series Foundation Models\n  and Large Language Models for Health Sensing","summary":"  Large language models (LLMs) show promise for health applications when\ncombined with behavioral sensing data. Traditional approaches convert sensor\ndata into text prompts, but this process is prone to errors, computationally\nexpensive, and requires domain expertise. These challenges are particularly\nacute when processing extended time series data. While time series foundation\nmodels (TFMs) have recently emerged as powerful tools for learning\nrepresentations from temporal data, bridging TFMs and LLMs remains challenging.\nHere, we present Time2Lang, a framework that directly maps TFM outputs to LLM\nrepresentations without intermediate text conversion. Our approach first trains\non synthetic data using periodicity prediction as a pretext task, followed by\nevaluation on mental health classification tasks. We validate Time2Lang on two\nlongitudinal wearable and mobile sensing datasets: daily depression prediction\nusing step count data (17,251 days from 256 participants) and flourishing\nclassification based on conversation duration (46 participants over 10 weeks).\nTime2Lang maintains near constant inference times regardless of input length,\nunlike traditional prompting methods. The generated embeddings preserve\nessential time-series characteristics such as auto-correlation. Our results\ndemonstrate that TFMs and LLMs can be effectively integrated while minimizing\ninformation loss and enabling performance transfer across these distinct\nmodeling paradigms. To our knowledge, we are the first to integrate a TFM and\nan LLM for health, thus establishing a foundation for future research combining\ngeneral-purpose large models for complex healthcare tasks.\n","authors":["Arvind Pillai","Dimitris Spathis","Subigya Nepal","Amanda C Collins","Daniel M Mackin","Michael V Heinz","Tess Z Griffin","Nicholas C Jacobson","Andrew Campbell"],"pdf_url":"https://arxiv.org/pdf/2502.07608v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.08077v1","updated":"2025-02-12T02:44:41Z","published":"2025-02-12T02:44:41Z","title":"Cascading Bandits Robust to Adversarial Corruptions","summary":"  Online learning to rank sequentially recommends a small list of items to\nusers from a large candidate set and receives the users' click feedback. In\nmany real-world scenarios, users browse the recommended list in order and click\nthe first attractive item without checking the rest. Such behaviors are usually\nformulated as the cascade model. Many recent works study algorithms for\ncascading bandits, an online learning to rank framework in the cascade model.\nHowever, the performance of existing methods may drop significantly if part of\nthe user feedback is adversarially corrupted (e.g., click fraud). In this work,\nwe study how to resist adversarial corruptions in cascading bandits. We first\nformulate the ``\\textit{Cascading Bandits with Adversarial Corruptions}\" (CBAC)\nproblem, which assumes that there is an adaptive adversary that may manipulate\nthe user feedback. Then we propose two robust algorithms for this problem,\nwhich assume the corruption level is known and agnostic, respectively. We show\nthat both algorithms can achieve logarithmic regret when the algorithm is not\nunder attack, and the regret increases linearly with the corruption level. The\nexperimental results also verify the robustness of our methods.\n","authors":["Jize Xie","Cheng Chen","Zhiyong Wang","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2502.08077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15664v4","updated":"2025-02-12T02:32:30Z","published":"2024-06-21T21:44:27Z","title":"Flat Posterior Does Matter For Bayesian Model Averaging","summary":"  Bayesian neural networks (BNNs) estimate the posterior distribution of model\nparameters and utilize posterior samples for Bayesian Model Aver- aging (BMA)\nin prediction. However, despite the crucial role of flatness in the loss\nlandscape in improving the generalization of neural networks, its impact on BMA\nhas been largely overlooked. In this work, we explore how posterior flatness\ninfluences BMA generalization and empirically demonstrate that (1) most\napproximate Bayesian inference methods fail to yield a flat posterior and (2)\nBMA predictions, without considering posterior flatness, are less effective at\nimproving generalization. To address this, we propose Flat Posterior-aware\nBayesian Model Averaging (FP-BMA), a novel training objective that explicitly\nencourages flat posteriors in a principled Bayesian manner. We also introduce a\nFlat Posterior-aware Bayesian Transfer Learning scheme that enhances\ngeneralization in downstream tasks. Empirically, we show that FP-BMA\nsuccessfully captures flat posteriors, improving generalization performance.\n","authors":["Sungjun Lim","Jeyoon Yeom","Sooyon Kim","Hoyoon Byun","Jinho Kang","Yohan Jung","Jiyoung Jung","Kyungwoo Song"],"pdf_url":"https://arxiv.org/pdf/2406.15664v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10281v2","updated":"2025-02-12T02:11:10Z","published":"2024-06-12T05:13:09Z","title":"Watermarking Language Models with Error Correcting Codes","summary":"  Recent progress in large language models enables the creation of realistic\nmachine-generated content. Watermarking is a promising approach to distinguish\nmachine-generated text from human text, embedding statistical signals in the\noutput that are ideally undetectable to humans. We propose a watermarking\nframework that encodes such signals through an error correcting code. Our\nmethod, termed robust binary code (RBC) watermark, introduces no distortion\ncompared to the original probability distribution, and no noticeable\ndegradation in quality. We evaluate our watermark on base and instruction\nfine-tuned models and find our watermark is robust to edits, deletions, and\ntranslations. We provide an information-theoretic perspective on watermarking,\na powerful statistical test for detection and for generating p-values, and\ntheoretical guarantees. Our empirical findings suggest our watermark is fast,\npowerful, and robust, comparing favorably to the state-of-the-art.\n","authors":["Patrick Chao","Yan Sun","Edgar Dobriban","Hamed Hassani"],"pdf_url":"https://arxiv.org/pdf/2406.10281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08063v1","updated":"2025-02-12T02:04:46Z","published":"2025-02-12T02:04:46Z","title":"Multi-Agent Performative Prediction Beyond the Insensitivity Assumption:\n  A Case Study for Mortgage Competition","summary":"  Performative prediction models account for feedback loops in decision-making\nprocesses where predictions influence future data distributions. While existing\nwork largely assumes insensitivity of data distributions to small strategy\nchanges, this assumption usually fails in real-world competitive (i.e.\nmulti-agent) settings. For example, in Bertrand-type competitions, a small\nreduction in one firm's price can lead that firm to capture the entire demand,\nwhile all others sharply lose all of their customers.\n  We study a representative setting of multi-agent performative prediction in\nwhich insensitivity assumptions do not hold, and investigate the convergence of\nnatural dynamics. To do so, we focus on a specific game that we call the ''Bank\nGame'', where two lenders compete over interest rates and credit score\nthresholds. Consumers act similarly as to in a Bertrand Competition, with each\nconsumer selecting the firm with the lowest interest rate that they are\neligible for based on the firms' credit thresholds. Our analysis characterizes\nthe equilibria of this game and demonstrates that when both firms use a common\nand natural no-regret learning dynamic -- exponential weights -- with proper\ninitialization, the dynamics always converge to stable outcomes despite the\ngeneral-sum structure. Notably, our setting admits multiple stable equilibria,\nwith convergence dependent on initial conditions. We also provide theoretical\nconvergence results in the stochastic case when the utility matrix is not fully\nknown, but each learner can observe sufficiently many samples of consumers at\neach time step to estimate it, showing robustness to slight mis-specifications.\nFinally, we provide experimental results that validate our theoretical\nfindings.\n","authors":["Guanghui Wang","Krishna Acharya","Lokranjan Lakshmikanthan","Vidya Muthukumar","Juba Ziani"],"pdf_url":"https://arxiv.org/pdf/2502.08063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.09113v2","updated":"2025-02-12T02:00:12Z","published":"2024-05-15T06:11:24Z","title":"Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained\n  Optimization","summary":"  Recent research indicates that large language models (LLMs) are susceptible\nto jailbreaking attacks that can generate harmful content. This paper\nintroduces a novel token-level attack method, Adaptive Dense-to-Sparse\nConstrained Optimization (ADC), which has been shown to successfully jailbreak\nmultiple open-source LLMs. Drawing inspiration from the difficulties of\ndiscrete token optimization, our method relaxes the discrete jailbreak\noptimization into a continuous optimization process while gradually increasing\nthe sparsity of the optimizing vectors. This technique effectively bridges the\ngap between discrete and continuous space optimization. Experimental results\ndemonstrate that our method is more effective and efficient than\nstate-of-the-art token-level methods. On Harmbench, our approach achieves the\nhighest attack success rate on seven out of eight LLMs compared to the latest\njailbreak methods. Trigger Warning: This paper contains model behavior that can\nbe offensive in nature.\n","authors":["Kai Hu","Weichen Yu","Yining Li","Kai Chen","Tianjun Yao","Xiang Li","Wenhe Liu","Lijun Yu","Zhiqiang Shen","Matt Fredrikson"],"pdf_url":"https://arxiv.org/pdf/2405.09113v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03647v2","updated":"2025-02-12T01:59:32Z","published":"2023-10-05T16:21:42Z","title":"Rethinking Algorithmic Fairness for Human-AI Collaboration","summary":"  Existing approaches to algorithmic fairness aim to ensure equitable outcomes\nif human decision-makers comply perfectly with algorithmic decisions. However,\nperfect compliance with the algorithm is rarely a reality or even a desirable\noutcome in human-AI collaboration. Yet, recent studies have shown that\nselective compliance with fair algorithms can amplify discrimination relative\nto the prior human policy. As a consequence, ensuring equitable outcomes\nrequires fundamentally different algorithmic design principles that ensure\nrobustness to the decision-maker's (a priori unknown) compliance pattern. We\ndefine the notion of compliance-robustly fair algorithmic recommendations that\nare guaranteed to (weakly) improve fairness in decisions, regardless of the\nhuman's compliance pattern. We propose a simple optimization strategy to\nidentify the best performance-improving compliance-robustly fair policy.\nHowever, we show that it may be infeasible to design algorithmic\nrecommendations that are simultaneously fair in isolation, compliance-robustly\nfair, and more accurate than the human policy; thus, if our goal is to improve\nthe equity and accuracy of human-AI collaboration, it may not be desirable to\nenforce traditional algorithmic fairness constraints. We illustrate the value\nof our approach on criminal sentencing data before and after the introduction\nof an algorithmic risk assessment tool in Virginia.\n","authors":["Haosen Ge","Hamsa Bastani","Osbert Bastani"],"pdf_url":"https://arxiv.org/pdf/2310.03647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08059v1","updated":"2025-02-12T01:54:21Z","published":"2025-02-12T01:54:21Z","title":"On Mechanistic Circuits for Extractive Question-Answering","summary":"  Large language models are increasingly used to process documents and\nfacilitate question-answering on them. In our paper, we extract mechanistic\ncircuits for this real-world language modeling task: context-augmented language\nmodeling for extractive question-answering (QA) tasks and understand the\npotential benefits of circuits towards downstream applications such as data\nattribution to context information. We extract circuits as a function of\ninternal model components (e.g., attention heads, MLPs) using causal mediation\nanalysis techniques. Leveraging the extracted circuits, we first understand the\ninterplay between the model's usage of parametric memory and retrieved context\ntowards a better mechanistic understanding of context-augmented language\nmodels. We then identify a small set of attention heads in our circuit which\nperforms reliable data attribution by default, thereby obtaining attribution\nfor free in just the model's forward pass. Using this insight, we then\nintroduce ATTNATTRIB, a fast data attribution algorithm which obtains\nstate-of-the-art attribution results across various extractive QA benchmarks.\nFinally, we show the possibility to steer the language model towards answering\nfrom the context, instead of the parametric memory by using the attribution\nfrom ATTNATTRIB as an additional signal during the forward pass. Beyond\nmechanistic understanding, our paper provides tangible applications of circuits\nin the form of reliable data attribution and model steering.\n","authors":["Samyadeep Basu","Vlad Morariu","Zichao Wang","Ryan Rossi","Cherry Zhao","Soheil Feizi","Varun Manjunatha"],"pdf_url":"https://arxiv.org/pdf/2502.08059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08058v1","updated":"2025-02-12T01:50:12Z","published":"2025-02-12T01:50:12Z","title":"General Coded Computing: Adversarial Settings","summary":"  Conventional coded computing frameworks are predominantly tailored for\nstructured computations, such as matrix multiplication and polynomial\nevaluation. Such tasks allow the reuse of tools and techniques from algebraic\ncoding theory to improve the reliability of distributed systems in the presence\nof stragglers and adversarial servers.\n  This paper lays the foundation for general coded computing, which extends the\napplicability of coded computing to handle a wide class of computations. In\naddition, it particularly addresses the challenging problem of managing\nadversarial servers. We demonstrate that, in the proposed scheme, for a system\nwith $N$ servers, where $\\mathcal{O}(N^a)$, $a \\in [0,1)$, are adversarial, the\nsupremum of the average approximation error over all adversarial strategies\ndecays at a rate of $N^{\\frac{6}{5}(a-1)}$, under minimal assumptions on the\ncomputing tasks. Furthermore, we show that within a general framework, the\nproposed scheme achieves optimal adversarial robustness, in terms of maximum\nnumber of adversarial servers it can tolerate. This marks a significant step\ntoward practical and reliable general coded computing. Implementation results\nfurther validate the effectiveness of the proposed method in handling various\ncomputations, including inference in deep neural networks.\n","authors":["Parsa Moradi","Hanzaleh Akbarinodehi","Mohammad Ali Maddah-Ali"],"pdf_url":"https://arxiv.org/pdf/2502.08058v1.pdf","comment":"18 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.08055v2","updated":"2025-02-12T01:49:15Z","published":"2024-03-12T20:02:39Z","title":"DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design\n  and Prediction","summary":"  This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of\n3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional\nneural network model, both aimed at aerodynamic car design through machine\nlearning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million\nsurface mesh faces and comprehensive aerodynamic performance data comprising of\nfull 3D pressure, velocity fields, and wall-shear stresses, addresses the\ncritical need for extensive datasets to train deep learning models in\nengineering applications. It is 60\\% larger than the previously available\nlargest public dataset of cars, and is the only open-source dataset that also\nmodels wheels and underbody. RegDGCNN leverages this large-scale dataset to\nprovide high-precision drag estimates directly from 3D meshes, bypassing\ntraditional limitations such as the need for 2D image rendering or Signed\nDistance Fields (SDF). By enabling fast drag estimation in seconds, RegDGCNN\nfacilitates rapid aerodynamic assessments, offering a substantial leap towards\nintegrating data-driven methods in automotive design. Together, DrivAerNet and\nRegDGCNN promise to accelerate the car design process and contribute to the\ndevelopment of more efficient cars. To lay the groundwork for future\ninnovations in the field, the dataset and code used in our study are publicly\naccessible at https://github.com/Mohamedelrefaie/DrivAerNet.\n","authors":["Mohamed Elrefaie","Angela Dai","Faez Ahmed"],"pdf_url":"https://arxiv.org/pdf/2403.08055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08056v1","updated":"2025-02-12T01:36:27Z","published":"2025-02-12T01:36:27Z","title":"Cognify: Supercharging Gen-AI Workflows With Hierarchical Autotuning","summary":"  Today's gen-AI workflows that involve multiple ML model calls, tool/API\ncalls, data retrieval, or generic code execution are often tuned manually in an\nad-hoc way that is both time-consuming and error-prone. In this paper, we\npropose a systematic approach for automatically tuning gen-AI workflows. Our\nkey insight is that gen-AI workflows can benefit from structure, operator, and\nprompt changes, but unique properties of gen-AI workflows require new\noptimization techniques. We propose AdaSeek, an adaptive hierarchical search\nalgorithm for autotuning gen-AI workflows. AdaSeek organizes workflow tuning\nmethods into different layers based on the user-specified total search budget\nand distributes the budget across different layers based on the complexity of\neach layer. During its hierarchical search, AdaSeek redistributes the search\nbudget from less useful to more promising tuning configurations based on\nworkflow-level evaluation results. We implement AdaSeek in a workflow\nautotuning framework called Cognify and evaluate Cognify using six types of\nworkflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify\nimproves these workflows' generation quality by up to 2.8x, reduces execution\nmonetary cost by up to 10x, and reduces end-to-end latency by 2.7x.\n","authors":["Zijian He","Reyna Abhyankar","Vikranth Srivatsa","Yiying Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.08056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19671v2","updated":"2025-02-12T01:34:12Z","published":"2024-11-29T12:56:43Z","title":"On the Performance Analysis of Momentum Method: A Frequency Domain\n  Perspective","summary":"  Momentum-based optimizers are widely adopted for training neural networks.\nHowever, the optimal selection of momentum coefficients remains elusive. This\nuncertainty impedes a clear understanding of the role of momentum in stochastic\ngradient methods. In this paper, we present a frequency domain analysis\nframework that interprets the momentum method as a time-variant filter for\ngradients, where adjustments to momentum coefficients modify the filter\ncharacteristics. Our experiments support this perspective and provide a deeper\nunderstanding of the mechanism involved. Moreover, our analysis reveals the\nfollowing significant findings: high-frequency gradient components are\nundesired in the late stages of training; preserving the original gradient in\nthe early stages, and gradually amplifying low-frequency gradient components\nduring training both enhance performance. Based on these insights, we propose\nFrequency Stochastic Gradient Descent with Momentum (FSGDM), a heuristic\noptimizer that dynamically adjusts the momentum filtering characteristic with\nan empirically effective dynamic magnitude response. Experimental results\ndemonstrate the superiority of FSGDM over conventional momentum optimizers.\n","authors":["Xianliang Li","Jun Luo","Zhiwei Zheng","Hanxiao Wang","Li Luo","Lingkun Wen","Linlong Wu","Sheng Xu"],"pdf_url":"https://arxiv.org/pdf/2411.19671v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2501.14427v3","updated":"2025-02-12T01:33:00Z","published":"2025-01-24T11:55:57Z","title":"GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand\n  Graphs Better","summary":"  The success of Large Language Models (LLMs) in various domains has led\nresearchers to apply them to graph-related problems by converting graph data\ninto natural language text. However, unlike graph data, natural language\ninherently has sequential order. We observe a counter-intuitive fact that when\nthe order of nodes or edges in the natural language description of a graph is\nshuffled, despite describing the same graph, model performance fluctuates\nbetween high performance and random guessing. Additionally, due to LLMs'\nlimited input context length, current methods typically randomly sample\nneighbors of target nodes as representatives of their neighborhood, which may\nnot always be effective for accurate reasoning. To address these gaps, we\nintroduce GraphSOS (Graph Sampling and Order Selection). This novel model\nframework features an Order Selector Module to ensure proper serialization\norder of the graph and a Subgraph Sampling Module to sample subgraphs with\nbetter structure for better reasoning. Furthermore, we propose Graph CoT\nobtained through distillation, and enhance LLM's reasoning and zero-shot\nlearning capabilities for graph tasks through instruction tuning. Experiments\non multiple datasets for node classification and graph question-answering\ndemonstrate that GraphSOS improves LLMs' performance and generalization ability\non graph tasks.\n","authors":["Xu Chu","Hanlin Xue","Zhijie Tan","Bingce Wang","Tong Mo","Weiping Li"],"pdf_url":"https://arxiv.org/pdf/2501.14427v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08055v1","updated":"2025-02-12T01:31:39Z","published":"2025-02-12T01:31:39Z","title":"SLVR: Securely Leveraging Client Validation for Robust Federated\n  Learning","summary":"  Federated Learning (FL) enables collaborative model training while keeping\nclient data private. However, exposing individual client updates makes FL\nvulnerable to reconstruction attacks. Secure aggregation mitigates such privacy\nrisks but prevents the server from verifying the validity of each client\nupdate, creating a privacy-robustness tradeoff. Recent efforts attempt to\naddress this tradeoff by enforcing checks on client updates using\nzero-knowledge proofs, but they support limited predicates and often depend on\npublic validation data. We propose SLVR, a general framework that securely\nleverages clients' private data through secure multi-party computation. By\nutilizing clients' data, SLVR not only eliminates the need for public\nvalidation data, but also enables a wider range of checks for robustness,\nincluding cross-client accuracy validation. It also adapts naturally to\ndistribution shifts in client data as it can securely refresh its validation\ndata up-to-date. Our empirical evaluations show that SLVR improves robustness\nagainst model poisoning attacks, particularly outperforming existing methods by\nup to 50% under adaptive attacks. Additionally, SLVR demonstrates effective\nadaptability and stable convergence under various distribution shift scenarios.\n","authors":["Jihye Choi","Sai Rahul Rachuri","Ke Wang","Somesh Jha","Yizhen Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08055v1.pdf","comment":"29 pages"},{"id":"http://arxiv.org/abs/2502.08054v1","updated":"2025-02-12T01:31:01Z","published":"2025-02-12T01:31:01Z","title":"COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual\n  Occluded Grasping","summary":"  This paper addresses the challenge of occluded robot grasping, i.e. grasping\nin situations where the desired grasp poses are kinematically infeasible due to\nenvironmental constraints such as surface collisions. Traditional robot\nmanipulation approaches struggle with the complexity of non-prehensile or\nbimanual strategies commonly used by humans in these circumstances.\nState-of-the-art reinforcement learning (RL) methods are unsuitable due to the\ninherent complexity of the task. In contrast, learning from demonstration\nrequires collecting a significant number of expert demonstrations, which is\noften infeasible. Instead, inspired by human bimanual manipulation strategies,\nwhere two hands coordinate to stabilise and reorient objects, we focus on a\nbimanual robotic setup to tackle this challenge. In particular, we introduce\nConstraint-based Manipulation for Bimanual Occluded Grasping (COMBO-Grasp), a\nlearning-based approach which leverages two coordinated policies: a constraint\npolicy trained using self-supervised datasets to generate stabilising poses and\na grasping policy trained using RL that reorients and grasps the target object.\nA key contribution lies in value function-guided policy coordination.\nSpecifically, during RL training for the grasping policy, the constraint\npolicy's output is refined through gradients from a jointly trained value\nfunction, improving bimanual coordination and task performance. Lastly,\nCOMBO-Grasp employs teacher-student policy distillation to effectively deploy\npoint cloud-based policies in real-world environments. Empirical evaluations\ndemonstrate that COMBO-Grasp significantly improves task success rates compared\nto competitive baseline approaches, with successful generalisation to unseen\nobjects in both simulated and real-world environments.\n","authors":["Jun Yamada","Alexander L. Mitchell","Jack Collins","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2502.08054v1.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.04692v3","updated":"2025-02-12T01:02:51Z","published":"2025-02-07T06:37:05Z","title":"STRIDE: Automating Reward Design, Deep Reinforcement Learning Training\n  and Feedback Optimization in Humanoid Robotics Locomotion","summary":"  Humanoid robotics presents significant challenges in artificial intelligence,\nrequiring precise coordination and control of high-degree-of-freedom systems.\nDesigning effective reward functions for deep reinforcement learning (DRL) in\nthis domain remains a critical bottleneck, demanding extensive manual effort,\ndomain expertise, and iterative refinement. To overcome these challenges, we\nintroduce STRIDE, a novel framework built on agentic engineering to automate\nreward design, DRL training, and feedback optimization for humanoid robot\nlocomotion tasks. By combining the structured principles of agentic engineering\nwith large language models (LLMs) for code-writing, zero-shot generation, and\nin-context optimization, STRIDE generates, evaluates, and iteratively refines\nreward functions without relying on task-specific prompts or templates. Across\ndiverse environments featuring humanoid robot morphologies, STRIDE outperforms\nthe state-of-the-art reward design framework EUREKA, achieving an average\nimprovement of round 250% in efficiency and task performance. Using\nSTRIDE-generated rewards, simulated humanoid robots achieve sprint-level\nlocomotion across complex terrains, highlighting its ability to advance DRL\nworkflows and humanoid robotics research.\n","authors":["Zhenwei Wu","Jinxiong Lu","Yuxiao Chen","Yunxin Liu","Yueting Zhuang","Luhui Hu"],"pdf_url":"https://arxiv.org/pdf/2502.04692v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08041v1","updated":"2025-02-12T00:57:53Z","published":"2025-02-12T00:57:53Z","title":"The Art of Misclassification: Too Many Classes, Not Enough Points","summary":"  Classification is a ubiquitous and fundamental problem in artificial\nintelligence and machine learning, with extensive efforts dedicated to\ndeveloping more powerful classifiers and larger datasets. However, the\nclassification task is ultimately constrained by the intrinsic properties of\ndatasets, independently of computational power or model complexity. In this\nwork, we introduce a formal entropy-based measure of classificability, which\nquantifies the inherent difficulty of a classification problem by assessing the\nuncertainty in class assignments given feature representations. This measure\ncaptures the degree of class overlap and aligns with human intuition, serving\nas an upper bound on classification performance for classification problems.\nOur results establish a theoretical limit beyond which no classifier can\nimprove the classification accuracy, regardless of the architecture or amount\nof data, in a given problem. Our approach provides a principled framework for\nunderstanding when classification is inherently fallible and fundamentally\nambiguous.\n","authors":["Mario Franco","Gerardo Febres","Nelson Fern√°ndez","Carlos Gershenson"],"pdf_url":"https://arxiv.org/pdf/2502.08041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16381v2","updated":"2025-02-12T00:48:57Z","published":"2024-05-25T23:53:07Z","title":"Trivialized Momentum Facilitates Diffusion Generative Modeling on Lie\n  Groups","summary":"  The generative modeling of data on manifolds is an important task, for which\ndiffusion models in flat spaces typically need nontrivial adaptations. This\narticle demonstrates how a technique called `trivialization' can transfer the\neffectiveness of diffusion models in Euclidean spaces to Lie groups. In\nparticular, an auxiliary momentum variable was algorithmically introduced to\nhelp transport the position variable between data distribution and a fixed,\neasy-to-sample distribution. Normally, this would incur further difficulty for\nmanifold data because momentum lives in a space that changes with the position.\nHowever, our trivialization technique creates a new momentum variable that\nstays in a simple fixed vector space. This design, together with a manifold\npreserving integrator, simplifies implementation and avoids inaccuracies\ncreated by approximations such as projections to tangent space and manifold,\nwhich were typically used in prior work, hence facilitating generation with\nhigh-fidelity and efficiency. The resulting method achieves state-of-the-art\nperformance on protein and RNA torsion angle generation and sophisticated torus\ndatasets. We also, arguably for the first time, tackle the generation of data\non high-dimensional Special Orthogonal and Unitary groups, the latter essential\nfor quantum problems. Code is available at\nhttps://github.com/yuchen-zhu-zyc/TDM.\n","authors":["Yuchen Zhu","Tianrong Chen","Lingkai Kong","Evangelos A. Theodorou","Molei Tao"],"pdf_url":"https://arxiv.org/pdf/2405.16381v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2309.13481v4","updated":"2025-02-12T00:36:52Z","published":"2023-09-23T21:39:51Z","title":"Offline to Online Learning for Real-Time Bandwidth Estimation","summary":"  Real-time video applications require accurate bandwidth estimation (BWE) to\nmaintain user experience across varying network conditions. However, increasing\nnetwork heterogeneity challenges general-purpose BWE algorithms, necessitating\nsolutions that adapt to end-user environments. While widely adopted,\nheuristic-based methods are difficult to individualize without extensive domain\nexpertise. Conversely, online reinforcement learning (RL) offers ease of\ncustomization but neglects prior domain expertise and suffers from sample\ninefficiency. Thus, we present Merlin, an imitation learning-based solution\nthat replaces the manual parameter tuning of heuristic-based methods with\ndata-driven updates to streamline end-user personalization. Our key insight is\nthat transforming heuristic-based BWE algorithms into neural networks\nfacilitates data-driven personalization. Merlin utilizes Behavioral Cloning to\nefficiently learn from offline telemetry logs, capturing heuristic policies\nwithout live network interactions. The cloned policy can then be seamlessly\ntailored to end user network conditions through online finetuning. In real\nintercontinental videoconferencing calls, Merlin matches our heuristic's policy\nwith no statistically significant differences in user quality of experience\n(QoE). Finetuning Merlin's control policy to end-user environments enables QoE\nimprovements of up to 7.8% compared to the heuristic policy. Lastly, our\nIL-based design performs competitively with current state-of-the-art online RL\ntechniques but converges with 80% fewer videoconferencing samples, facilitating\npractical end-user personalization.\n","authors":["Aashish Gottipati","Sami Khairy","Gabriel Mittag","Vishak Gopal","Ross Cutler"],"pdf_url":"https://arxiv.org/pdf/2309.13481v4.pdf","comment":"8 pages, under review. Updated content, added finetuning evaluations,\n  updated title, added IEEE copyright"},{"id":"http://arxiv.org/abs/2502.08033v1","updated":"2025-02-12T00:26:01Z","published":"2025-02-12T00:26:01Z","title":"End-to-End Predictive Planner for Autonomous Driving with Consistency\n  Models","summary":"  Trajectory prediction and planning are fundamental components for autonomous\nvehicles to navigate safely and efficiently in dynamic environments.\nTraditionally, these components have often been treated as separate modules,\nlimiting the ability to perform interactive planning and leading to\ncomputational inefficiency in multi-agent scenarios. In this paper, we present\na novel unified and data-driven framework that integrates prediction and\nplanning with a single consistency model. Trained on real-world human driving\ndatasets, our consistency model generates samples from high-dimensional,\nmultimodal joint trajectory distributions of the ego and multiple surrounding\nagents, enabling end-to-end predictive planning. It effectively produces\ninteractive behaviors, such as proactive nudging and yielding to ensure both\nsafe and efficient interactions with other road users. To incorporate\nadditional planning constraints on the ego vehicle, we propose an alternating\ndirection method for multi-objective guidance in online guided sampling.\nCompared to diffusion models, our consistency model achieves better performance\nwith fewer sampling steps, making it more suitable for real-time deployment.\nExperimental results on Waymo Open Motion Dataset (WOMD) demonstrate our\nmethod's superiority in trajectory quality, constraint satisfaction, and\ninteractive behavior compared to various existing approaches.\n","authors":["Anjian Li","Sangjae Bae","David Isele","Ryne Beeson","Faizan M. Tariq"],"pdf_url":"https://arxiv.org/pdf/2502.08033v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04047v3","updated":"2025-02-12T00:23:36Z","published":"2024-10-05T06:04:19Z","title":"Multi-Step Time Series Inference Agent for Reasoning and Automated Task\n  Execution","summary":"  Time series analysis is crucial in real-world applications, yet traditional\nmethods focus on isolated tasks only, and recent studies on time series\nreasoning remain limited to simple, single-step inference constrained to\nnatural language answer. In this work, we propose a practical novel task:\nmulti-step time series inference that demands both compositional reasoning and\ncomputation precision of time series analysis. To address such challenge, we\npropose a simple but effective program-aided inference agent that leverages\nLLMs' reasoning ability to decompose complex tasks into structured execution\npipelines. By integrating in-context learning, self-correction, and\nprogram-aided execution, our proposed approach ensures accurate and\ninterpretable results. To benchmark performance, we introduce a new dataset and\na unified evaluation framework with task-specific success criteria. Experiments\nshow that our approach outperforms standalone general purpose LLMs in both\nbasic time series concept understanding as well as multi-step time series\ninference task, highlighting the importance of hybrid approaches that combine\nreasoning with computational precision.\n","authors":["Wen Ye","Yizhou Zhang","Wei Yang","Defu Cao","Lumingyuan Tang","Jie Cai","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2410.04047v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19243v3","updated":"2025-02-12T00:08:30Z","published":"2024-03-28T08:58:20Z","title":"Efficient Learning With Sine-Activated Low-rank Matrices","summary":"  Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.\n","authors":["Yiping Ji","Hemanth Saratchandran","Cameron Gordon","Zeyu Zhang","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19243v3.pdf","comment":"The first two authors contributed equally. Paper accepted at ICLR\n  2025"},{"id":"http://arxiv.org/abs/2209.05935v4","updated":"2025-02-12T00:03:03Z","published":"2022-09-13T12:31:33Z","title":"Variational Causal Inference","summary":"  Estimating an individual's potential outcomes under counterfactual treatments\nis a challenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, impulse\nresponses, human faces) and covariates are relatively limited. In this case, to\nconstruct one's outcome under a counterfactual treatment, it is crucial to\nleverage individual information contained in its observed factual outcome on\ntop of the covariates. We propose a deep variational Bayesian framework that\nrigorously integrates two main sources of information for outcome construction\nunder a counterfactual treatment: one source is the individual features\nembedded in the high-dimensional factual outcome; the other source is the\nresponse distribution of similar subjects (subjects with the same covariates)\nthat factually received this treatment of interest.\n","authors":["Yulun Wu","Layne C. Price","Zichen Wang","Vassilis N. Ioannidis","Robert A. Barton","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2209.05935v4.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.21144v4","updated":"2025-02-12T19:20:49Z","published":"2024-10-28T15:44:35Z","title":"Enhancing Learned Image Compression via Cross Window-based Attention","summary":"  In recent years, learned image compression methods have demonstrated superior\nrate-distortion performance compared to traditional image compression methods.\nRecent methods utilize convolutional neural networks (CNN), variational\nautoencoders (VAE), invertible neural networks (INN), and transformers. Despite\ntheir significant contributions, a main drawback of these models is their poor\nperformance in capturing local redundancy. Therefore, to leverage global\nfeatures along with local redundancy, we propose a CNN-based solution\nintegrated with a feature encoding module. The feature encoding module encodes\nimportant features before feeding them to the CNN and then utilizes cross-scale\nwindow-based attention, which further captures local redundancy. Cross-scale\nwindow-based attention is inspired by the attention mechanism in transformers\nand effectively enlarges the receptive field. Both the feature encoding module\nand the cross-scale window-based attention module in our architecture are\nflexible and can be incorporated into any other network architecture. We\nevaluate our method on the Kodak and CLIC datasets and demonstrate that our\napproach is effective and on par with state-of-the-art methods. Our code is\npublicly available at https://github.com/prmudgal/CWAM_IC_ISVC. .\n","authors":["Priyanka Mudgal","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.21144v4.pdf","comment":"Paper accepted and presented in ISVC'24. Copyrights stay with ISVC\n  Our code is available at: https://github.com/prmudgal/CWAM_IC_ISVC"},{"id":"http://arxiv.org/abs/2502.04328v2","updated":"2025-02-12T18:40:46Z","published":"2025-02-06T18:59:55Z","title":"Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive\n  Modality Alignment","summary":"  Recent advances in large language models, particularly following GPT-4o, have\nsparked increasing interest in developing omni-modal models capable of\nunderstanding more modalities. While some open-source alternatives have\nemerged, there is still a notable lag behind specialized single-modality models\nin performance. In this paper, we present Ola, an Omni-modal language model\nthat achieves competitive performance across image, video, and audio\nunderstanding compared to specialized counterparts. The core design of Ola lies\nin its progressive modality alignment strategy that extends the supporting\nmodality of the language model progressively. Our training pipeline begins with\nthe most distinct modalities: image and text, then gradually expands the skill\nsets of the model using speech data that connects language and audio knowledge,\nand video data that connects all modalities. The progressive learning pipeline\nalso enables us to maintain a relatively small size of the cross-modal\nalignment data, making developing omni-modal from existing vision-language\nmodels easy and less costly. Moreover, to unlock an advanced interactive\nexperience like GPT-4o, we further design a sentence-wise decoding solution for\nstreaming speech generation. Extensive experiments demonstrate that Ola\nsurpasses existing open omni-modal LLMs across all modalities while achieving\nhighly competitive performance compared to state-of-the-art specialized models\nof similar sizes. We aim to make Ola a fully open omni-modal understanding\nsolution to advance future research in this emerging field. Model weights,\ncode, and data are open-sourced at https://github.com/Ola-Omni/Ola.\n","authors":["Zuyan Liu","Yuhao Dong","Jiahui Wang","Ziwei Liu","Winston Hu","Jiwen Lu","Yongming Rao"],"pdf_url":"https://arxiv.org/pdf/2502.04328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.19702v2","updated":"2025-02-12T16:47:30Z","published":"2024-10-25T17:19:55Z","title":"TimeSuite: Improving MLLMs for Long Video Understanding via Grounded\n  Tuning","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in short video understanding. However, understanding long-form\nvideos still remains challenging for MLLMs. This paper proposes TimeSuite, a\ncollection of new designs to adapt the existing short-form video MLLMs for long\nvideo understanding, including a simple yet efficient framework to process long\nvideo sequence, a high-quality video dataset for grounded tuning of MLLMs, and\na carefully-designed instruction tuning task to explicitly incorporate the\ngrounding supervision in the traditional QA format. Specifically, based on\nVideoChat, we propose our long-video MLLM, coined as VideoChat-T, by\nimplementing a token shuffling to compress long video tokens and introducing\nTemporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of\nvisual representation. Meanwhile, we introduce the TimePro, a comprehensive\ngrounding-centric instruction tuning dataset composed of 9 tasks and 349k\nhigh-quality grounded annotations. Notably, we design a new instruction tuning\ntask type, called Temporal Grounded Caption, to peform detailed video\ndescriptions with the corresponding time stamps prediction. This explicit\ntemporal location prediction will guide MLLM to correctly attend on the visual\ncontent when generating description, and thus reduce the hallucination risk\ncaused by the LLMs. Experimental results demonstrate that our TimeSuite\nprovides a successful solution to enhance the long video understanding\ncapability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the\nbenchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T\nexhibits robust zero-shot temporal grounding capabilities, significantly\noutperforming the existing state-of-the-art MLLMs. After fine-tuning, it\nperforms on par with the traditional supervised expert models.\n","authors":["Xiangyu Zeng","Kunchang Li","Chenting Wang","Xinhao Li","Tianxiang Jiang","Ziang Yan","Songze Li","Yansong Shi","Zhengrong Yue","Yi Wang","Yali Wang","Yu Qiao","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.19702v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.08556v1","updated":"2025-02-12T16:38:40Z","published":"2025-02-12T16:38:40Z","title":"Human-Centric Foundation Models: Perception, Generation and Agentic\n  Modeling","summary":"  Human understanding and generation are critical for modeling digital humans\nand humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)\ninspired by the success of generalist models, such as large language and vision\nmodels, have emerged to unify diverse human-centric tasks into a single\nframework, surpassing traditional task-specific approaches. In this survey, we\npresent a comprehensive overview of HcFMs by proposing a taxonomy that\ncategorizes current approaches into four groups: (1) Human-centric Perception\nFoundation Models that capture fine-grained features for multi-modal 2D and 3D\nunderstanding. (2) Human-centric AIGC Foundation Models that generate\nhigh-fidelity, diverse human-related content. (3) Unified Perception and\nGeneration Models that integrate these capabilities to enhance both human\nunderstanding and synthesis. (4) Human-centric Agentic Foundation Models that\nextend beyond perception and generation to learn human-like intelligence and\ninteractive behaviors for humanoid embodied tasks. We review state-of-the-art\ntechniques, discuss emerging challenges and future research directions. This\nsurvey aims to serve as a roadmap for researchers and practitioners working\ntowards more robust, versatile, and intelligent digital human and embodiments\nmodeling.\n","authors":["Shixiang Tang","Yizhou Wang","Lu Chen","Yuan Wang","Sida Peng","Dan Xu","Wanli Ouyang"],"pdf_url":"https://arxiv.org/pdf/2502.08556v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.08513v1","updated":"2025-02-12T15:46:47Z","published":"2025-02-12T15:46:47Z","title":"\"You'll Be Alice Adventuring in Wonderland!\" Processes, Challenges, and\n  Opportunities of Creating Animated Virtual Reality Stories","summary":"  Animated virtual reality (VR) stories, combining the presence of VR and the\nartistry of computer animation, offer a compelling way to deliver messages and\nevoke emotions. Motivated by the growing demand for immersive narrative\nexperiences, more creators are creating animated VR stories. However, a\nholistic understanding of their creation processes and challenges involved in\ncrafting these stories is still limited. Based on semi-structured interviews\nwith 21 animated VR story creators, we identify ten common stages in their\nend-to-end creation processes, ranging from idea generation to evaluation,\nwhich form diverse workflows that are story-driven or visual-driven.\nAdditionally, we highlight nine unique issues that arise during the creation\nprocess, such as a lack of reference material for multi-element plots, the\nabsence of specific functionalities for story integration, and inadequate\nsupport for audience evaluation. We compare the creation of animated VR stories\nto general XR applications and distill several future research opportunities.\n","authors":["Lin-Ping Yuan","Feilin Han","Liwenhan Xie","Junjie Zhang","Jian Zhao","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2502.08513v1.pdf","comment":"Conditionally accepted to the ACM Conference on Human Factors in\n  Computing Systems (CHI'25)"},{"id":"http://arxiv.org/abs/2502.08438v1","updated":"2025-02-12T14:22:59Z","published":"2025-02-12T14:22:59Z","title":"Composite Sketch+Text Queries for Retrieving Objects with Elusive Names\n  and Complex Interactions","summary":"  Non-native speakers with limited vocabulary often struggle to name specific\nobjects despite being able to visualize them, e.g., people outside Australia\nsearching for numbats. Further, users may want to search for such elusive\nobjects with difficult-to-sketch interactions, e.g., numbat digging in the\nground. In such common but complex situations, users desire a search interface\nthat accepts composite multimodal queries comprising hand-drawn sketches of\ndifficult-to-name but easy-to-draw objects and text describing\ndifficult-to-sketch but easy-to-verbalize object attributes or interaction with\nthe scene. This novel problem statement distinctly differs from the previously\nwell-researched TBIR (text-based image retrieval) and SBIR (sketch-based image\nretrieval) problems. To study this under-explored task, we curate a dataset,\nCSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M\nqueries and 108K natural scene images. Further, as a solution to this problem,\nwe propose a pretrained multimodal transformer-based baseline, STNET\n(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant\nobjects in the natural scene image, and encodes the text and image to perform\nimage retrieval. In addition to contrastive learning, we propose multiple\ntraining objectives that improve the performance of our model. Extensive\nexperiments show that our proposed method outperforms several state-of-the-art\nretrieval methods for text-only, sketch-only, and composite query modalities.\nWe make the dataset and code available at our project website.\n","authors":["Prajwal Gatti","Kshitij Parikh","Dhriti Prasanna Paul","Manish Gupta","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.08438v1.pdf","comment":"Accepted at AAAI 2024, 9 pages. Project Website:\n  https://vl2g.github.io/projects/cstbir"},{"id":"http://arxiv.org/abs/2407.14093v3","updated":"2025-02-12T08:49:34Z","published":"2024-07-19T07:57:48Z","title":"Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large\n  Language Models","summary":"  Recently, mixture of experts (MoE) has become a popular paradigm for\nachieving the trade-off between modal capacity and efficiency of multi-modal\nlarge language models (MLLMs). Different from previous efforts, we are\ndedicated to exploring the dynamic expert path in an already exist MLLM and\nshow that a standard MLLM can be also a mixture of experts. To approach this\ntarget, we propose a novel dynamic expert scheme for MLLMs, termed Routing\nExperts (RoE), which can achieve example-dependent optimal path routing without\nobvious structure tweaks. Meanwhile, a new regularization of structure sparsity\nis also introduced to enforce MLLMs to learn more short-cut inference, ensuring\nthe efficiency. In addition, we also realize the first attempt of aligning the\ntraining and inference schemes of MLLMs in terms of network routing. To\nvalidate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,\nLLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL\nbenchmarks. The experiment results not only show the great advantages of our\nRoE in improving MLLMs' efficiency, but also yield obvious advantages than\nMoE-LLaVA in both performance and speed, e.g., an average performance gain of\n3.3% on 5 benchmarks while being faster.\n","authors":["Qiong Wu","Zhaoxi Ke","Yiyi Zhou","Xiaoshuai Sun","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2407.14093v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07531v2","updated":"2025-02-12T07:35:56Z","published":"2025-02-11T13:11:59Z","title":"VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video\n  Generation","summary":"  Recent image-to-video generation methods have demonstrated success in\nenabling control over one or two visual elements, such as camera trajectory or\nobject motion. However, these methods are unable to offer control over multiple\nvisual elements due to limitations in data and network efficacy. In this paper,\nwe introduce VidCRAFT3, a novel framework for precise image-to-video generation\nthat enables control over camera motion, object motion, and lighting direction\nsimultaneously. To better decouple control over each visual element, we propose\nthe Spatial Triple-Attention Transformer, which integrates lighting direction,\ntext, and image in a symmetric way. Since most real-world video datasets lack\nlighting annotations, we construct a high-quality synthetic video dataset, the\nVideoLightingDirection (VLD) dataset. This dataset includes lighting direction\nannotations and objects of diverse appearance, enabling VidCRAFT3 to\neffectively handle strong light transmission and reflection effects.\nAdditionally, we propose a three-stage training strategy that eliminates the\nneed for training data annotated with multiple visual elements (camera motion,\nobject motion, and lighting direction) simultaneously. Extensive experiments on\nbenchmark datasets demonstrate the efficacy of VidCRAFT3 in producing\nhigh-quality video content, surpassing existing state-of-the-art methods in\nterms of control granularity and visual coherence. All code and data will be\npublicly available.\n","authors":["Sixiao Zheng","Zimian Peng","Yanpeng Zhou","Yi Zhu","Hang Xu","Xiangru Huang","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2502.07531v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07328v2","updated":"2025-02-12T04:00:14Z","published":"2025-02-11T07:46:29Z","title":"Music for All: Exploring Multicultural Representations in Music\n  Generation Models","summary":"  The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.\n","authors":["Atharva Mehta","Shivam Chauhan","Amirbek Djanibekov","Atharva Kulkarni","Gus Xia","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2502.07328v2.pdf","comment":"17 pages, 5 figures, accepted to NAACL'25"},{"id":"http://arxiv.org/abs/2502.08674v1","updated":"2025-02-12T03:32:28Z","published":"2025-02-12T03:32:28Z","title":"COutfitGAN: Learning to Synthesize Compatible Outfits Supervised by\n  Silhouette Masks and Fashion Styles","summary":"  How to recommend outfits has gained considerable attention in both academia\nand industry in recent years. Many studies have been carried out regarding\nfashion compatibility learning, to determine whether the fashion items in an\noutfit are compatible or not. These methods mainly focus on evaluating the\ncompatibility of existing outfits and rarely consider applying such knowledge\nto 'design' new fashion items. We propose the new task of generating\ncomplementary and compatible fashion items based on an arbitrary number of\ngiven fashion items. In particular, given some fashion items that can make up\nan outfit, the aim of this paper is to synthesize photo-realistic images of\nother, complementary, fashion items that are compatible with the given ones. To\nachieve this, we propose an outfit generation framework, referred to as\nCOutfitGAN, which includes a pyramid style extractor, an outfit generator, a\nUNet-based real/fake discriminator, and a collocation discriminator. To train\nand evaluate this framework, we collected a large-scale fashion outfit dataset\nwith over 200K outfits and 800K fashion items from the Internet. Extensive\nexperiments show that COutfitGAN outperforms other baselines in terms of\nsimilarity, authenticity, and compatibility measurements.\n","authors":["Dongliang Zhou","Haijun Zhang","Qun Li","Jianghong Ma","Xiaofei Xu"],"pdf_url":"https://arxiv.org/pdf/2502.08674v1.pdf","comment":"This paper was accepted by IEEE TMM"}]},"2025-02-11T00:00:00Z":{"Machine Learning":[{"id":"http://arxiv.org/abs/2210.00116v3","updated":"2025-02-11T23:58:24Z","published":"2022-09-30T22:13:57Z","title":"Predicting Cellular Responses with Variational Causal Inference and\n  Refined Relational Information","summary":"  Predicting the responses of a cell under perturbations may bring important\nbenefits to drug discovery and personalized therapeutics. In this work, we\npropose a novel graph variational Bayesian causal inference framework to\npredict a cell's gene expressions under counterfactual perturbations\n(perturbations that this cell did not factually receive), leveraging\ninformation representing biological knowledge in the form of gene regulatory\nnetworks (GRNs) to aid individualized cellular response predictions. Aiming at\na data-adaptive GRN, we also developed an adjacency matrix updating technique\nfor graph convolutional networks and used it to refine GRNs during\npre-training, which generated more insights on gene relations and enhanced\nmodel performance. Additionally, we propose a robust estimator within our\nframework for the asymptotically efficient estimation of marginal perturbation\neffect, which is yet to be carried out in previous works. With extensive\nexperiments, we exhibited the advantage of our approach over state-of-the-art\ndeep learning models for individual response prediction.\n","authors":["Yulun Wu","Robert A. Barton","Zichen Wang","Vassilis N. Ioannidis","Carlo De Donno","Layne C. Price","Luis F. Voloch","George Karypis"],"pdf_url":"https://arxiv.org/pdf/2210.00116v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12730v2","updated":"2025-02-11T23:56:02Z","published":"2024-10-16T16:44:12Z","title":"Counterfactual Generative Modeling with Variational Causal Inference","summary":"  Estimating an individual's counterfactual outcomes under interventions is a\nchallenging task for traditional causal inference and supervised learning\napproaches when the outcome is high-dimensional (e.g. gene expressions, facial\nimages) and covariates are relatively limited. In this case, to predict one's\noutcomes under counterfactual treatments, it is crucial to leverage individual\ninformation contained in the observed outcome in addition to the covariates.\nPrior works using variational inference in counterfactual generative modeling\nhave been focusing on neural adaptations and model variants within the\nconditional variational autoencoder formulation, which we argue is\nfundamentally ill-suited to the notion of counterfactual in causal inference.\nIn this work, we present a novel variational Bayesian causal inference\nframework and its theoretical backings to properly handle counterfactual\ngenerative modeling tasks, through which we are able to conduct counterfactual\nsupervision end-to-end during training without any counterfactual samples, and\nencourage disentangled exogenous noise abduction that aids the correct\nidentification of causal effect in counterfactual generations. In experiments,\nwe demonstrate the advantage of our framework compared to state-of-the-art\nmodels in counterfactual generative modeling on multiple benchmarks.\n","authors":["Yulun Wu","Louie McConnell","Claudia Iriondo"],"pdf_url":"https://arxiv.org/pdf/2410.12730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08024v1","updated":"2025-02-11T23:53:16Z","published":"2025-02-11T23:53:16Z","title":"Initialization Matters: Unraveling the Impact of Pre-Training on\n  Federated Learning","summary":"  Initializing with pre-trained models when learning on downstream tasks is\nbecoming standard practice in machine learning. Several recent works explore\nthe benefits of pre-trained initialization in a federated learning (FL)\nsetting, where the downstream training is performed at the edge clients with\nheterogeneous data distribution. These works show that starting from a\npre-trained model can substantially reduce the adverse impact of data\nheterogeneity on the test performance of a model trained in a federated\nsetting, with no changes to the standard FedAvg training algorithm. In this\nwork, we provide a deeper theoretical understanding of this phenomenon. To do\nso, we study the class of two-layer convolutional neural networks (CNNs) and\nprovide bounds on the training error convergence and test error of such a\nnetwork trained with FedAvg. We introduce the notion of aligned and misaligned\nfilters at initialization and show that the data heterogeneity only affects\nlearning on misaligned filters. Starting with a pre-trained model typically\nresults in fewer misaligned filters at initialization, thus producing a lower\ntest error even when the model is trained in a federated setting with data\nheterogeneity. Experiments in synthetic settings and practical FL training on\nCNNs verify our theoretical findings.\n","authors":["Divyansh Jhunjhunwala","Pranay Sharma","Zheng Xu","Gauri Joshi"],"pdf_url":"https://arxiv.org/pdf/2502.08024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01618v3","updated":"2025-02-11T23:52:26Z","published":"2025-02-03T18:50:50Z","title":"A Probabilistic Inference Approach to Inference-Time Scaling of LLMs\n  using Particle-Based Monte Carlo Methods","summary":"  Large language models (LLMs) have achieved significant performance gains via\nscaling up model sizes and/or data. However, recent evidence suggests\ndiminishing returns from such approaches, motivating scaling the computation\nspent at inference time. Existing inference-time scaling methods, usually with\nreward models, cast the task as a search problem, which tends to be vulnerable\nto reward hacking as a consequence of approximation errors in reward models. In\nthis paper, we instead cast inference-time scaling as a probabilistic inference\ntask and leverage sampling-based techniques to explore the typical set of the\nstate distribution of a state-space model with an approximate likelihood,\nrather than optimize for its mode directly. We propose a novel inference-time\nscaling approach by adapting particle-based Monte Carlo methods to this task.\nOur empirical evaluation demonstrates that our methods have a 4-16x better\nscaling rate over our deterministic search counterparts on various challenging\nmathematical reasoning tasks. Using our approach, we show that\nQwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,\nwhile Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.\nOur work not only presents an effective method to inference-time scaling, but\nalso connects the rich literature in probabilistic inference with\ninference-time scaling of LLMs to develop more robust algorithms in future\nwork. Code, videos, and further information available at\nhttps://probabilistic-inference-scaling.github.io.\n","authors":["Isha Puri","Shivchander Sudalairaj","Guangxuan Xu","Kai Xu","Akash Srivastava"],"pdf_url":"https://arxiv.org/pdf/2502.01618v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08021v1","updated":"2025-02-11T23:40:55Z","published":"2025-02-11T23:40:55Z","title":"Model Selection for Off-policy Evaluation: New Algorithms and\n  Experimental Protocol","summary":"  Holdout validation and hyperparameter tuning from data is a long-standing\nproblem in offline reinforcement learning (RL). A standard framework is to use\noff-policy evaluation (OPE) methods to evaluate and select the policies, but\nOPE either incurs exponential variance (e.g., importance sampling) or has\nhyperparameters on their own (e.g., FQE and model-based). In this work we focus\non hyperparameter tuning for OPE itself, which is even more under-investigated.\nConcretely, we select among candidate value functions (\"model-free\") or\ndynamics (\"model-based\") to best assess the performance of a target policy. Our\ncontributions are two fold. We develop: (1) new model-free and model-based\nselectors with theoretical guarantees, and (2) a new experimental protocol for\nempirically evaluating them. Compared to the model-free protocol in prior\nworks, our new protocol allows for more stable generation of candidate value\nfunctions, better control of misspecification, and evaluation of model-free and\nmodel-based methods alike. We exemplify the protocol on a Gym environment, and\nfind that our new model-free selector, LSTD-Tournament, demonstrates promising\nempirical performance.\n","authors":["Pai Liu","Lingfeng Zhao","Shivangi Agarwal","Jinghan Liu","Audrey Huang","Philip Amortila","Nan Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.08021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08003v6","updated":"2025-02-11T23:18:12Z","published":"2024-10-10T14:58:18Z","title":"More Experts Than Galaxies: Conditionally-overlapping Experts With\n  Biologically-Inspired Fixed Routing","summary":"  The evolution of biological neural systems has led to both modularity and\nsparse coding, which enables energy efficiency and robustness across the\ndiversity of tasks in the lifespan. In contrast, standard neural networks rely\non dense, non-specialized architectures, where all model parameters are\nsimultaneously updated to learn multiple tasks, leading to interference.\nCurrent sparse neural network approaches aim to alleviate this issue but are\nhindered by limitations such as 1) trainable gating functions that cause\nrepresentation collapse, 2) disjoint experts that result in redundant\ncomputation and slow learning, and 3) reliance on explicit input or task IDs\nthat limit flexibility and scalability. In this paper we propose Conditionally\nOverlapping Mixture of ExperTs (COMET), a general deep learning method that\naddresses these challenges by inducing a modular, sparse architecture with an\nexponential number of overlapping experts. COMET replaces the trainable gating\nfunction used in Sparse Mixture of Experts with a fixed, biologically inspired\nrandom projection applied to individual input representations. This design\ncauses the degree of expert overlap to depend on input similarity, so that\nsimilar inputs tend to share more parameters. This results in faster learning\nper update step and improved out-of-sample generalization. We demonstrate the\neffectiveness of COMET on a range of tasks, including image classification,\nlanguage modeling, and regression, using several popular deep learning\narchitectures.\n","authors":["Sagi Shaier","Francisco Pereira","Katharina von der Wense","Lawrence E Hunter","Matt Jones"],"pdf_url":"https://arxiv.org/pdf/2410.08003v6.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.08008v1","updated":"2025-02-11T23:07:14Z","published":"2025-02-11T23:07:14Z","title":"An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models","summary":"  Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.\n","authors":["Kasra Ahmadi","Rouzbeh Behnia","Reza Ebrahimi","Mehran Mozaffari Kermani","Jeremiah Birrell","Jason Pacheco","Attila A Yavuz"],"pdf_url":"https://arxiv.org/pdf/2502.08008v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.08007v1","updated":"2025-02-11T23:06:43Z","published":"2025-02-11T23:06:43Z","title":"The Role of Randomness in Stability","summary":"  Stability is a central property in learning and statistics promising the\noutput of an algorithm $A$ does not change substantially when applied to\nsimilar datasets $S$ and $S'$. It is an elementary fact that any sufficiently\nstable algorithm (e.g.\\ one returning the same result with high probability,\nsatisfying privacy guarantees, etc.) must be randomized. This raises a natural\nquestion: can we quantify how much randomness is needed for algorithmic\nstability?\n  We study the randomness complexity of two influential notions of stability in\nlearning: replicability, which promises $A$ usually outputs the same result\nwhen run over samples from the same distribution (and shared random coins), and\ndifferential privacy, which promises the output distribution of $A$ remains\nsimilar under neighboring datasets. The randomness complexity of these notions\nwas studied recently in (Dixon et al. ICML 2024) and (Cannone et al. ITCS 2024)\nfor basic $d$-dimensional tasks (e.g. estimating the bias of $d$ coins), but\nlittle is known about the measures more generally or in complex settings like\nclassification.\n  Toward this end, we prove a `weak-to-strong' boosting theorem for stability:\nthe randomness complexity of a task $M$ (either under replicability or DP) is\ntightly controlled by the best replication probability of any deterministic\nalgorithm solving the task, a weak measure called `global stability' that is\nuniversally capped at $\\frac{1}{2}$ (Chase et al. FOCS 2023). Using this, we\ncharacterize the randomness complexity of PAC Learning: a class has bounded\nrandomness complexity iff it has finite Littlestone dimension, and moreover\nscales at worst logarithmically in the excess error of the learner. This\nresolves a question of (Chase et al. STOC 2024) who asked for such a\ncharacterization in the equivalent language of (error-dependent)\n`list-replicability'.\n","authors":["Max Hopkins","Shay Moran"],"pdf_url":"https://arxiv.org/pdf/2502.08007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08781v2","updated":"2025-02-11T23:05:30Z","published":"2024-12-11T21:23:24Z","title":"GMem: A Modular Approach for Ultra-Efficient Generative Models","summary":"  Recent studies indicate that the denoising process in deep generative\ndiffusion models implicitly learns and memorizes semantic information from the\ndata distribution. These findings suggest that capturing more complex data\ndistributions requires larger neural networks, leading to a substantial\nincrease in computational demands, which in turn become the primary bottleneck\nin both training and inference of diffusion models. To this end, we introduce\nGMem: A Modular Approach for Ultra-Efficient Generative Models. Our approach\nGMem decouples the memory capacity from model and implements it as a separate,\nimmutable memory set that preserves the essential semantic information in the\ndata. The results are significant: GMem enhances both training, sampling\nefficiency, and diversity generation. This design on one hand reduces the\nreliance on network for memorize complex data distribution and thus enhancing\nboth training and sampling efficiency. On ImageNet at $256 \\times 256$\nresolution, GMem achieves a $50\\times$ training speedup compared to SiT,\nreaching FID $=7.66$ in fewer than $28$ epochs ($\\sim 4$ hours training time),\nwhile SiT requires $1400$ epochs. Without classifier-free guidance, GMem\nachieves state-of-the-art (SoTA) performance FID $=1.53$ in $160$ epochs with\nonly $\\sim 20$ hours of training, outperforming LightningDiT which requires\n$800$ epochs and $\\sim 95$ hours to attain FID $=2.17$.\n","authors":["Yi Tang","Peng Sun","Zhenglin Cheng","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2412.08781v2.pdf","comment":"9 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.08006v1","updated":"2025-02-11T23:05:16Z","published":"2025-02-11T23:05:16Z","title":"Greed is Good: Guided Generation from a Greedy Perspective","summary":"  Training-free guided generation is a widely used and powerful technique that\nallows the end user to exert further control over the generative process of\ndiffusion models. In this work, we explore the guided generation from the\nperspective of optimizing the solution trajectory of a neural differential\nequation in a greedy manner. We present such a strategy as a unifying view on\ntraining-free guidance by showing that the greedy strategy is a first-order\ndiscretization of end-to-end optimization techniques. We show that a greedy\nguidance strategy makes good decisions and compare it to a guidance strategy\nusing the ideal gradients found via the continuous adjoint equations. We then\nshow how other popular training-free guidance strategies can be viewed in a\nunified manner from this perspective.\n","authors":["Zander W. Blasingame","Chen Liu"],"pdf_url":"https://arxiv.org/pdf/2502.08006v1.pdf","comment":"Initial preprint"},{"id":"http://arxiv.org/abs/2502.08005v1","updated":"2025-02-11T23:02:14Z","published":"2025-02-11T23:02:14Z","title":"Towards Training One-Step Diffusion Models Without Distillation","summary":"  Recent advances in one-step generative models typically follow a two-stage\nprocess: first training a teacher diffusion model and then distilling it into a\none-step student model. This distillation process traditionally relies on both\nthe teacher model's score function to compute the distillation loss and its\nweights for student initialization. In this paper, we explore whether one-step\ngenerative models can be trained directly without this distillation process.\nFirst, we show that the teacher's score function is not essential and propose a\nfamily of distillation methods that achieve competitive results without relying\non score estimation. Next, we demonstrate that initialization from teacher\nweights is indispensable in successful training. Surprisingly, we find that\nthis benefit is not due to improved ``input-output\" mapping but rather the\nlearned feature representations, which dominate distillation quality. Our\nfindings provide a better understanding of the role of initialization in\none-step model training and its impact on distillation quality.\n","authors":["Mingtian Zhang","Jiajun He","Wenlin Chen","Zijing Ou","Jos√© Miguel Hern√°ndez-Lobato","Bernhard Sch√∂lkopf","David Barber"],"pdf_url":"https://arxiv.org/pdf/2502.08005v1.pdf","comment":"13 pages, Technical Report"},{"id":"http://arxiv.org/abs/2502.08004v1","updated":"2025-02-11T22:58:18Z","published":"2025-02-11T22:58:18Z","title":"Optimizing Likelihoods via Mutual Information: Bridging Simulation-Based\n  Inference and Bayesian Optimal Experimental Design","summary":"  Simulation-based inference (SBI) is a method to perform inference on a\nvariety of complex scientific models with challenging inference (inverse)\nproblems. Bayesian Optimal Experimental Design (BOED) aims to efficiently use\nexperimental resources to make better inferences. Various stochastic\ngradient-based BOED methods have been proposed as an alternative to Bayesian\noptimization and other experimental design heuristics to maximize information\ngain from an experiment. We demonstrate a link via mutual information bounds\nbetween SBI and stochastic gradient-based variational inference methods that\npermits BOED to be used in SBI applications as SBI-BOED. This link allows\nsimultaneous optimization of experimental designs and optimization of amortized\ninference functions. We evaluate the pitfalls of naive design optimization\nusing this method in a standard SBI task and demonstrate the utility of a\nwell-chosen design distribution in BOED. We compare this approach on SBI-based\nmodels in real-world simulators in epidemiology and biology, showing notable\nimprovements in inference.\n","authors":["Vincent D. Zaballa","Elliot E. Hui"],"pdf_url":"https://arxiv.org/pdf/2502.08004v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2502.08003v1","updated":"2025-02-11T22:57:19Z","published":"2025-02-11T22:57:19Z","title":"Heterogeneous Multi-agent Multi-armed Bandits on Stochastic Block Models","summary":"  We study a novel heterogeneous multi-agent multi-armed bandit problem with a\ncluster structure induced by stochastic block models, influencing not only\ngraph topology, but also reward heterogeneity. Specifically, agents are\ndistributed on random graphs based on stochastic block models - a generalized\nErdos-Renyi model with heterogeneous edge probabilities: agents are grouped\ninto clusters (known or unknown); edge probabilities for agents within the same\ncluster differ from those across clusters. In addition, the cluster structure\nin stochastic block model also determines our heterogeneous rewards. Rewards\ndistributions of the same arm vary across agents in different clusters but\nremain consistent within a cluster, unifying homogeneous and heterogeneous\nsettings and varying degree of heterogeneity, and rewards are independent\nsamples from these distributions. The objective is to minimize system-wide\nregret across all agents. To address this, we propose a novel algorithm\napplicable to both known and unknown cluster settings. The algorithm combines\nan averaging-based consensus approach with a newly introduced information\naggregation and weighting technique, resulting in a UCB-type strategy. It\naccounts for graph randomness, leverages both intra-cluster (homogeneous) and\ninter-cluster (heterogeneous) information from rewards and graphs, and\nincorporates cluster detection for unknown cluster settings. We derive optimal\ninstance-dependent regret upper bounds of order $\\log{T}$ under sub-Gaussian\nrewards. Importantly, our regret bounds capture the degree of heterogeneity in\nthe system (an additional layer of complexity), exhibit smaller constants,\nscale better for large systems, and impose significantly relaxed assumptions on\nedge probabilities. In contrast, prior works have not accounted for this\nrefined problem complexity, rely on more stringent assumptions, and exhibit\nlimited scalability.\n","authors":["Mengfan Xu","Liren Shan","Fatemeh Ghaffari","Xuchuang Wang","Xutong Liu","Mohammad Hajiesmaili"],"pdf_url":"https://arxiv.org/pdf/2502.08003v1.pdf","comment":"55 pages"},{"id":"http://arxiv.org/abs/2502.08001v1","updated":"2025-02-11T22:48:49Z","published":"2025-02-11T22:48:49Z","title":"Unveiling Client Privacy Leakage from Public Dataset Usage in Federated\n  Distillation","summary":"  Federated Distillation (FD) has emerged as a popular federated training\nframework, enabling clients to collaboratively train models without sharing\nprivate data. Public Dataset-Assisted Federated Distillation (PDA-FD), which\nleverages public datasets for knowledge sharing, has become widely adopted.\nAlthough PDA-FD enhances privacy compared to traditional Federated Learning, we\ndemonstrate that the use of public datasets still poses significant privacy\nrisks to clients' private training data. This paper presents the first\ncomprehensive privacy analysis of PDA-FD in presence of an honest-but-curious\nserver. We show that the server can exploit clients' inference results on\npublic datasets to extract two critical types of private information: label\ndistributions and membership information of the private training dataset. To\nquantify these vulnerabilities, we introduce two novel attacks specifically\ndesigned for the PDA-FD setting: a label distribution inference attack and\ninnovative membership inference methods based on Likelihood Ratio Attack\n(LiRA). Through extensive evaluation of three representative PDA-FD frameworks\n(FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance,\nwith label distribution attacks reaching minimal KL-divergence and membership\ninference attacks maintaining high True Positive Rates under low False Positive\nRate constraints. Our findings reveal significant privacy risks in current\nPDA-FD frameworks and emphasize the need for more robust privacy protection\nmechanisms in collaborative learning systems.\n","authors":["Haonan Shi","Tu Ouyang","An Wang"],"pdf_url":"https://arxiv.org/pdf/2502.08001v1.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.07998v1","updated":"2025-02-11T22:34:49Z","published":"2025-02-11T22:34:49Z","title":"Adaptive kernel predictors from feature-learning infinite limits of\n  neural networks","summary":"  Previous influential work showed that infinite width limits of neural\nnetworks in the lazy training regime are described by kernel machines. Here, we\nshow that neural networks trained in the rich, feature learning infinite-width\nregime in two different settings are also described by kernel machines, but\nwith data-dependent kernels. For both cases, we provide explicit expressions\nfor the kernel predictors and prescriptions to numerically calculate them. To\nderive the first predictor, we study the large-width limit of feature-learning\nBayesian networks, showing how feature learning leads to task-relevant\nadaptation of layer kernels and preactivation densities. The saddle point\nequations governing this limit result in a min-max optimization problem that\ndefines the kernel predictor. To derive the second predictor, we study gradient\nflow training of randomly initialized networks trained with weight decay in the\ninfinite-width limit using dynamical mean field theory (DMFT). The fixed point\nequations of the arising DMFT defines the task-adapted internal representations\nand the kernel predictor. We compare our kernel predictors to kernels derived\nfrom lazy regime and demonstrate that our adaptive kernels achieve lower test\nloss on benchmark datasets.\n","authors":["Clarissa Lauditi","Blake Bordelon","Cengiz Pehlevan"],"pdf_url":"https://arxiv.org/pdf/2502.07998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03464v2","updated":"2025-02-11T22:29:52Z","published":"2024-08-06T22:39:34Z","title":"Vision Foundation Models in Remote Sensing: A Survey","summary":"  Artificial Intelligence (AI) technologies have profoundly transformed the\nfield of remote sensing, revolutionizing data collection, processing, and\nanalysis. Traditionally reliant on manual interpretation and task-specific\nmodels, remote sensing research has been significantly enhanced by the advent\nof foundation models-large-scale, pre-trained AI models capable of performing a\nwide array of tasks with unprecedented accuracy and efficiency. This paper\nprovides a comprehensive survey of foundation models in the remote sensing\ndomain. We categorize these models based on their architectures, pre-training\ndatasets, and methodologies. Through detailed performance comparisons, we\nhighlight emerging trends and the significant advancements achieved by those\nfoundation models. Additionally, we discuss technical challenges, practical\nimplications, and future research directions, addressing the need for\nhigh-quality data, computational resources, and improved model generalization.\nOur research also finds that pre-training methods, particularly self-supervised\nlearning techniques like contrastive learning and masked autoencoders,\nremarkably enhance the performance and robustness of foundation models. This\nsurvey aims to serve as a resource for researchers and practitioners by\nproviding a panorama of advances and promising pathways for continued\ndevelopment and application of foundation models in remote sensing.\n","authors":["Siqi Lu","Junlin Guo","James R Zimmer-Dauphinee","Jordan M Nieusma","Xiao Wang","Parker VanValkenburgh","Steven A Wernke","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2408.03464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00972v3","updated":"2025-02-11T22:20:47Z","published":"2024-02-01T19:41:04Z","title":"Closure Discovery for Coarse-Grained Partial Differential Equations\n  Using Grid-based Reinforcement Learning","summary":"  Reliable predictions of critical phenomena, such as weather, wildfires and\nepidemics often rely on models described by Partial Differential Equations\n(PDEs). However, simulations that capture the full range of spatio-temporal\nscales described by such PDEs are often prohibitively expensive. Consequently,\ncoarse-grained simulations are usually deployed that adopt various heuristics\nand empirical closure terms to account for the missing information. We propose\na novel and systematic approach for identifying closures in under-resolved PDEs\nusing grid-based Reinforcement Learning. This formulation incorporates\ninductive bias and exploits locality by deploying a central policy represented\nefficiently by a Fully Convolutional Network (FCN). We demonstrate the\ncapabilities and limitations of our framework through numerical solutions of\nthe advection equation and the Burgers' equation. Our results show accurate\npredictions for in- and out-of-distribution test cases as well as a significant\nspeedup compared to resolving all scales.\n","authors":["Jan-Philipp von Bassewitz","Sebastian Kaltenbach","Petros Koumoutsakos"],"pdf_url":"https://arxiv.org/pdf/2402.00972v3.pdf","comment":"Conference on Parsimony and Learning (CPAL)"},{"id":"http://arxiv.org/abs/2502.07993v1","updated":"2025-02-11T22:19:56Z","published":"2025-02-11T22:19:56Z","title":"What is a Sketch-and-Precondition Derivation for Low-Rank Approximation?\n  Inverse Power Error or Inverse Power Estimation?","summary":"  Randomized sketching accelerates large-scale numerical linear algebra by\nreducing computa- tional complexity. While the traditional sketch-and-solve\napproach reduces the problem size di- rectly through sketching, the\nsketch-and-precondition method leverages sketching to construct a computational\nfriendly preconditioner. This preconditioner improves the convergence speed of\niterative solvers applied to the original problem, maintaining accuracy in the\nfull space. Further- more, the convergence rate of the solver improves at least\nlinearly with the sketch size. Despite its potential, developing a\nsketch-and-precondition framework for randomized algorithms in low- rank matrix\napproximation remains an open challenge. We introduce the Error-Powered\nSketched Inverse Iteration (EPSI) Method via run sketched Newton iteration for\nthe Lagrange form as a sketch-and-precondition variant for randomized low-rank\napproximation. Our method achieves theoretical guarantees, including a\nconvergence rate that improves at least linearly with the sketch size.\n","authors":["Ruihan Xu","Yiping Lu"],"pdf_url":"https://arxiv.org/pdf/2502.07993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07990v1","updated":"2025-02-11T22:14:30Z","published":"2025-02-11T22:14:30Z","title":"Learning Effective Dynamics across Spatio-Temporal Scales of Complex\n  Flows","summary":"  Modeling and simulation of complex fluid flows with dynamics that span\nmultiple spatio-temporal scales is a fundamental challenge in many scientific\nand engineering domains. Full-scale resolving simulations for systems such as\nhighly turbulent flows are not feasible in the foreseeable future, and\nreduced-order models must capture dynamics that involve interactions across\nscales. In the present work, we propose a novel framework, Graph-based Learning\nof Effective Dynamics (Graph-LED), that leverages graph neural networks (GNNs),\nas well as an attention-based autoregressive model, to extract the effective\ndynamics from a small amount of simulation data. GNNs represent flow fields on\nunstructured meshes as graphs and effectively handle complex geometries and\nnon-uniform grids. The proposed method combines a GNN based, dimensionality\nreduction for variable-size unstructured meshes with an autoregressive temporal\nattention model that can learn temporal dependencies automatically. We\nevaluated the proposed approach on a suite of fluid dynamics problems,\nincluding flow past a cylinder and flow over a backward-facing step over a\nrange of Reynolds numbers. The results demonstrate robust and effective\nforecasting of spatio-temporal physics; in the case of the flow past a\ncylinder, both small-scale effects that occur close to the cylinder as well as\nits wake are accurately captured.\n","authors":["Han Gao","Sebastian Kaltenbach","Petros Koumoutsakos"],"pdf_url":"https://arxiv.org/pdf/2502.07990v1.pdf","comment":"Conference on Parsimony and Learning (CPAL)"},{"id":"http://arxiv.org/abs/2502.07980v1","updated":"2025-02-11T21:53:48Z","published":"2025-02-11T21:53:48Z","title":"CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning\n  Capabilities of LLMs","summary":"  The role of Large Language Models (LLMs) has not been extensively explored in\nanalog circuit design, which could benefit from a reasoning-based approach that\ntranscends traditional optimization techniques. In particular, despite their\ngrowing relevance, there are no benchmarks to assess LLMs' reasoning capability\nabout circuits. Therefore, we created the CIRCUIT dataset consisting of 510\nquestion-answer pairs spanning various levels of analog-circuit-related\nsubjects. The best-performing model on our dataset, GPT-4o, achieves 48.04%\naccuracy when evaluated on the final numerical answer. To evaluate the\nrobustness of LLMs on our dataset, we introduced a unique feature that enables\nunit-test-like evaluation by grouping questions into unit tests. In this case,\nGPT-4o can only pass 27.45% of the unit tests, highlighting that the most\nadvanced LLMs still struggle with understanding circuits, which requires\nmulti-level reasoning, particularly when involving circuit topologies. This\ncircuit-specific benchmark highlights LLMs' limitations, offering valuable\ninsights for advancing their application in analog integrated circuit design.\n","authors":["Lejla Skelic","Yan Xu","Matthew Cox","Wenjie Lu","Tao Yu","Ruonan Han"],"pdf_url":"https://arxiv.org/pdf/2502.07980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07978v1","updated":"2025-02-11T21:52:19Z","published":"2025-02-11T21:52:19Z","title":"A Survey of In-Context Reinforcement Learning","summary":"  Reinforcement learning (RL) agents typically optimize their policies by\nperforming expensive backward passes to update their network parameters.\nHowever, some agents can solve new tasks without updating any parameters by\nsimply conditioning on additional context such as their action-observation\nhistories. This paper surveys work on such behavior, known as in-context\nreinforcement learning.\n","authors":["Amir Moeini","Jiuqi Wang","Jacob Beck","Ethan Blaser","Shimon Whiteson","Rohan Chandra","Shangtong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07977v1","updated":"2025-02-11T21:48:10Z","published":"2025-02-11T21:48:10Z","title":"RESIST: Resilient Decentralized Learning Using Consensus Gradient\n  Descent","summary":"  Empirical risk minimization (ERM) is a cornerstone of modern machine learning\n(ML), supported by advances in optimization theory that ensure efficient\nsolutions with provable algorithmic convergence rates, which measure the speed\nat which optimization algorithms approach a solution, and statistical learning\nrates, which characterize how well the solution generalizes to unseen data.\nPrivacy, memory, computational, and communications constraints increasingly\nnecessitate data collection, processing, and storage across network-connected\ndevices. In many applications, these networks operate in decentralized settings\nwhere a central server cannot be assumed, requiring decentralized ML algorithms\nthat are both efficient and resilient. Decentralized learning, however, faces\nsignificant challenges, including an increased attack surface for adversarial\ninterference during decentralized learning processes. This paper focuses on the\nman-in-the-middle (MITM) attack, which can cause models to deviate\nsignificantly from their intended ERM solutions. To address this challenge, we\npropose RESIST (Resilient dEcentralized learning using conSensus gradIent\ndeScenT), an optimization algorithm designed to be robust against adversarially\ncompromised communication links. RESIST achieves algorithmic and statistical\nconvergence for strongly convex, Polyak-Lojasiewicz, and nonconvex ERM\nproblems. Experimental results demonstrate the robustness and scalability of\nRESIST for real-world decentralized learning in adversarial environments.\n","authors":["Cheng Fang","Rishabh Dixit","Waheed U. Bajwa","Mert Gurbuzbalaban"],"pdf_url":"https://arxiv.org/pdf/2502.07977v1.pdf","comment":"preprint of a journal paper; 100 pages and 17 figures"},{"id":"http://arxiv.org/abs/2405.02119v2","updated":"2025-02-11T21:40:27Z","published":"2024-05-03T14:19:40Z","title":"EnvId: A Metric Learning Approach for Forensic Few-Shot Identification\n  of Unseen Environments","summary":"  Audio recordings may provide important evidence in criminal investigations.\nOne such case is the forensic association of a recorded audio to its recording\nlocation. For example, a voice message may be the only investigative cue to\nnarrow down the candidate sites for a crime. Up to now, several works provide\nsupervised classification tools for closed-set recording environment\nidentification under relatively clean recording conditions. However, in\nforensic investigations, the candidate locations are case-specific. Thus,\nsupervised learning techniques are not applicable without retraining a\nclassifier on a sufficient amount of training samples for each case and\nrespective candidate set. In addition, a forensic tool has to deal with audio\nmaterial from uncontrolled sources with variable properties and quality. In\nthis work, we therefore attempt a major step towards practical forensic\napplication scenarios. We propose a representation learning framework called\nEnvId, short for environment identification. EnvId avoids case-specific\nretraining by modeling the task as a few-shot classification problem. We\ndemonstrate that EnvId can handle forensically challenging material. It\nprovides good quality predictions even under unseen signal degradations,\nout-of-distribution reverberation characteristics or recording position\nmismatches.\n","authors":["Denise Moussa","Germans Hirsch","Christian Riess"],"pdf_url":"https://arxiv.org/pdf/2405.02119v2.pdf","comment":"Accepted at TIFS"},{"id":"http://arxiv.org/abs/2502.07975v1","updated":"2025-02-11T21:40:11Z","published":"2025-02-11T21:40:11Z","title":"Sink equilibria and the attractors of learning in games","summary":"  Characterizing the limit behavior -- that is, the attractors -- of learning\ndynamics is one of the most fundamental open questions in game theory. In\nrecent work in this front, it was conjectured that the attractors of the\nreplicator dynamic are in one-to-one correspondence with the sink equilibria of\nthe game -- the sink strongly connected components of a game's preference graph\n-- , and it was established that they do stand in at least one-to-many\ncorrespondence with them. We make threefold progress on the problem of\ncharacterizing attractors. First, we show through a topological construction\nthat the one-to-one conjecture is false. Second, we make progress on the\nattractor characterization problem for two-player games by establishing that\nthe one-to-one conjecture is true in the absence of a local pattern called a\nweak local source -- a pattern that is absent from zero-sum games. Finally, we\nlook -- for the first time in this context -- at fictitious play, the\nlongest-studied learning dynamic, and examine to what extent the conjecture\ngeneralizes there. We establish that under fictitious play, sink equilibria\nalways contain attractors (sometimes strictly), and every attractor corresponds\nto a strongly connected set of nodes in the preference graph.\n","authors":["Oliver Biggar","Christos Papadimitriou"],"pdf_url":"https://arxiv.org/pdf/2502.07975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16868v2","updated":"2025-02-11T21:39:32Z","published":"2024-08-29T19:22:37Z","title":"Characterization of point-source transient events with a rolling-shutter\n  compressed sensing system","summary":"  Point-source transient events (PSTEs) - optical events that are both\nextremely fast and extremely small - pose several challenges to an imaging\nsystem. Due to their speed, accurately characterizing such events often\nrequires detectors with very high frame rates. Due to their size, accurately\ndetecting such events requires maintaining coverage over an extended\nfield-of-view, often through the use of imaging focal plane arrays (FPA) with a\nglobal shutter readout. Traditional imaging systems that meet these\nrequirements are costly in terms of price, size, weight, power consumption, and\ndata bandwidth, and there is a need for cheaper solutions with adequate\ntemporal and spatial coverage. To address these issues, we develop a novel\ncompressed sensing algorithm adapted to the rolling shutter readout of an\nimaging system. This approach enables reconstruction of a PSTE signature at the\nsampling rate of the rolling shutter, offering a 1-2 order of magnitude\ntemporal speedup and a proportional reduction in data bandwidth. We present\nempirical results demonstrating accurate recovery of PSTEs using measurements\nthat are spatially undersampled by a factor of 25, and our simulations show\nthat, relative to other compressed sensing algorithms, our algorithm is both\nfaster and yields higher quality reconstructions. We also present theoretical\nresults characterizing our algorithm and corroborating simulations. The\npotential impact of our work includes the development of much faster, cheaper\nsensor solutions for PSTE detection and characterization.\n","authors":["Frank Qiu","Joshua Michalenko","Lilian K. Casias","Cameron J. Radosevich","Jon Slater","Eric A. Shields"],"pdf_url":"https://arxiv.org/pdf/2408.16868v2.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.07974v1","updated":"2025-02-11T21:37:19Z","published":"2025-02-11T21:37:19Z","title":"From Hazard Identification to Controller Design: Proactive and\n  LLM-Supported Safety Engineering for ML-Powered Systems","summary":"  Machine learning (ML) components are increasingly integrated into software\nproducts, yet their complexity and inherent uncertainty often lead to\nunintended and hazardous consequences, both for individuals and society at\nlarge. Despite these risks, practitioners seldom adopt proactive approaches to\nanticipate and mitigate hazards before they occur. Traditional safety\nengineering approaches, such as Failure Mode and Effects Analysis (FMEA) and\nSystem Theoretic Process Analysis (STPA), offer systematic frameworks for early\nrisk identification but are rarely adopted. This position paper advocates for\nintegrating hazard analysis into the development of any ML-powered software\nproduct and calls for greater support to make this process accessible to\ndevelopers. By using large language models (LLMs) to partially automate a\nmodified STPA process with human oversight at critical steps, we expect to\naddress two key challenges: the heavy dependency on highly experienced safety\nengineering experts, and the time-consuming, labor-intensive nature of\ntraditional hazard analysis, which often impedes its integration into\nreal-world development workflows. We illustrate our approach with a running\nexample, demonstrating that many seemingly unanticipated issues can, in fact,\nbe anticipated.\n","authors":["Yining Hong","Christopher S. Timperley","Christian K√§stner"],"pdf_url":"https://arxiv.org/pdf/2502.07974v1.pdf","comment":"Accepted for publication at the International Conference on AI\n  Engineering (CAIN) 2025"},{"id":"http://arxiv.org/abs/2307.07735v3","updated":"2025-02-11T21:37:03Z","published":"2023-07-15T07:19:29Z","title":"Faster Algorithms for Structured Linear and Kernel Support Vector\n  Machines","summary":"  Quadratic programming is a ubiquitous prototype in convex programming. Many\nmachine learning problems can be formulated as quadratic programming, including\nthe famous Support Vector Machines (SVMs). Linear and kernel SVMs have been\namong the most popular models in machine learning over the past three decades,\nprior to the deep learning era.\n  Generally, a quadratic program has an input size of $\\Theta(n^2)$, where $n$\nis the number of variables. Assuming the Strong Exponential Time Hypothesis\n($\\textsf{SETH}$), it is known that no $O(n^{2-o(1)})$ time algorithm exists\nwhen the quadratic objective matrix is positive semidefinite (Backurs, Indyk,\nand Schmidt, NeurIPS'17). However, problems such as SVMs usually admit much\nsmaller input sizes: one is given $n$ data points, each of dimension $d$, and\n$d$ is oftentimes much smaller than $n$. Furthermore, the SVM program has only\n$O(1)$ equality linear constraints. This suggests that faster algorithms are\nfeasible, provided the program exhibits certain structures.\n  In this work, we design the first nearly-linear time algorithm for solving\nquadratic programs whenever the quadratic objective admits a low-rank\nfactorization, and the number of linear constraints is small. Consequently, we\nobtain results for SVMs:\n  * For linear SVM when the input data is $d$-dimensional, our algorithm runs\nin time $\\widetilde O(nd^{(\\omega+1)/2}\\log(1/\\epsilon))$ where $\\omega\\approx\n2.37$ is the fast matrix multiplication exponent;\n  * For Gaussian kernel SVM, when the data dimension $d = {\\color{black}O(\\log\nn)}$ and the squared dataset radius is sub-logarithmic in $n$, our algorithm\nruns in time $O(n^{1+o(1)}\\log(1/\\epsilon))$. We also prove that when the\nsquared dataset radius is at least $\\Omega(\\log^2 n)$, then\n$\\Omega(n^{2-o(1)})$ time is required. This improves upon the prior best lower\nbound in both the dimension $d$ and the squared dataset radius.\n","authors":["Yuzhou Gu","Zhao Song","Lichen Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.07735v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07971v1","updated":"2025-02-11T21:35:13Z","published":"2025-02-11T21:35:13Z","title":"ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval","summary":"  Document retrieval is a core component of question-answering systems, as it\nenables conditioning answer generation on new and large-scale corpora. While\neffective, the standard practice of encoding documents into high-dimensional\nembeddings for similarity search entails large memory and compute footprints,\nand also makes it hard to inspect the inner workings of the system. In this\npaper, we propose a tree-based method for organizing and representing reference\ndocuments at various granular levels, which offers the flexibility to balance\ncost and utility, and eases the inspection of the corpus content and retrieval\noperations. Our method, called ReTreever, jointly learns a routing function per\ninternal node of a binary tree such that query and reference documents are\nassigned to similar tree branches, hence directly optimizing for retrieval\nperformance. Our evaluations show that ReTreever generally preserves full\nrepresentation accuracy. Its hierarchical structure further provides strong\ncoarse representations and enhances transparency by indirectly learning\nmeaningful semantic groupings. Among hierarchical retrieval methods, ReTreever\nachieves the best retrieval accuracy at the lowest latency, proving that this\nfamily of techniques can be viable in practical applications.\n","authors":["Shubham Gupta","Zichao Li","Tianyi Chen","Cem Subakan","Siva Reddy","Perouz Taslakian","Valentina Zantedeschi"],"pdf_url":"https://arxiv.org/pdf/2502.07971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07968v1","updated":"2025-02-11T21:24:13Z","published":"2025-02-11T21:24:13Z","title":"Generative Risk Minimization for Out-of-Distribution Generalization on\n  Graphs","summary":"  Out-of-distribution (OOD) generalization on graphs aims at dealing with\nscenarios where the test graph distribution differs from the training graph\ndistributions. Compared to i.i.d. data like images, the OOD generalization\nproblem on graph-structured data remains challenging due to the non-i.i.d.\nproperty and complex structural information on graphs. Recently, several works\non graph OOD generalization have explored extracting invariant subgraphs that\nshare crucial classification information across different distributions.\nNevertheless, such a strategy could be suboptimal for entirely capturing the\ninvariant information, as the extraction of discrete structures could\npotentially lead to the loss of invariant information or the involvement of\nspurious information. In this paper, we propose an innovative framework, named\nGenerative Risk Minimization (GRM), designed to generate an invariant subgraph\nfor each input graph to be classified, instead of extraction. To address the\nchallenge of optimization in the absence of optimal invariant subgraphs (i.e.,\nground truths), we derive a tractable form of the proposed GRM objective by\nintroducing a latent causal variable, and its effectiveness is validated by our\ntheoretical analysis. We further conduct extensive experiments across a variety\nof real-world graph datasets for both node-level and graph-level OOD\ngeneralization, and the results demonstrate the superiority of our framework\nGRM.\n","authors":["Song Wang","Zhen Tan","Yaochen Zhu","Chuxu Zhang","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2502.07968v1.pdf","comment":"TMLR 02/2025"},{"id":"http://arxiv.org/abs/2502.07964v1","updated":"2025-02-11T21:21:28Z","published":"2025-02-11T21:21:28Z","title":"New tools for comparing classical and neural ODE models for tumor growth","summary":"  A new computational tool TumorGrowth.jl for modeling tumor growth is\nintroduced. The tool allows the comparison of standard textbook models, such as\nGeneral Bertalanffy and Gompertz, with some newer models, including, for the\nfirst time, neural ODE models. As an application, we revisit a human meta-study\nof non-small cell lung cancer and bladder cancer lesions, in patients\nundergoing two different treatment options, to determine if previously reported\nperformance differences are statistically significant, and if newer, more\ncomplex models perform any better. In a population of examples with at least\nfour time-volume measurements available for calibration, and an average of\nabout 6.3, our main conclusion is that the General Bertalanffy model has\nsuperior performance, on average. However, where more measurements are\navailable, we argue that more complex models, capable of capturing rebound and\nrelapse behavior, may be better choices.\n","authors":["Anthony D. Blaom","Samuel Okon"],"pdf_url":"https://arxiv.org/pdf/2502.07964v1.pdf","comment":"9 pages, 2 figures. Related software is archived at\n  https://github.com/ablaom/TumorGrowth.jl"},{"id":"http://arxiv.org/abs/2502.07962v1","updated":"2025-02-11T21:20:48Z","published":"2025-02-11T21:20:48Z","title":"ESPFormer: Doubly-Stochastic Attention with Expected Sliced Transport\n  Plans","summary":"  While self-attention has been instrumental in the success of Transformers, it\ncan lead to over-concentration on a few tokens during training, resulting in\nsuboptimal information flow. Enforcing doubly-stochastic constraints in\nattention matrices has been shown to improve structure and balance in attention\ndistributions. However, existing methods rely on iterative Sinkhorn\nnormalization, which is computationally costly. In this paper, we introduce a\nnovel, fully parallelizable doubly-stochastic attention mechanism based on\nsliced optimal transport, leveraging Expected Sliced Transport Plans (ESP).\nUnlike prior approaches, our method enforces double stochasticity without\niterative Sinkhorn normalization, significantly enhancing efficiency. To ensure\ndifferentiability, we incorporate a temperature-based soft sorting technique,\nenabling seamless integration into deep learning models. Experiments across\nmultiple benchmark datasets, including image classification, point cloud\nclassification, sentiment analysis, and neural machine translation, demonstrate\nthat our enhanced attention regularization consistently improves performance\nacross diverse applications.\n","authors":["Ashkan Shahbazi","Elaheh Akbari","Darian Salehi","Xinran Liu","Navid Naderializadeh","Soheil Kolouri"],"pdf_url":"https://arxiv.org/pdf/2502.07962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07951v1","updated":"2025-02-11T21:00:01Z","published":"2025-02-11T21:00:01Z","title":"Federated Self-supervised Domain Generalization for Label-efficient\n  Polyp Segmentation","summary":"  Employing self-supervised learning (SSL) methodologies assumes par-amount\nsignificance in handling unlabeled polyp datasets when building deep\nlearning-based automatic polyp segmentation models. However, the intricate\nprivacy dynamics surrounding medical data often preclude seamless data sharing\namong disparate medical centers. Federated learning (FL) emerges as a\nformidable solution to this privacy conundrum, yet within the realm of FL,\noptimizing model generalization stands as a pressing imperative. Robust\ngeneralization capabilities are imperative to ensure the model's efficacy\nacross diverse geographical domains post-training on localized client datasets.\nIn this paper, a Federated self-supervised Domain Generalization method is\nproposed to enhance the generalization capacity of federated and\nLabel-efficient intestinal polyp segmentation, named LFDG. Based on a classical\nSSL method, DropPos, LFDG proposes an adversarial learning-based data\naugmentation method (SSADA) to enhance the data diversity. LFDG further\nproposes a relaxation module based on Source-reconstruction and\nAugmentation-masking (SRAM) to maintain stability in feature learning. We have\nvalidated LFDG on polyp images from six medical centers. The performance of our\nmethod achieves 3.80% and 3.92% better than the baseline and other recent FL\nmethods and SSL methods, respectively.\n","authors":["Xinyi Tan","Jiacheng Wang","Liansheng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07951v1.pdf","comment":"Accepted at ADSMI @ MICCAI 2024"},{"id":"http://arxiv.org/abs/2502.07949v1","updated":"2025-02-11T20:57:46Z","published":"2025-02-11T20:57:46Z","title":"VSC-RL: Advancing Autonomous Vision-Language Agents with Variational\n  Subgoal-Conditioned Reinforcement Learning","summary":"  State-of-the-art (SOTA) reinforcement learning (RL) methods enable the\nvision-language agents to learn from interactions with the environment without\nhuman supervision. However, they struggle with learning inefficiencies in\ntackling real-world complex sequential decision-making tasks, especially with\nsparse reward signals and long-horizon dependencies. To effectively address the\nissue, we introduce Variational Subgoal-Conditioned RL (VSC-RL), which\nreformulates the vision-language sequential decision-making task as a\nvariational goal-conditioned RL problem, allowing us to leverage advanced\noptimization methods to enhance learning efficiency. Specifically, VSC-RL\noptimizes the SubGoal Evidence Lower BOund (SGC-ELBO), which consists of (a)\nmaximizing the subgoal-conditioned return via RL and (b) minimizing the\nsubgoal-conditioned difference with the reference policy. We theoretically\ndemonstrate that SGC-ELBO is equivalent to the original optimization objective,\nensuring improved learning efficiency without sacrificing performance\nguarantees. Additionally, for real-world complex decision-making tasks, VSC-RL\nleverages the vision-language model to autonomously decompose the goal into\nfeasible subgoals, enabling efficient learning. Across various benchmarks,\nincluding challenging real-world mobile device control tasks, VSC-RL\nsignificantly outperforms the SOTA vision-language agents, achieving superior\nperformance and remarkable improvement in learning efficiency.\n","authors":["Qingyuan Wu","Jianheng Liu","Jianye Hao","Jun Wang","Kun Shao"],"pdf_url":"https://arxiv.org/pdf/2502.07949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07945v1","updated":"2025-02-11T20:49:13Z","published":"2025-02-11T20:49:13Z","title":"SurGrID: Controllable Surgical Simulation via Scene Graph to Image\n  Diffusion","summary":"  Surgical simulation offers a promising addition to conventional surgical\ntraining. However, available simulation tools lack photorealism and rely on\nhardcoded behaviour. Denoising Diffusion Models are a promising alternative for\nhigh-fidelity image synthesis, but existing state-of-the-art conditioning\nmethods fall short in providing precise control or interactivity over the\ngenerated scenes.\n  We introduce SurGrID, a Scene Graph to Image Diffusion Model, allowing for\ncontrollable surgical scene synthesis by leveraging Scene Graphs. These graphs\nencode a surgical scene's components' spatial and semantic information, which\nare then translated into an intermediate representation using our novel\npre-training step that explicitly captures local and global information.\n  Our proposed method improves the fidelity of generated images and their\ncoherence with the graph input over the state-of-the-art. Further, we\ndemonstrate the simulation's realism and controllability in a user assessment\nstudy involving clinical experts.\n  Scene Graphs can be effectively used for precise and interactive conditioning\nof Denoising Diffusion Models for simulating surgical scenes, enabling high\nfidelity and interactive control over the generated content.\n","authors":["Yannik Frisch","Ssharvien Kumar Sivakumar","√áaƒühan K√∂ksal","Elsa B√∂hm","Felix Wagner","Adrian Gericke","Ghazal Ghazaei","Anirban Mukhopadhyay"],"pdf_url":"https://arxiv.org/pdf/2502.07945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07942v1","updated":"2025-02-11T20:41:49Z","published":"2025-02-11T20:41:49Z","title":"Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths\n  of Large and Small LLMs","summary":"  Web browsing agents powered by large language models (LLMs) have shown\ntremendous potential in automating complex web-based tasks. Existing approaches\ntypically rely on large LLMs (e.g., GPT-4o) to explore web environments and\ngenerate trajectory data, which is then used either for demonstration retrieval\n(for large LLMs) or to distill small LLMs (e.g., Llama3) in a process that\nremains decoupled from the exploration. In this paper, we propose\nAgentSymbiotic, an iterative framework that couples data synthesis with\ntask-performance, yielding a \"symbiotic improvement\" for both large and small\nLLMs. Our study uncovers a complementary dynamic between LLM types: while large\nLLMs excel at generating high-quality trajectories for distillation, the\ndistilled small LLMs-owing to their distinct reasoning capabilities-often\nchoose actions that diverge from those of their larger counterparts. This\ndivergence drives the exploration of novel trajectories, thereby enriching the\nsynthesized data. However, we also observe that the performance of small LLMs\nbecomes a bottleneck in this iterative enhancement process. To address this, we\npropose two innovations in LLM distillation: a speculative data synthesis\nstrategy that mitigates off-policy bias, and a multi-task learning approach\ndesigned to boost the reasoning capabilities of the student LLM. Furthermore,\nwe introduce a Hybrid Mode for Privacy Preservation to address user privacy\nconcerns. Evaluated on the WEBARENA benchmark, AgentSymbiotic achieves SOTA\nperformance with both LLM types. Our best Large LLM agent reaches 52%,\nsurpassing the previous best of 45%, while our 8B distilled model demonstrates\na competitive 49%, exceeding the prior best of 28%. Code will be released upon\nacceptance.\n","authors":["Ruichen Zhang","Mufan Qiu","Zhen Tan","Mohan Zhang","Vincent Lu","Jie Peng","Kaidi Xu","Leandro Z. Agudelo","Peter Qian","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07939v1","updated":"2025-02-11T20:36:23Z","published":"2025-02-11T20:36:23Z","title":"Discrete Markov Probabilistic Models","summary":"  This paper introduces the Discrete Markov Probabilistic Model (DMPM), a novel\nalgorithm for discrete data generation. The algorithm operates in the space of\nbits $\\{0,1\\}^d$, where the noising process is a continuous-time Markov chain\nthat can be sampled exactly via a Poissonian clock that flips labels uniformly\nat random. The time-reversal process, like the forward noise process, is a jump\nprocess, with its intensity governed by a discrete analogue of the classical\nscore function. Crucially, this intensity is proven to be the conditional\nexpectation of a function of the forward process, strengthening its theoretical\nalignment with score-based generative models while ensuring robustness and\nefficiency. We further establish convergence bounds for the algorithm under\nminimal assumptions and demonstrate its effectiveness through experiments on\nlow-dimensional Bernoulli-distributed datasets and high-dimensional binary\nMNIST data. The results highlight its strong performance in generating discrete\nstructures. This work bridges theoretical foundations and practical\napplications, advancing the development of effective and theoretically grounded\ndiscrete generative modeling.\n","authors":["Le-Tuyet-Nhi Pham","Dario Shariatian","Antonio Ocello","Giovanni Conforti","Alain Durmus"],"pdf_url":"https://arxiv.org/pdf/2502.07939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03361v3","updated":"2025-02-11T20:33:14Z","published":"2024-06-05T15:14:58Z","title":"What Matters in Hierarchical Search for Combinatorial Reasoning\n  Problems?","summary":"  Efficiently tackling combinatorial reasoning problems, particularly the\nnotorious NP-hard tasks, remains a significant challenge for AI research.\nRecent efforts have sought to enhance planning by incorporating hierarchical\nhigh-level search strategies, known as subgoal methods. While promising, their\nperformance against traditional low-level planners is inconsistent, raising\nquestions about their application contexts. In this study, we conduct an\nin-depth exploration of subgoal-planning methods for combinatorial reasoning.\nWe identify the attributes pivotal for leveraging the advantages of high-level\nsearch: hard-to-learn value functions, complex action spaces, presence of dead\nends in the environment, or using data collected from diverse experts. We\npropose a consistent evaluation methodology to achieve meaningful comparisons\nbetween methods and reevaluate the state-of-the-art algorithms.\n","authors":["Micha≈Ç Zawalski","Gracjan G√≥ral","Micha≈Ç Tyrolski","Emilia Wi≈õnios","Franciszek Budrowski","Marek Cygan","≈Åukasz Kuci≈Ñski","Piotr Mi≈Ço≈õ"],"pdf_url":"https://arxiv.org/pdf/2406.03361v3.pdf","comment":"Accepted for Generative Models for Decision Making Workshop at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2502.07937v1","updated":"2025-02-11T20:31:59Z","published":"2025-02-11T20:31:59Z","title":"Active Advantage-Aligned Online Reinforcement Learning with Offline Data","summary":"  Online reinforcement learning (RL) enhances policies through direct\ninteractions with the environment, but faces challenges related to sample\nefficiency. In contrast, offline RL leverages extensive pre-collected data to\nlearn policies, but often produces suboptimal results due to limited data\ncoverage. Recent efforts have sought to integrate offline and online RL in\norder to harness the advantages of both approaches. However, effectively\ncombining online and offline RL remains challenging due to issues that include\ncatastrophic forgetting, lack of robustness and sample efficiency. In an effort\nto address these challenges, we introduce A3 RL , a novel method that actively\nselects data from combined online and offline sources to optimize policy\nimprovement. We provide theoretical guarantee that validates the effectiveness\nour active sampling strategy and conduct thorough empirical experiments showing\nthat our method outperforms existing state-of-the-art online RL techniques that\nutilize offline data. Our code will be publicly available at:\nhttps://github.com/xuefeng-cs/A3RL.\n","authors":["Xuefeng Liu","Hung T. C. Le","Siyu Chen","Rick Stevens","Zhuoran Yang","Matthew R. Walter","Yuxin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06111v2","updated":"2025-02-11T20:25:11Z","published":"2025-02-10T02:46:29Z","title":"CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science\n  Research Repositories","summary":"  The increasing complexity of computer science research projects demands more\neffective tools for deploying code repositories. Large Language Models (LLMs),\nsuch as Anthropic Claude and Meta Llama, have demonstrated significant\nadvancements across various fields of computer science research, including the\nautomation of diverse software engineering tasks. To evaluate the effectiveness\nof LLMs in handling complex code development tasks of research projects,\nparticularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark\nfor Computer Science Research projects. This benchmark assesses LLMs from\nvarious aspects including accuracy, efficiency, and deployment script quality,\naiming to explore their potential in conducting computer science research\nautonomously. We also introduce a novel framework, CSR-Agents, that utilizes\nmultiple LLM agents to automate the deployment of GitHub code repositories of\ncomputer science research projects. Specifically, by checking instructions from\nmarkdown files and interpreting repository structures, the model generates and\niteratively improves bash commands that set up the experimental environments\nand deploy the code to conduct research tasks. Preliminary results from\nCSR-Bench indicate that LLM agents can significantly enhance the workflow of\nrepository deployment, thereby boosting developer productivity and improving\nthe management of developmental workflows.\n","authors":["Yijia Xiao","Runhui Wang","Luyang Kong","Davor Golac","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06111v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11105v2","updated":"2025-02-11T20:18:12Z","published":"2024-10-14T21:33:10Z","title":"Emulators for stellar profiles in binary population modeling","summary":"  Knowledge about the internal physical structure of stars is crucial to\nunderstanding their evolution. The novel binary population synthesis code\nPOSYDON includes a module for interpolating the stellar and binary properties\nof any system at the end of binary MESA evolution based on a pre-computed set\nof models. In this work, we present a new emulation method for predicting\nstellar profiles, i.e., the internal stellar structure along the radial axis,\nusing machine learning techniques. We use principal component analysis for\ndimensionality reduction and fully-connected feed-forward neural networks for\nmaking predictions. We find accuracy to be comparable to that of nearest\nneighbor approximation, with a strong advantage in terms of memory and storage\nefficiency. By providing a versatile framework for modeling stellar internal\nstructure, the emulation method presented here will enable faster simulations\nof higher physical fidelity, offering a foundation for a wide range of\nlarge-scale population studies of stellar and binary evolution.\n","authors":["Elizabeth Teng","Ugur Demir","Zoheyr Doctor","Philipp M. Srivastava","Shamal Lalvani","Vicky Kalogera","Aggelos Katsaggelos","Jeff J. Andrews","Simone S. Bavera","Max M. Briel","Seth Gossage","Konstantinos Kovlakas","Matthias U. Kruckow","Kyle Akira Rocha","Meng Sun","Zepei Xing","Emmanouil Zapartas"],"pdf_url":"https://arxiv.org/pdf/2410.11105v2.pdf","comment":"12 pages, 10 figures. Accepted for publication by Astronomy and\n  Computing"},{"id":"http://arxiv.org/abs/2502.07923v1","updated":"2025-02-11T19:54:11Z","published":"2025-02-11T19:54:11Z","title":"Sign Operator for Coping with Heavy-Tailed Noise: High Probability\n  Convergence Bounds with Extensions to Distributed Optimization and Comparison\n  Oracle","summary":"  The growing popularity of AI optimization problems involving severely\ncorrupted data has increased the demand for methods capable of handling\nheavy-tailed noise, i.e., noise with bounded $\\kappa$-th moment, $\\kappa \\in\n(1,2]$. For the widely used clipping technique, effectiveness heavily depends\non the careful tuning of clipping levels throughout training. In this paper, we\ndemonstrate that using only the sign of the input, without introducing\nadditional hyperparameters, is sufficient to cope with heavy-tailed noise\neffectively. For smooth non-convex functions, we prove that SignSGD achieves\noptimal sample complexity $\\tilde{O}\\left(\\varepsilon^{-\\frac{3\\kappa -\n2}{\\kappa - 1}}\\right)$ with high probability for attaining an average gradient\nnorm accuracy of $\\varepsilon$. Under the assumption of symmetric noise, we use\nSignSGD with Majority Voting to extend this bound to the distributed\noptimization or reduce the sample complexity to $\\tilde{O}(\\varepsilon^{-4})$\nin the case of a single worker with arbitrary parameters. Furthermore, we\nexplore the application of the sign operator in zeroth-order optimization with\nan oracle that can only compare function values at two different points. We\npropose a novel method, MajorityVote-CompsSGD, and provide the first-known\nhigh-probability bound $\\tilde{O}(\\varepsilon^{-6})$ for the number of\ncomparisons under symmetric noise assumption. Our theoretical findings are\nsupported by the superior performance of sign-based methods in training Large\nLanguage Models.\n","authors":["Nikita Kornilov","Philip Zmushko","Andrei Semenov","Alexander Gasnikov","Alexander Beznosikov"],"pdf_url":"https://arxiv.org/pdf/2502.07923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05684v2","updated":"2025-02-11T19:45:20Z","published":"2025-02-08T20:33:06Z","title":"Machine Unlearning via Information Theoretic Regularization","summary":"  How can we effectively remove or \"unlearn\" undesirable information, such as\nspecific features or individual data points, from a learning outcome while\nminimizing utility loss and ensuring rigorous guarantees? We introduce a\nmathematical framework based on information-theoretic regularization to address\nboth feature and data point unlearning. For feature unlearning, we derive a\nunified solution that simultaneously optimizes diverse learning objectives,\nincluding entropy, conditional entropy, KL-divergence, and the energy of\nconditional probability. For data point unlearning, we first propose a novel\ndefinition that serves as a practical condition for unlearning via retraining,\nis easy to verify, and aligns with the principles of differential privacy from\nan inference perspective. Then, we provide provable guarantees for our\nframework on data point unlearning. By combining flexibility in learning\nobjectives with simplicity in regularization design, our approach is highly\nadaptable and practical for a wide range of machine learning and AI\napplications.\n","authors":["Shizhou Xu","Thomas Strohmer"],"pdf_url":"https://arxiv.org/pdf/2502.05684v2.pdf","comment":"31 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.11295v4","updated":"2025-02-11T19:42:26Z","published":"2024-09-17T15:49:44Z","title":"EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage","summary":"  Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.\n","authors":["Zeyi Liao","Lingbo Mo","Chejian Xu","Mintong Kang","Jiawei Zhang","Chaowei Xiao","Yuan Tian","Bo Li","Huan Sun"],"pdf_url":"https://arxiv.org/pdf/2409.11295v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07905v1","updated":"2025-02-11T19:21:23Z","published":"2025-02-11T19:21:23Z","title":"DeepSeek on a Trip: Inducing Targeted Visual Hallucinations via\n  Representation Vulnerabilities","summary":"  Multimodal Large Language Models (MLLMs) represent the cutting edge of AI\ntechnology, with DeepSeek models emerging as a leading open-source alternative\noffering competitive performance to closed-source systems. While these models\ndemonstrate remarkable capabilities, their vision-language integration\nmechanisms introduce specific vulnerabilities. We implement an adapted\nembedding manipulation attack on DeepSeek Janus that induces targeted visual\nhallucinations through systematic optimization of image embeddings. Through\nextensive experimentation across COCO, DALL-E 3, and SVIT datasets, we achieve\nhallucination rates of up to 98.0% while maintaining high visual fidelity (SSIM\n> 0.88) of the manipulated images on open-ended questions. Our analysis\ndemonstrates that both 1B and 7B variants of DeepSeek Janus are susceptible to\nthese attacks, with closed-form evaluation showing consistently higher\nhallucination rates compared to open-ended questioning. We introduce a novel\nmulti-prompt hallucination detection framework using LLaMA-3.1 8B Instruct for\nrobust evaluation. The implications of these findings are particularly\nconcerning given DeepSeek's open-source nature and widespread deployment\npotential. This research emphasizes the critical need for embedding-level\nsecurity measures in MLLM deployment pipelines and contributes to the broader\ndiscussion of responsible AI implementation.\n","authors":["Chashi Mahiul Islam","Samuel Jacob Chacko","Preston Horne","Xiuwen Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07905v1.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.06128v2","updated":"2025-02-11T19:21:07Z","published":"2024-10-08T15:31:33Z","title":"Zero-Shot Learning of Causal Models","summary":"  With the increasing acquisition of datasets over time, we now have access to\nprecise and varied descriptions of the world, encompassing a broad range of\nphenomena. These datasets can be seen as observations from unknown causal\ngenerative processes, commonly described by Structural Causal Models (SCMs).\nRecovering SCMs from observations poses formidable challenges, and often\nrequires us to learn a specific generative model for each dataset. In this\nwork, we propose to learn a \\emph{single} model capable of inferring the SCMs\nin a zero-shot manner. Rather than learning a specific SCM for each dataset, we\nenable the Fixed-Point Approach (FiP)~\\citep{scetbon2024fip} to infer the\ngenerative SCMs conditionally on their empirical representations. As a\nby-product, our approach can perform zero-shot generation of new dataset\nsamples and intervened samples. We demonstrate via experiments that our\namortized procedure achieves performances on par with SoTA methods trained\nspecifically for each dataset on both in and out-of-distribution problems. To\nthe best of our knowledge, this is the first time that SCMs are inferred in a\nzero-shot manner from observations, paving the way for a paradigmatic shift\ntoward the assimilation of causal knowledge across datasets. The code is\navailable on Github.\n","authors":["Divyat Mahajan","Jannes Gladrow","Agrin Hilmkil","Cheng Zhang","Meyer Scetbon"],"pdf_url":"https://arxiv.org/pdf/2410.06128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00075v3","updated":"2025-02-11T19:08:08Z","published":"2024-06-21T19:18:16Z","title":"Logicbreaks: A Framework for Understanding Subversion of Rule-based\n  Inference","summary":"  We study how to subvert large language models (LLMs) from following\nprompt-specified rules. We first formalize rule-following as inference in\npropositional Horn logic, a mathematical system in which rules have the form\n\"if $P$ and $Q$, then $R$\" for some propositions $P$, $Q$, and $R$. Next, we\nprove that although small transformers can faithfully follow such rules,\nmaliciously crafted prompts can still mislead both theoretical constructions\nand models learned from data. Furthermore, we demonstrate that popular attack\nalgorithms on LLMs find adversarial prompts and induce attention patterns that\nalign with our theory. Our novel logic-based framework provides a foundation\nfor studying LLMs in rule-based settings, enabling a formal analysis of tasks\nlike logical reasoning and jailbreak attacks.\n","authors":["Anton Xue","Avishree Khare","Rajeev Alur","Surbhi Goel","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2407.00075v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06663v2","updated":"2025-02-11T19:01:39Z","published":"2025-02-10T16:51:03Z","title":"EfficientLLM: Scalable Pruning-Aware Pretraining for\n  Architecture-Agnostic Edge Language Models","summary":"  Modern large language models (LLMs) driven by scaling laws, achieve\nintelligence emergency in large model sizes. Recently, the increasing concerns\nabout cloud costs, latency, and privacy make it an urgent requirement to\ndevelop compact edge language models. Distinguished from direct pretraining\nthat bounded by the scaling law, this work proposes the pruning-aware\npretraining, focusing on retaining performance of much larger optimized models.\nIt features following characteristics: 1) Data-scalable: we introduce minimal\nparameter groups in LLM and continuously optimize structural pruning, extending\npost-training pruning methods like LLM-Pruner and SparseGPT into the\npretraining phase. 2) Architecture-agnostic: the LLM architecture is\nauto-designed using saliency-driven pruning, which is the first time to exceed\nSoTA human-designed LLMs in modern pretraining. We reveal that it achieves\ntop-quality edge language models, termed EfficientLLM, by scaling up LLM\ncompression and extending its boundary. EfficientLLM significantly outperforms\nSoTA baselines with $100M \\sim 1B$ parameters, such as MobileLLM, SmolLM,\nQwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first\nattempt, EfficientLLM bridges the performance gap between traditional LLM\ncompression and direct pretraining methods, and we will fully open source at\nhttps://github.com/Xingrun-Xing2/EfficientLLM.\n","authors":["Xingrun Xing","Zheng Liu","Shitao Xiao","Boyan Gao","Yiming Liang","Wanpeng Zhang","Haokun Lin","Guoqi Li","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07891v1","updated":"2025-02-11T19:00:58Z","published":"2025-02-11T19:00:58Z","title":"The Observational Partial Order of Causal Structures with Latent\n  Variables","summary":"  For two causal structures with the same set of visible variables, one is said\nto observationally dominate the other if the set of distributions over the\nvisible variables realizable by the first contains the set of distributions\nover the visible variables realizable by the second. Knowing such dominance\nrelations is useful for adjudicating between these structures given\nobservational data. We here consider the problem of determining the partial\norder of equivalence classes of causal structures with latent variables\nrelative to observational dominance. We provide a complete characterization of\nthe dominance order in the case of three visible variables, and a partial\ncharacterization in the case of four visible variables. Our techniques also\nhelp to identify which observational equivalence classes have a set of\nrealizable distributions that is characterized by nontrivial inequality\nconstraints, analogous to Bell inequalities and instrumental inequalities. We\nfind evidence that as one increases the number of visible variables, the\nequivalence classes satisfying nontrivial inequality constraints become\nubiquitous. (Because such classes are the ones for which there can be a\ndifference in the distributions that are quantumly and classically realizable,\nthis implies that the potential for quantum-classical gaps is also ubiquitous.)\nFurthermore, we find evidence that constraint-based causal discovery algorithms\nthat rely solely on conditional independence constraints have a significantly\nweaker distinguishing power among observational equivalence classes than\nalgorithms that go beyond these (i.e., algorithms that also leverage nested\nMarkov constraints and inequality constraints).\n","authors":["Marina Maciel Ansanelli","Elie Wolfe","Robert W. Spekkens"],"pdf_url":"https://arxiv.org/pdf/2502.07891v1.pdf","comment":"48 pages, 30 figures"},{"id":"http://arxiv.org/abs/2502.07889v1","updated":"2025-02-11T19:00:05Z","published":"2025-02-11T19:00:05Z","title":"A unifying account of warm start guarantees for patches of quantum\n  landscapes","summary":"  Barren plateaus are fundamentally a statement about quantum loss landscapes\non average but there can, and generally will, exist patches of barren plateau\nlandscapes with substantial gradients. Previous work has studied certain\nclasses of parameterized quantum circuits and found example regions where\ngradients vanish at worst polynomially in system size. Here we present a\ngeneral bound that unifies all these previous cases and that can tackle\nphysically-motivated ans\\\"atze that could not be analyzed previously.\nConcretely, we analytically prove a lower-bound on the variance of the loss\nthat can be used to show that in a non-exponentially narrow region around a\npoint with curvature the loss variance cannot decay exponentially fast. This\nresult is complemented by numerics and an upper-bound that suggest that any\nloss function with a barren plateau will have exponentially vanishing gradients\nin any constant radius subregion. Our work thus suggests that while there are\nhopes to be able to warm-start variational quantum algorithms, any\ninitialization strategy that cannot get increasingly close to the region of\nattraction with increasing problem size is likely inadequate.\n","authors":["Hela Mhiri","Ricard Puig","Sacha Lerch","Manuel S. Rudolph","Thiparat Chotibut","Supanut Thanasilp","Zo√´ Holmes"],"pdf_url":"https://arxiv.org/pdf/2502.07889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07783v1","updated":"2025-02-11T18:59:57Z","published":"2025-02-11T18:59:57Z","title":"Curvature Tuning: Provable Training-free Model Steering From a Single\n  Parameter","summary":"  The scaling of model size and data size has reshaped the paradigm of AI. As a\nresult, the common protocol to leverage the latest models is to steer them\ntowards a specific downstream task of interest through {\\em fine-tuning}.\nDespite its importance, the main methods for fine-tuning remain limited to full\nor low-rank adapters--containing countless hyper-parameters and lacking\ninterpretability. In this paper, we take a step back and demonstrate how novel\nand explainable post-training steering solutions can be derived theoretically\nfrom {\\em spline operators}, a rich mathematical framing of Deep Networks that\nwas recently developed. Our method--coined \\textbf{Curvature Tuning (CT)}--has\na single parameter that provably modulates the curvature of the model's\ndecision boundary henceforth allowing training-free steering. This makes CT\nboth more efficient and interpretable than conventional fine-tuning methods. We\nempirically validate its effectiveness in improving generalization and\nrobustness of pretrained models. For example, CT improves out-of-distribution\ntransfer performances of ResNet-18/50 by 2.57\\%/1.74\\% across seventeen\ndownstream datasets, and improves RobustBench robust accuracy by\n11.76\\%/348.44\\%. Additionally, we apply CT to ReLU-based Swin-T/S, improving\ntheir generalization on nine downstream datasets by 2.43\\%/3.33\\%. Our code is\navailable at\n\\href{https://github.com/Leon-Leyang/curvature-tuning}{https://github.com/Leon-Leyang/curvature-tuning}.\n","authors":["Leyang Hu","Randall Balestriero"],"pdf_url":"https://arxiv.org/pdf/2502.07783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02749v3","updated":"2025-02-11T18:59:47Z","published":"2024-10-03T17:57:22Z","title":"Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis","summary":"  Software engineers mainly write code by editing existing programs. In\ncontrast, language models (LMs) autoregressively synthesize programs in a\nsingle pass. One explanation for this is the scarcity of sequential edit data.\nWhile high-quality instruction data for code synthesis is scarce, edit data for\nsynthesis is even scarcer. To fill this gap, we develop a synthetic data\ngeneration algorithm called LintSeq. This algorithm refactors programs into\nsequences of synthetic edits by using a linter to procedurally sample across\ninterdependent lines of source code. Synthetic edits sampled with LintSeq\nreflect the syntax and semantics of their programming language. To test the\nalgorithm, we use it to refactor a dataset of instruction + program pairs into\ninstruction + program-diff-sequence tuples. Then, we fine-tune a series of\nsmaller LMs ranging from 2.6B to 14B parameters on both the re-factored and\noriginal versions of this dataset. We perform comprehensive evaluations\ncomparing edit sequence code LMs against baselines on HumanEval, MBPP(+),\nCodeContests, DS-1000, and BigCodeBench. We show that models fine-tuned to\niteratively synthesize code match or outperform baselines on pass@1, and\nexhibit better scaling across higher pass@k as a function of total test-time\nFLOPs. Finally, we also pretrain our own tiny LMs for code understanding. We\nshow that fine-tuning these models to synthesize code edit-by-edit results in\nstrong performance on HumanEval and MBPP(+) compared to existing code language\nmodels of similar scale such as CodeT5+, AlphaCode, and Codex.\n","authors":["Ulyana Piterbarg","Lerrel Pinto","Rob Fergus"],"pdf_url":"https://arxiv.org/pdf/2410.02749v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07780v1","updated":"2025-02-11T18:59:35Z","published":"2025-02-11T18:59:35Z","title":"DarwinLM: Evolutionary Structured Pruning of Large Language Models","summary":"  Large Language Models (LLMs) have achieved significant success across various\nNLP tasks. However, their massive computational costs limit their widespread\nuse, particularly in real-time applications. Structured pruning offers an\neffective solution by compressing models and directly providing end-to-end\nspeed improvements, regardless of the hardware environment. Meanwhile,\ndifferent components of the model exhibit varying sensitivities towards\npruning, calling for \\emph{non-uniform} model compression. However, a pruning\nmethod should not only identify a capable substructure, but also account for\npost-compression training. To this end, we propose \\sysname, a method for\n\\emph{training-aware} structured pruning. \\sysname builds upon an evolutionary\nsearch process, generating multiple offspring models in each generation through\nmutation, and selecting the fittest for survival. To assess the effect of\npost-training, we incorporate a lightweight, multistep training process within\nthe offspring population, progressively increasing the number of tokens and\neliminating poorly performing models in each selection stage. We validate our\nmethod through extensive experiments on Llama-2-7B, Llama-3.1-8B and\nQwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured\npruning. For instance, \\sysname surpasses ShearedLlama while requiring\n$5\\times$ less training data during post-compression training.\n","authors":["Shengkun Tang","Oliver Sieberling","Eldar Kurtic","Zhiqiang Shen","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2502.07780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07776v1","updated":"2025-02-11T18:58:04Z","published":"2025-02-11T18:58:04Z","title":"Auditing Prompt Caching in Language Model APIs","summary":"  Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.\n","authors":["Chenchen Gu","Xiang Lisa Li","Rohith Kuditipudi","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2502.07776v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.07774v1","updated":"2025-02-11T18:57:18Z","published":"2025-02-11T18:57:18Z","title":"Optimistic Interior Point Methods for Sequential Hypothesis Testing by\n  Betting","summary":"  The technique of \"testing by betting\" frames nonparametric sequential\nhypothesis testing as a multiple-round game, where a player bets on future\nobservations that arrive in a streaming fashion, accumulates wealth that\nquantifies evidence against the null hypothesis, and rejects the null once the\nwealth exceeds a specified threshold while controlling the false positive\nerror. Designing an online learning algorithm that achieves a small regret in\nthe game can help rapidly accumulate the bettor's wealth, which in turn can\nshorten the time to reject the null hypothesis under the alternative $H_1$.\nHowever, many of the existing works employ the Online Newton Step (ONS) to\nupdate within a halved decision space to avoid a gradient explosion issue,\nwhich is potentially conservative for rapid wealth accumulation. In this paper,\nwe introduce a novel strategy utilizing interior-point methods in optimization\nthat allows updates across the entire interior of the decision space without\nthe risk of gradient explosion. Our approach not only maintains strong\nstatistical guarantees but also facilitates faster null hypothesis rejection in\ncritical scenarios, overcoming the limitations of existing approaches.\n","authors":["Can Chen","Jun-Kun Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07774v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07771v1","updated":"2025-02-11T18:55:57Z","published":"2025-02-11T18:55:57Z","title":"Breaking Down Bias: On The Limits of Generalizable Pruning Strategies","summary":"  We employ model pruning to examine how LLMs conceptualize racial biases, and\nwhether a generalizable mitigation strategy for such biases appears feasible.\nOur analysis yields several novel insights. We find that pruning can be an\neffective method to reduce bias without significantly increasing anomalous\nmodel behavior. Neuron-based pruning strategies generally yield better results\nthan approaches pruning entire attention heads. However, our results also show\nthat the effectiveness of either approach quickly deteriorates as pruning\nstrategies become more generalized. For instance, a model that is trained on\nremoving racial biases in the context of financial decision-making poorly\ngeneralizes to biases in commercial transactions. Overall, our analysis\nsuggests that racial biases are only partially represented as a general concept\nwithin language models. The other part of these biases is highly\ncontext-specific, suggesting that generalizable mitigation strategies may be of\nlimited effectiveness. Our findings have important implications for legal\nframeworks surrounding AI. In particular, they suggest that an effective\nmitigation strategy should include the allocation of legal responsibility on\nthose that deploy models in a specific use case.\n","authors":["Sibo Ma","Alejandro Salinas","Peter Henderson","Julian Nyarko"],"pdf_url":"https://arxiv.org/pdf/2502.07771v1.pdf","comment":"28 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2502.06774v2","updated":"2025-02-11T18:54:30Z","published":"2025-02-10T18:52:22Z","title":"ENFORCE: Exact Nonlinear Constrained Learning with Adaptive-depth Neural\n  Projection","summary":"  Ensuring neural networks adhere to domain-specific constraints is crucial for\naddressing safety and ethical concerns while also enhancing prediction\naccuracy. Despite the nonlinear nature of most real-world tasks, existing\nmethods are predominantly limited to affine or convex constraints. We introduce\nENFORCE, a neural network architecture that guarantees predictions to satisfy\nnonlinear constraints exactly. ENFORCE is trained with standard unconstrained\ngradient-based optimizers (e.g., Adam) and leverages autodifferentiation and\nlocal neural projections to enforce any $\\mathcal{C}^1$ constraint to arbitrary\ntolerance $\\epsilon$. We build an adaptive-depth neural projection (AdaNP)\nmodule that dynamically adjusts its complexity to suit the specific problem and\nthe required tolerance levels. ENFORCE guarantees satisfaction of equality\nconstraints that are nonlinear in both inputs and outputs of the neural network\nwith minimal (and adjustable) computational cost.\n","authors":["Giacomo Lastrucci","Artur M. Schweidtmann"],"pdf_url":"https://arxiv.org/pdf/2502.06774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07764v1","updated":"2025-02-11T18:47:53Z","published":"2025-02-11T18:47:53Z","title":"Polynomial-Time Approximability of Constrained Reinforcement Learning","summary":"  We study the computational complexity of approximating general constrained\nMarkov decision processes. Our primary contribution is the design of a\npolynomial time $(0,\\epsilon)$-additive bicriteria approximation algorithm for\nfinding optimal constrained policies across a broad class of recursively\ncomputable constraints, including almost-sure, chance, expectation, and their\nanytime variants. Matching lower bounds imply our approximation guarantees are\noptimal so long as $P \\neq NP$. The generality of our approach results in\nanswers to several long-standing open complexity questions in the constrained\nreinforcement learning literature. Specifically, we are the first to prove\npolynomial-time approximability for the following settings: policies under\nchance constraints, deterministic policies under multiple expectation\nconstraints, policies under non-homogeneous constraints (i.e., constraints of\ndifferent types), and policies under constraints for continuous-state\nprocesses.\n","authors":["Jeremy McMahan"],"pdf_url":"https://arxiv.org/pdf/2502.07764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.19392v2","updated":"2025-02-11T18:45:12Z","published":"2025-01-31T18:47:42Z","title":"Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models","summary":"  Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.\n","authors":["Alina Shutova","Vladimir Malinovskii","Vage Egiazarian","Denis Kuznedelev","Denis Mazur","Nikita Surkov","Ivan Ermakov","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2501.19392v2.pdf","comment":"Preprint, under review"},{"id":"http://arxiv.org/abs/2407.10366v2","updated":"2025-02-11T18:44:46Z","published":"2024-07-15T00:13:53Z","title":"Accessing Vision Foundation Models via ImageNet-1K","summary":"  Vision foundation models are renowned for the generalization ability due to\nmassive training data. Nevertheless, they demand tremendous training resources,\nand the training data is often inaccessible, e.g., CLIP, DINOv2, posing great\nchallenges to developing derivatives that could facilitate the research. In\nthis work, we offer a very simple and general solution, named \\textit{Proteus},\nto distill foundation models into smaller equivalents on ImageNet-1K without\naccess to the original training data. Specifically, we remove the designs from\nconventional knowledge distillation settings that result in dataset bias and\npresent three levels of training objectives, i.e., token, patch, and feature,\nto maximize the efficacy of knowledge transfer. In this manner, Proteus is\ntrained at ImageNet-level costs with surprising ability, facilitating the\naccessibility of training foundation models for the broader research community.\nWhen leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the\nperformance of the Oracle method DINOv2-L/14 (142M training data) across 19\nbenchmarks and outperforms other vision foundation models including CLIP-L/14\n(400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M) with a significantly\nsmaller training set of 1.2M images.\n","authors":["Yitian Zhang","Xu Ma","Yue Bai","Huan Wang","Yun Fu"],"pdf_url":"https://arxiv.org/pdf/2407.10366v2.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.07760v1","updated":"2025-02-11T18:43:07Z","published":"2025-02-11T18:43:07Z","title":"Scalable Fingerprinting of Large Language Models","summary":"  Model fingerprinting has emerged as a powerful tool for model owners to\nidentify their shared model given API access. However, to lower false discovery\nrate, fight fingerprint leakage, and defend against coalitions of model users\nattempting to bypass detection, we argue that {\\em scalability} is critical,\ni.e., scaling up the number of fingerprints one can embed into a model. Hence,\nwe pose scalability as a crucial requirement for fingerprinting schemes. We\nexperiment with fingerprint design at a scale significantly larger than\npreviously considered, and introduce a new method, dubbed Perinucleus sampling,\nto generate scalable, persistent, and harmless fingerprints. We demonstrate\nthat this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two\norders of magnitude more than existing schemes -- without degrading the model's\nutility. Our inserted fingerprints persist even after supervised fine-tuning on\nstandard post-training data. We further address security risks for\nfingerprinting, and theoretically and empirically show how a scalable\nfingerprinting scheme like ours can mitigate these risks.\n","authors":["Anshul Nasery","Jonathan Hayase","Creston Brooks","Peiyao Sheng","Himanshu Tyagi","Pramod Viswanath","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07760v1.pdf","comment":"23 pages 15 figures"},{"id":"http://arxiv.org/abs/2502.07758v1","updated":"2025-02-11T18:38:02Z","published":"2025-02-11T18:38:02Z","title":"Novel computational workflows for natural and biomedical image\n  processing based on hypercomplex algebras","summary":"  Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.\n","authors":["Nektarios A. Valous","Eckhard Hitzer","Drago≈ü Du≈üe","Rodrigo Rojas Moraleda","Ferdinand Popp","Meggy Suarez-Carmona","Anna Berthel","Ismini Papageorgiou","Carlo Fremd","Alexander R√∂lle","Christina C. Westhoff","B√©n√©dicte Lenoir","Niels Halama","Inka Z√∂rnig","Dirk J√§ger"],"pdf_url":"https://arxiv.org/pdf/2502.07758v1.pdf","comment":"24 pages, 18 figures, 14 tables"},{"id":"http://arxiv.org/abs/2502.07752v1","updated":"2025-02-11T18:27:19Z","published":"2025-02-11T18:27:19Z","title":"Towards Efficient Optimizer Design for LLM via Structured Fisher\n  Approximation with a Low-Rank Extension","summary":"  Designing efficient optimizers for large language models (LLMs) with\nlow-memory requirements and fast convergence is an important and challenging\nproblem. This paper makes a step towards the systematic design of such\noptimizers through the lens of structured Fisher information matrix (FIM)\napproximation. We show that many state-of-the-art efficient optimizers can be\nviewed as solutions to FIM approximation (under the Frobenius norm) with\nspecific structural assumptions. Building on these insights, we propose two\ndesign recommendations of practical efficient optimizers for LLMs, involving\nthe careful selection of structural assumptions to balance generality and\nefficiency, and enhancing memory efficiency of optimizers with general\nstructures through a novel low-rank extension framework. We demonstrate how to\nuse each design approach by deriving new memory-efficient optimizers: Row and\nColumn Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation\n(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the\neffectiveness, showing faster and better convergence than existing\nmemory-efficient baselines and Adam with little memory overhead. Notably, Alice\nachieves better than 2x faster convergence over Adam, while RACS delivers\nstrong performance on the 1B model with SGD-like memory.\n","authors":["Wenbo Gong","Meyer Scetbon","Chao Ma","Edward Meeds"],"pdf_url":"https://arxiv.org/pdf/2502.07752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07750v1","updated":"2025-02-11T18:25:48Z","published":"2025-02-11T18:25:48Z","title":"PFedDST: Personalized Federated Learning with Decentralized Selection\n  Training","summary":"  Distributed Learning (DL) enables the training of machine learning models\nacross multiple devices, yet it faces challenges like non-IID data\ndistributions and device capability disparities, which can impede training\nefficiency. Communication bottlenecks further complicate traditional Federated\nLearning (FL) setups. To mitigate these issues, we introduce the Personalized\nFederated Learning with Decentralized Selection Training (PFedDST) framework.\nPFedDST enhances model training by allowing devices to strategically evaluate\nand select peers based on a comprehensive communication score. This score\nintegrates loss, task similarity, and selection frequency, ensuring optimal\npeer connections. This selection strategy is tailored to increase local\npersonalization and promote beneficial peer collaborations to strengthen the\nstability and efficiency of the training process. Our experiments demonstrate\nthat PFedDST not only enhances model accuracy but also accelerates convergence.\nThis approach outperforms state-of-the-art methods in handling data\nheterogeneity, delivering both faster and more effective training in diverse\nand decentralized systems.\n","authors":["Mengchen Fan","Keren Li","Tianyun Zhang","Qing Tian","Baocheng Geng"],"pdf_url":"https://arxiv.org/pdf/2502.07750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07749v1","updated":"2025-02-11T18:25:14Z","published":"2025-02-11T18:25:14Z","title":"Whole-Genome Phenotype Prediction with Machine Learning: Open Problems\n  in Bacterial Genomics","summary":"  How can we identify causal genetic mechanisms that govern bacterial traits?\nInitial efforts entrusting machine learning models to handle the task of\npredicting phenotype from genotype return high accuracy scores. However,\nattempts to extract any meaning from the predictive models are found to be\ncorrupted by falsely identified \"causal\" features. Relying solely on pattern\nrecognition and correlations is unreliable, significantly so in bacterial\ngenomics settings where high-dimensionality and spurious associations are the\nnorm. Though it is not yet clear whether we can overcome this hurdle,\nsignificant efforts are being made towards discovering potential high-risk\nbacterial genetic variants. In view of this, we set up open problems\nsurrounding phenotype prediction from bacterial whole-genome datasets and\nextending those to learning causal effects, and discuss challenges that impact\nthe reliability of a machine's decision-making when faced with datasets of this\nnature.\n","authors":["Tamsin James","Ben Williamson","Peter Tino","Nicole Wheeler"],"pdf_url":"https://arxiv.org/pdf/2502.07749v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2402.08096v3","updated":"2025-02-11T18:25:07Z","published":"2024-02-12T22:32:12Z","title":"An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation\n  during Multi-stage Fine-tuning","summary":"  Incrementally fine-tuning foundational models on new tasks or domains is now\nthe de facto approach in NLP. A known pitfall of this approach is the\n\\emph{catastrophic forgetting} of prior knowledge that happens during\nfine-tuning. A common approach to alleviate such forgetting is to rehearse\nsamples from prior tasks during fine-tuning. Several existing works assume a\nfixed memory buffer to store prior task examples, while relying on inferences\n(forward passes) with the model at hand for choosing examples for rehearsal\nfrom the buffer. However, given the increasing computational cost of model\ninference, and decreasing cost of data storage, we focus on the setting to\nrehearse samples with a fixed computational budget instead of a fixed memory\nbudget. We propose a sampling scheme, \\texttt{\\bf mix-cd}, that prioritizes\nrehearsal of ``collateral damage'' samples, which are samples predicted\ncorrectly by the prior model but forgotten by the incrementally tuned one. The\ncrux of our scheme is a procedure to efficiently estimate the density of\ncollateral damage samples without incurring additional model inferences. Our\napproach is computationally efficient, easy to implement, and outperforms\nseveral leading continual learning methods in compute-constrained settings. All\nthe code will be publicly available at\nhttps://github.com/jybai/mix-cd-rehearsal.\n","authors":["Andrew Bai","Chih-Kuan Yeh","Cho-Jui Hsieh","Ankur Taly"],"pdf_url":"https://arxiv.org/pdf/2402.08096v3.pdf","comment":"13 pages, 9 figures. Published in NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2402.09401v2","updated":"2025-02-11T18:18:59Z","published":"2024-02-14T18:58:40Z","title":"Reinforcement Learning from Human Feedback with Active Queries","summary":"  Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$\ninstance-dependent regret bound and an $\\tilde{O}(d^2/\\Delta^2)$ query\ncomplexity, where $d$ is the dimension of feature space and $\\Delta$ is the\nsub-optimality gap over all the contexts. We then propose ADPO, a practical\nversion of our algorithm based on direct preference optimization (DPO) and\napply it to fine-tuning LLMs. Our experiments show that ADPO, while only making\nabout half of queries for human preference, matches the performance of the\nstate-of-the-art DPO method.\n","authors":["Kaixuan Ji","Jiafan He","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2402.09401v2.pdf","comment":"28 pages, 1 figure, 4 table"},{"id":"http://arxiv.org/abs/2502.07746v1","updated":"2025-02-11T18:13:29Z","published":"2025-02-11T18:13:29Z","title":"HiPoNet: A Topology-Preserving Multi-View Neural Network For High\n  Dimensional Point Cloud and Single-Cell Data","summary":"  In this paper, we propose HiPoNet, an end-to-end differentiable neural\nnetwork for regression, classification, and representation learning on\nhigh-dimensional point clouds. Single-cell data can have high dimensionality\nexceeding the capabilities of existing methods point cloud tailored for 3D\ndata. Moreover, modern single-cell and spatial experiments now yield entire\ncohorts of datasets (i.e. one on every patient), necessitating models that can\nprocess large, high-dimensional point clouds at scale. Most current approaches\nbuild a single nearest-neighbor graph, discarding important geometric\ninformation. In contrast, HiPoNet forms higher-order simplicial complexes\nthrough learnable feature reweighting, generating multiple data views that\ndisentangle distinct biological processes. It then employs simplicial wavelet\ntransforms to extract multi-scale features - capturing both local and global\ntopology. We empirically show that these components preserve topological\ninformation in the learned representations, and that HiPoNet significantly\noutperforms state-of-the-art point-cloud and graph-based models on single cell.\nWe also show an application of HiPoNet on spatial transcriptomics datasets\nusing spatial co-ordinates as one of the views. Overall, HiPoNet offers a\nrobust and scalable solution for high-dimensional data analysis.\n","authors":["Siddharth Viswanath","Hiren Madhu","Dhananjay Bhaskar","Jake Kovalic","Dave Johnson","Rex Ying","Christopher Tape","Ian Adelstein","Michael Perlmutter","Smita Krishnaswamy"],"pdf_url":"https://arxiv.org/pdf/2502.07746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01985v4","updated":"2025-02-11T18:09:35Z","published":"2024-09-03T15:26:51Z","title":"UNSURE: self-supervised learning with Unknown Noise level and Stein's\n  Unbiased Risk Estimate","summary":"  Recently, many self-supervised learning methods for image reconstruction have\nbeen proposed that can learn from noisy data alone, bypassing the need for\nground-truth references. Most existing methods cluster around two classes: i)\nStein's Unbiased Risk Estimate (SURE) and similar approaches that assume full\nknowledge of the noise distribution, and ii) Noise2Self and similar\ncross-validation methods that require very mild knowledge about the noise\ndistribution. The first class of methods tends to be impractical, as the noise\nlevel is often unknown in real-world applications, and the second class is\noften suboptimal compared to supervised learning. In this paper, we provide a\ntheoretical framework that characterizes this expressivity-robustness trade-off\nand propose a new approach based on SURE, but unlike the standard SURE, does\nnot require knowledge about the noise level. Throughout a series of\nexperiments, we show that the proposed estimator outperforms other existing\nself-supervised methods on various imaging inverse problems.\n","authors":["Juli√°n Tachella","Mike Davies","Laurent Jacques"],"pdf_url":"https://arxiv.org/pdf/2409.01985v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04463v2","updated":"2025-02-11T18:06:02Z","published":"2025-02-06T19:18:16Z","title":"Training Language Models to Reason Efficiently","summary":"  Scaling model size and training data has led to great advances in the\nperformance of Large Language Models (LLMs). However, the diminishing returns\nof this approach necessitate alternative methods to improve model capabilities,\nparticularly in tasks requiring advanced reasoning. Large reasoning models,\nwhich leverage long chain-of-thoughts, bring unprecedented breakthroughs in\nproblem-solving capabilities but at a substantial deployment cost associated to\nlonger generations. Reducing inference costs is crucial for the economic\nfeasibility, user experience, and environmental sustainability of these models.\n  In this work, we propose to train large reasoning models to reason\nefficiently. More precisely, we use reinforcement learning (RL) to train\nreasoning models to dynamically allocate inference-time compute based on task\ncomplexity. Our method incentivizes models to minimize unnecessary\ncomputational overhead while maintaining accuracy, thereby achieving\nsubstantial efficiency gains. It enables the derivation of a family of\nreasoning models with varying efficiency levels, controlled via a single\nhyperparameter. Experiments on two open-weight large reasoning models\ndemonstrate significant reductions in inference cost while preserving most of\nthe accuracy.\n","authors":["Daman Arora","Andrea Zanette"],"pdf_url":"https://arxiv.org/pdf/2502.04463v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07741v1","updated":"2025-02-11T18:05:54Z","published":"2025-02-11T18:05:54Z","title":"Advancing climate model interpretability: Feature attribution for Arctic\n  melt anomalies","summary":"  The focus of our work is improving the interpretability of anomalies in\nclimate models and advancing our understanding of Arctic melt dynamics. The\nArctic and Antarctic ice sheets are experiencing rapid surface melting and\nincreased freshwater runoff, contributing significantly to global sea level\nrise. Understanding the mechanisms driving snowmelt in these regions is\ncrucial. ERA5, a widely used reanalysis dataset in polar climate studies,\noffers extensive climate variables and global data assimilation. However, its\nsnowmelt model employs an energy imbalance approach that may oversimplify the\ncomplexity of surface melt. In contrast, the Glacier Energy and Mass Balance\n(GEMB) model incorporates additional physical processes, such as snow\naccumulation, firn densification, and meltwater percolation/refreezing,\nproviding a more detailed representation of surface melt dynamics. In this\nresearch, we focus on analyzing surface snowmelt dynamics of the Greenland Ice\nSheet using feature attribution for anomalous melt events in ERA5 and GEMB\nmodels. We present a novel unsupervised attribution method leveraging\ncounterfactual explanation method to analyze detected anomalies in ERA5 and\nGEMB. Our anomaly detection results are validated using MEaSUREs ground-truth\ndata, and the attributions are evaluated against established feature ranking\nmethods, including XGBoost, Shapley values, and Random Forest. Our attribution\nframework identifies the physics behind each model and the climate features\ndriving melt anomalies. These findings demonstrate the utility of our\nattribution method in enhancing the interpretability of anomalies in climate\nmodels and advancing our understanding of Arctic melt dynamics.\n","authors":["Tolulope Ale","Nicole-Jeanne Schlegel","Vandana P. Janeja"],"pdf_url":"https://arxiv.org/pdf/2502.07741v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2408.15332v2","updated":"2025-02-11T18:01:40Z","published":"2024-08-27T18:00:06Z","title":"What makes math problems hard for reinforcement learning: a case study","summary":"  Using a long-standing conjecture from combinatorial group theory, we explore,\nfrom multiple perspectives, the challenges of finding rare instances carrying\ndisproportionately high rewards. Based on lessons learned in the context\ndefined by the Andrews-Curtis conjecture, we propose algorithmic enhancements\nand a topological hardness measure with implications for a broad class of\nsearch problems. As part of our study, we also address several open\nmathematical questions. Notably, we demonstrate the length reducibility of all\nbut two presentations in the Akbulut-Kirby series (1981), and resolve various\npotential counterexamples in the Miller-Schupp series (1991), including three\ninfinite subfamilies.\n","authors":["Ali Shehper","Anibal M. Medina-Mardones","Lucas Fagan","Bart≈Çomiej Lewandowski","Angus Gruen","Yang Qiu","Piotr Kucharski","Zhenghan Wang","Sergei Gukov"],"pdf_url":"https://arxiv.org/pdf/2408.15332v2.pdf","comment":"58 pages, 25 figures, 1 table. Try it:\n  https://github.com/shehper/AC-Solver"},{"id":"http://arxiv.org/abs/2502.07739v1","updated":"2025-02-11T17:59:35Z","published":"2025-02-11T17:59:35Z","title":"HRP: High-Rank Preheating for Superior LoRA Initialization","summary":"  This paper studies the crucial impact of initialization on the convergence\nproperties of Low-Rank Adaptation (LoRA). We theoretically demonstrate that\nrandom initialization, a widely used schema, will likely lead LoRA to random\nlow-rank results, rather than the best low-rank result. While this issue can be\nmitigated by adjusting initialization towards a well-informed direction, it\nrelies on prior knowledge of the target, which is typically unknown in\nreal-world scenarios. To approximate this well-informed initial direction, we\npropose High-Rank Preheating (HRP), which fine-tunes high-rank LoRA for a few\nsteps and uses the singular value decomposition of the preheated result as a\nsuperior initialization. HRP initialization is theory-supported to combine the\nconvergence strengths of high-rank LoRA and the generalization strengths of\nlow-rank LoRA. Extensive experiments demonstrate that HRP significantly\nenhances LoRA's effectiveness across various models and tasks, achieving\nperformance comparable to full-parameter fine-tuning and outperforming other\ninitialization strategies.\n","authors":["Yuzhu Chen","Yingjie Wang","Shi Fu","Li Shen","Yongcheng Jing","Xinmei Tian","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2502.07739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07735v1","updated":"2025-02-11T17:55:03Z","published":"2025-02-11T17:55:03Z","title":"Revisiting Non-Acyclic GFlowNets in Discrete Environments","summary":"  Generative Flow Networks (GFlowNets) are a family of generative models that\nlearn to sample objects from a given probability distribution, potentially\nknown up to a normalizing constant. Instead of working in the object space,\nGFlowNets proceed by sampling trajectories in an appropriately constructed\ndirected acyclic graph environment, greatly relying on the acyclicity of the\ngraph. In our paper, we revisit the theory that relaxes the acyclicity\nassumption and present a simpler theoretical framework for non-acyclic\nGFlowNets in discrete environments. Moreover, we provide various novel\ntheoretical insights related to training with fixed backward policies, the\nnature of flow functions, and connections between entropy-regularized RL and\nnon-acyclic GFlowNets, which naturally generalize the respective concepts and\ntheoretical results from the acyclic setting. In addition, we experimentally\nre-examine the concept of loss stability in non-acyclic GFlowNet training, as\nwell as validate our own theoretical findings.\n","authors":["Nikita Morozov","Ian Maksimov","Daniil Tiapkin","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2502.07735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20562v2","updated":"2025-02-11T17:53:46Z","published":"2024-09-30T17:59:03Z","title":"SpaceMesh: A Continuous Representation for Learning Manifold Surface\n  Meshes","summary":"  Meshes are ubiquitous in visual computing and simulation, yet most existing\nmachine learning techniques represent meshes only indirectly, e.g. as the level\nset of a scalar field or deformation of a template, or as a disordered triangle\nsoup lacking local structure. This work presents a scheme to directly generate\nmanifold, polygonal meshes of complex connectivity as the output of a neural\nnetwork. Our key innovation is to define a continuous latent connectivity space\nat each mesh vertex, which implies the discrete mesh. In particular, our vertex\nembeddings generate cyclic neighbor relationships in a halfedge mesh\nrepresentation, which gives a guarantee of edge-manifoldness and the ability to\nrepresent general polygonal meshes. This representation is well-suited to\nmachine learning and stochastic optimization, without restriction on\nconnectivity or topology. We first explore the basic properties of this\nrepresentation, then use it to fit distributions of meshes from large datasets.\nThe resulting models generate diverse meshes with tessellation structure\nlearned from the dataset population, with concise details and high-quality mesh\nelements. In applications, this approach not only yields high-quality outputs\nfrom generative models, but also enables directly learning challenging geometry\nprocessing tasks such as mesh repair.\n","authors":["Tianchang Shen","Zhaoshuo Li","Marc Law","Matan Atzmon","Sanja Fidler","James Lucas","Jun Gao","Nicholas Sharp"],"pdf_url":"https://arxiv.org/pdf/2409.20562v2.pdf","comment":"published at SIGGRAPH Asia 2024"},{"id":"http://arxiv.org/abs/2502.07732v1","updated":"2025-02-11T17:51:52Z","published":"2025-02-11T17:51:52Z","title":"Economics of Sourcing Human Data","summary":"  Progress in AI has relied on human-generated data, from annotator\nmarketplaces to the wider Internet. However, the widespread use of large\nlanguage models now threatens the quality and integrity of human-generated data\non these very platforms. We argue that this issue goes beyond the immediate\nchallenge of filtering AI-generated content--it reveals deeper flaws in how\ndata collection systems are designed. Existing systems often prioritize speed,\nscale, and efficiency at the cost of intrinsic human motivation, leading to\ndeclining engagement and data quality. We propose that rethinking data\ncollection systems to align with contributors' intrinsic motivations--rather\nthan relying solely on external incentives--can help sustain high-quality data\nsourcing at scale while maintaining contributor trust and long-term\nparticipation.\n","authors":["Sebastin Santy","Prasanta Bhattacharya","Manoel Horta Ribeiro","Kelsey Allen","Sewoong Oh"],"pdf_url":"https://arxiv.org/pdf/2502.07732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06530v3","updated":"2025-02-11T17:49:04Z","published":"2024-10-09T04:07:20Z","title":"TopoTune : A Framework for Generalized Combinatorial Complex Neural\n  Networks","summary":"  Graph Neural Networks (GNNs) excel in learning from relational datasets,\nprocessing node and edge features in a way that preserves the symmetries of the\ngraph domain. However, many complex systems -- such as biological or social\nnetworks--involve multiway complex interactions that are more naturally\nrepresented by higher-order topological domains. The emerging field of\nTopological Deep Learning (TDL) aims to accommodate and leverage these\nhigher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly\ngeneral TDL models, have been shown to be more expressive and better performing\nthan GNNs. However, differently from the GNN ecosystem, TDL lacks a principled\nand standardized framework for easily defining new architectures, restricting\nits accessibility and applicability. To address this issue, we introduce\nGeneralized CCNNs (GCCNs), a novel simple yet powerful family of TDL models\nthat can be used to systematically transform any (graph) neural network into\nits TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while\nextensive experiments on a diverse class of GCCNs show that these architectures\nconsistently match or outperform CCNNs, often with less model complexity. In an\neffort to accelerate and democratize TDL, we introduce TopoTune, a lightweight\nsoftware for defining, building, and training GCCNs with unprecedented\nflexibility and ease.\n","authors":["Mathilde Papillon","Guillermo Bern√°rdez","Claudio Battiloro","Nina Miolane"],"pdf_url":"https://arxiv.org/pdf/2410.06530v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15065v2","updated":"2025-02-11T17:47:11Z","published":"2024-08-27T13:48:15Z","title":"The Benefits of Balance: From Information Projections to Variance\n  Reduction","summary":"  Data balancing across multiple modalities and sources appears in various\nforms in foundation models in machine learning and AI, e.g. in CLIP and DINO.\nWe show that data balancing across modalities and sources actually offers an\nunsuspected benefit: variance reduction. We present a non-asymptotic\nstatistical bound that quantifies this variance reduction effect and relates it\nto the eigenvalue decay of Markov operators. Furthermore, we describe how\nvarious forms of data balancing in contrastive multimodal learning and\nself-supervised clustering can be better understood, and even improved upon,\nowing to our variance reduction viewpoint.\n","authors":["Lang Liu","Ronak Mehta","Soumik Pal","Zaid Harchaoui"],"pdf_url":"https://arxiv.org/pdf/2408.15065v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08281v3","updated":"2025-02-11T17:43:59Z","published":"2024-01-16T11:12:36Z","title":"The Faiss library","summary":"  Vector databases typically manage large collections of embedding vectors.\nCurrently, AI applications are growing rapidly, and so is the number of\nembeddings that need to be stored and indexed. The Faiss library is dedicated\nto vector similarity search, a core functionality of vector databases. Faiss is\na toolkit of indexing methods and related primitives used to search, cluster,\ncompress and transform vectors. This paper describes the trade-off space of\nvector search and the design principles of Faiss in terms of structure,\napproach to optimization and interfacing. We benchmark key features of the\nlibrary and discuss a few selected applications to highlight its broad\napplicability.\n","authors":["Matthijs Douze","Alexandr Guzhva","Chengqi Deng","Jeff Johnson","Gergely Szilvasy","Pierre-Emmanuel Mazar√©","Maria Lomeli","Lucas Hosseini","Herv√© J√©gou"],"pdf_url":"https://arxiv.org/pdf/2401.08281v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08731v3","updated":"2025-02-11T17:38:13Z","published":"2023-10-12T21:38:07Z","title":"Novelty Detection in Reinforcement Learning with World Models","summary":"  Reinforcement learning (RL) using world models has found significant recent\nsuccesses. However, when a sudden change to world mechanics or properties\noccurs then agent performance and reliability can dramatically decline. We\nrefer to the sudden change in visual properties or state transitions as\nnovelties. Implementing novelty detection within generated world model\nframeworks is a crucial task for protecting the agent when deployed. In this\npaper, we propose straightforward bounding approaches to incorporate novelty\ndetection into world model RL agents, by utilizing the misalignment of the\nworld model's hallucinated states and the true observed states as an anomaly\nscore. We provide effective approaches to detecting novelties in a distribution\nof transitions learned by an agent in a world model. Finally, we show the\nadvantage of our work in a novel environment compared to traditional machine\nlearning novelty detection methods as well as currently accepted RL focused\nnovelty detection algorithms.\n","authors":["Geigh Zollicoffer","Kenneth Eaton","Jonathan Balloch","Julia Kim","Wei Zhou","Robert Wright","Mark O. Riedl"],"pdf_url":"https://arxiv.org/pdf/2310.08731v3.pdf","comment":"RLC Safety 2024"},{"id":"http://arxiv.org/abs/2501.11779v2","updated":"2025-02-11T17:36:32Z","published":"2025-01-20T23:10:13Z","title":"Glinthawk: A Two-Tiered Architecture for Offline LLM Inference","summary":"  We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.\n","authors":["Pouya Hamadanian","Sadjad Fouladi"],"pdf_url":"https://arxiv.org/pdf/2501.11779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04667v2","updated":"2025-02-11T17:36:13Z","published":"2025-01-08T18:28:12Z","title":"Natural Variational Annealing for Multimodal Optimization","summary":"  We introduce a new multimodal optimization approach called Natural\nVariational Annealing (NVA) that combines the strengths of three foundational\nconcepts to simultaneously search for multiple global and local modes of\nblack-box nonconvex objectives. First, it implements a simultaneous search by\nusing variational posteriors, such as, mixtures of Gaussians. Second, it\napplies annealing to gradually trade off exploration for exploitation. Finally,\nit learns the variational search distribution using natural-gradient learning\nwhere updates resemble well-known and easy-to-implement algorithms. The three\nconcepts come together in NVA giving rise to new algorithms and also allowing\nus to incorporate \"fitness shaping\", a core concept from evolutionary\nalgorithms. We assess the quality of search on simulations and compare them to\nmethods using gradient descent and evolution strategies. We also provide an\napplication to a real-world inverse problem in planetary science.\n","authors":["T√¢m Le Minh","Julyan Arbel","Thomas M√∂llenhoff","Mohammad Emtiyaz Khan","Florence Forbes"],"pdf_url":"https://arxiv.org/pdf/2501.04667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07721v1","updated":"2025-02-11T17:33:48Z","published":"2025-02-11T17:33:48Z","title":"TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning","summary":"  The prevalence of noisy labels in real-world datasets poses a significant\nimpediment to the effective deployment of deep learning models. While\nmeta-learning strategies have emerged as a promising approach for addressing\nthis challenge, existing methods often suffer from limited transferability and\ntask-specific designs. This paper introduces TMLC-Net, a novel Transferable\nMeta-Learner for Correcting Noisy Labels, designed to overcome these\nlimitations. TMLC-Net learns a general-purpose label correction strategy that\ncan be readily applied across diverse datasets and model architectures without\nrequiring extensive retraining or fine-tuning. Our approach integrates three\ncore components: (1) Normalized Noise Perception, which captures and normalizes\ntraining dynamics to handle distribution shifts; (2) Time-Series Encoding,\nwhich models the temporal evolution of sample statistics using a recurrent\nneural network; and (3) Subclass Decoding, which predicts a corrected label\ndistribution based on the learned representations. We conduct extensive\nexperiments on benchmark datasets with various noise types and levels,\ndemonstrating that TMLC-Net consistently outperforms state-of-the-art methods\nin terms of both accuracy and robustness to label noise. Furthermore, we\nanalyze the transferability of TMLC-Net, showcasing its adaptability to new\ndatasets and noise conditions, and establishing its potential as a broadly\napplicable solution for robust deep learning in noisy environments.\n","authors":["Mengyang Li"],"pdf_url":"https://arxiv.org/pdf/2502.07721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10868v2","updated":"2025-02-11T17:31:55Z","published":"2024-10-08T11:24:59Z","title":"Large Continual Instruction Assistant","summary":"  Continual Instruction Tuning (CIT) is adopted to continually instruct Large\nModels to follow human intent data by data. It is observed that existing\ngradient update would heavily destroy the performance on previous datasets\nduring CIT process. Instead, Exponential Moving Average (EMA), owns the ability\nto trace previous parameters, which can aid in decreasing forgetting.\nNonetheless, its stable balance weight fails to deal with the ever-changing\ndatasets, leading to the out-of-balance between plasticity and stability. In\nthis paper, we propose a general continual instruction tuning framework to\naddress the challenge. Starting from the trade-off prerequisite and EMA update,\nwe propose the plasticity and stability ideal condition. Based on Taylor\nexpansion in the loss function, we find the optimal balance weight can be\nautomatically determined by the gradients and learned parameters. Therefore, we\npropose a stable-plasticity balanced coefficient to avoid knowledge confusion.\nBased on the semantic similarity of the instructions, we can determine whether\nto retrain or expand the training parameters and allocate the most suitable\nparameters for the testing instances. Extensive experiments across multiple\ncontinual instruction tuning benchmarks demonstrate that our approach not only\nenhances anti-forgetting capabilities but also significantly improves overall\ncontinual tuning performance. For example, based on LLaVA-7B, the forgetting is\nreduced from 5.42 to 1.93. Our code will be made publicly available soon.\n","authors":["Jingyang Qiao","Zhizhong Zhang","Xin Tan","Yanyun Qu","Shouhong Ding","Yuan Xie"],"pdf_url":"https://arxiv.org/pdf/2410.10868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10763v2","updated":"2025-02-11T17:28:34Z","published":"2024-03-16T02:06:14Z","title":"Drago: Primal-Dual Coupled Variance Reduction for Faster\n  Distributionally Robust Optimization","summary":"  We consider the penalized distributionally robust optimization (DRO) problem\nwith a closed, convex uncertainty set, a setting that encompasses learning\nusing $f$-DRO and spectral/$L$-risk minimization. We present Drago, a\nstochastic primal-dual algorithm that combines cyclic and randomized components\nwith a carefully regularized primal update to achieve dual variance reduction.\nOwing to its design, Drago enjoys a state-of-the-art linear convergence rate on\nstrongly convex-strongly concave DRO problems with a fine-grained dependency on\nprimal and dual condition numbers. Theoretical results are supported by\nnumerical benchmarks on regression and classification tasks.\n","authors":["Ronak Mehta","Jelena Diakonikolas","Zaid Harchaoui"],"pdf_url":"https://arxiv.org/pdf/2403.10763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18922v3","updated":"2025-02-11T17:23:13Z","published":"2024-04-29T17:58:30Z","title":"DPO Meets PPO: Reinforced Token Optimization for RLHF","summary":"  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nlarge language models, its open-source implementation is still largely\nsub-optimal. To address these issues, we introduce a framework that models RLHF\nproblems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Under this framework, we introduce an\nalgorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the\ntoken-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive experiments\ndemonstrate that \\texttt{RTO} performs better than PPO and other direct\npreference learning algorithms. In particular, RTO outperforms PPO by 7.5\npoints on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code\nand models are available at\n\\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.\n","authors":["Han Zhong","Zikang Shan","Guhao Feng","Wei Xiong","Xinle Cheng","Li Zhao","Di He","Jiang Bian","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2404.18922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07862v1","updated":"2025-02-11T17:19:44Z","published":"2025-02-11T17:19:44Z","title":"ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise\n  and Compute Resources","summary":"  Multimodal deep learning systems are deployed in dynamic scenarios due to the\nrobustness afforded by multiple sensing modalities. Nevertheless, they struggle\nwith varying compute resource availability (due to multi-tenancy, device\nheterogeneity, etc.) and fluctuating quality of inputs (from sensor feed\ncorruption, environmental noise, etc.). Current multimodal systems employ\nstatic resource provisioning and cannot easily adapt when compute resources\nchange over time. Additionally, their reliance on processing sensor data with\nfixed feature extractors is ill-equipped to handle variations in modality\nquality. Consequently, uninformative modalities, such as those with high noise,\nneedlessly consume resources better allocated towards other modalities. We\npropose ADMN, a layer-wise Adaptive Depth Multimodal Network capable of\ntackling both challenges - it adjusts the total number of active layers across\nall modalities to meet compute resource constraints, and continually\nreallocates layers across input modalities according to their modality quality.\nOur evaluations showcase ADMN can match the accuracy of state-of-the-art\nnetworks while reducing up to 75% of their floating-point operations.\n","authors":["Jason Wu","Kang Yang","Lance Kaplan","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2502.07862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07861v1","updated":"2025-02-11T17:18:17Z","published":"2025-02-11T17:18:17Z","title":"BalanceKV: KV Cache Compression through Discrepancy Theory","summary":"  Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods.\n","authors":["Insu Han","Michael Kapralov","Ekaterina Kochetkova","Kshiteej Sheth","Amir Zandieh"],"pdf_url":"https://arxiv.org/pdf/2502.07861v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07715v1","updated":"2025-02-11T17:15:55Z","published":"2025-02-11T17:15:55Z","title":"Near-Optimal Sample Complexity in Reward-Free Kernel-Based Reinforcement\n  Learning","summary":"  Reinforcement Learning (RL) problems are being considered under increasingly\nmore complex structures. While tabular and linear models have been thoroughly\nexplored, the analytical study of RL under nonlinear function approximation,\nespecially kernel-based models, has recently gained traction for their strong\nrepresentational capacity and theoretical tractability. In this context, we\nexamine the question of statistical efficiency in kernel-based RL within the\nreward-free RL framework, specifically asking: how many samples are required to\ndesign a near-optimal policy? Existing work addresses this question under\nrestrictive assumptions about the class of kernel functions. We first explore\nthis question by assuming a generative model, then relax this assumption at the\ncost of increasing the sample complexity by a factor of H, the length of the\nepisode. We tackle this fundamental problem using a broad class of kernels and\na simpler algorithm compared to prior work. Our approach derives new confidence\nintervals for kernel ridge regression, specific to our RL setting, which may be\nof broader applicability. We further validate our theoretical findings through\nsimulations.\n","authors":["Aya Kayal","Sattar Vakili","Laura Toni","Alberto Bernacchia"],"pdf_url":"https://arxiv.org/pdf/2502.07715v1.pdf","comment":"Accepted at AISTATS 2025"},{"id":"http://arxiv.org/abs/2409.05701v3","updated":"2025-02-11T17:14:43Z","published":"2024-09-09T15:13:56Z","title":"pFedGPA: Diffusion-based Generative Parameter Aggregation for\n  Personalized Federated Learning","summary":"  Federated Learning (FL) offers a decentralized approach to model training,\nwhere data remains local and only model parameters are shared between the\nclients and the central server. Traditional methods, such as Federated\nAveraging (FedAvg), linearly aggregate these parameters which are usually\ntrained on heterogeneous data distributions, potentially overlooking the\ncomplex, high-dimensional nature of the parameter space. This can result in\ndegraded performance of the aggregated model. While personalized FL approaches\ncan mitigate the heterogeneous data issue to some extent, the limitation of\nlinear aggregation remains unresolved. To alleviate this issue, we investigate\nthe generative approach of diffusion model and propose a novel generative\nparameter aggregation framework for personalized FL, \\texttt{pFedGPA}. In this\nframework, we deploy a diffusion model on the server to integrate the diverse\nparameter distributions and propose a parameter inversion method to efficiently\ngenerate a set of personalized parameters for each client. This inversion\nmethod transforms the uploaded parameters into a latent code, which is then\naggregated through denoising sampling to produce the final personalized\nparameters. By encoding the dependence of a client's model parameters on the\nspecific data distribution using the high-capacity diffusion model,\n\\texttt{pFedGPA} can effectively decouple the complexity of the overall\ndistribution of all clients' model parameters from the complexity of each\nindividual client's parameter distribution. Our experimental results\nconsistently demonstrate the superior performance of the proposed method across\nmultiple datasets, surpassing baseline approaches.\n","authors":["Jiahao Lai","Jiaqi Li","Jian Xu","Yanru Wu","Boshi Tang","Siqi Chen","Yongfeng Huang","Wenbo Ding","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2409.05701v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17165v3","updated":"2025-02-11T17:06:45Z","published":"2023-11-28T19:01:09Z","title":"(Ir)rationality in AI: State of the Art, Research Challenges and Open\n  Questions","summary":"  The concept of rationality is central to the field of artificial\nintelligence. Whether we are seeking to simulate human reasoning, or the goal\nis to achieve bounded optimality, we generally seek to make artificial agents\nas rational as possible. Despite the centrality of the concept within AI, there\nis no unified definition of what constitutes a rational agent. This article\nprovides a survey of rationality and irrationality in artificial intelligence,\nand sets out the open questions in this area. The understanding of rationality\nin other fields has influenced its conception within artificial intelligence,\nin particular work in economics, philosophy and psychology. Focusing on the\nbehaviour of artificial agents, we consider irrational behaviours that can\nprove to be optimal in certain scenarios. Some methods have been developed to\ndeal with irrational agents, both in terms of identification and interaction,\nhowever work in this area remains limited. Methods that have up to now been\ndeveloped for other purposes, namely adversarial scenarios, may be adapted to\nsuit interactions with artificial agents. We further discuss the interplay\nbetween human and artificial agents, and the role that rationality plays within\nthis interaction; many questions remain in this area, relating to potentially\nirrational behaviour of both humans and artificial agents.\n","authors":["Olivia Macmillan-Scott","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2311.17165v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16247v2","updated":"2025-02-11T16:54:45Z","published":"2024-12-20T00:01:16Z","title":"Towards scientific discovery with dictionary learning: Extracting\n  biological concepts from microscopy foundation models","summary":"  Dictionary learning (DL) has emerged as a powerful interpretability tool for\nlarge language models. By extracting known concepts (e.g., Golden-Gate Bridge)\nfrom human-interpretable data (e.g., text), sparse DL can elucidate a model's\ninner workings. In this work, we ask if DL can also be used to discover unknown\nconcepts from less human-interpretable scientific data (e.g., cell images),\nultimately enabling modern approaches to scientific discovery. As a first step,\nwe use DL algorithms to study microscopy foundation models trained on\nmulti-cell image data, where little prior knowledge exists regarding which\nhigh-level concepts should arise. We show that sparse dictionaries indeed\nextract biologically-meaningful concepts such as cell type and genetic\nperturbation type. We also propose Iterative Codebook Feature Learning~(ICFL)\nand combine it with a pre-processing step which uses PCA whitening from a\ncontrol dataset. In our experiments, we demonstrate that both ICFL and PCA\nimprove the selectivity of extracted features compared to TopK sparse\nautoencoders.\n","authors":["Konstantin Donhauser","Kristina Ulicna","Gemma Elyse Moran","Aditya Ravuri","Kian Kenyon-Dean","Cian Eastwood","Jason Hartford"],"pdf_url":"https://arxiv.org/pdf/2412.16247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10737v3","updated":"2025-02-11T16:47:17Z","published":"2024-06-15T20:47:38Z","title":"DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation","summary":"  Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained\nmodels to continually changing, unseen target domains. While existing CTTA\nmethods assume structured domain changes with uniform durations, real-world\nenvironments often exhibit dynamic patterns where domains recur with varying\nfrequencies and durations. Current approaches, which adapt the same parameters\nacross different domains, struggle in such dynamic conditions-they face\nconvergence issues with brief domain exposures, risk forgetting previously\nlearned knowledge, or misapplying it to irrelevant domains. To remedy this, we\npropose DPCore, a method designed for robust performance across diverse domain\nchange patterns while ensuring computational efficiency. DPCore integrates\nthree key components: Visual Prompt Adaptation for efficient domain alignment,\na Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism\nthat intelligently adjusts existing prompts for similar domains while creating\nnew ones for substantially different domains. Extensive experiments on four\nbenchmarks demonstrate that DPCore consistently outperforms various CTTA\nmethods, achieving state-of-the-art performance in both structured and dynamic\nsettings while reducing trainable parameters by 99% and computation time by 64%\ncompared to previous approaches.\n","authors":["Yunbei Zhang","Akshay Mehra","Shuaicheng Niu","Jihun Hamm"],"pdf_url":"https://arxiv.org/pdf/2406.10737v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18768v3","updated":"2025-02-11T16:24:23Z","published":"2024-09-27T14:12:49Z","title":"Learning from Demonstration with Implicit Nonlinear Dynamics Models","summary":"  Learning from Demonstration (LfD) is a useful paradigm for training policies\nthat solve tasks involving complex motions, such as those encountered in\nrobotic manipulation. In practice, the successful application of LfD requires\novercoming error accumulation during policy execution, i.e. the problem of\ndrift due to errors compounding over time and the consequent\nout-of-distribution behaviours. Existing works seek to address this problem\nthrough scaling data collection, correcting policy errors with a\nhuman-in-the-loop, temporally ensembling policy predictions or through learning\na dynamical system model with convergence guarantees. In this work, we propose\nand validate an alternative approach to overcoming this issue. Inspired by\nreservoir computing, we develop a recurrent neural network layer that includes\na fixed nonlinear dynamical system with tunable dynamical properties for\nmodelling temporal dynamics. We validate the efficacy of our neural network\nlayer on the task of reproducing human handwriting motions using the LASA Human\nHandwriting Dataset. Through empirical experiments we demonstrate that\nincorporating our layer into existing neural network architectures addresses\nthe issue of compounding errors in LfD. Furthermore, we perform a comparative\nevaluation against existing approaches including a temporal ensemble of policy\npredictions and an Echo State Network (ESN) implementation. We find that our\napproach yields greater policy precision and robustness on the handwriting task\nwhile also generalising to multiple dynamics regimes and maintaining\ncompetitive latency scores.\n","authors":["Peter David Fagan","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2409.18768v3.pdf","comment":"21 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.02153v2","updated":"2025-02-11T16:23:39Z","published":"2024-12-03T04:28:14Z","title":"Revisiting the Initial Steps in Adaptive Gradient Descent Optimization","summary":"  Adaptive gradient optimization methods, such as Adam, are prevalent in\ntraining deep neural networks across diverse machine learning tasks due to\ntheir ability to achieve faster convergence. However, these methods often\nsuffer from suboptimal generalization compared to stochastic gradient descent\n(SGD) and exhibit instability, particularly when training Transformer models.\nIn this work, we show the standard initialization of the second-order moment\nestimation ($v_0 =0$) as a significant factor contributing to these\nlimitations. We introduce simple yet effective solutions: initializing the\nsecond-order moment estimation with non-zero values, using either data-driven\nor random initialization strategies. Empirical evaluations demonstrate that our\napproach not only stabilizes convergence but also enhances the final\nperformance of adaptive gradient optimizers. Furthermore, by adopting the\nproposed initialization strategies, Adam achieves performance comparable to\nmany recently proposed variants of adaptive gradient optimization methods. Our\ncode is available at https://github.com/Walleclipse/Adam_Initialization.\n","authors":["Abulikemu Abuduweili","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2412.02153v2.pdf","comment":"Conference on Parsimony and Learning (CPAL) 2025"},{"id":"http://arxiv.org/abs/2409.05907v2","updated":"2025-02-11T16:22:45Z","published":"2024-09-06T15:47:40Z","title":"Programming Refusal with Conditional Activation Steering","summary":"  LLMs have shown remarkable capabilities, but precisely controlling their\nresponse behavior remains challenging. Existing activation steering methods\nalter LLM behavior indiscriminately, limiting their practical applicability in\nsettings where selective responses are essential, such as content moderation or\ndomain-specific assistants. In this paper, we propose Conditional Activation\nSteering (CAST), which analyzes LLM activation patterns during inference to\nselectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts\nactivate distinct patterns in the model's hidden states. Using CAST, one can\nsystematically control LLM behavior with rules like \"if input is about hate\nspeech or adult content, then refuse\" or \"if input is not about legal advice,\nthen refuse.\" This allows for selective modification of responses to specific\ncontent while maintaining normal responses to other content, all without\nrequiring weight optimization. We release an open-source implementation of our\nframework at <github.com/IBM/activation-steering>.\n","authors":["Bruce W. Lee","Inkit Padhi","Karthikeyan Natesan Ramamurthy","Erik Miehling","Pierre Dognin","Manish Nagireddy","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2409.05907v2.pdf","comment":"ICLR 2025, Spotlight"},{"id":"http://arxiv.org/abs/2502.07858v1","updated":"2025-02-11T16:22:06Z","published":"2025-02-11T16:22:06Z","title":"MAAT: Mamba Adaptive Anomaly Transformer with association discrepancy\n  for time series","summary":"  Anomaly detection in time series is essential for industrial monitoring and\nenvironmental sensing, yet distinguishing anomalies from complex patterns\nremains challenging. Existing methods like the Anomaly Transformer and\nDCdetector have progressed, but they face limitations such as sensitivity to\nshort-term contexts and inefficiency in noisy, non-stationary environments.\n  To overcome these issues, we introduce MAAT, an improved architecture that\nenhances association discrepancy modeling and reconstruction quality. MAAT\nfeatures Sparse Attention, efficiently capturing long-range dependencies by\nfocusing on relevant time steps, thereby reducing computational redundancy.\nAdditionally, a Mamba-Selective State Space Model is incorporated into the\nreconstruction module, utilizing a skip connection and Gated Attention to\nimprove anomaly localization and detection performance.\n  Extensive experiments show that MAAT significantly outperforms previous\nmethods, achieving better anomaly distinguishability and generalization across\nvarious time series applications, setting a new standard for unsupervised time\nseries anomaly detection in real-world scenarios.\n","authors":["Abdellah Zakaria Sellam","Ilyes Benaissa","Abdelmalik Taleb-Ahmed","Luigi Patrono","Cosimo Distante"],"pdf_url":"https://arxiv.org/pdf/2502.07858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07857v1","updated":"2025-02-11T16:20:57Z","published":"2025-02-11T16:20:57Z","title":"SNAP: Sequential Non-Ancestor Pruning for Targeted Causal Effect\n  Estimation With an Unknown Graph","summary":"  Causal discovery can be computationally demanding for large numbers of\nvariables. If we only wish to estimate the causal effects on a small subset of\ntarget variables, we might not need to learn the causal graph for all\nvariables, but only a small subgraph that includes the targets and their\nadjustment sets. In this paper, we focus on identifying causal effects between\ntarget variables in a computationally and statistically efficient way. This\ntask combines causal discovery and effect estimation, aligning the discovery\nobjective with the effects to be estimated. We show that definite non-ancestors\nof the targets are unnecessary to learn causal relations between the targets\nand to identify efficient adjustments sets. We sequentially identify and prune\nthese definite non-ancestors with our Sequential Non-Ancestor Pruning (SNAP)\nframework, which can be used either as a preprocessing step to standard causal\ndiscovery methods, or as a standalone sound and complete causal discovery\nalgorithm. Our results on synthetic and real data show that both approaches\nsubstantially reduce the number of independence tests and the computation time\nwithout compromising the quality of causal effect estimations.\n","authors":["M√°ty√°s Schubert","Tom Claassen","Sara Magliacane"],"pdf_url":"https://arxiv.org/pdf/2502.07857v1.pdf","comment":"Accepted at AISTATS 2025"},{"id":"http://arxiv.org/abs/2502.06314v2","updated":"2025-02-11T16:04:15Z","published":"2025-02-10T10:06:46Z","title":"From Pixels to Components: Eigenvector Masking for Visual Representation\n  Learning","summary":"  Predicting masked from visible parts of an image is a powerful\nself-supervised approach for visual representation learning. However, the\ncommon practice of masking random patches of pixels exhibits certain failure\nmodes, which can prevent learning meaningful high-level features, as required\nfor downstream tasks. We propose an alternative masking strategy that operates\non a suitable transformation of the data rather than on the raw pixels.\nSpecifically, we perform principal component analysis and then randomly mask a\nsubset of components, which accounts for a fixed ratio of the data variance.\nThe learning task then amounts to reconstructing the masked components from the\nvisible ones. Compared to local patches of pixels, the principal components of\nimages carry more global information. We thus posit that predicting masked from\nvisible components involves more high-level features, allowing our masking\nstrategy to extract more useful representations. This is corroborated by our\nempirical findings which demonstrate improved image classification performance\nfor component over pixel masking. Our method thus constitutes a simple and\nrobust data-driven alternative to traditional masked image modeling approaches.\n","authors":["Alice Bizeul","Thomas Sutter","Alain Ryser","Bernhard Sch√∂lkopf","Julius von K√ºgelgen","Julia E. Vogt"],"pdf_url":"https://arxiv.org/pdf/2502.06314v2.pdf","comment":"Preprint. Under review"},{"id":"http://arxiv.org/abs/2402.11355v5","updated":"2025-02-11T16:03:35Z","published":"2024-02-17T18:12:02Z","title":"A Practical Method for Generating String Counterfactuals","summary":"  Interventions targeting the representation space of language models (LMs)\nhave emerged as an effective means to influence model behavior. Such methods\nare employed, for example, to eliminate or alter the encoding of demographic\ninformation such as gender within the model's representations and, in so doing,\ncreate a counterfactual representation. However, because the intervention\noperates within the representation space, understanding precisely what aspects\nof the text it modifies poses a challenge. In this paper, we give a method to\nconvert representation counterfactuals into string counterfactuals. We\ndemonstrate that this approach enables us to analyze the linguistic alterations\ncorresponding to a given representation space intervention and to interpret the\nfeatures utilized to encode a specific concept. Moreover, the resulting\ncounterfactuals can be used to mitigate bias in classification through data\naugmentation.\n","authors":["Matan Avitan","Ryan Cotterell","Yoav Goldberg","Shauli Ravfogel"],"pdf_url":"https://arxiv.org/pdf/2402.11355v5.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.11061v8","updated":"2025-02-11T15:59:51Z","published":"2024-10-14T20:14:39Z","title":"Learning to Optimize for Mixed-Integer Non-linear Programming","summary":"  Mixed-integer nonlinear programs (MINLPs) arise in diverse domains such as\nenergy systems and transportation but are notoriously difficult to solve,\nparticularly on a large scale. While learning-to-optimize methods have been\nsuccessful at continuous optimization, extending them to MINLPs is still\nchallenging due to the integer constraints. To overcome this, we propose a\nnovel deep-learning approach with two learnable correction layers to ensure\nsolution integrality and a post-processing step to improve solution\nfeasibility. Our experiments show that this is the first general method capable\nof efficiently solving large-scale MINLPs with up to tens of thousands of\nvariables in milliseconds, delivering high-quality solutions even when\ntraditional solvers and heuristics fail. This is the first general learning\nmethod for MINLP, successfully solving some of the largest instances reported\nto date.\n","authors":["Bo Tang","Elias B. Khalil","J√°n Drgo≈àa"],"pdf_url":"https://arxiv.org/pdf/2410.11061v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12042v3","updated":"2025-02-11T15:58:10Z","published":"2024-06-17T19:22:04Z","title":"Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image\n  Diffusion Models","summary":"  Text-to-image (T2I) diffusion models have demonstrated impressive image\ngeneration capabilities. Still, their computational intensity prohibits\nresource-constrained organizations from deploying T2I models after fine-tuning\nthem on their internal target data. While pruning techniques offer a potential\nsolution to reduce the computational burden of T2I models, static pruning\nmethods use the same pruned model for all input prompts, overlooking the\nvarying capacity requirements of different prompts. Dynamic pruning addresses\nthis issue by utilizing a separate sub-network for each prompt, but it prevents\nbatch parallelism on GPUs. To overcome these limitations, we introduce Adaptive\nPrompt-Tailored Pruning (APTP), a novel prompt-based pruning method designed\nfor T2I diffusion models. Central to our approach is a prompt router model,\nwhich learns to determine the required capacity for an input text prompt and\nroutes it to an architecture code, given a total desired compute budget for\nprompts. Each architecture code represents a specialized model tailored to the\nprompts assigned to it, and the number of codes is a hyperparameter. We train\nthe prompt router and architecture codes using contrastive learning, ensuring\nthat similar prompts are mapped to nearby codes. Further, we employ optimal\ntransport to prevent the codes from collapsing into a single one. We\ndemonstrate APTP's effectiveness by pruning Stable Diffusion (SD) V2.1 using\nCC3M and COCO as target datasets. APTP outperforms the single-model pruning\nbaselines in terms of FID, CLIP, and CMMD scores. Our analysis of the clusters\nlearned by APTP reveals they are semantically meaningful. We also show that\nAPTP can automatically discover previously empirically found challenging\nprompts for SD, e.g. prompts for generating text images, assigning them to\nhigher capacity codes.\n","authors":["Alireza Ganjdanesh","Reza Shirkavand","Shangqian Gao","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2406.12042v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07661v1","updated":"2025-02-11T15:51:23Z","published":"2025-02-11T15:51:23Z","title":"Partial-Label Learning with Conformal Candidate Cleaning","summary":"  Real-world data is often ambiguous; for example, human annotation produces\ninstances with multiple conflicting class labels. Partial-label learning (PLL)\naims at training a classifier in this challenging setting, where each instance\nis associated with a set of candidate labels and one correct, but unknown,\nclass label. A multitude of algorithms targeting this setting exists and, to\nenhance their prediction quality, several extensions that are applicable across\na wide range of PLL methods have been introduced. While many of these\nextensions rely on heuristics, this article proposes a novel enhancing method\nthat incrementally prunes candidate sets using conformal prediction. To work\naround the missing labeled validation set, which is typically required for\nconformal prediction, we propose a strategy that alternates between training a\nPLL classifier to label the validation set, leveraging these predicted class\nlabels for calibration, and pruning candidate labels that are not part of the\nresulting conformal sets. In this sense, our method alternates between\nempirical risk minimization and candidate set pruning. We establish that our\npruning method preserves the conformal validity with respect to the unknown\nground truth. Our extensive experiments on artificial and real-world data show\nthat the proposed approach significantly improves the test set accuracies of\nseveral state-of-the-art PLL classifiers.\n","authors":["Tobias Fuchs","Florian Kalinke"],"pdf_url":"https://arxiv.org/pdf/2502.07661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11160v4","updated":"2025-02-11T15:50:27Z","published":"2023-04-21T17:59:08Z","title":"Isotonic Mechanism for Exponential Family Estimation in Machine Learning\n  Peer Review","summary":"  In 2023, the International Conference on Machine Learning (ICML) required\nauthors with multiple submissions to rank their submissions based on perceived\nquality. In this paper, we aim to employ these author-specified rankings to\nenhance peer review in machine learning and artificial intelligence conferences\nby extending the Isotonic Mechanism to exponential family distributions. This\nmechanism generates adjusted scores that closely align with the original scores\nwhile adhering to author-specified rankings. Despite its applicability to a\nbroad spectrum of exponential family distributions, implementing this mechanism\ndoes not require knowledge of the specific distribution form. We demonstrate\nthat an author is incentivized to provide accurate rankings when her utility\ntakes the form of a convex additive function of the adjusted review scores. For\na certain subclass of exponential family distributions, we prove that the\nauthor reports truthfully only if the question involves only pairwise\ncomparisons between her submissions, thus indicating the optimality of ranking\nin truthful information elicitation. Moreover, we show that the adjusted scores\nimprove dramatically the estimation accuracy compared to the original scores\nand achieve nearly minimax optimality when the ground-truth scores have bounded\ntotal variation. We conclude with a numerical analysis of the ICML 2023 ranking\ndata, showing substantial estimation gains in approximating a proxy\nground-truth quality of the papers using the Isotonic Mechanism.\n","authors":["Yuling Yan","Weijie J. Su","Jianqing Fan"],"pdf_url":"https://arxiv.org/pdf/2304.11160v4.pdf","comment":"accepted to the Journal of the Royal Statistical Society: Series B"},{"id":"http://arxiv.org/abs/2502.07657v1","updated":"2025-02-11T15:46:03Z","published":"2025-02-11T15:46:03Z","title":"Private Low-Rank Approximation for Covariance Matrices, Dyson Brownian\n  Motion, and Eigenvalue-Gap Bounds for Gaussian Perturbations","summary":"  We consider the problem of approximating a $d \\times d$ covariance matrix $M$\nwith a rank-$k$ matrix under $(\\varepsilon,\\delta)$-differential privacy. We\npresent and analyze a complex variant of the Gaussian mechanism and obtain\nupper bounds on the Frobenius norm of the difference between the matrix output\nby this mechanism and the best rank-$k$ approximation to $M$. Our analysis\nprovides improvements over previous bounds, particularly when the spectrum of\n$M$ satisfies natural structural assumptions. The novel insight is to view the\naddition of Gaussian noise to a matrix as a continuous-time matrix Brownian\nmotion. This viewpoint allows us to track the evolution of eigenvalues and\neigenvectors of the matrix, which are governed by stochastic differential\nequations discovered by Dyson. These equations enable us to upper bound the\nFrobenius distance between the best rank-$k$ approximation of $M$ and that of a\nGaussian perturbation of $M$ as an integral that involves inverse eigenvalue\ngaps of the stochastically evolving matrix, as opposed to a sum of perturbation\nbounds obtained via Davis-Kahan-type theorems. Subsequently, again using the\nDyson Brownian motion viewpoint, we show that the eigenvalues of the matrix $M$\nperturbed by Gaussian noise have large gaps with high probability. These\nresults also contribute to the analysis of low-rank approximations under\naverage-case perturbations, and to an understanding of eigenvalue gaps for\nrandom matrices, both of which may be of independent interest.\n","authors":["Oren Mangoubi","Nisheeth K. Vishnoi"],"pdf_url":"https://arxiv.org/pdf/2502.07657v1.pdf","comment":"Published in Journal of the ACM. arXiv admin note: substantial text\n  overlap with arXiv:2306.16648"},{"id":"http://arxiv.org/abs/2502.07656v1","updated":"2025-02-11T15:43:49Z","published":"2025-02-11T15:43:49Z","title":"A Unifying Framework for Causal Imitation Learning with Hidden\n  Confounders","summary":"  We propose a general and unifying framework for causal Imitation Learning\n(IL) with hidden confounders that subsumes several existing confounded IL\nsettings from the literature. Our framework accounts for two types of hidden\nconfounders: (a) those observed by the expert, which thus influence the\nexpert's policy, and (b) confounding noise hidden to both the expert and the IL\nalgorithm. For additional flexibility, we also introduce a confounding noise\nhorizon and time-varying expert-observable hidden variables. We show that\ncausal IL in our framework can be reduced to a set of Conditional Moment\nRestrictions (CMRs) by leveraging trajectory histories as instruments to learn\na history-dependent policy. We propose DML-IL, a novel algorithm that uses\ninstrumental variable regression to solve these CMRs and learn a policy. We\nprovide a bound on the imitation gap for DML-IL, which recovers prior results\nas special cases. Empirical evaluation on a toy environment with continues\nstate-action spaces and multiple Mujoco tasks demonstrate that DML-IL\noutperforms state-of-the-art causal IL algorithms.\n","authors":["Daqian Shao","Thomas Kleine Buening","Marta Kwiatkowska"],"pdf_url":"https://arxiv.org/pdf/2502.07656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03736v3","updated":"2025-02-11T15:42:19Z","published":"2024-06-06T04:22:11Z","title":"Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data","summary":"  Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.\n","authors":["Jingyang Ou","Shen Nie","Kaiwen Xue","Fengqi Zhu","Jiacheng Sun","Zhenguo Li","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.03736v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07650v1","updated":"2025-02-11T15:39:47Z","published":"2025-02-11T15:39:47Z","title":"Guiding Time-Varying Generative Models with Natural Gradients on\n  Exponential Family Manifold","summary":"  Optimising probabilistic models is a well-studied field in statistics.\nHowever, its connection with the training of generative models remains largely\nunder-explored. In this paper, we show that the evolution of time-varying\ngenerative models can be projected onto an exponential family manifold,\nnaturally creating a link between the parameters of a generative model and\nthose of a probabilistic model. We then train the generative model by moving\nits projection on the manifold according to the natural gradient descent\nscheme. This approach also allows us to approximate the natural gradient of the\nKL divergence efficiently without relying on MCMC for intractable models.\nFurthermore, we propose particle versions of the algorithm, which feature\nclosed-form update rules for any parametric model within the exponential\nfamily. Through toy and real-world experiments, we validate the effectiveness\nof the proposed algorithms.\n","authors":["Song Liu","Leyang Wang","Yakun Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07646v1","updated":"2025-02-11T15:35:15Z","published":"2025-02-11T15:35:15Z","title":"Causal Additive Models with Unobserved Causal Paths and Backdoor Paths","summary":"  Causal additive models have been employed as tractable yet expressive\nframeworks for causal discovery involving hidden variables. State-of-the-art\nmethodologies suggest that determining the causal relationship between a pair\nof variables is infeasible in the presence of an unobserved backdoor or an\nunobserved causal path. Contrary to this assumption, we theoretically show that\nresolving the causal direction is feasible in certain scenarios by\nincorporating two novel components into the theory. The first component\nintroduces a novel characterization of regression sets within independence\nbetween regression residuals. The second component leverages conditional\nindependence among the observed variables. We also provide a search algorithm\nthat integrates these innovations and demonstrate its competitive performance\nagainst existing methods.\n","authors":["Thong Pham","Takashi Nicholas Maeda","Shohei Shimizu"],"pdf_url":"https://arxiv.org/pdf/2502.07646v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.07642v1","updated":"2025-02-11T15:33:17Z","published":"2025-02-11T15:33:17Z","title":"FoQA: A Faroese Question-Answering Dataset","summary":"  We present FoQA, a Faroese extractive question-answering (QA) dataset with\n2,000 samples, created using a semi-automated approach combining Large Language\nModels (LLMs) and human validation. The dataset was generated from Faroese\nWikipedia articles using GPT-4-turbo for initial QA generation, followed by\nquestion rephrasing to increase complexity and native speaker validation to\nensure quality. We provide baseline performance metrics for FoQA across\nmultiple models, including LLMs and BERT, demonstrating its effectiveness in\nevaluating Faroese QA performance. The dataset is released in three versions: a\nvalidated set of 2,000 samples, a complete set of all 10,001 generated samples,\nand a set of 2,395 rejected samples for error analysis.\n","authors":["Annika Simonsen","Dan Saattrup Nielsen","Hafsteinn Einarsson"],"pdf_url":"https://arxiv.org/pdf/2502.07642v1.pdf","comment":"Camera-ready version for RESOURCEFUL workshop, 2025"},{"id":"http://arxiv.org/abs/2502.07640v1","updated":"2025-02-11T15:27:35Z","published":"2025-02-11T15:27:35Z","title":"Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving","summary":"  We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. The final prover outperforms all existing\nopen-source models in whole-proof generation. On the miniF2F benchmark, it\nachieves a 57.6% success rate (Pass@32), exceeding the previous best\nopen-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7\nproblems (Pass@512), ranking first on the leaderboard. Furthermore, it\ngenerates 29.7K formal proofs for Lean Workbook problems, nearly doubling the\n15.7K produced by earlier works.\n","authors":["Yong Lin","Shange Tang","Bohan Lyu","Jiayun Wu","Hongzhou Lin","Kaiyu Yang","Jia Li","Mengzhou Xia","Danqi Chen","Sanjeev Arora","Chi Jin"],"pdf_url":"https://arxiv.org/pdf/2502.07640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07636v1","updated":"2025-02-11T15:23:14Z","published":"2025-02-11T15:23:14Z","title":"Consistency Training with Physical Constraints","summary":"  We propose a physics-aware Consistency Training (CT) method that accelerates\nsampling in Diffusion Models with physical constraints. Our approach leverages\na two-stage strategy: (1) learning the noise-to-data mapping via CT, and (2)\nincorporating physics constraints as a regularizer. Experiments on toy examples\nshow that our method generates samples in a single step while adhering to the\nimposed constraints. This approach has the potential to efficiently solve\npartial differential equations (PDEs) using deep generative modeling.\n","authors":["Che-Chia Chang","Chen-Yang Dai","Te-Sheng Lin","Ming-Chih Lai","Chieh-Hsin Lai"],"pdf_url":"https://arxiv.org/pdf/2502.07636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07635v1","updated":"2025-02-11T15:23:05Z","published":"2025-02-11T15:23:05Z","title":"Distributed Value Decomposition Networks with Networked Agents","summary":"  We investigate the problem of distributed training under partial\nobservability, whereby cooperative multi-agent reinforcement learning agents\n(MARL) maximize the expected cumulative joint reward. We propose distributed\nvalue decomposition networks (DVDN) that generate a joint Q-function that\nfactorizes into agent-wise Q-functions. Whereas the original value\ndecomposition networks rely on centralized training, our approach is suitable\nfor domains where centralized training is not possible and agents must learn by\ninteracting with the physical environment in a decentralized manner while\ncommunicating with their peers. DVDN overcomes the need for centralized\ntraining by locally estimating the shared objective. We contribute with two\ninnovative algorithms, DVDN and DVDN (GT), for the heterogeneous and\nhomogeneous agents settings respectively. Empirically, both algorithms\napproximate the performance of value decomposition networks, in spite of the\ninformation loss during communication, as demonstrated in ten MARL tasks in\nthree standard environments.\n","authors":["Guilherme S. Varela","Alberto Sardinha","Francisco S. Melo"],"pdf_url":"https://arxiv.org/pdf/2502.07635v1.pdf","comment":"21 pages, 15 figures, to be published in Proceedings of the 24th\n  International Conference on Autonomous Agents and Multiagent Systems (AAMAS\n  2025), Detroit, Michigan, USA, May 19 - 23, 2025, IFAAMAS"},{"id":"http://arxiv.org/abs/2502.04907v2","updated":"2025-02-11T15:17:43Z","published":"2025-02-07T13:23:40Z","title":"Scalable and consistent embedding of probability measures into Hilbert\n  spaces via measure quantization","summary":"  This paper is focused on statistical learning from data that come as\nprobability measures. In this setting, popular approaches consist in embedding\nsuch data into a Hilbert space with either Linearized Optimal Transport or\nKernel Mean Embedding. However, the cost of computing such embeddings prohibits\ntheir direct use in large-scale settings. We study two methods based on measure\nquantization for approximating input probability measures with discrete\nmeasures of small-support size. The first one is based on optimal quantization\nof each input measure, while the second one relies on mean-measure\nquantization. We study the consistency of such approximations, and its\nimplication for scalable embeddings of probability measures into a Hilbert\nspace at a low computational cost. We finally illustrate our findings with\nvarious numerical experiments.\n","authors":["Erell Gachon","Elsa Cazelles","J√©r√©mie Bigot"],"pdf_url":"https://arxiv.org/pdf/2502.04907v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07630v1","updated":"2025-02-11T15:17:29Z","published":"2025-02-11T15:17:29Z","title":"Rethinking Timing Residuals: Advancing PET Detectors with Explicit TOF\n  Corrections","summary":"  PET is a functional imaging method that visualizes metabolic processes. TOF\ninformation can be derived from coincident detector signals and incorporated\ninto image reconstruction to enhance the SNR. PET detectors are typically\nassessed by their CTR, but timing performance is degraded by various factors.\nResearch on timing calibration seeks to mitigate these degradations and restore\naccurate timing information. While many calibration methods use analytical\napproaches, machine learning techniques have recently gained attention due to\ntheir flexibility. We developed a residual physics-based calibration approach\nthat combines prior domain knowledge with the power of machine learning models.\nThis approach begins with an initial analytical calibration addressing\nfirst-order skews. The remaining deviations, regarded as residual effects, are\nused to train machine learning models to eliminate higher-order skews. The key\nadvantage is that the experimenter guides the learning process through the\ndefinition of timing residuals. In earlier studies, we developed models that\ndirectly predicted the expected time difference, which offered corrections only\nimplicitly (implicit correction models). In this study, we introduce a new\ndefinition for timing residuals, enabling us to train models that directly\npredict correction values (explicit correction models). The explicit correction\napproach significantly simplifies data acquisition, improves linearity, and\nenhances timing performance from $371 \\pm 6$ ps to $281 \\pm 5$ ps for\ncoincidences from 430 keV to 590 keV. Additionally, the new definition reduces\nmodel size, making it suitable for high-throughput applications like PET\nscanners. Experiments were conducted using two detector stacks composed of $4\n\\times 4$ LYSO:Ce,Ca crystals ($3.8\\times 3.8\\times 20$ mm$^{3}$) coupled to $4\n\\times 4$ Broadcom NUV-MT SiPMs and digitized with the TOFPET2 ASIC.\n","authors":["Stephan Naunheim","Luis Lopes de Paiva","Vanessa Nadig","Yannick Kuhl","Stefan Gundacker","Florian Mueller","Volkmar Schulz"],"pdf_url":"https://arxiv.org/pdf/2502.07630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13779v2","updated":"2025-02-11T15:09:49Z","published":"2024-01-24T20:00:23Z","title":"Faster Convergence with Less Communication: Broadcast-Based Subgraph\n  Sampling for Decentralized Learning over Wireless Networks","summary":"  Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely\nadopted algorithm for decentralized training of machine learning models across\nnetworked agents. A crucial part of D-SGD is the consensus-based model\naveraging, which heavily relies on information exchange and fusion among the\nnodes. Specifically, for consensus averaging over wireless networks,\ncommunication coordination is necessary to determine when and how a node can\naccess the channel and transmit (or receive) information to (or from) its\nneighbors. In this work, we propose $\\texttt{BASS}$, a broadcast-based subgraph\nsampling method designed to accelerate the convergence of D-SGD while\nconsidering the actual communication cost per iteration. $\\texttt{BASS}$\ncreates a set of mixing matrix candidates that represent sparser subgraphs of\nthe base topology. In each consensus iteration, one mixing matrix is sampled,\nleading to a specific scheduling decision that activates multiple\ncollision-free subsets of nodes. The sampling occurs in a probabilistic manner,\nand the elements of the mixing matrices, along with their sampling\nprobabilities, are jointly optimized. Simulation results demonstrate that\n$\\texttt{BASS}$ enables faster convergence with fewer transmission slots\ncompared to existing link-based scheduling methods. In conclusion, the inherent\nbroadcasting nature of wireless channels offers intrinsic advantages in\naccelerating the convergence of decentralized optimization and learning.\n","authors":["Daniel P√©rez Herrera","Zheng Chen","Erik G. Larsson"],"pdf_url":"https://arxiv.org/pdf/2401.13779v2.pdf","comment":"14 pages, 10 figures, accepted for publication at IEEE Open Journals\n  of Communication. arXiv admin note: text overlap with arXiv:2310.16106"},{"id":"http://arxiv.org/abs/2502.07620v1","updated":"2025-02-11T15:09:05Z","published":"2025-02-11T15:09:05Z","title":"Causal-Informed Contrastive Learning: Towards Bias-Resilient\n  Pre-training under Concept Drift","summary":"  The evolution of large-scale contrastive pre-training propelled by top-tier\ndatasets has reached a transition point in the scaling law. Consequently,\nsustaining and enhancing a model's pre-training capabilities in drift\nenvironments have surfaced as a notable challenge. In this paper, we initially\nuncover that contrastive pre-training methods are significantly impacted by\nconcept drift wherein distributions change unpredictably, resulting in notable\nbiases in the feature space of the pre-trained model. Empowered by causal\ninference, we construct a structural causal graph to analyze the impact of\nconcept drift to contrastive pre-training systemically, and propose the causal\ninterventional contrastive objective. Upon achieving this, we devise a\nresilient contrastive pre-training approach to accommodate the data stream of\nconcept drift, with simple and scalable implementation. Extensive experiments\non various downstream tasks demonstrate our resilient contrastive pre-training\neffectively mitigates the bias stemming from the concept drift data stream.\nCodes are available at https://anonymous.4open.science/r/ResilientCL/.\n","authors":["Xiaoyu Yang","Jie Lu","En Yu"],"pdf_url":"https://arxiv.org/pdf/2502.07620v1.pdf","comment":"17pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.07616v1","updated":"2025-02-11T15:05:26Z","published":"2025-02-11T15:05:26Z","title":"Tractable Transformers for Flexible Conditional Generation","summary":"  Non-autoregressive (NAR) generative models are valuable because they can\nhandle diverse conditional generation tasks in a more principled way than their\nautoregressive (AR) counterparts, which are constrained by sequential\ndependency requirements. Recent advancements in NAR models, such as diffusion\nlanguage models, have demonstrated superior performance in unconditional\ngeneration compared to AR models (e.g., GPTs) of similar sizes. However, such\nimprovements do not always lead to improved conditional generation performance.\nWe show that a key reason for this gap is the difficulty in generalizing to\nconditional probability queries unseen during training. As a result, strong\nunconditional generation performance does not guarantee high-quality\nconditional generation. This paper proposes Tractable Transformers\n(Tracformer), a Transformer-based generative model that is more robust to\ndifferent conditional generation tasks. Unlike existing models that rely solely\non global contextual features derived from full inputs, Tracformers incorporate\na sparse Transformer encoder to capture both local and global contextual\ninformation. This information is routed through a decoder for conditional\ngeneration. Empirical results demonstrate that Tracformers achieve\nstate-of-the-art conditional generation performance on text modeling compared\nto recent diffusion and AR model baselines.\n","authors":["Anji Liu","Xuejie Liu","Dayuan Zhao","Mathias Niepert","Yitao Liang","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2502.07616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07041v3","updated":"2025-02-11T14:57:40Z","published":"2024-12-09T23:01:04Z","title":"Generalized Least Squares Kernelized Tensor Factorization","summary":"  Completing multidimensional tensor-structured data with missing entries is a\nfundamental task for many real-world applications involving incomplete or\ncorrupted datasets. For data with spatial or temporal side information,\nlow-rank factorization models with smoothness constraints have demonstrated\nstrong performance. Although effective at capturing global and long-range\ncorrelations, these models often struggle to capture short-scale,\nhigh-frequency variations in the data. To address this limitation, we propose\nthe Generalized Least Squares Kernelized Tensor Factorization (GLSKF) framework\nfor tensor completion. GLSKF integrates smoothness-constrained low-rank\nfactorization with a locally correlated residual process; the resulting\nadditive structure enables effective characterization of both global\ndependencies and local variations. Specifically, we define the covariance norm\nto enforce the smoothness of factor matrices in the global low-rank\nfactorization, and use structured covariance/kernel functions to model the\nlocal processes. For model estimation, we develop an alternating least squares\n(ALS) procedure with closed-form solutions for each subproblem. GLSKF utilizes\nzero-padding and slicing operations based on projection matrices which preserve\nthe Kronecker structure of covariances, facilitating efficient computations\nthrough the conjugate gradient (CG) method. The proposed framework is evaluated\non four real-world datasets across diverse tasks. Experimental results\ndemonstrate that GLSKF achieves superior performance and scalability,\nestablishing it as a novel solution for multidimensional tensor completion.\n","authors":["Mengying Lei","Lijun Sun"],"pdf_url":"https://arxiv.org/pdf/2412.07041v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07606v1","updated":"2025-02-11T14:56:16Z","published":"2025-02-11T14:56:16Z","title":"Algorithmic Aspects of Strategic Trading","summary":"  Algorithmic trading in modern financial markets is widely acknowledged to\nexhibit strategic, game-theoretic behaviors whose complexity can be difficult\nto model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress\nin the setting of trading for position building. Here parties wish to buy or\nsell a fixed number of shares in a fixed time period in the presence of both\ntemporary and permanent market impact, resulting in exponentially large\nstrategy spaces. While these papers primarily consider the existence and\nstructural properties of equilibrium strategies, in this work we focus on the\nalgorithmic aspects of the proposed model. We give an efficient algorithm for\ncomputing best responses, and show that while the temporary impact only setting\nyields a potential game, best response dynamics do not generally converge for\nthe general setting, for which no fast algorithm for (Nash) equilibrium\ncomputation is known. This leads us to consider the broader notion of Coarse\nCorrelated Equilibria (CCE), which we show can be computed efficiently via an\nimplementation of Follow the Perturbed Leader (FTPL). We illustrate the model\nand our results with an experimental investigation, where FTPL exhibits\ninteresting behavior in different regimes of the relative weighting between\ntemporary and permanent market impact.\n","authors":["Michael Kearns","Mirah Shi"],"pdf_url":"https://arxiv.org/pdf/2502.07606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02525v4","updated":"2025-02-11T14:54:12Z","published":"2022-09-06T14:30:18Z","title":"Generalisation under gradient descent via deterministic PAC-Bayes","summary":"  We establish disintegrated PAC-Bayesian generalisation bounds for models\ntrained with gradient descent methods or continuous gradient flows. Contrary to\nstandard practice in the PAC-Bayesian setting, our result applies to\noptimisation algorithms that are deterministic, without requiring any\nde-randomisation step. Our bounds are fully computable, depending on the\ndensity of the initial distribution and the Hessian of the training objective\nover the trajectory. We show that our framework can be applied to a variety of\niterative optimisation algorithms, including stochastic gradient descent (SGD),\nmomentum-based schemes, and damped Hamiltonian dynamics.\n","authors":["Eugenio Clerico","Tyler Farghly","George Deligiannidis","Benjamin Guedj","Arnaud Doucet"],"pdf_url":"https://arxiv.org/pdf/2209.02525v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07591v1","updated":"2025-02-11T14:40:57Z","published":"2025-02-11T14:40:57Z","title":"DMWM: Dual-Mind World Model with Long-Term Imagination","summary":"  Imagination in world models is crucial for enabling agents to learn\nlong-horizon policy in a sample-efficient manner. Existing recurrent\nstate-space model (RSSM)-based world models depend on single-step statistical\ninference to capture the environment dynamics, and, hence, they are unable to\nperform long-term imagination tasks due to the accumulation of prediction\nerrors. Inspired by the dual-process theory of human cognition, we propose a\nnovel dual-mind world model (DMWM) framework that integrates logical reasoning\nto enable imagination with logical consistency. DMWM is composed of two\ncomponents: an RSSM-based System 1 (RSSM-S1) component that handles state\ntransitions in an intuitive manner and a logic-integrated neural network-based\nSystem 2 (LINN-S2) component that guides the imagination process through\nhierarchical deep logical reasoning. The inter-system feedback mechanism is\ndesigned to ensure that the imagination process follows the logical rules of\nthe real environment. The proposed framework is evaluated on benchmark tasks\nthat require long-term planning from the DMControl suite. Extensive\nexperimental results demonstrate that the proposed framework yields significant\nimprovements in terms of logical coherence, trial efficiency, data efficiency\nand long-term imagination over the state-of-the-art world models.\n","authors":["Lingyi Wang","Rashed Shelim","Walid Saad","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2502.07591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20385v2","updated":"2025-02-11T14:37:00Z","published":"2024-12-29T07:21:13Z","title":"A Particle Algorithm for Mean-Field Variational Inference","summary":"  Variational inference is a fast and scalable alternative to Markov chain\nMonte Carlo and has been widely applied to posterior inference tasks in\nstatistics and machine learning. A traditional approach for implementing\nmean-field variational inference (MFVI) is coordinate ascent variational\ninference (CAVI), which relies crucially on parametric assumptions on complete\nconditionals. In this paper, we introduce a novel particle-based algorithm for\nmean-field variational inference, which we term PArticle VI (PAVI). Notably,\nour algorithm does not rely on parametric assumptions on complete conditionals,\nand it applies to the nonparametric setting. We provide non-asymptotic\nfinite-particle convergence guarantee for our algorithm. To our knowledge, this\nis the first end-to-end guarantee for particle-based MFVI.\n","authors":["Qiang Du","Kaizheng Wang","Edith Zhang","Chenyang Zhong"],"pdf_url":"https://arxiv.org/pdf/2412.20385v2.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2502.07587v1","updated":"2025-02-11T14:36:39Z","published":"2025-02-11T14:36:39Z","title":"SEMU: Singular Value Decomposition for Efficient Machine Unlearning","summary":"  While the capabilities of generative foundational models have advanced\nrapidly in recent years, methods to prevent harmful and unsafe behaviors remain\nunderdeveloped. Among the pressing challenges in AI safety, machine unlearning\n(MU) has become increasingly critical to meet upcoming safety regulations. Most\nexisting MU approaches focus on altering the most significant parameters of the\nmodel. However, these methods often require fine-tuning substantial portions of\nthe model, resulting in high computational costs and training instabilities,\nwhich are typically mitigated by access to the original training dataset.\n  In this work, we address these limitations by leveraging Singular Value\nDecomposition (SVD) to create a compact, low-dimensional projection that\nenables the selective forgetting of specific data points. We propose Singular\nValue Decomposition for Efficient Machine Unlearning (SEMU), a novel approach\ndesigned to optimize MU in two key aspects. First, SEMU minimizes the number of\nmodel parameters that need to be modified, effectively removing unwanted\nknowledge while making only minimal changes to the model's weights. Second,\nSEMU eliminates the dependency on the original training dataset, preserving the\nmodel's previously acquired knowledge without additional data requirements.\n  Extensive experiments demonstrate that SEMU achieves competitive performance\nwhile significantly improving efficiency in terms of both data usage and the\nnumber of modified parameters.\n","authors":["Marcin Sendera","≈Åukasz Struski","Kamil KsiƒÖ≈ºek","Kryspin Musiol","Jacek Tabor","Dawid Rymarczyk"],"pdf_url":"https://arxiv.org/pdf/2502.07587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07584v1","updated":"2025-02-11T14:31:32Z","published":"2025-02-11T14:31:32Z","title":"Understanding the Generalization Error of Markov algorithms through\n  Poissonization","summary":"  Using continuous-time stochastic differential equation (SDE) proxies to\nstochastic optimization algorithms has proven fruitful for understanding their\ngeneralization abilities. A significant part of these approaches are based on\nthe so-called ``entropy flows'', which greatly simplify the generalization\nanalysis. Unfortunately, such well-structured entropy flows cannot be obtained\nfor most discrete-time algorithms, and the existing SDE approaches remain\nlimited to specific noise and algorithmic structures. We aim to alleviate this\nissue by introducing a generic framework for analyzing the generalization error\nof Markov algorithms through `Poissonization', a continuous-time approximation\nof discrete-time processes with formal approximation guarantees. Through this\napproach, we first develop a novel entropy flow, which directly leads to\nPAC-Bayesian generalization bounds. We then draw novel links to modified\nversions of the celebrated logarithmic Sobolev inequalities (LSI), identify\ncases where such LSIs are satisfied, and obtain improved bounds. Beyond its\ngenerality, our framework allows exploiting specific properties of learning\nalgorithms. In particular, we incorporate the noise structure of different\nalgorithm types - namely, those with additional noise injections (noisy) and\nthose without (non-noisy) - through various technical tools. This illustrates\nthe capacity of our methods to achieve known (yet, Poissonized) and new\ngeneralization bounds.\n","authors":["Benjamin Dupuis","Maxime Haddouche","George Deligiannidis","Umut Simsekli"],"pdf_url":"https://arxiv.org/pdf/2502.07584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07580v1","updated":"2025-02-11T14:27:10Z","published":"2025-02-11T14:27:10Z","title":"Generative Modeling with Bayesian Sample Inference","summary":"  We derive a novel generative model from the simple act of Gaussian posterior\ninference. Treating the generated sample as an unknown variable to infer lets\nus formulate the sampling process in the language of Bayesian probability. Our\nmodel uses a sequence of prediction and posterior update steps to narrow down\nthe unknown sample from a broad initial belief. In addition to a rigorous\ntheoretical analysis, we establish a connection between our model and diffusion\nmodels and show that it includes Bayesian Flow Networks (BFNs) as a special\ncase. In our experiments, we demonstrate improved performance over both BFNs\nand Variational Diffusion Models, achieving competitive likelihood scores on\nCIFAR10 and ImageNet.\n","authors":["Marten Lienen","Marcel Kollovieh","Stephan G√ºnnemann"],"pdf_url":"https://arxiv.org/pdf/2502.07580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07579v1","updated":"2025-02-11T14:25:52Z","published":"2025-02-11T14:25:52Z","title":"Single-Step Consistent Diffusion Samplers","summary":"  Sampling from unnormalized target distributions is a fundamental yet\nchallenging task in machine learning and statistics. Existing sampling\nalgorithms typically require many iterative steps to produce high-quality\nsamples, leading to high computational costs that limit their practicality in\ntime-sensitive or resource-constrained settings. In this work, we introduce\nconsistent diffusion samplers, a new class of samplers designed to generate\nhigh-fidelity samples in a single step. We first develop a distillation\nalgorithm to train a consistent diffusion sampler from a pretrained diffusion\nmodel without pre-collecting large datasets of samples. Our algorithm leverages\nincomplete sampling trajectories and noisy intermediate states directly from\nthe diffusion process. We further propose a method to train a consistent\ndiffusion sampler from scratch, fully amortizing exploration by training a\nsingle model that both performs diffusion sampling and skips intermediate steps\nusing a self-consistency loss. Through extensive experiments on a variety of\nunnormalized distributions, we show that our approach yields high-fidelity\nsamples using less than 1% of the network evaluations required by traditional\ndiffusion samplers.\n","authors":["Pascal Jutras-Dub√©","Patrick Pynadath","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03006v2","updated":"2025-02-11T14:19:32Z","published":"2024-11-05T11:12:11Z","title":"Neural Networks and (Virtual) Extended Formulations","summary":"  Neural networks with piecewise linear activation functions, such as rectified\nlinear units (ReLU) or maxout, are among the most fundamental models in modern\nmachine learning. We make a step towards proving lower bounds on the size of\nsuch neural networks by linking their representative capabilities to the notion\nof the extension complexity $\\mathrm{xc}(P)$ of a polytope $P$. This is a\nwell-studied quantity in combinatorial optimization and polyhedral geometry\ndescribing the number of inequalities needed to model $P$ as a linear program.\nWe show that $\\mathrm{xc}(P)$ is a lower bound on the size of any monotone or\ninput-convex neural network that solves the linear optimization problem over\n$P$. This implies exponential lower bounds on such neural networks for a\nvariety of problems, including the polynomially solvable maximum weight\nmatching problem.\n  In an attempt to prove similar bounds also for general neural networks, we\nintroduce the notion of virtual extension complexity $\\mathrm{vxc}(P)$, which\ngeneralizes $\\mathrm{xc}(P)$ and describes the number of inequalities needed to\nrepresent the linear optimization problem over $P$ as a difference of two\nlinear programs. We prove that $\\mathrm{vxc}(P)$ is a lower bound on the size\nof any neural network that optimizes over $P$. While it remains an open\nquestion to derive useful lower bounds on $\\mathrm{vxc}(P)$, we argue that this\nquantity deserves to be studied independently from neural networks by proving\nthat one can efficiently optimize over a polytope $P$ using a small virtual\nextended formulation.\n","authors":["Christoph Hertrich","Georg Loho"],"pdf_url":"https://arxiv.org/pdf/2411.03006v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04315v3","updated":"2025-02-11T14:01:39Z","published":"2025-02-06T18:57:06Z","title":"ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time\n  Clusters","summary":"  Recent advances in large language models (LLMs) have shown remarkable\nperformance across diverse tasks. However, these models are typically deployed\nwith fixed weights, which limits their ability to adapt dynamically to the\nvariability inherent in real-world data during inference. This paper introduces\nChameleonLLM, a novel framework that enables inference-time adaptation of LLMs\nby leveraging batch-aware clustering and on-the-fly generation of low-rank\nupdates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation\n(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable\nmasks), our method dynamically generates adaptive modifications to the decoder\nweights based on the aggregated statistics of clustered batches. By\nintelligently grouping similar inputs and computing context-aware low-rank\nupdates via a hyper-network, ChameleonLLM achieves significant performance\ngains, outperforming conventional LoRA methods while eliminating the overhead\nof maintaining multiple expert models. Our experiments highlight the potential\nof our approach to serve as a versatile and highly adaptive solution for\nlanguage model inference. ChameleonLLM is open-sourced to ensure the\nreproducibility of our experiments:\nhttps://anonymous.4open.science/r/ChamaleonLLM/\n","authors":["Kamer Ali Yuksel","Hassan Sawaf"],"pdf_url":"https://arxiv.org/pdf/2502.04315v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07563v1","updated":"2025-02-11T14:01:39Z","published":"2025-02-11T14:01:39Z","title":"LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its\n  Hybrid","summary":"  Linear sequence modeling approaches, such as linear attention, provide\nadvantages like linear-time training and constant-memory inference over\nsequence lengths. However, existing sequence parallelism (SP) methods are\neither not optimized for the right-product-first feature of linear attention or\nuse a ring-style communication strategy, which results in lower computation\nparallelism, limits their scalability for longer sequences in distributed\nsystems. In this paper, we introduce LASP-2, a new SP method to enhance both\ncommunication and computation parallelism when training linear attention\ntransformer models with very-long input sequences. Compared to previous work\nLASP, LASP-2 rethinks the minimal communication requirement for SP on linear\nattention layers, reorganizes the whole communication-computation workflow of\nLASP. In this way, only one single AllGather collective communication is needed\non intermediate memory states, whose sizes are independent of the sequence\nlength, leading to significant improvements of both communication and\ncomputation parallelism, as well as their overlap. Additionally, we extend\nLASP-2 to LASP-2H by applying similar communication redesign to standard\nattention modules, offering an efficient SP solution for hybrid models that\nblend linear and standard attention layers. Our evaluation on a Linear-Llama3\nmodel, a variant of Llama3 with linear attention replacing standard attention,\ndemonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2\nachieves training speed improvements of 15.2% over LASP and 36.6% over Ring\nAttention, with a sequence length of 2048K across 64 GPUs. The Code is released\nas a part of: https://github.com/OpenSparseLLMs/Linear-MoE.\n","authors":["Weigao Sun","Disen Lan","Yiran Zhong","Xiaoye Qu","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.07563v1.pdf","comment":"Technical report, 17 pages"},{"id":"http://arxiv.org/abs/2410.00535v3","updated":"2025-02-11T13:59:11Z","published":"2024-10-01T09:21:29Z","title":"The Causal Information Bottleneck and Optimal Causal Variable\n  Abstractions","summary":"  To effectively study complex causal systems, it is often useful to construct\nabstractions of parts of the system by discarding irrelevant details while\npreserving key features. The Information Bottleneck (IB) method is a widely\nused approach to construct variable abstractions by compressing random\nvariables while retaining predictive power over a target variable. Traditional\nmethods like IB are purely statistical and ignore underlying causal structures,\nmaking them ill-suited for causal tasks. We propose the Causal Information\nBottleneck (CIB), a causal extension of the IB, which compresses a set of\nchosen variables while maintaining causal control over a target variable. This\nmethod produces abstractions of (sets of) variables which are causally\ninterpretable, give us insight about the interactions between the abstracted\nvariables and the target variable, and can be used when reasoning about\ninterventions. We present experimental results demonstrating that the learned\nabstractions accurately capture causal relations as intended.\n","authors":["Francisco N. F. Q. Simoes","Mehdi Dastani","Thijs van Ommen"],"pdf_url":"https://arxiv.org/pdf/2410.00535v3.pdf","comment":"Submitted to UAI 2025. Code available at\n  github.com/francisco-simoes/cib-optimization-psagd"},{"id":"http://arxiv.org/abs/2501.18005v3","updated":"2025-02-11T13:58:39Z","published":"2025-01-29T21:40:32Z","title":"Fault Localization via Fine-tuning Large Language Models with Mutation\n  Generated Stack Traces","summary":"  Abrupt and unexpected terminations of software are termed as software\ncrashes. They can be challenging to analyze. Finding the root cause requires\nextensive manual effort and expertise to connect information sources like stack\ntraces, source code, and logs. Typical approaches to fault localization require\neither test failures or source code. Crashes occurring in production\nenvironments, such as that of SAP HANA, provide solely crash logs and stack\ntraces. We present a novel approach to localize faults based only on the stack\ntrace information and no additional runtime information, by fine-tuning large\nlanguage models (LLMs). We address complex cases where the root cause of a\ncrash differs from the technical cause, and is not located in the innermost\nframe of the stack trace. As the number of historic crashes is insufficient to\nfine-tune LLMs, we augment our dataset by leveraging code mutators to inject\nsynthetic crashes into the code base. By fine-tuning on 64,369 crashes\nresulting from 4.1 million mutations of the HANA code base, we can correctly\npredict the root cause location of a crash with an accuracy of 66.9\\% while\nbaselines only achieve 12.6% and 10.6%. We substantiate the generalizability of\nour approach by evaluating on two additional open-source databases, SQLite and\nDuckDB, achieving accuracies of 63% and 74%, respectively. Across all our\nexperiments, fine-tuning consistently outperformed prompting non-finetuned LLMs\nfor localizing faults in our datasets.\n","authors":["Neetha Jambigi","Bartosz Bogacz","Moritz Mueller","Thomas Bach","Michael Felderer"],"pdf_url":"https://arxiv.org/pdf/2501.18005v3.pdf","comment":"I do not have the necessary approvals to out the paper on Arxiv from\n  my organization yet. I was too soon to do this"},{"id":"http://arxiv.org/abs/2502.07553v1","updated":"2025-02-11T13:41:30Z","published":"2025-02-11T13:41:30Z","title":"Attention Learning is Needed to Efficiently Learn Parity Function","summary":"  Transformers, with their attention mechanisms, have emerged as the\nstate-of-the-art architectures of sequential modeling and empirically\noutperform feed-forward neural networks (FFNNs) across many fields, such as\nnatural language processing and computer vision. However, their generalization\nability, particularly for low-sensitivity functions, remains less studied. We\nbridge this gap by analyzing transformers on the $k$-parity problem. Daniely\nand Malach (NeurIPS 2020) show that FFNNs with one hidden layer and $O(nk^7\n\\log k)$ parameters can learn $k$-parity, where the input length $n$ is\ntypically much larger than $k$. In this paper, we prove that FFNNs require at\nleast $\\Omega(n)$ parameters to learn $k$-parity, while transformers require\nonly $O(k)$ parameters, surpassing the theoretical lower bound needed by FFNNs.\nWe further prove that this parameter efficiency cannot be achieved with fixed\nattention heads. Our work establishes transformers as theoretically superior to\nFFNNs in learning parity function, showing how their attention mechanisms\nenable parameter-efficient generalization in functions with low sensitivity.\n","authors":["Yaomengxi Han","Debarghya Ghoshdastidar"],"pdf_url":"https://arxiv.org/pdf/2502.07553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07551v1","updated":"2025-02-11T13:40:15Z","published":"2025-02-11T13:40:15Z","title":"Early Stopping Against Label Noise Without Validation Data","summary":"  Early stopping methods in deep learning face the challenge of balancing the\nvolume of training and validation data, especially in the presence of label\nnoise. Concretely, sparing more data for validation from training data would\nlimit the performance of the learned model, yet insufficient validation data\ncould result in a sub-optimal selection of the desired model. In this paper, we\npropose a novel early stopping method called Label Wave, which does not require\nvalidation data for selecting the desired model in the presence of label noise.\nIt works by tracking the changes in the model's predictions on the training set\nduring the training process, aiming to halt training before the model unduly\nfits mislabeled data. This method is empirically supported by our observation\nthat minimum fluctuations in predictions typically occur at the training epoch\nbefore the model excessively fits mislabeled data. Through extensive\nexperiments, we show both the effectiveness of the Label Wave method across\nvarious settings and its capability to enhance the performance of existing\nmethods for learning with noisy labels.\n","authors":["Suqin Yuan","Lei Feng","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07551v1.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2502.07549v1","updated":"2025-02-11T13:39:35Z","published":"2025-02-11T13:39:35Z","title":"HGTUL: A Hypergraph-based Model For Trajectory User Linking","summary":"  Trajectory User Linking (TUL), which links anonymous trajectories with users\nwho generate them, plays a crucial role in modeling human mobility. Despite\nsignificant advancements in this field, existing studies primarily neglect the\nhigh-order inter-trajectory relationships, which represent complex associations\namong multiple trajectories, manifested through multi-location co-occurrence\npatterns emerging when trajectories intersect at various Points of Interest\n(POIs). Furthermore, they also overlook the variable influence of POIs on\ndifferent trajectories, as well as the user class imbalance problem caused by\ndisparities in user activity levels and check-in frequencies. To address these\nlimitations, we propose a novel HyperGraph-based multi-perspective Trajectory\nUser Linking model (HGTUL). Our model learns trajectory representations from\nboth relational and spatio-temporal perspectives: (1) it captures high-order\nassociations among trajectories by constructing a trajectory hypergraph and\nleverages a hypergraph attention network to learn the variable impact of POIs\non trajectories; (2) it models the spatio-temporal characteristics of\ntrajectories by incorporating their temporal and spatial information into a\nsequential encoder. Moreover, we design a data balancing method to effectively\naddress the user class imbalance problem and experimentally validate its\nsignificance in TUL. Extensive experiments on three real-world datasets\ndemonstrate that HGTUL outperforms state-of-the-art baselines, achieving\nimprovements of 2.57%~20.09% and 5.68%~26.00% in ACC@1 and Macro-F1 metrics,\nrespectively.\n","authors":["Fengjie Chang","Xinning Zhu","Zheng Hu","Yang Qin"],"pdf_url":"https://arxiv.org/pdf/2502.07549v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.01175v4","updated":"2025-02-11T13:35:23Z","published":"2024-06-03T10:14:32Z","title":"NeoRL: Efficient Exploration for Nonepisodic RL","summary":"  We study the problem of nonepisodic reinforcement learning (RL) for nonlinear\ndynamical systems, where the system dynamics are unknown and the RL agent has\nto learn from a single trajectory, i.e., without resets. We propose Nonepisodic\nOptimistic RL (NeoRL), an approach based on the principle of optimism in the\nface of uncertainty. NeoRL uses well-calibrated probabilistic models and plans\noptimistically w.r.t. the epistemic uncertainty about the unknown dynamics.\nUnder continuity and bounded energy assumptions on the system, we provide a\nfirst-of-its-kind regret bound of $O(\\Gamma_T \\sqrt{T})$ for general nonlinear\nsystems with Gaussian process dynamics. We compare NeoRL to other baselines on\nseveral deep RL environments and empirically demonstrate that NeoRL achieves\nthe optimal average cost while incurring the least regret.\n","authors":["Bhavya Sukhija","Lenart Treven","Florian D√∂rfler","Stelian Coros","Andreas Krause"],"pdf_url":"https://arxiv.org/pdf/2406.01175v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07547v1","updated":"2025-02-11T13:34:09Z","published":"2025-02-11T13:34:09Z","title":"Instance-dependent Early Stopping","summary":"  In machine learning practice, early stopping has been widely used to\nregularize models and can save computational costs by halting the training\nprocess when the model's performance on a validation set stops improving.\nHowever, conventional early stopping applies the same stopping criterion to all\ninstances without considering their individual learning statuses, which leads\nto redundant computations on instances that are already well-learned. To\nfurther improve the efficiency, we propose an Instance-dependent Early Stopping\n(IES) method that adapts the early stopping mechanism from the entire training\nset to the instance level, based on the core principle that once the model has\nmastered an instance, the training on it should stop. IES considers an instance\nas mastered if the second-order differences of its loss value remain within a\nsmall range around zero. This offers a more consistent measure of an instance's\nlearning status compared with directly using the loss value, and thus allows\nfor a unified threshold to determine when an instance can be excluded from\nfurther backpropagation. We show that excluding mastered instances from\nbackpropagation can increase the gradient norms, thereby accelerating the\ndecrease of the training loss and speeding up the training process. Extensive\nexperiments on benchmarks demonstrate that IES method can reduce\nbackpropagation instances by 10%-50% while maintaining or even slightly\nimproving the test accuracy and transfer learning performance of a model.\n","authors":["Suqin Yuan","Runqi Lin","Lei Feng","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07547v1.pdf","comment":"Accepted by ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2410.14281v2","updated":"2025-02-11T13:28:10Z","published":"2024-10-18T08:38:12Z","title":"PLMTrajRec: A Scalable and Generalizable Trajectory Recovery Method with\n  Pre-trained Language Models","summary":"  Spatiotemporal trajectory data is crucial for various applications. However,\nissues such as device malfunctions and network instability often cause sparse\ntrajectories, leading to lost detailed movement information. Recovering the\nmissing points in sparse trajectories to restore the detailed information is\nthus essential. Despite recent progress, several challenges remain. First, the\nlack of large-scale dense trajectory data makes it difficult to train a\ntrajectory recovery model from scratch. Second, the varying spatiotemporal\ncorrelations in sparse trajectories make it hard to generalize recovery across\ndifferent sampling intervals. Third, the lack of location information\ncomplicates the extraction of road conditions for missing points.\n  To address these challenges, we propose a novel trajectory recovery model\ncalled PLMTrajRec. It leverages the scalability of a pre-trained language model\n(PLM) and can be fine-tuned with only a limited set of dense trajectories. To\nhandle different sampling intervals in sparse trajectories, we first convert\neach trajectory's sampling interval and movement features into natural language\nrepresentations, allowing the PLM to recognize its interval. We then introduce\na trajectory encoder to unify trajectories of varying intervals into a single\ninterval and capture their spatiotemporal relationships. To obtain road\nconditions for missing points, we propose an area flow-guided implicit\ntrajectory prompt, which models road conditions by collecting traffic flows in\neach region. We also introduce a road condition passing mechanism that uses\nobserved points' road conditions to infer those of the missing points.\nExperiments on two public trajectory datasets with three sampling intervals\neach demonstrate the effectiveness, scalability, and generalization ability of\nPLMTrajRec.\n","authors":["Tonglong Wei","Yan Lin","Youfang Lin","Shengnan Guo","Jilin Hu","Haitao Yuan","Gao Cong","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2410.14281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01567v2","updated":"2025-02-11T13:27:59Z","published":"2024-05-24T18:39:20Z","title":"MeMo: Meaningful, Modular Controllers via Noise Injection","summary":"  Robots are often built from standardized assemblies, (e.g. arms, legs, or\nfingers), but each robot must be trained from scratch to control all the\nactuators of all the parts together. In this paper we demonstrate a new\napproach that takes a single robot and its controller as input and produces a\nset of modular controllers for each of these assemblies such that when a new\nrobot is built from the same parts, its control can be quickly learned by\nreusing the modular controllers. We achieve this with a framework called MeMo\nwhich learns (Me)aningful, (Mo)dular controllers. Specifically, we propose a\nnovel modularity objective to learn an appropriate division of labor among the\nmodules. We demonstrate that this objective can be optimized simultaneously\nwith standard behavior cloning loss via noise injection. We benchmark our\nframework in locomotion and grasping environments on simple to complex robot\nmorphology transfer. We also show that the modules help in task transfer. On\nboth structure and task transfer, MeMo achieves improved training efficiency to\ngraph neural network and Transformer baselines.\n","authors":["Megan Tjandrasuwita","Jie Xu","Armando Solar-Lezama","Wojciech Matusik"],"pdf_url":"https://arxiv.org/pdf/2407.01567v2.pdf","comment":"NeurIPS 2024; 29 pages, 21 figures"},{"id":"http://arxiv.org/abs/2502.07854v1","updated":"2025-02-11T13:12:06Z","published":"2025-02-11T13:12:06Z","title":"Advancing Heat Demand Forecasting with Attention Mechanisms:\n  Opportunities and Challenges","summary":"  Global leaders and policymakers are unified in their unequivocal commitment\nto decarbonization efforts in support of Net-Zero agreements. District Heating\nSystems (DHS), while contributing to carbon emissions due to the continued\nreliance on fossil fuels for heat production, are embracing more sustainable\npractices albeit with some sense of vulnerability as it could constrain their\nability to adapt to dynamic demand and production scenarios. As demographic\ndemands grow and renewables become the central strategy in decarbonizing the\nheating sector, the need for accurate demand forecasting has intensified.\nAdvances in digitization have paved the way for Machine Learning (ML) based\nsolutions to become the industry standard for modeling complex time series\npatterns. In this paper, we focus on building a Deep Learning (DL) model that\nuses deconstructed components of independent and dependent variables that\naffect heat demand as features to perform multi-step ahead forecasting of head\ndemand. The model represents the input features in a time-frequency space and\nuses an attention mechanism to generate accurate forecasts. The proposed method\nis evaluated on a real-world dataset and the forecasting performance is\nassessed against LSTM and CNN-based forecasting models. Across different supply\nzones, the attention-based models outperforms the baselines quantitatively and\nqualitatively, with an Mean Absolute Error (MAE) of 0.105 with a standard\ndeviation of 0.06kW h and a Mean Absolute Percentage Error (MAPE) of 5.4% with\na standard deviation of 2.8%, in comparison the second best model with a MAE of\n0.10 with a standard deviation of 0.06kW h and a MAPE of 5.6% with a standard\ndeviation of 3%.\n","authors":["Adithya Ramachandran","Thorkil Flensmark B. Neergaard","Andreas Maier","Siming Bayer"],"pdf_url":"https://arxiv.org/pdf/2502.07854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07529v1","updated":"2025-02-11T13:10:34Z","published":"2025-02-11T13:10:34Z","title":"Training Deep Learning Models with Norm-Constrained LMOs","summary":"  In this work, we study optimization methods that leverage the linear\nminimization oracle (LMO) over a norm-ball. We propose a new stochastic family\nof algorithms that uses the LMO to adapt to the geometry of the problem and,\nperhaps surprisingly, show that they can be applied to unconstrained problems.\nThe resulting update rule unifies several existing optimization methods under a\nsingle framework. Furthermore, we propose an explicit choice of norm for deep\narchitectures, which, as a side benefit, leads to the transferability of\nhyperparameters across model sizes. Experimentally, we demonstrate significant\nspeedups on nanoGPT training without any reliance on Adam. The proposed method\nis memory-efficient, requiring only one set of model weights and one set of\ngradients, which can be stored in half-precision.\n","authors":["Thomas Pethick","Wanyun Xie","Kimon Antonakopoulos","Zhenyu Zhu","Antonio Silveti-Falls","Volkan Cevher"],"pdf_url":"https://arxiv.org/pdf/2502.07529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07528v1","updated":"2025-02-11T13:09:09Z","published":"2025-02-11T13:09:09Z","title":"Forecasting the future development in quality and value of professional\n  football players for applications in team management","summary":"  Transfers in professional football (soccer) are risky investments because of\nthe large transfer fees and high risks involved. Although data-driven models\ncan be used to improve transfer decisions, existing models focus on describing\nplayers' historical progress, leaving their future performance unknown.\nMoreover, recent developments have called for the use of explainable models\ncombined with uncertainty quantification of predictions. This paper assesses\nexplainable machine learning models based on predictive accuracy and\nuncertainty quantification methods for the prediction of the future development\nin quality and transfer value of professional football players. Using a\nhistorical data set of data-driven indicators describing player quality and the\ntransfer value of a football player, the models are trained to forecast player\nquality and player value one year ahead. These two prediction problems\ndemonstrate the efficacy of tree-based models, particularly random forest and\nXGBoost, in making accurate predictions. In general, the random forest model is\nfound to be the most suitable model because it provides accurate predictions as\nwell as an uncertainty quantification method that naturally arises from the\nbagging procedure of the random forest model. Additionally, our research shows\nthat the development of player performance contains nonlinear patterns and\ninteractions between variables, and that time series information can provide\nuseful information for the modeling of player performance metrics. Our research\nprovides models to help football clubs make more informed, data-driven transfer\ndecisions by forecasting player quality and transfer value.\n","authors":["Koen W. van Arem","Floris Goes-Smit","Jakob S√∂hl"],"pdf_url":"https://arxiv.org/pdf/2502.07528v1.pdf","comment":"The article itself is on the pages 1-27. The data set used in this\n  article is described in the appendix at the pages 28-35"},{"id":"http://arxiv.org/abs/2502.07527v1","updated":"2025-02-11T13:08:03Z","published":"2025-02-11T13:08:03Z","title":"NatureLM: Deciphering the Language of Nature for Scientific Discovery","summary":"  Foundation models have revolutionized natural language processing and\nartificial intelligence, significantly enhancing how machines comprehend and\ngenerate human languages. Inspired by the success of these foundation models,\nresearchers have developed foundation models for individual scientific domains,\nincluding small molecules, materials, proteins, DNA, and RNA. However, these\nmodels are typically trained in isolation, lacking the ability to integrate\nacross different scientific domains. Recognizing that entities within these\ndomains can all be represented as sequences, which together form the \"language\nof nature\", we introduce Nature Language Model (briefly, NatureLM), a\nsequence-based science foundation model designed for scientific discovery.\nPre-trained with data from multiple scientific domains, NatureLM offers a\nunified, versatile model that enables various applications including: (i)\ngenerating and optimizing small molecules, proteins, RNA, and materials using\ntext instructions; (ii) cross-domain generation/design, such as\nprotein-to-molecule and protein-to-RNA generation; and (iii) achieving\nstate-of-the-art performance in tasks like SMILES-to-IUPAC translation and\nretrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach\nfor various scientific tasks, including drug discovery (hit\ngeneration/optimization, ADMET optimization, synthesis), novel material design,\nand the development of therapeutic proteins or nucleotides. We have developed\nNatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion\nparameters) and observed a clear improvement in performance as the model size\nincreases.\n","authors":["Yingce Xia","Peiran Jin","Shufang Xie","Liang He","Chuan Cao","Renqian Luo","Guoqing Liu","Yue Wang","Zequn Liu","Yuan-Jyue Chen","Zekun Guo","Yeqi Bai","Pan Deng","Yaosen Min","Ziheng Lu","Hongxia Hao","Han Yang","Jielan Li","Chang Liu","Jia Zhang","Jianwei Zhu","Kehan Wu","Wei Zhang","Kaiyuan Gao","Qizhi Pei","Qian Wang","Xixian Liu","Yanting Li","Houtian Zhu","Yeqing Lu","Mingqian Ma","Zun Wang","Tian Xie","Krzysztof Maziarz","Marwin Segler","Zhao Yang","Zilong Chen","Yu Shi","Shuxin Zheng","Lijun Wu","Chen Hu","Peggy Dai","Tie-Yan Liu","Haiguang Liu","Tao Qin"],"pdf_url":"https://arxiv.org/pdf/2502.07527v1.pdf","comment":"81 pages"},{"id":"http://arxiv.org/abs/2502.07523v1","updated":"2025-02-11T12:55:32Z","published":"2025-02-11T12:55:32Z","title":"Scaling Off-Policy Reinforcement Learning with Batch and Weight\n  Normalization","summary":"  Reinforcement learning has achieved significant milestones, but sample\nefficiency remains a bottleneck for real-world applications. Recently, CrossQ\nhas demonstrated state-of-the-art sample efficiency with a low update-to-data\n(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with\nhigher UTD ratios. We identify challenges in the training dynamics, which are\nemphasized by higher UTD ratios. To address these, we integrate weight\nnormalization into the CrossQ framework, a solution that stabilizes training,\nhas been shown to prevent potential loss of plasticity and keeps the effective\nlearning rate constant. Our proposed approach reliably scales with increasing\nUTD ratios, achieving competitive performance across 25 challenging continuous\ncontrol tasks on the DeepMind Control Suite and Myosuite benchmarks, notably\nthe complex dog and humanoid environments. This work eliminates the need for\ndrastic interventions, such as network resets, and offers a simple yet robust\npathway for improving sample efficiency and scalability in model-free\nreinforcement learning.\n","authors":["Daniel Palenicek","Florian Vogt","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2502.07523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11107v2","updated":"2025-02-11T12:51:40Z","published":"2024-07-15T15:22:52Z","title":"Latent Linear Quadratic Regulator for Robotic Control Tasks","summary":"  Model predictive control (MPC) has played a more crucial role in various\nrobotic control tasks, but its high computational requirements are concerning,\nespecially for nonlinear dynamical models. This paper presents a\n$\\textbf{la}$tent $\\textbf{l}$inear $\\textbf{q}$uadratic $\\textbf{r}$egulator\n(LaLQR) that maps the state space into a latent space, on which the dynamical\nmodel is linear and the cost function is quadratic, allowing the efficient\napplication of LQR. We jointly learn this alternative system by imitating the\noriginal MPC. Experiments show LaLQR's superior efficiency and generalization\ncompared to other baselines.\n","authors":["Yuan Zhang","Shaohui Yang","Toshiyuki Ohtsuka","Colin Jones","Joschka Boedecker"],"pdf_url":"https://arxiv.org/pdf/2407.11107v2.pdf","comment":"Accepted at RSS 2024 workshop on Koopman Operators in Robotics"},{"id":"http://arxiv.org/abs/2501.02737v2","updated":"2025-02-11T12:38:35Z","published":"2025-01-06T03:11:12Z","title":"Holistic Semantic Representation for Navigational Trajectory Generation","summary":"  Trajectory generation has garnered significant attention from researchers in\nthe field of spatio-temporal analysis, as it can generate substantial\nsynthesized human mobility trajectories that enhance user privacy and alleviate\ndata scarcity. However, existing trajectory generation methods often focus on\nimproving trajectory generation quality from a singular perspective, lacking a\ncomprehensive semantic understanding across various scales. Consequently, we\nare inspired to develop a HOlistic SEmantic Representation (HOSER) framework\nfor navigational trajectory generation. Given an origin-and-destination (OD)\npair and the starting time point of a latent trajectory, we first propose a\nRoad Network Encoder to expand the receptive field of road- and zone-level\nsemantics. Second, we design a Multi-Granularity Trajectory Encoder to\nintegrate the spatio-temporal semantics of the generated trajectory at both the\npoint and trajectory levels. Finally, we employ a Destination-Oriented\nNavigator to seamlessly integrate destination-oriented guidance. Extensive\nexperiments on three real-world datasets demonstrate that HOSER outperforms\nstate-of-the-art baselines by a significant margin. Moreover, the model's\nperformance in few-shot learning and zero-shot learning scenarios further\nverifies the effectiveness of our holistic semantic representation.\n","authors":["Ji Cao","Tongya Zheng","Qinghong Guo","Yu Wang","Junshu Dai","Shunyu Liu","Jie Yang","Jie Song","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2501.02737v2.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.07516v1","updated":"2025-02-11T12:36:00Z","published":"2025-02-11T12:36:00Z","title":"The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation","summary":"  Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study adopts a data-driven approach and presents\nthe first systematic attempt to identify prompts and text tokens in MIMIC-CXR\nthat contribute the most to training data memorization. Our analysis reveals an\nunexpected finding: prompts containing traces of de-identification procedures\nare among the most memorized, with de-identification markers contributing the\nmost. Furthermore, we also find existing inference-time memorization mitigation\nstrategies are ineffective and fail to sufficiently reduce the model's reliance\non memorized text tokens highlighting a broader issue in T2I synthesis with\nMIMIC-CXR. On this front, we propose actionable strategies to enhance privacy\nand improve the reliability of generative models in medical imaging. Finally,\nour results provide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset.\n","authors":["Raman Dutt"],"pdf_url":"https://arxiv.org/pdf/2502.07516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07514v1","updated":"2025-02-11T12:33:33Z","published":"2025-02-11T12:33:33Z","title":"A Near-optimal, Scalable and Corruption-tolerant Framework for\n  Stochastic Bandits: From Single-Agent to Multi-Agent and Beyond","summary":"  We investigate various stochastic bandit problems in the presence of\nadversarial corruption. A seminal contribution to this area is the\nBARBAR~\\citep{gupta2019better} algorithm, which is both simple and efficient,\ntolerating significant levels of corruption with nearly no degradation in\nperformance. However, its regret upper bound exhibits a complexity of $O(KC)$,\nwhile the lower bound is $\\Omega(C)$. In this paper, we enhance the BARBAR\nalgorithm by proposing a novel framework called BARBAT, which eliminates the\nfactor of $K$ and achieves an optimal regret bound up to a logarithmic factor.\nWe also demonstrate how BARBAT can be extended to various settings, including\ngraph bandits, combinatorial semi-bandits, batched bandits and multi-agent\nbandits. In comparison to the Follow-The-Regularized-Leader (FTRL) family of\nmethods, which provide a best-of-both-worlds guarantee, our approach is more\nefficient and parallelizable. Notably, FTRL-based methods face challenges in\nscaling to batched and multi-agent settings.\n","authors":["Zicheng Hu","Cheng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07799v3","updated":"2025-02-11T12:32:59Z","published":"2023-10-11T18:32:21Z","title":"Domain-invariant Clinical Representation Learning by Bridging Data\n  Distribution Shift across EMR Datasets","summary":"  Emerging diseases present challenges in symptom recognition and timely\nclinical intervention due to limited available information. An effective\nprognostic model could assist physicians in making accurate diagnoses and\ndesigning personalized treatment plans to prevent adverse outcomes. However, in\nthe early stages of disease emergence, several factors hamper model\ndevelopment: limited data collection, insufficient clinical experience, and\nprivacy and ethical concerns restrict data availability and complicate accurate\nlabel assignment. Furthermore, Electronic Medical Record (EMR) data from\ndifferent diseases or sources often exhibit significant cross-dataset feature\nmisalignment, severely impacting the effectiveness of deep learning models. We\npresent a domain-invariant representation learning method that constructs a\ntransition model between source and target datasets. By constraining the\ndistribution shift of features generated across different domains, we capture\ndomain-invariant features specifically relevant to downstream tasks, developing\na unified domain-invariant encoder that achieves better feature representation\nacross various task domains. Experimental results across multiple target tasks\ndemonstrate that our proposed model surpasses competing baseline methods and\nachieves faster training convergence, particularly when working with limited\ndata. Extensive experiments validate our method's effectiveness in providing\nmore accurate predictions for emerging pandemics and other diseases. Code is\npublicly available at https://github.com/wang1yuhang/domain_invariant_network.\n","authors":["Zhongji Zhang","Yuhang Wang","Yinghao Zhu","Xinyu Ma","Yasha Wang","Junyi Gao","Liantao Ma","Wen Tang","Xiaoyun Zhang","Ling Wang"],"pdf_url":"https://arxiv.org/pdf/2310.07799v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17472v4","updated":"2025-02-11T12:29:00Z","published":"2024-02-27T12:53:15Z","title":"RAGFormer: Learning Semantic Attributes and Topological Structure for\n  Fraud Detection","summary":"  Fraud detection remains a challenging task due to the complex and deceptive\nnature of fraudulent activities. Current approaches primarily concentrate on\nlearning only one perspective of the graph: either the topological structure of\nthe graph or the attributes of individual nodes. However, we conduct empirical\nstudies to reveal that these two types of features, while nearly orthogonal,\nare each independently effective. As a result, previous methods can not fully\ncapture the comprehensive characteristics of the fraud graph. To address this\ndilemma, we present a novel framework called Relation-Aware GNN with\ntransFormer~(RAGFormer) which simultaneously embeds both semantic and\ntopological features into a target node. The simple yet effective network\nconsists of a semantic encoder, a topology encoder, and an attention fusion\nmodule. The semantic encoder utilizes Transformer to learn semantic features\nand node interactions across different relations. We introduce Relation-Aware\nGNN as the topology encoder to learn topological features and node interactions\nwithin each relation. These two complementary features are interleaved through\nan attention fusion module to support prediction by both orthogonal features.\nExtensive experiments on two popular public datasets demonstrate that RAGFormer\nachieves state-of-the-art performance. The significant improvement of RAGFormer\nin an industrial credit card fraud detection dataset further validates the\napplicability of our method in real-world business scenarios.\n","authors":["Haolin Li","Shuyang Jiang","Lifeng Zhang","Siyuan Du","Guangnan Ye","Hongfeng Chai"],"pdf_url":"https://arxiv.org/pdf/2402.17472v4.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.07510v1","updated":"2025-02-11T12:28:47Z","published":"2025-02-11T12:28:47Z","title":"Joint Metric Space Embedding by Unbalanced OT with Gromov-Wasserstein\n  Marginal Penalization","summary":"  We propose a new approach for unsupervised alignment of heterogeneous\ndatasets, which maps data from two different domains without any known\ncorrespondences to a common metric space. Our method is based on an unbalanced\noptimal transport problem with Gromov-Wasserstein marginal penalization. It can\nbe seen as a counterpart to the recently introduced joint multidimensional\nscaling method. We prove that there exists a minimizer of our functional and\nthat for penalization parameters going to infinity, the corresponding sequence\nof minimizers converges to a minimizer of the so-called embedded Wasserstein\ndistance. Our model can be reformulated as a quadratic, multi-marginal,\nunbalanced optimal transport problem, for which a bi-convex relaxation admits a\nnumerical solver via block-coordinate descent. We provide numerical examples\nfor joint embeddings in Euclidean as well as non-Euclidean spaces.\n","authors":["Florian Beier","Moritz Piening","Robert Beinert","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2502.07510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00134v4","updated":"2025-02-11T12:28:36Z","published":"2024-08-29T12:55:10Z","title":"MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale","summary":"  Multi-agent pathfinding (MAPF) is a problem that generally requires finding\ncollision-free paths for multiple agents in a shared environment. Solving MAPF\noptimally, even under restrictive assumptions, is NP-hard, yet efficient\nsolutions for this problem are critical for numerous applications, such as\nautomated warehouses and transportation systems. Recently, learning-based\napproaches to MAPF have gained attention, particularly those leveraging deep\nreinforcement learning. Typically, such learning-based MAPF solvers are\naugmented with additional components like single-agent planning or\ncommunication. Orthogonally, in this work we rely solely on imitation learning\nthat leverages a large dataset of expert MAPF solutions and transformer-based\nneural network to create a foundation model for MAPF called MAPF-GPT. The\nlatter is capable of generating actions without additional heuristics or\ncommunication. MAPF-GPT demonstrates zero-shot learning abilities when solving\nthe MAPF problems that are not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable MAPF solvers\non a diverse range of problem instances and is computationally efficient during\ninference.\n","authors":["Anton Andreychuk","Konstantin Yakovlev","Aleksandr Panov","Alexey Skrynnik"],"pdf_url":"https://arxiv.org/pdf/2409.00134v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00568v3","updated":"2025-02-11T12:25:42Z","published":"2025-02-01T21:28:30Z","title":"Generating crossmodal gene expression from cancer histopathology\n  improves multimodal AI predictions","summary":"  Emerging research has highlighted that artificial intelligence based\nmultimodal fusion of digital pathology and transcriptomic features can improve\ncancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction.\nHowever, such direct fusion for joint decision is impractical in real clinical\nsettings, where histopathology is still the gold standard for diagnosis and\ntranscriptomic tests are rarely requested, at least in the public healthcare\nsystem. With our novel diffusion based crossmodal generative AI model PathGen,\nwe show that genomic expressions synthesized from digital histopathology\njointly predicts cancer grading and patient survival risk with high accuracy\n(state-of-the-art performance), certainty (through conformal coverage\nguarantee) and interpretability (through distributed attention maps). PathGen\ncode is available for open use by the research community through GitHub at\nhttps://github.com/Samiran-Dey/PathGen.\n","authors":["Samiran Dey","Christopher R. S. Banerji","Partha Basuchowdhuri","Sanjoy K. Saha","Deepak Parashar","Tapabrata Chakraborti"],"pdf_url":"https://arxiv.org/pdf/2502.00568v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00524v3","updated":"2025-02-11T12:25:17Z","published":"2023-01-02T04:27:25Z","title":"Learning Confident Classifiers in the Presence of Label Noise","summary":"  The success of Deep Neural Network (DNN) models significantly depends on the\nquality of provided annotations. In medical image segmentation, for example,\nhaving multiple expert annotations for each data point is common to minimize\nsubjective annotation bias. Then, the goal of estimation is to filter out the\nlabel noise and recover the ground-truth masks, which are not explicitly given.\nThis paper proposes a probabilistic model for noisy observations that allows us\nto build a confident classification and segmentation models. To accomplish it,\nwe explicitly model label noise and introduce a new information-based\nregularization that pushes the network to recover the ground-truth labels. In\naddition, for segmentation task we adjust the loss function by prioritizing\nlearning in high-confidence regions where all the annotators agree on labeling.\nWe evaluate the proposed method on a series of classification tasks such as\nnoisy versions of MNIST, CIFAR-10, Fashion-MNIST datasets as well as CIFAR-10N,\nwhich is real-world dataset with noisy human annotations. Additionally, for\nsegmentation task, we consider several medical imaging datasets, such as, LIDC\nand RIGA that reflect real-world inter-variability among multiple annotators.\nOur experiments show that our algorithm outperforms state-of-the-art solutions\nfor the considered classification and segmentation problems.\n","authors":["Asma Ahmed Hashmi","Aigerim Zhumabayeva","Nikita Kotelevskii","Artem Agafonov","Mohammad Yaqub","Maxim Panov","Martin Tak√°ƒç"],"pdf_url":"https://arxiv.org/pdf/2301.00524v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04078v2","updated":"2025-02-11T12:16:21Z","published":"2024-12-05T11:24:27Z","title":"Mind the Gap: Towards Generalizable Autonomous Penetration Testing via\n  Domain Randomization and Meta-Reinforcement Learning","summary":"  With increasing numbers of vulnerabilities exposed on the internet,\nautonomous penetration testing (pentesting) has emerged as a promising research\narea. Reinforcement learning (RL) is a natural fit for studying this topic.\nHowever, two key challenges limit the applicability of RL-based autonomous\npentesting in real-world scenarios: (a) training environment dilemma --\ntraining agents in simulated environments is sample-efficient while ensuring\ntheir realism remains challenging; (b) poor generalization ability -- agents'\npolicies often perform poorly when transferred to unseen scenarios, with even\nslight changes potentially causing significant generalization gap. To this end,\nwe propose GAP, a generalizable autonomous pentesting framework that aims to\nrealizes efficient policy training in realistic environments and train\ngeneralizable agents capable of drawing inferences about other cases from one\ninstance. GAP introduces a Real-to-Sim-to-Real pipeline that (a) enables\nend-to-end policy learning in unknown real environments while constructing\nrealistic simulations; (b) improves agents' generalization ability by\nleveraging domain randomization and meta-RL learning.Specially, we are among\nthe first to apply domain randomization in autonomous pentesting and propose a\nlarge language model-powered domain randomization method for synthetic\nenvironment generation. We further apply meta-RL to improve agents'\ngeneralization ability in unseen environments by leveraging synthetic\nenvironments. The combination of two methods effectively bridges the\ngeneralization gap and improves agents' policy adaptation\nperformance.Experiments are conducted on various vulnerable virtual machines,\nwith results showing that GAP can enable policy learning in various realistic\nenvironments, achieve zero-shot policy transfer in similar environments, and\nrealize rapid policy adaptation in dissimilar environments.\n","authors":["Shicheng Zhou","Jingju Liu","Yuliang Lu","Jiahai Yang","Yue Zhang","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2412.04078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07503v1","updated":"2025-02-11T12:11:40Z","published":"2025-02-11T12:11:40Z","title":"Harnessing Language's Fractal Geometry with Recursive Inference Scaling","summary":"  Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation.\n","authors":["Ibrahim Alabdulmohsin","Xiaohua Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.07503v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.04411v2","updated":"2025-02-11T12:09:51Z","published":"2025-02-06T11:26:30Z","title":"Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and\n  Uncertainty Based Routing","summary":"  Model merging aggregates Large Language Models (LLMs) finetuned on different\ntasks into a stronger one. However, parameter conflicts between models leads to\nperformance degradation in averaging. While model routing addresses this issue\nby selecting individual models during inference, it imposes excessive storage\nand compute costs, and fails to leverage the common knowledge from different\nmodels. In this work, we observe that different layers exhibit varying levels\nof parameter conflicts. Building on this insight, we average layers with\nminimal parameter conflicts and use a novel task-level expert routing for\nlayers with significant conflicts. To further reduce storage costs, inspired by\ntask arithmetic sparsity, we decouple multiple fine-tuned experts into a dense\nexpert and several sparse experts. Considering the out-of-distribution samples,\nwe select and merge appropriate experts based on the task uncertainty of the\ninput data. We conduct extensive experiments on both LLaMA and Qwen with\nvarying parameter scales, and evaluate on real-world reasoning tasks. Results\ndemonstrate that our method consistently achieves significant performance\nimprovements while requiring less system cost compared to existing methods.\n","authors":["Kunfeng Lai","Zhenheng Tang","Xinglin Pan","Peijie Dong","Xiang Liu","Haolan Chen","Li Shen","Bo Li","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2502.04411v2.pdf","comment":"work in progress. arXiv admin note: text overlap with\n  arXiv:2405.09673 by other authors"},{"id":"http://arxiv.org/abs/2502.06403v2","updated":"2025-02-11T12:08:04Z","published":"2025-02-10T12:44:49Z","title":"The AI off-switch problem as a signalling game: bounded rationality and\n  incomparability","summary":"  The off-switch problem is a critical challenge in AI control: if an AI system\nresists being switched off, it poses a significant risk. In this paper, we\nmodel the off-switch problem as a signalling game, where a human decision-maker\ncommunicates its preferences about some underlying decision problem to an AI\nagent, which then selects actions to maximise the human's utility. We assume\nthat the human is a bounded rational agent and explore various bounded\nrationality mechanisms. Using real machine learning models, we reprove prior\nresults and demonstrate that a necessary condition for an AI system to refrain\nfrom disabling its off-switch is its uncertainty about the human's utility. We\nalso analyse how message costs influence optimal strategies and extend the\nanalysis to scenarios involving incomparability.\n","authors":["Alessio benavoli","Alessandro facchini","Marco Zaffalon"],"pdf_url":"https://arxiv.org/pdf/2502.06403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07500v1","updated":"2025-02-11T12:03:18Z","published":"2025-02-11T12:03:18Z","title":"Unified Graph Networks (UGN): A Deep Neural Framework for Solving Graph\n  Problems","summary":"  Deep neural networks have enabled researchers to create powerful generalized\nframeworks, such as transformers, that can be used to solve well-studied\nproblems in various application domains, such as text and image. However, such\ngeneralized frameworks are not available for solving graph problems. Graph\nstructures are ubiquitous in many applications around us and many graph\nproblems have been widely studied over years. In recent times, there has been a\nsurge in deep neural network based approaches to solve graph problems, with\ngrowing availability of graph structured datasets across diverse domains.\nNevertheless, existing methods are mostly tailored to solve a specific task and\nlack the capability to create a generalized model leading to solutions for\ndifferent downstream tasks. In this work, we propose a novel,\nresource-efficient framework named \\emph{U}nified \\emph{G}raph \\emph{N}etwork\n(UGN) by leveraging the feature extraction capability of graph convolutional\nneural networks (GCN) and 2-dimensional convolutional neural networks (Conv2D).\nUGN unifies various graph learning tasks, such as link prediction, node\nclassification, community detection, graph-to-graph translation, knowledge\ngraph completion, and more, within a cohesive framework, while exercising\nminimal task-specific extensions (e.g., formation of supernodes for coarsening\nmassive networks to increase scalability, use of \\textit{mean target\nconnectivity matrix} (MTCM) representation for achieving scalability in graph\ntranslation task, etc.) to enhance the generalization capability of graph\nlearning and analysis. We test the novel UGN framework for six uncorrelated\ngraph problems, using twelve different datasets. Experimental results show that\nUGN outperforms the state-of-the-art baselines by a significant margin on ten\ndatasets, while producing comparable results on the remaining dataset.\n","authors":["Rudrajit Dawn","Madhusudan Ghosh","Partha Basuchowdhuri","Sudip Kumar Naskar"],"pdf_url":"https://arxiv.org/pdf/2502.07500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07497v1","updated":"2025-02-11T11:59:03Z","published":"2025-02-11T11:59:03Z","title":"On Training-Conditional Conformal Prediction and Binomial Proportion\n  Confidence Intervals","summary":"  Estimating the expectation of a Bernoulli random variable based on N\nindependent trials is a classical problem in statistics, typically addressed\nusing Binomial Proportion Confidence Intervals (BPCI). In the control systems\ncommunity, many critical tasks-such as certifying the statistical safety of\ndynamical systems-can be formulated as BPCI problems. Conformal Prediction\n(CP), a distribution-free technique for uncertainty quantification, has gained\nsignificant attention in recent years and has been applied to various control\nsystems problems, particularly to address uncertainties in learned dynamics or\ncontrollers. A variant known as training-conditional CP was recently employed\nto tackle the problem of safety certification. In this note, we highlight that\nthe use of training-conditional CP in this context does not provide valid\nsafety guarantees. We demonstrate why CP is unsuitable for BPCI problems and\nargue that traditional BPCI methods are better suited for statistical safety\ncertification.\n","authors":["Rudi Coppola","Manuel Mazo Jr"],"pdf_url":"https://arxiv.org/pdf/2502.07497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12600v2","updated":"2025-02-11T11:57:39Z","published":"2024-06-18T13:31:15Z","title":"Generalization bounds for mixing processes via delayed online-to-PAC\n  conversions","summary":"  We study the generalization error of statistical learning algorithms in a\nnon-i.i.d. setting, where the training data is sampled from a stationary mixing\nprocess. We develop an analytic framework for this scenario based on a\nreduction to online learning with delayed feedback. In particular, we show that\nthe existence of an online learning algorithm with bounded regret (against a\nfixed statistical learning algorithm in a specially constructed game of online\nlearning with delayed feedback) implies low generalization error of said\nstatistical learning method even if the data sequence is sampled from a mixing\ntime series. The rates demonstrate a trade-off between the amount of delay in\nthe online learning game and the degree of dependence between consecutive data\npoints, with near-optimal rates recovered in a number of well-studied settings\nwhen the delay is tuned appropriately as a function of the mixing time of the\nprocess.\n","authors":["Baptiste Abeles","Eugenio Clerico","Gergely Neu"],"pdf_url":"https://arxiv.org/pdf/2406.12600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07495v1","updated":"2025-02-11T11:54:56Z","published":"2025-02-11T11:54:56Z","title":"LLM-Sketch: Enhancing Network Sketches with LLM","summary":"  Network stream mining is fundamental to many network operations. Sketches, as\ncompact data structures that offer low memory overhead with bounded accuracy,\nhave emerged as a promising solution for network stream mining. Recent studies\nattempt to optimize sketches using machine learning; however, these approaches\nface the challenges of lacking adaptivity to dynamic networks and incurring\nhigh training costs. In this paper, we propose LLM-Sketch, based on the insight\nthat fields beyond the flow IDs in packet headers can also help infer flow\nsizes. By using a two-tier data structure and separately recording large and\nsmall flows, LLM-Sketch improves accuracy while minimizing memory usage.\nFurthermore, it leverages fine-tuned large language models (LLMs) to reliably\nestimate flow sizes. We evaluate LLM-Sketch on three representative tasks, and\nthe results demonstrate that LLM-Sketch outperforms state-of-the-art methods by\nachieving a $7.5\\times$ accuracy improvement.\n","authors":["Yuanpeng Li","Zhen Xu","Zongwei Lv","Yannan Hu","Yong Cui","Tong Yang"],"pdf_url":"https://arxiv.org/pdf/2502.07495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07491v1","updated":"2025-02-11T11:51:07Z","published":"2025-02-11T11:51:07Z","title":"Exploring Patterns Behind Sports","summary":"  This paper presents a comprehensive framework for time series prediction\nusing a hybrid model that combines ARIMA and LSTM. The model incorporates\nfeature engineering techniques, including embedding and PCA, to transform raw\ndata into a lower-dimensional representation while retaining key information.\nThe embedding technique is used to convert categorical data into continuous\nvectors, facilitating the capture of complex relationships. PCA is applied to\nreduce dimensionality and extract principal components, enhancing model\nperformance and computational efficiency. To handle both linear and nonlinear\npatterns in the data, the ARIMA model captures linear trends, while the LSTM\nmodel models complex nonlinear dependencies. The hybrid model is trained on\nhistorical data and achieves high accuracy, as demonstrated by low RMSE and MAE\nscores. Additionally, the paper employs the run test to assess the randomness\nof sequences, providing insights into the underlying patterns. Ablation studies\nare conducted to validate the roles of different components in the model,\ndemonstrating the significance of each module. The paper also utilizes the SHAP\nmethod to quantify the impact of traditional advantages on the predicted\nresults, offering a detailed understanding of feature importance. The KNN\nmethod is used to determine the optimal prediction interval, further enhancing\nthe model's accuracy. The results highlight the effectiveness of combining\ntraditional statistical methods with modern deep learning techniques for robust\ntime series forecasting in Sports.\n","authors":["Chang Liu","Chengcheng Ma","XuanQi Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12956v2","updated":"2025-02-11T11:50:15Z","published":"2025-01-22T15:29:09Z","title":"GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models","summary":"  Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial resource requirements. While low-bit quantized weights can\nreduce memory usage and improve inference efficiency, current hardware lacks\nnative support for mixed-precision General Matrix Multiplication (mpGEMM),\nresulting in inefficient dequantization-based implementations. Moreover,\nuniform quantization methods often fail to capture weight distributions\nadequately, leading to performance degradation. We propose GANQ (GPU-Adaptive\nNon-Uniform Quantization), a layer-wise post-training non-uniform quantization\nframework optimized for hardware-efficient lookup table-based mpGEMM. GANQ\nachieves superior quantization performance by utilizing a training-free,\nGPU-adaptive optimization algorithm to efficiently reduce layer-wise\nquantization errors. Extensive experiments demonstrate GANQ's ability to reduce\nthe perplexity gap from the FP16 baseline compared to state-of-the-art methods\nfor both 3-bit and 4-bit quantization. Furthermore, when deployed on a single\nNVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\\times$ speedup\nover the baseline, advancing memory and inference efficiency in LLM deployment.\n","authors":["Pengxiang Zhao","Xiaoming Yuan"],"pdf_url":"https://arxiv.org/pdf/2501.12956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07490v1","updated":"2025-02-11T11:49:03Z","published":"2025-02-11T11:49:03Z","title":"Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn\n  More","summary":"  Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.\n","authors":["Xialie Zhuang","Zhikai Jia","Jianjin Li","Zhenyu Zhang","Li Shen","Zheng Cao","Shiwei Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07490v1.pdf","comment":"15 pages,7 figures"},{"id":"http://arxiv.org/abs/2502.07489v1","updated":"2025-02-11T11:48:22Z","published":"2025-02-11T11:48:22Z","title":"Physiome-ODE: A Benchmark for Irregularly Sampled Multivariate Time\n  Series Forecasting Based on Biological ODEs","summary":"  State-of-the-art methods for forecasting irregularly sampled time series with\nmissing values predominantly rely on just four datasets and a few small toy\nexamples for evaluation. While ordinary differential equations (ODE) are the\nprevalent models in science and engineering, a baseline model that forecasts a\nconstant value outperforms ODE-based models from the last five years on three\nof these existing datasets. This unintuitive finding hampers further research\non ODE-based models, a more plausible model family. In this paper, we develop a\nmethodology to generate irregularly sampled multivariate time series (IMTS)\ndatasets from ordinary differential equations and to select challenging\ninstances via rejection sampling. Using this methodology, we create\nPhysiome-ODE, a large and sophisticated benchmark of IMTS datasets consisting\nof 50 individual datasets, derived from real-world ordinary differential\nequations from research in biology. Physiome-ODE is the first benchmark for\nIMTS forecasting that we are aware of and an order of magnitude larger than the\ncurrent evaluation setting of four datasets. Using our benchmark Physiome-ODE,\nwe show qualitatively completely different results than those derived from the\ncurrent four datasets: on Physiome-ODE ODE-based models can play to their\nstrength and our benchmark can differentiate in a meaningful way between\ndifferent IMTS forecasting models. This way, we expect to give a new impulse to\nresearch on ODE-based time series modeling.\n","authors":["Christian Kl√∂tergens","Vijaya Krishna Yalavarthi","Randolf Scholz","Maximilian Stubbemann","Stefan Born","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2502.07489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07488v1","updated":"2025-02-11T11:48:04Z","published":"2025-02-11T11:48:04Z","title":"Improving Adaptive Moment Optimization via Preconditioner\n  Diagonalization","summary":"  Modern adaptive optimization methods, such as Adam and its variants, have\nemerged as the most widely used tools in deep learning over recent years. These\nalgorithms offer automatic mechanisms for dynamically adjusting the update step\nbased on estimates of gradient statistics. Compared to traditional algorithms\nlike Stochastic Gradient Descent, these adaptive methods are typically more\nrobust to model scale and hyperparameter tuning. However, the gradient\nstatistics employed by these methods often do not leverage sufficient gradient\ncovariance information, leading to suboptimal updates in certain directions of\nthe parameter space and potentially slower convergence. In this work, we keep\ntrack of such covariance statistics in the form of a structured preconditioner\nmatrix. Unlike other works, our approach does not apply direct approximations\nto estimate this matrix. We instead implement an invertible transformation that\nmaps the preconditioner matrix into a new space where it becomes approximately\ndiagonal. This enables a diagonal approximation of the preconditioner matrix in\nthe transformed space, offering several computational advantages. Empirical\nresults show that our approach can substantially enhance the convergence speed\nof modern adaptive optimizers. Notably, for large language models like LLaMA,\nwe can achieve a speedup of 2x compared to the baseline Adam. Additionally, our\nmethod can be integrated with memory-efficient optimizers like Adafactor to\nmanage computational overhead.\n","authors":["Son Nguyen","Bo Liu","Lizhang Chen","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2502.07488v1.pdf","comment":"19 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.07480v1","updated":"2025-02-11T11:41:09Z","published":"2025-02-11T11:41:09Z","title":"Overfitting Regimes of Nadaraya-Watson Interpolators","summary":"  In recent years, there has been much interest in understanding the\ngeneralization behavior of interpolating predictors, which overfit on noisy\ntraining data. Whereas standard analyses are concerned with whether a method is\nconsistent or not, recent observations have shown that even inconsistent\npredictors can generalize well. In this work, we revisit the classic\ninterpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method),\nand study its generalization capabilities through this modern viewpoint. In\nparticular, by varying a single bandwidth-like hyperparameter, we prove the\nexistence of multiple overfitting behaviors, ranging non-monotonically from\ncatastrophic, through benign, to tempered. Our results highlight how even\nclassical interpolating methods can exhibit intricate generalization behaviors.\nNumerical experiments complement our theory, demonstrating the same phenomena.\n","authors":["Daniel Barzilai","Guy Kornowski","Ohad Shamir"],"pdf_url":"https://arxiv.org/pdf/2502.07480v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2410.13341v2","updated":"2025-02-11T11:39:18Z","published":"2024-10-17T08:49:42Z","title":"Limits to scalable evaluation at the frontier: LLM as Judge won't beat\n  twice the data","summary":"  High quality annotations are increasingly a bottleneck in the explosively\ngrowing machine learning ecosystem. Scalable evaluation methods that avoid\ncostly annotation have therefore become an important research ambition. Many\nhope to use strong existing models in lieu of costly labels to provide cheap\nmodel evaluations. Unfortunately, this method of using models as judges\nintroduces biases, such as self-preferencing, that can distort model\ncomparisons. An emerging family of debiasing tools promises to fix these issues\nby using a few high quality labels to debias a large number of model judgments.\nIn this paper, we study how far such debiasing methods, in principle, can go.\nOur main result shows that when the judge is no more accurate than the\nevaluated model, no debiasing method can decrease the required amount of ground\ntruth labels by more than half. Our result speaks to the severe limitations of\nthe LLM-as-a-judge paradigm at the evaluation frontier where the goal is to\nassess newly released models that are possibly better than the judge. Through\nan empirical evaluation, we demonstrate that the sample size savings achievable\nin practice are even more modest than what our theoretical limit suggests.\nAlong the way, our work provides new observations about debiasing methods for\nmodel evaluation, and points out promising avenues for future work.\n","authors":["Florian E. Dorner","Vivian Y. Nastl","Moritz Hardt"],"pdf_url":"https://arxiv.org/pdf/2410.13341v2.pdf","comment":"ICLR 2025; 28 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.07469v1","updated":"2025-02-11T11:25:10Z","published":"2025-02-11T11:25:10Z","title":"5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma\n  Turbulence","summary":"  Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to achieving commercially viable fusion\npower is understanding plasma turbulence, which can significantly degrade\nplasma confinement. Modelling turbulence is crucial to design performing plasma\nscenarios for next-generation reactor-class devices and current experimental\nmachines. The nonlinear gyrokinetic equation underpinning turbulence modelling\nevolves a 5D distribution function over time. Solving this equation numerically\nis extremely expensive, requiring up to weeks for a single run to converge,\nmaking it unfeasible for iterative optimisation and control studies. In this\nwork, we propose a method for training neural surrogates for 5D gyrokinetic\nsimulations. Our method extends a hierarchical vision transformer to five\ndimensions and is trained on the 5D distribution function for the adiabatic\nelectron approximation. We demonstrate that our model can accurately infer\ndownstream physical quantities such as heat flux time trace and electrostatic\npotentials for single-step predictions two orders of magnitude faster than\nnumerical codes. Our work paves the way towards neural surrogates for plasma\nturbulence simulations to accelerate deployment of commercial energy production\nvia nuclear fusion.\n","authors":["Gianluca Galletti","Fabian Paischer","Paul Setinek","William Hornsby","Lorenzo Zanisi","Naomi Carey","Stanislas Pamela","Johannes Brandstetter"],"pdf_url":"https://arxiv.org/pdf/2502.07469v1.pdf","comment":"6 pages (+ references and appendix)"},{"id":"http://arxiv.org/abs/2409.13467v3","updated":"2025-02-11T11:25:03Z","published":"2024-09-20T12:55:43Z","title":"Higher-Order Message Passing for Glycan Representation Learning","summary":"  Glycans are the most complex biological sequence, with monosaccharides\nforming extended, non-linear sequences. As post-translational modifications,\nthey modulate protein structure, function, and interactions. Due to their\ndiversity and complexity, predictive models of glycan properties and functions\nare still insufficient.\n  Graph Neural Networks (GNNs) are deep learning models designed to process and\nanalyze graph-structured data. These architectures leverage the connectivity\nand relational information in graphs to learn effective representations of\nnodes, edges, and entire graphs. Iteratively aggregating information from\nneighboring nodes, GNNs capture complex patterns within graph data, making them\nparticularly well-suited for tasks such as link prediction or graph\nclassification across domains.\n  This work presents a new model architecture based on combinatorial complexes\nand higher-order message passing to extract features from glycan structures\ninto a latent space representation. The architecture is evaluated on an\nimproved GlycanML benchmark suite, establishing a new state-of-the-art\nperformance. We envision that these improvements will spur further advances in\ncomputational glycosciences and reveal the roles of glycans in biology.\n","authors":["Roman Joeres","Daniel Bojar"],"pdf_url":"https://arxiv.org/pdf/2409.13467v3.pdf","comment":"Accepted to MLSB Workshop at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2502.07460v1","updated":"2025-02-11T11:11:05Z","published":"2025-02-11T11:11:05Z","title":"Logarithmic Regret for Online KL-Regularized Reinforcement Learning","summary":"  Recent advances in Reinforcement Learning from Human Feedback (RLHF) have\nshown that KL-regularization plays a pivotal role in improving the efficiency\nof RL fine-tuning for large language models (LLMs). Despite its empirical\nadvantage, the theoretical difference between KL-regularized RL and standard RL\nremains largely under-explored. While there is a recent line of work on the\ntheoretical analysis of KL-regularized objective in decision making\n\\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses\neither reduce to the traditional RL setting or rely on strong coverage\nassumptions. In this paper, we propose an optimism-based KL-regularized online\ncontextual bandit algorithm, and provide a novel analysis of its regret. By\ncarefully leveraging the benign optimization landscape induced by the\nKL-regularization and the optimistic reward estimation, our algorithm achieves\nan $\\mathcal{O}\\big(\\eta\\log (N_{\\mathcal R} T)\\cdot d_{\\mathcal R}\\big)$\nlogarithmic regret bound, where $\\eta, N_{\\mathcal R},T,d_{\\mathcal R}$ denote\nthe KL-regularization parameter, the cardinality of the reward function class,\nnumber of rounds, and the complexity of the reward function class. Furthermore,\nwe extend our algorithm and analysis to reinforcement learning by developing a\nnovel decomposition over transition steps and also obtain a similar logarithmic\nregret bound.\n","authors":["Heyang Zhao","Chenlu Ye","Wei Xiong","Quanquan Gu","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17438v2","updated":"2025-02-11T11:10:32Z","published":"2024-11-26T13:54:24Z","title":"Object-centric proto-symbolic behavioural reasoning from pixels","summary":"  Autonomous intelligent agents must bridge computational challenges at\ndisparate levels of abstraction, from the low-level spaces of sensory input and\nmotor commands to the high-level domain of abstract reasoning and planning. A\nkey question in designing such agents is how best to instantiate the\nrepresentational space that will interface between these two levels -- ideally\nwithout requiring supervision in the form of expensive data annotations. These\nobjectives can be efficiently achieved by representing the world in terms of\nobjects (grounded in perception and action). In this work, we present a novel,\nbrain-inspired, deep-learning architecture that learns from pixels to\ninterpret, control, and reason about its environment, using object-centric\nrepresentations. We show the utility of our approach through tasks in synthetic\nenvironments that require a combination of (high-level) logical reasoning and\n(low-level) continuous control. Results show that the agent can learn emergent\nconditional behavioural reasoning, such as $(A \\to B) \\land (\\neg A \\to C)$, as\nwell as logical composition $(A \\to B) \\land (A \\to C) \\vdash A \\to (B \\land\nC)$ and XOR operations, and successfully controls its environment to satisfy\nobjectives deduced from these logical rules. The agent can adapt online to\nunexpected changes in its environment and is robust to mild violations of its\nworld model, thanks to dynamic internal desired goal generation. While the\npresent results are limited to synthetic settings (2D and 3D activated versions\nof dSprites), which fall short of real-world levels of complexity, the proposed\narchitecture shows how to manipulate grounded object representations, as a key\ninductive bias for unsupervised learning, to enable behavioral reasoning.\n","authors":["Ruben van Bergen","Justus H√ºbotter","Pablo Lanillos"],"pdf_url":"https://arxiv.org/pdf/2411.17438v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13735v2","updated":"2025-02-11T11:09:52Z","published":"2024-10-17T16:37:03Z","title":"Generative Conformal Prediction with Vectorized Non-Conformity Scores","summary":"  Conformal prediction (CP) provides model-agnostic uncertainty quantification\nwith guaranteed coverage, but conventional methods often produce overly\nconservative uncertainty sets, especially in multi-dimensional settings. This\nlimitation arises from simplistic non-conformity scores that rely solely on\nprediction error, failing to capture the prediction error distribution's\ncomplexity. To address this, we propose a generative conformal prediction\nframework with vectorized non-conformity scores, leveraging a generative model\nto sample multiple predictions from the fitted data distribution. By computing\nnon-conformity scores across these samples and estimating empirical quantiles\nat different density levels, we construct adaptive uncertainty sets using\ndensity-ranked uncertainty balls. This approach enables more precise\nuncertainty allocation -- yielding larger prediction sets in high-confidence\nregions and smaller or excluded sets in low-confidence regions -- enhancing\nboth flexibility and efficiency. We establish theoretical guarantees for\nstatistical validity and demonstrate through extensive numerical experiments\nthat our method outperforms state-of-the-art techniques on synthetic and\nreal-world datasets.\n","authors":["Minxing Zheng","Shixiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.13735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11676v3","updated":"2025-02-11T11:09:19Z","published":"2024-07-16T12:52:29Z","title":"SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with\n  Realistic Validation On Diverse Modalities","summary":"  Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a\nlabeled source domain to perform well on an unlabeled target domain with some\ndata distribution shift. While many methods have been proposed in the\nliterature, fair and realistic evaluation remains an open question,\nparticularly due to methodological difficulties in selecting hyperparameters in\nthe unsupervised setting. With SKADA-bench, we propose a framework to evaluate\nDA methods on diverse modalities, beyond computer vision task that have been\nlargely explored in the literature. We present a complete and fair evaluation\nof existing shallow algorithms, including reweighting, mapping, and subspace\nalignment. Realistic hyperparameter selection is performed with nested\ncross-validation and various unsupervised model selection scores, on both\nsimulated datasets with controlled shifts and real-world datasets across\ndiverse modalities, such as images, text, biomedical, and tabular data. Our\nbenchmark highlights the importance of realistic validation and provides\npractical guidance for real-life applications, with key insights into the\nchoice and impact of model selection approaches. SKADA-bench is open-source,\nreproducible, and can be easily extended with novel DA methods, datasets, and\nmodel selection criteria without requiring re-evaluating competitors.\nSKADA-bench is available on Github at\nhttps://github.com/scikit-adaptation/skada-bench.\n","authors":["Yanis Lalou","Th√©o Gnassounou","Antoine Collas","Antoine de Mathelin","Oleksii Kachaiev","Ambroise Odonnat","Alexandre Gramfort","Thomas Moreau","R√©mi Flamary"],"pdf_url":"https://arxiv.org/pdf/2407.11676v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12502v3","updated":"2025-02-11T11:03:24Z","published":"2024-11-19T13:40:49Z","title":"Transformer Neural Processes - Kernel Regression","summary":"  Neural Processes (NPs) are a rapidly evolving class of models designed to\ndirectly model the posterior predictive distribution of stochastic processes.\nOriginally developed as a scalable alternative to Gaussian Processes (GPs),\nwhich are limited by $O(n^3)$ runtime complexity, the most accurate modern NPs\ncan often rival GPs but still suffer from an $O(n^2)$ bottleneck due to their\nattention mechanism. We introduce the Transformer Neural Process - Kernel\nRegression (TNP-KR), a scalable NP featuring: (1) a Kernel Regression Block\n(KRBlock), a simple, extensible, and parameter efficient transformer block with\ncomplexity $O(n_c^2 + n_c n_t)$, where $n_c$ and $n_t$ are the number of\ncontext and test points, respectively; (2) a kernel-based attention bias; and\n(3) two novel attention mechanisms: scan attention (SA), a memory-efficient\nscan-based attention that when paired with a kernel-based bias can make TNP-KR\ntranslation invariant, and deep kernel attention (DKA), a Performer-style\nattention that implicitly incoporates a distance bias and further reduces\ncomplexity to $O(n_c)$. These enhancements enable both TNP-KR variants to\nperform inference with 100K context points on over 1M test points in under a\nminute on a single 24GB GPU. On benchmarks spanning meta regression, Bayesian\noptimization, image completion, and epidemiology, TNP-KR with DKA outperforms\nits Performer counterpart on nearly every benchmark, while TNP-KR with SA\nachieves state-of-the-art results.\n","authors":["Daniel Jenson","Jhonathan Navott","Mengyan Zhang","Makkunda Sharma","Elizaveta Semenova","Seth Flaxman"],"pdf_url":"https://arxiv.org/pdf/2411.12502v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07222v2","updated":"2025-02-11T11:02:10Z","published":"2024-06-11T13:01:50Z","title":"Improving Autoformalization using Type Checking","summary":"  Autoformalization, the automatic translation of unconstrained natural\nlanguage into formal languages, has garnered significant attention due to its\npotential applications in theorem proving, formal verification, and LLM output\nchecking. In this work, we analyze both current autoformalization methods and\nthe processes used to evaluate them, focusing specifically on the Lean 4\ntheorem proving language. We demonstrate that scaling type-check filtering with\nself-consistency techniques on top of existing methods significantly improves\nperformance, achieving absolute accuracy gains of up to +18.4\\% on ProofNet. To\nsupport reproducibility and further research, we release our code, including\nnew symbolic equivalence for Lean formulas. We also release new benchmarks: a\nnew research-level mathematics dataset RLM25, a corrected ProofNet, and\nProofNetVerif with labeled correct and incorrect autoformalization pairs for\nevaluating metrics.\n","authors":["Auguste Poiroux","Gail Weiss","Viktor Kunƒçak","Antoine Bosselut"],"pdf_url":"https://arxiv.org/pdf/2406.07222v2.pdf","comment":"New benchmarks released, see\n  https://github.com/augustepoiroux/RLMEval ,\n  https://huggingface.co/datasets/PAug/ProofNetSharp , and\n  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see\n  https://github.com/augustepoiroux/LeanInteract"},{"id":"http://arxiv.org/abs/2502.07456v1","updated":"2025-02-11T11:00:58Z","published":"2025-02-11T11:00:58Z","title":"FedAPA: Server-side Gradient-Based Adaptive Personalized Aggregation for\n  Federated Learning on Heterogeneous Data","summary":"  Personalized federated learning (PFL) tailors models to clients' unique data\ndistributions while preserving privacy. However, existing\naggregation-weight-based PFL methods often struggle with heterogeneous data,\nfacing challenges in accuracy, computational efficiency, and communication\noverhead. We propose FedAPA, a novel PFL method featuring a server-side,\ngradient-based adaptive aggregation strategy to generate personalized models,\nby updating aggregation weights based on gradients of client-parameter changes\nwith respect to the aggregation weights in a centralized manner. FedAPA\nguarantees theoretical convergence and achieves superior accuracy and\ncomputational efficiency compared to 10 PFL competitors across three datasets,\nwith competitive communication overhead.\n","authors":["Yuxia Sun","Aoxiang Sun","Siyi Pan","Zhixiao Fu","Jingcai Guo"],"pdf_url":"https://arxiv.org/pdf/2502.07456v1.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.04625v2","updated":"2025-02-11T10:58:16Z","published":"2024-11-07T11:22:46Z","title":"Sharp Analysis for KL-Regularized Contextual Bandits and RLHF","summary":"  Reverse-Kullback-Leibler (KL) regularization has emerged to be a predominant\ntechnique used to enhance policy optimization in reinforcement learning (RL)\nand reinforcement learning from human feedback (RLHF), which forces the learned\npolicy to stay close to a reference policy. While the effectiveness and\nnecessity of KL-regularization have been empirically demonstrated in various\npractical scenarios, current theoretical analysis of KL-regularized RLHF still\nobtains the same $\\mathcal{O}(1 / \\epsilon^2)$ sample complexity as problems\nwithout KL-regularization. To understand the fundamental distinction between\npolicy learning objectives with KL-regularization and ones without\nKL-regularization, we are the first to theoretically demonstrate the power of\nKL-regularization by providing a sharp analysis for KL-regularized contextual\nbandits and RLHF, revealing an $\\mathcal{O}(1 / \\epsilon)$ sample complexity\nwhen $\\epsilon$ is sufficiently small.\n  We further explore the role of data coverage in contextual bandits and RLHF.\nWhile the coverage assumption is commonly employed in offline RLHF to link the\nsamples from the reference policy to the optimal policy, often at the cost of a\nmultiplicative dependence on the coverage coefficient, its impact on the sample\ncomplexity of online RLHF remains unclear. Previous theoretical analyses of\nonline RLHF typically require explicit exploration and additional structural\nassumptions on the reward function class. In contrast, we show that with\nsufficient coverage from the reference policy, a simple two-stage mixed\nsampling strategy can achieve a sample complexity with only an additive\ndependence on the coverage coefficient. Our results provide a comprehensive\nunderstanding of the roles of KL-regularization and data coverage in RLHF,\nshedding light on the design of more efficient RLHF algorithms.\n","authors":["Heyang Zhao","Chenlu Ye","Quanquan Gu","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.04625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13481v2","updated":"2025-02-11T10:56:50Z","published":"2024-05-22T09:47:54Z","title":"Locally Private Estimation with Public Features","summary":"  We initiate the study of locally differentially private (LDP) learning with\npublic features. We define semi-feature LDP, where some features are publicly\navailable while the remaining ones, along with the label, require protection\nunder local differential privacy. Under semi-feature LDP, we demonstrate that\nthe mini-max convergence rate for non-parametric regression is significantly\nreduced compared to that of classical LDP. Then we propose HistOfTree, an\nestimator that fully leverages the information contained in both public and\nprivate features. Theoretically, HistOfTree reaches the mini-max optimal\nconvergence rate. Empirically, HistOfTree achieves superior performance on both\nsynthetic and real data. We also explore scenarios where users have the\nflexibility to select features for protection manually. In such cases, we\npropose an estimator and a data-driven parameter tuning strategy, leading to\nanalogous theoretical and empirical results.\n","authors":["Yuheng Ma","Ke Jia","Hanfang Yang"],"pdf_url":"https://arxiv.org/pdf/2405.13481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06358v2","updated":"2025-02-11T10:54:40Z","published":"2025-02-10T11:20:10Z","title":"Towards bandit-based prompt-tuning for in-the-wild foundation agents","summary":"  Prompting has emerged as the dominant paradigm for adapting large,\npre-trained transformer-based models to downstream tasks. The Prompting\nDecision Transformer (PDT) enables large-scale, multi-task offline\nreinforcement learning pre-training by leveraging stochastic trajectory prompts\nto identify the target task. However, these prompts are sampled uniformly from\nexpert demonstrations, overlooking a critical limitation: Not all prompts are\nequally informative for differentiating between tasks. To address this, we\npropose an inference time bandit-based prompt-tuning framework that explores\nand optimizes trajectory prompt selection to enhance task performance. Our\nexperiments indicate not only clear performance gains due to bandit-based\nprompt-tuning, but also better sample complexity, scalability, and prompt space\nexploration compared to prompt-tuning baselines.\n","authors":["Finn Rietz","Oleg Smirnov","Sara Karimi","Lele Cao"],"pdf_url":"https://arxiv.org/pdf/2502.06358v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07445v1","updated":"2025-02-11T10:43:36Z","published":"2025-02-11T10:43:36Z","title":"Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon","summary":"  Large language models (LLMs) often appear to excel on public benchmarks, but\nthese high scores may mask an overreliance on dataset-specific surface cues\nrather than true language understanding. We introduce the Chameleon Benchmark\nOverfit Detector (C-BOD), a meta-evaluation framework that systematically\ndistorts benchmark prompts via a parametric transformation and detects\noverfitting of LLMs. By rephrasing inputs while preserving their semantic\ncontent and labels, C-BOD exposes whether a model's performance is driven by\nmemorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our\nmethod reveals an average performance degradation of 2.15% under modest\nperturbations, with 20 out of 26 models exhibiting statistically significant\ndifferences. Notably, models with higher baseline accuracy exhibit larger\nperformance differences under perturbation, and larger LLMs tend to be more\nsensitive to rephrasings indicating that both cases may overrely on fixed\nprompt patterns. In contrast, the Llama family and models with lower baseline\naccuracy show insignificant degradation, suggesting reduced dependency on\nsuperficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows\neasy integration into training pipelines to promote more robust language\nunderstanding. Our findings challenge the community to look beyond leaderboard\nscores and prioritize resilience and generalization in LLM evaluation.\n","authors":["Nurit Cohen-Inger","Yehonatan Elisha","Bracha Shapira","Lior Rokach","Seffi Cohen"],"pdf_url":"https://arxiv.org/pdf/2502.07445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11228v2","updated":"2025-02-11T10:35:04Z","published":"2024-09-17T14:21:02Z","title":"Learning Source Disentanglement in Neural Audio Codec","summary":"  Neural audio codecs have significantly advanced audio compression by\nefficiently converting continuous audio signals into discrete tokens. These\ncodecs preserve high-quality sound and enable sophisticated sound generation\nthrough generative models trained on these tokens. However, existing neural\ncodec models are typically trained on large, undifferentiated audio datasets,\nneglecting the essential discrepancies between sound domains like speech,\nmusic, and environmental sound effects. This oversight complicates data\nmodeling and poses additional challenges to the controllability of sound\ngeneration. To tackle these issues, we introduce the Source-Disentangled Neural\nAudio Codec (SD-Codec), a novel approach that combines audio coding and source\nseparation. By jointly learning audio resynthesis and separation, SD-Codec\nexplicitly assigns audio signals from different domains to distinct codebooks,\nsets of discrete representations. Experimental results indicate that SD-Codec\nnot only maintains competitive resynthesis quality but also, supported by the\nseparation results, demonstrates successful disentanglement of different\nsources in the latent space, thereby enhancing interpretability in audio codec\nand providing potential finer control over the audio generation process.\n","authors":["Xiaoyu Bie","Xubo Liu","Ga√´l Richard"],"pdf_url":"https://arxiv.org/pdf/2409.11228v2.pdf","comment":"ICASSP 2025, project page: https://xiaoyubie1994.github.io/sdcodec/"},{"id":"http://arxiv.org/abs/2502.07849v1","updated":"2025-02-11T10:29:29Z","published":"2025-02-11T10:29:29Z","title":"Understanding Classifier-Free Guidance: High-Dimensional Theory and\n  Non-Linear Generalizations","summary":"  Recent studies have raised concerns about the effectiveness of\nClassifier-Free Guidance (CFG), indicating that in low-dimensional settings, it\ncan lead to overshooting the target distribution and reducing sample diversity.\nIn this work, we demonstrate that in infinite and sufficiently high-dimensional\ncontexts CFG effectively reproduces the target distribution, revealing a\nblessing-of-dimensionality result. Additionally, we explore finite-dimensional\neffects, precisely characterizing overshoot and variance reduction. Based on\nour analysis, we introduce non-linear generalizations of CFG. Through numerical\nsimulations on Gaussian mixtures and experiments on class-conditional and\ntext-to-image diffusion models, we validate our analysis and show that our\nnon-linear CFG offers improved flexibility and generation quality without\nadditional computation cost.\n","authors":["Krunoslav Lehman Pavasovic","Jakob Verbeek","Giulio Biroli","Marc Mezard"],"pdf_url":"https://arxiv.org/pdf/2502.07849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2102.04259v4","updated":"2025-02-11T10:29:23Z","published":"2021-02-04T17:13:03Z","title":"Concentration of Non-Isotropic Random Tensors with Applications to\n  Learning and Empirical Risk Minimization","summary":"  Dimension is an inherent bottleneck to some modern learning tasks, where\noptimization methods suffer from the size of the data. In this paper, we study\nnon-isotropic distributions of data and develop tools that aim at reducing\nthese dimensional costs by a dependency on an effective dimension rather than\nthe ambient one. Based on non-asymptotic estimates of the metric entropy of\nellipsoids -- that prove to generalize to infinite dimensions -- and on a\nchaining argument, our uniform concentration bounds involve an effective\ndimension instead of the global dimension, improving over existing results. We\nshow the importance of taking advantage of non-isotropic properties in learning\nproblems with the following applications: i) we improve state-of-the-art\nresults in statistical preconditioning for communication-efficient distributed\noptimization, ii) we introduce a non-isotropic randomized smoothing for\nnon-smooth optimization. Both applications cover a class of functions that\nencompasses empirical risk minization (ERM) for linear models.\n","authors":["Mathieu Even","Laurent Massouli√©"],"pdf_url":"https://arxiv.org/pdf/2102.04259v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09591v2","updated":"2025-02-11T10:27:18Z","published":"2024-11-14T17:02:41Z","title":"Handling missing values in clinical machine learning: Insights from an\n  expert study","summary":"  Inherently interpretable machine learning (IML) models offer valuable support\nfor clinical decision-making but face challenges when features contain missing\nvalues. Traditional approaches, such as imputation or discarding incomplete\nrecords, are often impractical in scenarios where data is missing at test time.\nWe surveyed 55 clinicians from 29 French trauma centers, collecting 20 complete\nresponses to study their interaction with three IML models in a real-world\nclinical setting for predicting hemorrhagic shock with missing values. Our\nfindings reveal that while clinicians recognize the value of interpretability\nand are familiar with common IML approaches, traditional imputation techniques\noften conflict with their intuition. Instead of imputing unobserved values,\nthey rely on observed features combined with medical intuition and experience.\nAs a result, methods that natively handle missing values are preferred. These\nfindings underscore the need to integrate clinical reasoning into future IML\nmodels to enhance human-computer interaction.\n","authors":["Lena Stempfle","Arthur James","Julie Josse","Tobias Gauss","Fredrik D. Johansson"],"pdf_url":"https://arxiv.org/pdf/2411.09591v2.pdf","comment":"8 pages, 5 figures, restructured writing from previous version and\n  additional results"},{"id":"http://arxiv.org/abs/2502.07432v1","updated":"2025-02-11T10:20:04Z","published":"2025-02-11T10:20:04Z","title":"CapyMOA: Efficient Machine Learning for Data Streams in Python","summary":"  CapyMOA is an open-source library designed for efficient machine learning on\nstreaming data. It provides a structured framework for real-time learning and\nevaluation, featuring a flexible data representation. CapyMOA includes an\nextensible architecture that allows integration with external frameworks such\nas MOA and PyTorch, facilitating hybrid learning approaches that combine\ntraditional online algorithms with deep learning techniques. By emphasizing\nadaptability, scalability, and usability, CapyMOA allows researchers and\npractitioners to tackle dynamic learning challenges across various domains.\n","authors":["Heitor Murilo Gomes","Anton Lee","Nuwan Gunasekara","Yibin Sun","Guilherme Weigert Cassales","Justin Liu","Marco Heyden","Vitor Cerqueira","Maroua Bahri","Yun Sing Koh","Bernhard Pfahringer","Albert Bifet"],"pdf_url":"https://arxiv.org/pdf/2502.07432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18582v2","updated":"2025-02-11T10:16:04Z","published":"2025-01-30T18:54:22Z","title":"Accuracy and Robustness of Weight-Balancing Methods for Training PINNs","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as powerful tools for\nintegrating physics-based models with data by minimizing both data and physics\nlosses. However, this multi-objective optimization problem is notoriously\nchallenging, with some benchmark problems leading to unfeasible solutions. To\naddress these issues, various strategies have been proposed, including adaptive\nweight adjustments in the loss function. In this work, we introduce clear\ndefinitions of accuracy and robustness in the context of PINNs and propose a\nnovel training algorithm based on the Primal-Dual (PD) optimization framework.\nOur approach enhances the robustness of PINNs while maintaining comparable\nperformance to existing weight-balancing methods. Numerical experiments\ndemonstrate that the PD method consistently achieves reliable solutions across\nall investigated cases, even in the low-data regime, and can be easily\nimplemented, facilitating its practical adoption. The code is available at\nhttps://github.com/haoming-SHEN/Accuracy-and-Robustness-of-Weight-Balancing-Methods-for-Training-PINNs.git.\n","authors":["Matthieu Barreau","Haoming Shen"],"pdf_url":"https://arxiv.org/pdf/2501.18582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07425v1","updated":"2025-02-11T10:12:28Z","published":"2025-02-11T10:12:28Z","title":"Towards a Foundation Model for Physics-Informed Neural Networks:\n  Multi-PDE Learning with Active Sampling","summary":"  Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving partial differential equations (PDEs) by embedding physical laws\ninto neural network training. However, traditional PINN models are typically\ndesigned for single PDEs, limiting their generalizability across different\nphysical systems. In this work, we explore the potential of a foundation PINN\nmodel capable of solving multiple PDEs within a unified architecture. We\ninvestigate the efficacy of a single PINN framework trained on four distinct\nPDEs-the Simple Harmonic Oscillator (SHO), the 1D Heat Equation, the 1D Wave\nEquation, and the 2D Laplace Equation, demonstrating its ability to learn\ndiverse physical dynamics.\n  To enhance sample efficiency, we incorporate Active Learning (AL) using Monte\nCarlo (MC) Dropout-based uncertainty estimation, selecting the most informative\ntraining samples iteratively. We evaluate different active learning strategies,\ncomparing models trained on 10%, 20%, 30%, 40%, and 50% of the full dataset,\nand analyze their impact on solution accuracy. Our results indicate that\ntargeted uncertainty sampling significantly improves performance with fewer\ntraining samples, leading to efficient learning across multiple PDEs.\n  This work highlights the feasibility of a generalizable PINN-based foundation\nmodel, capable of adapting to different physics-based problems without\nredesigning network architectures. Our findings suggest that multi-PDE PINNs\nwith active learning can serve as an effective approach for reducing\ncomputational costs while maintaining high accuracy in physics-based deep\nlearning applications.\n","authors":["Keon Vin Park"],"pdf_url":"https://arxiv.org/pdf/2502.07425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07847v1","updated":"2025-02-11T10:10:15Z","published":"2025-02-11T10:10:15Z","title":"Technical note on calibrating vision-language models under covariate\n  shift","summary":"  Despite being a successful example of emerging capability, vision-language\nfoundation models for low-shot vision classification have a limited ability to\nsufficiently generalize to the target data distribution due to sample poverty,\nleading to sensitivity to variations in the data. A popular mitigation strategy\nis finetuning over multiple datasets, but domain generalization is expensive\nwhen practiced in this manner. This work examines both covariate shift between\npre-training data and the underspecified target data, and \\textit{confidence\nmisalignment}, where the model's prediction confidence amplified by the limited\ndata availability. We propose \\textit{Confidence-Calibrated Covariate Shift\nCorrection ($C3SC$)}, a unified framework to mitigate both covariate shift and\nconfidence misalignment. $C3SC$ leverages Fisher information penalty for\ncovariate shift correction and confidence misalignment penalty (CMP) to lower\nconfidence on misclassified examples. Experimental results across various\nvision and covariate shift datasets demonstrates that $C3SC$ significantly\nimproves in calibration (ECE) by $5.82\\%$ at maximum. $C3SC$ shows better\nrobustness as well by showing $3.5\\%$ improvement in accuracy metric on\nchallenging covariate shift datasets, making $C3SC$ a promising solution for\nreliable real-world vision-language low-shot applications under distribution\nshift.\n","authors":["Behraj Khan","Rizwan Qureshi","Tahir Syed"],"pdf_url":"https://arxiv.org/pdf/2502.07847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.00560v2","updated":"2025-02-11T10:02:55Z","published":"2024-12-31T17:46:51Z","title":"Re-evaluating Automatic LLM System Ranking for Alignment with Human\n  Preference","summary":"  Evaluating and ranking the capabilities of different LLMs is crucial for\nunderstanding their performance and alignment with human preferences. Due to\nthe high cost and time-consuming nature of human evaluations, an automatic LLM\nbencher (i.e., an automatic evaluation framework that aims to rank LLMs based\non their alignment with human preferences) is indispensable. An automatic LLM\nbencher consists of four components: the input set (e.g., a user instruction),\nthe evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise\ncomparison), and the aggregation method (e.g., the ELO rating system). However,\nprevious work has not thoroughly explored how to select these components or how\ntheir different combinations influence the results. In this work, through\ncontrolled experiments, we provide a series of recommendations on how to choose\neach component to better automate the evaluation of LLMs. Furthermore, we\ndiscovered that when evaluating LLMs with similar performance, the performance\nof the automatic LLM bencher declines sharply, underscoring the limitations of\ncurrent benchers and calling for future work. Lastly, we found that the\nevaluation models' performance at the instance level (e.g., the accuracy of\nselecting the best output) does not always align with their effectiveness when\nused as a component of a bencher, highlighting the importance of dedicated\nsystem-level evaluation of benchers.\n","authors":["Mingqi Gao","Yixin Liu","Xinyu Hu","Xiaojun Wan","Jonathan Bragg","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2501.00560v2.pdf","comment":"Findings of NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07422v1","updated":"2025-02-11T10:02:43Z","published":"2025-02-11T10:02:43Z","title":"MoENAS: Mixture-of-Expert based Neural Architecture Search for jointly\n  Accurate, Fair, and Robust Edge Deep Neural Networks","summary":"  There has been a surge in optimizing edge Deep Neural Networks (DNNs) for\naccuracy and efficiency using traditional optimization techniques such as\npruning, and more recently, employing automatic design methodologies. However,\nthe focus of these design techniques has often overlooked critical metrics such\nas fairness, robustness, and generalization. As a result, when evaluating SOTA\nedge DNNs' performance in image classification using the FACET dataset, we\nfound that they exhibit significant accuracy disparities (14.09%) across 10\ndifferent skin tones, alongside issues of non-robustness and poor\ngeneralizability. In response to these observations, we introduce\nMixture-of-Experts-based Neural Architecture Search (MoENAS), an automatic\ndesign technique that navigates through a space of mixture of experts to\ndiscover accurate, fair, robust, and general edge DNNs. MoENAS improves the\naccuracy by 4.02% compared to SOTA edge DNNs and reduces the skin tone accuracy\ndisparities from 14.09% to 5.60%, while enhancing robustness by 3.80% and\nminimizing overfitting to 0.21%, all while keeping model size close to\nstate-of-the-art models average size (+0.4M). With these improvements, MoENAS\nestablishes a new benchmark for edge DNN design, paving the way for the\ndevelopment of more inclusive and robust edge DNNs.\n","authors":["Lotfi Abdelkrim Mecharbat","Alberto Marchisio","Muhammad Shafique","Mohammad M. Ghassemi","Tuka Alhanai"],"pdf_url":"https://arxiv.org/pdf/2502.07422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07415v1","updated":"2025-02-11T09:52:06Z","published":"2025-02-11T09:52:06Z","title":"Quantification of model error for inverse problems in the Weak Neural\n  Variational Inference framework","summary":"  We present a novel extension of the Weak Neural Variational Inference (WNVI)\nframework for probabilistic material property estimation that explicitly\nquantifies model errors in PDE-based inverse problems. Traditional approaches\nassume the correctness of all governing equations, including potentially\nunreliable constitutive laws, which can lead to biased estimates and\nmisinterpretations. Our proposed framework addresses this limitation by\ndistinguishing between reliable governing equations, such as conservation laws,\nand uncertain constitutive relationships. By treating all state variables as\nlatent random variables, we enforce these equations through separate sets of\nresiduals, leveraging a virtual likelihood approach with weighted residuals.\nThis formulation not only identifies regions where constitutive laws break down\nbut also improves robustness against model uncertainties without relying on a\nfully trustworthy forward model. We demonstrate the effectiveness of our\napproach in the context of elastography, showing that it provides a structured,\ninterpretable, and computationally efficient alternative to traditional model\nerror correction techniques. Our findings suggest that the proposed framework\nenhances the accuracy and reliability of material property estimation by\noffering a principled way to incorporate uncertainty in constitutive modeling.\n","authors":["Vincent C. Scholz","P. S. Koutsourelakis"],"pdf_url":"https://arxiv.org/pdf/2502.07415v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.13483v2","updated":"2025-02-11T09:52:04Z","published":"2025-01-23T08:57:02Z","title":"Robust Amortized Bayesian Inference with Self-Consistency Losses on\n  Unlabeled Data","summary":"  Neural amortized Bayesian inference (ABI) can solve probabilistic inverse\nproblems orders of magnitude faster than classical methods. However, neural ABI\nis not yet sufficiently robust for widespread and safe applicability. In\nparticular, when performing inference on observations outside of the scope of\nthe simulated data seen during training, for example, because of model\nmisspecification, the posterior approximations are likely to become highly\nbiased. Due to the bad pre-asymptotic behavior of current neural posterior\nestimators in the out-of-simulation regime, the resulting estimation biases\ncannot be fixed in acceptable time by just simulating more training data. In\nthis proof-of-concept paper, we propose a semi-supervised approach that enables\ntraining not only on (labeled) simulated data generated from the model, but\nalso on unlabeled data originating from any source, including real-world data.\nTo achieve the latter, we exploit Bayesian self-consistency properties that can\nbe transformed into strictly proper losses without requiring knowledge of true\nparameter values, that is, without requiring data labels. The results of our\ninitial experiments show remarkable improvements in the robustness of ABI on\nout-of-simulation data. Even if the observed data is far away from both labeled\nand unlabeled training data, inference remains highly accurate. If our findings\nalso generalize to other scenarios and model classes, we believe that our new\nmethod represents a major breakthrough in neural ABI.\n","authors":["Aayush Mishra","Daniel Habermann","Marvin Schmitt","Stefan T. Radev","Paul-Christian B√ºrkner"],"pdf_url":"https://arxiv.org/pdf/2501.13483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07846v1","updated":"2025-02-11T09:51:25Z","published":"2025-02-11T09:51:25Z","title":"Memory Analysis on the Training Course of DeepSeek Models","summary":"  We present a theoretical analysis of GPU memory consumption during the\ntraining of DeepSeek models such as DeepSeek-v2 and DeepSeek-v3. Our primary\nobjective is to clarify the device-level memory requirements associated with\nvarious distributed training configurations. Specifically, we examine critical\nfactors influencing memory usage, including micro-batch size, activation\nrecomputation policies, 3D parallelism, and ZeRO optimizations. It is important\nto emphasize that the training policies discussed in this report are not\nrepresentative of DeepSeek's official configurations. Instead, they are\nexplored to provide a deeper understanding of memory dynamics in training of\nlarge-scale mixture-of-experts model.\n","authors":["Ping Zhang","Lei Su"],"pdf_url":"https://arxiv.org/pdf/2502.07846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07414v1","updated":"2025-02-11T09:51:22Z","published":"2025-02-11T09:51:22Z","title":"Sample Weight Averaging for Stable Prediction","summary":"  The challenge of Out-of-Distribution (OOD) generalization poses a\nfoundational concern for the application of machine learning algorithms to\nrisk-sensitive areas. Inspired by traditional importance weighting and\npropensity weighting methods, prior approaches employ an independence-based\nsample reweighting procedure. They aim at decorrelating covariates to\ncounteract the bias introduced by spurious correlations between unstable\nvariables and the outcome, thus enhancing generalization and fulfilling stable\nprediction under covariate shift. Nonetheless, these methods are prone to\nexperiencing an inflation of variance, primarily attributable to the reduced\nefficacy in utilizing training samples during the reweighting process. Existing\nremedies necessitate either environmental labels or substantially higher time\ncosts along with additional assumptions and supervised information. To mitigate\nthis issue, we propose SAmple Weight Averaging (SAWA), a simple yet efficacious\nstrategy that can be universally integrated into various sample reweighting\nalgorithms to decrease the variance and coefficient estimation error, thus\nboosting the covariate-shift generalization and achieving stable prediction\nacross different environments. We prove its rationality and benefits\ntheoretically. Experiments across synthetic datasets and real-world datasets\nconsistently underscore its superiority against covariate shift.\n","authors":["Han Yu","Yue He","Renzhe Xu","Dongbai Li","Jiayin Zhang","Wenchao Zou","Peng Cui"],"pdf_url":"https://arxiv.org/pdf/2502.07414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01419v2","updated":"2025-02-11T09:50:04Z","published":"2024-11-03T03:04:00Z","title":"PSformer: Parameter-efficient Transformer with Segment Attention for\n  Time Series Forecasting","summary":"  Time series forecasting remains a critical challenge across various domains,\noften complicated by high-dimensional data and long-term dependencies. This\npaper presents a novel transformer architecture for time series forecasting,\nincorporating two key innovations: parameter sharing (PS) and Spatial-Temporal\nSegment Attention (SegAtt). We also define the time series segment as the\nconcatenation of sequence patches from the same positions across different\nvariables. The proposed model, PSformer, reduces the number of training\nparameters through the parameter sharing mechanism, thereby improving model\nefficiency and scalability. The introduction of SegAtt could enhance the\ncapability of capturing local spatio-temporal dependencies by computing\nattention over the segments, and improve global representation by integrating\ninformation across segments. The combination of parameter sharing and SegAtt\nsignificantly improves the forecasting performance. Extensive experiments on\nbenchmark datasets demonstrate that PSformer outperforms popular baselines and\nother transformer-based approaches in terms of accuracy and scalability,\nestablishing itself as an accurate and scalable tool for time series\nforecasting.\n","authors":["Yanlong Wang","Jian Xu","Fei Ma","Shao-Lun Huang","Danny Dongning Sun","Xiao-Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.01419v2.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2502.07409v1","updated":"2025-02-11T09:42:13Z","published":"2025-02-11T09:42:13Z","title":"MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for\n  Few-Shot WSI Classification","summary":"  Whole slide pathology image classification presents challenges due to\ngigapixel image sizes and limited annotation labels, hindering model\ngeneralization. This paper introduces a prompt learning method to adapt large\nvision-language models for few-shot pathology classification. We first extend\nthe Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology\nimage tiles, into a vision-language model by adding adaptors and aligning it\nwith medical text encoders via contrastive learning on 923K image-text pairs.\nThe model is then used to extract visual features and text embeddings from\nfew-shot annotations and fine-tunes with learnable prompt embeddings. Unlike\nprior methods that combine prompts with frozen features using prefix embeddings\nor self-attention, we propose multi-granular attention that compares\ninteractions between learnable prompts with individual image patches and groups\nof them. This approach improves the model's ability to capture both\nfine-grained details and broader context, enhancing its recognition of complex\npatterns across sub-regions. To further improve accuracy, we leverage\n(unbalanced) optimal transport-based visual-text distance to secure model\nrobustness by mitigating perturbations that might occur during the data\naugmentation process. Empirical experiments on lung, kidney, and breast\npathology modalities validate the effectiveness of our approach; thereby, we\nsurpass several of the latest competitors and consistently improve performance\nacross diverse architectures, including CLIP, PLIP, and Prov-GigaPath\nintegrated PLIP. We release our implementations and pre-trained models at this\nMGPATH.\n","authors":["Anh-Tien Nguyen","Duy Minh Ho Nguyen","Nghiem Tuong Diep","Trung Quoc Nguyen","Nhat Ho","Jacqueline Michelle Metsch","Miriam Cindy Maurer","Daniel Sonntag","Hanibal Bohnenberger","Anne-Christin Hauschild"],"pdf_url":"https://arxiv.org/pdf/2502.07409v1.pdf","comment":"first version"},{"id":"http://arxiv.org/abs/2501.12747v2","updated":"2025-02-11T09:41:34Z","published":"2025-01-22T09:31:02Z","title":"Singular leaning coefficients and efficiency in learning theory","summary":"  Singular learning models with non-positive Fisher information matrices\ninclude neural networks, reduced-rank regression, Boltzmann machines, normal\nmixture models, and others. These models have been widely used in the\ndevelopment of learning machines. However, theoretical analysis is still in its\nearly stages. In this paper, we examine learning coefficients, which indicate\nthe general learning efficiency of deep linear learning models and three-layer\nneural network models with ReLU units. Finally, we extend the results to\ninclude the case of the Softmax function.\n","authors":["Miki Aoyagi"],"pdf_url":"https://arxiv.org/pdf/2501.12747v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2502.07408v1","updated":"2025-02-11T09:40:45Z","published":"2025-02-11T09:40:45Z","title":"No Data, No Optimization: A Lightweight Method To Disrupt Neural\n  Networks With Sign-Flips","summary":"  Deep Neural Networks (DNNs) can be catastrophically disrupted by flipping\nonly a handful of sign bits in their parameters. We introduce Deep Neural\nLesion (DNL), a data-free, lightweight method that locates these critical\nparameters and triggers massive accuracy drops. We validate its efficacy on a\nwide variety of computer vision models and datasets. The method requires no\ntraining data or optimization and can be carried out via common exploits\nsoftware, firmware or hardware based attack vectors. An enhanced variant that\nuses a single forward and backward pass further amplifies the damage beyond\nDNL's zero-pass approach. Flipping just two sign bits in ResNet50 on ImageNet\nreduces accuracy by 99.8\\%. We also show that selectively protecting a small\nfraction of vulnerable sign bits provides a practical defense against such\nattacks.\n","authors":["Ido Galil","Moshe Kimhi","Ran El-Yaniv"],"pdf_url":"https://arxiv.org/pdf/2502.07408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07400v1","updated":"2025-02-11T09:29:23Z","published":"2025-02-11T09:29:23Z","title":"Explainable Multimodal Machine Learning for Revealing Structure-Property\n  Relationships in Carbon Nanotube Fibers","summary":"  In this study, we propose Explainable Multimodal Machine Learning (EMML),\nwhich integrates the analysis of diverse data types (multimodal data) using\nfactor analysis for feature extraction with Explainable AI (XAI), for carbon\nnanotube (CNT) fibers prepared from aqueous dispersions. This method is a\npowerful approach to elucidate the mechanisms governing material properties,\nwhere multi-stage fabrication conditions and multiscale structures have complex\ninfluences. Thus, in our case, this approach helps us understand how different\nprocessing steps and structures at various scales impact the final properties\nof CNT fibers. The analysis targeted structures ranging from the nanoscale to\nthe macroscale, including aggregation size distributions of CNT dispersions and\nthe effective length of CNTs. Furthermore, because some types of data were\ndifficult to interpret using standard methods, challenging-to-interpret\ndistribution data were analyzed using Negative Matrix Factorization (NMF) for\nextracting key features that determine the outcome. Contribution analysis with\nSHapley Additive exPlanations (SHAP) demonstrated that small, uniformly\ndistributed aggregates are crucial for improving fracture strength, while CNTs\nwith long effective lengths are significant factors for enhancing electrical\nconductivity. The analysis also identified thresholds and trends for these key\nfactors to assist in defining the conditions needed to optimize CNT fiber\nproperties. EMML is not limited to CNT fibers but can be applied to the design\nof other materials derived from nanomaterials, making it a useful tool for\ndeveloping a wide range of advanced materials. This approach provides a\nfoundation for advancing data-driven materials research.\n","authors":["Daisuke Kimura","Naoko Tajima","Toshiya Okazaki","Shun Muroga"],"pdf_url":"https://arxiv.org/pdf/2502.07400v1.pdf","comment":"33 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.07397v1","updated":"2025-02-11T09:24:25Z","published":"2025-02-11T09:24:25Z","title":"Bandit Optimal Transport","summary":"  Despite the impressive progress in statistical Optimal Transport (OT) in\nrecent years, there has been little interest in the study of the\n\\emph{sequential learning} of OT. Surprisingly so, as this problem is both\npractically motivated and a challenging extension of existing settings such as\nlinear bandits. This article considers (for the first time) the stochastic\nbandit problem of learning to solve generic Kantorovich and entropic OT\nproblems from repeated interactions when the marginals are known but the cost\nis unknown. We provide $\\tilde{\\mathcal O}(\\sqrt{T})$ regret algorithms for\nboth problems by extending linear bandits on Hilbert spaces. These results\nprovide a reduction to infinite-dimensional linear bandits. To deal with the\ndimension, we provide a method to exploit the intrinsic regularity of the cost\nto learn, yielding corresponding regret bounds which interpolate between\n$\\tilde{\\mathcal O}(\\sqrt{T})$ and $\\tilde{\\mathcal O}(T)$.\n","authors":["Lorenzo Croissant"],"pdf_url":"https://arxiv.org/pdf/2502.07397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07394v1","updated":"2025-02-11T09:23:16Z","published":"2025-02-11T09:23:16Z","title":"Interpretable Rules for Online Failure Prediction: A Case Study on the\n  Metro do Porto dataset","summary":"  Due to their high predictive performance, predictive maintenance applications\nhave increasingly been approached with Deep Learning techniques in recent\nyears. However, as in other real-world application scenarios, the need for\nexplainability is often stated but not sufficiently addressed. This study will\nfocus on predicting failures on Metro trains in Porto, Portugal. While recent\nworks have found high-performing deep neural network architectures that feature\na parallel explainability pipeline, the generated explanations are fairly\ncomplicated and need help explaining why the failures are happening. This work\nproposes a simple online rule-based explainability approach with interpretable\nfeatures that leads to straightforward, interpretable rules. We showcase our\napproach on MetroPT2 and find that three specific sensors on the Metro do Porto\ntrains suffice to predict the failures present in the dataset with simple\nrules.\n","authors":["Matthias Jakobs","Bruno Veloso","Joao Gama"],"pdf_url":"https://arxiv.org/pdf/2502.07394v1.pdf","comment":"Under submission at Information Fusion"},{"id":"http://arxiv.org/abs/2410.03782v2","updated":"2025-02-11T09:21:41Z","published":"2024-10-03T16:25:35Z","title":"DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation","summary":"  Adapting a pre-trained foundation model on downstream tasks should ensure\nrobustness against distribution shifts without the need to retrain the whole\nmodel. Although existing weight interpolation methods are simple yet effective,\nwe argue their static nature limits downstream performance while achieving\nefficiency. In this work, we propose DaWin, a training-free dynamic weight\ninterpolation method that leverages the entropy of individual models over each\nunlabeled test sample to assess model expertise, and compute per-sample\ninterpolation coefficients dynamically. Unlike previous works that typically\nrely on additional training to learn such coefficients, our approach requires\nno training. Then, we propose a mixture modeling approach that greatly reduces\ninference overhead raised by dynamic interpolation. We validate DaWin on the\nlarge-scale visual recognition benchmarks, spanning 14 tasks across robust\nfine-tuning -- ImageNet and derived five distribution shift benchmarks -- and\nmulti-task learning with eight classification tasks. Results demonstrate that\nDaWin achieves significant performance gain in considered settings, with\nminimal computational overhead. We further discuss DaWin's analytic behavior to\nexplain its empirical success.\n","authors":["Changdae Oh","Yixuan Li","Kyungwoo Song","Sangdoo Yun","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2410.03782v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2310.03984v2","updated":"2025-02-11T09:07:15Z","published":"2023-10-06T02:45:21Z","title":"AURO: Reinforcement Learning for Adaptive User Retention Optimization in\n  Recommender Systems","summary":"  The field of Reinforcement Learning (RL) has garnered increasing attention\nfor its ability of optimizing user retention in recommender systems. A primary\nobstacle in this optimization process is the environment non-stationarity\nstemming from the continual and complex evolution of user behavior patterns\nover time, such as variations in interaction rates and retention propensities.\nThese changes pose significant challenges to existing RL algorithms for\nrecommendations, leading to issues with dynamics and reward distribution\nshifts. This paper introduces a novel approach called \\textbf{A}daptive\n\\textbf{U}ser \\textbf{R}etention \\textbf{O}ptimization (AURO) to address this\nchallenge. To navigate the recommendation policy in non-stationary\nenvironments, AURO introduces an state abstraction module in the policy\nnetwork. The module is trained with a new value-based loss function, aligning\nits output with the estimated performance of the current policy. As the policy\nperformance of RL is sensitive to environment drifts, the loss function enables\nthe state abstraction to be reflective of environment changes and notify the\nrecommendation policy to adapt accordingly. Additionally, the non-stationarity\nof the environment introduces the problem of implicit cold start, where the\nrecommendation policy continuously interacts with users displaying novel\nbehavior patterns. AURO encourages exploration guarded by performance-based\nrejection sampling to maintain a stable recommendation quality in the\ncost-sensitive online environment. Extensive empirical analysis are conducted\nin a user retention simulator, the MovieLens dataset, and a live short-video\nrecommendation platform, demonstrating AURO's superior performance against all\nevaluated baseline algorithms.\n","authors":["Zhenghai Xue","Qingpeng Cai","Bin Yang","Lantao Hu","Peng Jiang","Kun Gai","Bo An"],"pdf_url":"https://arxiv.org/pdf/2310.03984v2.pdf","comment":"The Web Conference 2025 (Oral)"},{"id":"http://arxiv.org/abs/2501.14588v2","updated":"2025-02-11T09:03:49Z","published":"2025-01-24T15:49:04Z","title":"Data Assetization via Resources-decoupled Federated Learning","summary":"  With the development of the digital economy, data is increasingly recognized\nas an essential resource for both work and life. However, due to privacy\nconcerns, data owners tend to maximize the value of data through the\ncirculation of information rather than direct data transfer. Federated learning\n(FL) provides an effective approach to collaborative training models while\npreserving privacy. However, as model parameters and training data grow, there\nare not only real differences in data resources between different data owners,\nbut also mismatches between data and computing resources. These challenges lead\nto inadequate collaboration among data owners, compute centers, and model\nowners, reducing the global utility of the three parties and the effectiveness\nof data assetization. In this work, we first propose a framework for\nresource-decoupled FL involving three parties. Then, we design a Tripartite\nStackelberg Model and theoretically analyze the Stackelberg-Nash equilibrium\n(SNE) for participants to optimize global utility. Next, we propose the\nQuality-aware Dynamic Resources-decoupled FL algorithm (QD-RDFL), in which we\nderive and solve the optimal strategies of all parties to achieve SNE using\nbackward induction. We also design a dynamic optimization mechanism to improve\nthe optimal strategy profile by evaluating the contribution of data quality\nfrom data owners to the global model during real training. Finally, our\nextensive experiments demonstrate that our method effectively encourages the\nlinkage of the three parties involved, maximizing the global utility and value\nof data assets.\n","authors":["Jianzhe Zhao","Feida Zhu","Lingyan He","Zixin Tang","Mingce Gao","Shiyu Yang","Guibing Guo"],"pdf_url":"https://arxiv.org/pdf/2501.14588v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18987v2","updated":"2025-02-11T09:03:09Z","published":"2024-10-09T17:19:22Z","title":"Point Cloud Synthesis Using Inner Product Transforms","summary":"  Point-cloud synthesis, i.e. the generation of novel point clouds from an\ninput distribution, remains a challenging task, for which numerous complex\nmachine-learning models have been devised. We develop a novel method that\nencodes geometrical-topological characteristics of point clouds using inner\nproducts, leading to a highly-efficient point cloud representation with\nprovable expressivity properties. Integrated into deep learning models, our\nencoding exhibits high quality in typical tasks like reconstruction,\ngeneration, and interpolation, with inference times orders of magnitude faster\nthan existing methods.\n","authors":["Ernst R√∂ell","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2410.18987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07373v1","updated":"2025-02-11T08:48:46Z","published":"2025-02-11T08:48:46Z","title":"EvoFlow: Evolving Diverse Agentic Workflows On The Fly","summary":"  The past two years have witnessed the evolution of large language model\n(LLM)-based multi-agent systems from labor-intensive manual design to partial\nautomation (\\textit{e.g.}, prompt engineering, communication topology) and\neventually to fully automated design. However, existing agentic automation\npipelines often lack LLM heterogeneity and focus on single-objective\nperformance optimization, limiting their potential to combine weaker models for\nmore customized and cost-effective solutions. To address this challenge, we\npropose EvoFlow, a niching evolutionary algorithm-based framework to\nautomatically search a population of heterogeneous and complexity-adaptive\nagentic workflows, rather than a single homogeneous, complex workflow.\nTechnically, EvoFlow performs \\textit{(1) tag-based retrieval} to extract\nparent workflows from an agentic population, evolves new workflows through\n\\textit{(2) crossover} and \\textit{(3) mutation}, and employs \\textit{(4)\nniching-based selection} to maintain population diversity and quality.\nExtensive evaluations across seven benchmarks demonstrate that EvoFlow is:\n\\textbf{(I) diverse}, evolving a population of workflows ranging from simple\nI/O tasks to complex multi-turn interactions; \\textbf{(II) high-performing},\noutperforming previous handcrafted and automated workflows by\n$1.23\\%\\sim29.86\\%$; \\textbf{(III) economical}, surpassing powerful\n\\llmname{o1-preview} at $12.4\\%$ of its inference cost using weaker open-source\nmodels.\n","authors":["Guibin Zhang","Kaijie Chen","Guancheng Wan","Heng Chang","Hong Cheng","Kun Wang","Shuyue Hu","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2502.07373v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07369v1","updated":"2025-02-11T08:43:41Z","published":"2025-02-11T08:43:41Z","title":"Uniform Kernel Prober","summary":"  The ability to identify useful features or representations of the input data\nbased on training data that achieves low prediction error on test data across\nmultiple prediction tasks is considered the key to multitask learning success.\nIn practice, however, one faces the issue of the choice of prediction tasks and\nthe availability of test data from the chosen tasks while comparing the\nrelative performance of different features. In this work, we develop a class of\npseudometrics called Uniform Kernel Prober (UKP) for comparing features or\nrepresentations learned by different statistical models such as neural networks\nwhen the downstream prediction tasks involve kernel ridge regression. The\nproposed pseudometric, UKP, between any two representations, provides a uniform\nmeasure of prediction error on test data corresponding to a general class of\nkernel ridge regression tasks for a given choice of a kernel without access to\ntest data. Additionally, desired invariances in representations can be\nsuccessfully captured by UKP only through the choice of the kernel function and\nthe pseudometric can be efficiently estimated from $n$ input data samples with\n$O(\\frac{1}{\\sqrt{n}})$ estimation error. We also experimentally demonstrate\nthe ability of UKP to discriminate between different types of features or\nrepresentations based on their generalization performance on downstream kernel\nridge regression tasks.\n","authors":["Soumya Mukherjee","Bharath K. Sriperumbudur"],"pdf_url":"https://arxiv.org/pdf/2502.07369v1.pdf","comment":"34 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.07365v1","updated":"2025-02-11T08:37:16Z","published":"2025-02-11T08:37:16Z","title":"LongReD: Mitigating Short-Text Degradation of Long-Context Large\n  Language Models via Restoration Distillation","summary":"  Large language models (LLMs) have gained extended context windows through\nscaling positional encodings and lightweight continual pre-training. However,\nthis often leads to degraded performance on short-text tasks, while the reasons\nfor this degradation remain insufficiently explored. In this work, we identify\ntwo primary factors contributing to this issue: distribution drift in hidden\nstates and attention scores, and catastrophic forgetting during continual\npre-training. To address these challenges, we propose Long Context Pre-training\nwith Restoration Distillation (LongReD), a novel approach designed to mitigate\nshort-text performance degradation through minimizing the distribution\ndiscrepancy between the extended and original models. Besides training on long\ntexts, LongReD distills the hidden state of selected layers from the original\nmodel on short texts. Additionally, LongReD also introduces a short-to-long\ndistillation, aligning the output distribution on short texts with that on long\ntexts by leveraging skipped positional indices. Experiments on common text\nbenchmarks demonstrate that LongReD effectively preserves the model's\nshort-text performance while maintaining comparable or even better capacity to\nhandle long texts than baselines.\n","authors":["Zican Dong","Junyi Li","Jinhao Jiang","Mingyu Xu","Wayne Xin Zhao","Bingning Wang","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07364v1","updated":"2025-02-11T08:36:38Z","published":"2025-02-11T08:36:38Z","title":"Effects of Random Edge-Dropping on Over-Squashing in Graph Neural\n  Networks","summary":"  Message Passing Neural Networks (MPNNs) are a class of Graph Neural Networks\n(GNNs) that leverage the graph topology to propagate messages across\nincreasingly larger neighborhoods. The message-passing scheme leads to two\ndistinct challenges: over-smoothing and over-squashing. While several\nalgorithms, e.g. DropEdge and its variants -- DropNode, DropAgg and DropGNN --\nhave successfully addressed the over-smoothing problem, their impact on\nover-squashing remains largely unexplored. This represents a critical gap in\nthe literature as failure to mitigate over-squashing would make these methods\nunsuitable for long-range tasks. In this work, we take the first step towards\nclosing this gap by studying the aforementioned algorithms in the context of\nover-squashing. We present novel theoretical results that characterize the\nnegative effects of DropEdge on sensitivity between distant nodes, suggesting\nits unsuitability for long-range tasks. Our findings are easily extended to its\nvariants, allowing us to build a comprehensive understanding of how they affect\nover-squashing. We evaluate these methods using real-world datasets,\ndemonstrating their detrimental effects. Specifically, we show that while\nDropEdge-variants improve test-time performance in short range tasks, they\ndeteriorate performance in long-range ones. Our theory explains these results\nas follows: random edge-dropping lowers the effective receptive field of GNNs,\nwhich although beneficial for short-range tasks, misaligns the models on\nlong-range ones. This forces the models to overfit to short-range artefacts in\nthe training set, resulting in poor generalization. Our conclusions highlight\nthe need to re-evaluate various methods designed for training deep GNNs, with a\nrenewed focus on modelling long-range interactions.\n","authors":["Jasraj Singh","Keyue Jiang","Brooks Paige","Laura Toni"],"pdf_url":"https://arxiv.org/pdf/2502.07364v1.pdf","comment":"24 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2502.07344v1","updated":"2025-02-11T08:16:48Z","published":"2025-02-11T08:16:48Z","title":"Integrating Physics and Data-Driven Approaches: An Explainable and\n  Uncertainty-Aware Hybrid Model for Wind Turbine Power Prediction","summary":"  The rapid growth of the wind energy sector underscores the urgent need to\noptimize turbine operations and ensure effective maintenance through early\nfault detection systems. While traditional empirical and physics-based models\noffer approximate predictions of power generation based on wind speed, they\noften fail to capture the complex, non-linear relationships between other input\nvariables and the resulting power output. Data-driven machine learning methods\npresent a promising avenue for improving wind turbine modeling by leveraging\nlarge datasets, enhancing prediction accuracy but often at the cost of\ninterpretability. In this study, we propose a hybrid semi-parametric model that\ncombines the strengths of both approaches, applied to a dataset from a wind\nfarm with four turbines. The model integrates a physics-inspired submodel,\nproviding a reasonable approximation of power generation, with a non-parametric\nsubmodel that predicts the residuals. This non-parametric submodel is trained\non a broader range of variables to account for phenomena not captured by the\nphysics-based component. The hybrid model achieves a 37% improvement in\nprediction accuracy over the physics-based model. To enhance interpretability,\nSHAP values are used to analyze the influence of input features on the residual\nsubmodel's output. Additionally, prediction uncertainties are quantified using\na conformalized quantile regression method. The combination of these\ntechniques, alongside the physics grounding of the parametric submodel,\nprovides a flexible, accurate, and reliable framework. Ultimately, this study\nopens the door for evaluating the impact of unmodeled variables on wind turbine\npower generation, offering a basis for potential optimization.\n","authors":["Alfonso Gij√≥n","Simone Eiraudo","Antonio Manjavacas","Daniele Salvatore Schiera","Miguel Molina-Solana","Juan G√≥mez-Romero"],"pdf_url":"https://arxiv.org/pdf/2502.07344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02929v2","updated":"2025-02-11T08:09:50Z","published":"2024-06-05T04:37:06Z","title":"ZeroDiff: Solidified Visual-Semantic Correlation in Zero-Shot Learning","summary":"  Zero-shot Learning (ZSL) aims to enable classifiers to identify unseen\nclasses. This is typically achieved by generating visual features for unseen\nclasses based on learned visual-semantic correlations from seen classes.\nHowever, most current generative approaches heavily rely on having a sufficient\nnumber of samples from seen classes. Our study reveals that a scarcity of seen\nclass samples results in a marked decrease in performance across many\ngenerative ZSL techniques. We argue, quantify, and empirically demonstrate that\nthis decline is largely attributable to spurious visual-semantic correlations.\nTo address this issue, we introduce ZeroDiff, an innovative generative\nframework for ZSL that incorporates diffusion mechanisms and contrastive\nrepresentations to enhance visual-semantic correlations. ZeroDiff comprises\nthree key components: (1) Diffusion augmentation, which naturally transforms\nlimited data into an expanded set of noised data to mitigate generative model\noverfitting; (2) Supervised-contrastive (SC)-based representations that\ndynamically characterize each limited sample to support visual feature\ngeneration; and (3) Multiple feature discriminators employing a\nWasserstein-distance-based mutual learning approach, evaluating generated\nfeatures from various perspectives, including pre-defined semantics, SC-based\nrepresentations, and the diffusion process. Extensive experiments on three\npopular ZSL benchmarks demonstrate that ZeroDiff not only achieves significant\nimprovements over existing ZSL methods but also maintains robust performance\neven with scarce training data. Our codes are available at\nhttps://github.com/FouriYe/ZeroDiff_ICLR25.\n","authors":["Zihan Ye","Shreyank N. Gowda","Xiaowei Huang","Haotian Xu","Yaochu Jin","Kaizhu Huang","Xiaobo Jin"],"pdf_url":"https://arxiv.org/pdf/2406.02929v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2409.03365v3","updated":"2025-02-11T08:08:01Z","published":"2024-09-05T09:10:40Z","title":"Spindle: Efficient Distributed Training of Multi-Task Large Models via\n  Wavefront Scheduling","summary":"  Recent foundation models are capable of handling multiple tasks and multiple\ndata modalities with the unified base model structure and several specialized\nmodel components. However, efficient training of such multi-task (MT)\nmulti-modal (MM) models poses significant system challenges due to the\nsophisticated model architecture and the heterogeneous workloads of different\ntasks and modalities.\n  In this paper, we propose Spindle, a brand new training system tailored for\nresource-efficient and high-performance training of MT MM models via wavefront\nscheduling. The key idea of Spindle is to decompose the model execution into\nwaves and address the joint optimization problem sequentially, including both\nheterogeneity-aware workload parallelization and dependency-driven execution\nscheduling. We build our system and evaluate it on various MT MM models.\nExperiments demonstrate the superior performance and efficiency of Spindle,\nwith speedup ratio up to 71% compared to state-of-the-art training systems.\n","authors":["Yujie Wang","Shenhan Zhu","Fangcheng Fu","Xupeng Miao","Jie Zhang","Juan Zhu","Fan Hong","Yong Li","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2409.03365v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02300v2","updated":"2025-02-11T08:07:17Z","published":"2025-02-04T13:13:35Z","title":"Density Ratio Estimation with Conditional Probability Paths","summary":"  Density ratio estimation in high dimensions can be reframed as integrating a\ncertain quantity, the time score, over probability paths which interpolate\nbetween the two densities. In practice, the time score has to be estimated\nbased on samples from the two densities. However, existing methods for this\nproblem remain computationally expensive and can yield inaccurate estimates.\nInspired by recent advances in generative modeling, we introduce a novel\nframework for time score estimation, based on a conditioning variable. Choosing\nthe conditioning variable judiciously enables a closed-form objective function.\nWe demonstrate that, compared to previous approaches, our approach results in\nfaster learning of the time score and competitive or better estimation\naccuracies of the density ratio on challenging tasks. Furthermore, we establish\ntheoretical guarantees on the error of the estimated density ratio.\n","authors":["Hanlin Yu","Arto Klami","Aapo Hyv√§rinen","Anna Korba","Omar Chehab"],"pdf_url":"https://arxiv.org/pdf/2502.02300v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08508v2","updated":"2025-02-11T08:04:24Z","published":"2024-02-13T15:03:02Z","title":"A PAC-Bayesian Link Between Generalisation and Flat Minima","summary":"  Modern machine learning usually involves predictors in the overparameterised\nsetting (number of trained parameters greater than dataset size), and their\ntraining yields not only good performance on training data, but also good\ngeneralisation capacity. This phenomenon challenges many theoretical results,\nand remains an open problem. To reach a better understanding, we provide novel\ngeneralisation bounds involving gradient terms. To do so, we combine the\nPAC-Bayes toolbox with Poincar\\'e and Log-Sobolev inequalities, avoiding an\nexplicit dependency on the dimension of the predictor space. Our results\nhighlight the positive influence of flat minima (being minima with a\nneighbourhood nearly minimising the learning problem as well) on generalisation\nperformance, involving directly the benefits of the optimisation phase.\n","authors":["Maxime Haddouche","Paul Viallard","Umut Simsekli","Benjamin Guedj"],"pdf_url":"https://arxiv.org/pdf/2402.08508v2.pdf","comment":"Published at International Conference on Algorithmic Learning Theory\n  2025"},{"id":"http://arxiv.org/abs/2502.07843v1","updated":"2025-02-11T07:56:19Z","published":"2025-02-11T07:56:19Z","title":"Emotional EEG Classification using Upscaled Connectivity Matrices","summary":"  In recent studies of emotional EEG classification, connectivity matrices have\nbeen successfully employed as input to convolutional neural networks (CNNs),\nwhich can effectively consider inter-regional interaction patterns in EEG.\nHowever, we find that such an approach has a limitation that important patterns\nin connectivity matrices may be lost during the convolutional operations in\nCNNs. To resolve this issue, we propose and validate an idea to upscale the\nconnectivity matrices to strengthen the local patterns. Experimental results\ndemonstrate that this simple idea can significantly enhance the classification\nperformance.\n","authors":["Chae-Won Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2502.07843v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07337v1","updated":"2025-02-11T07:55:41Z","published":"2025-02-11T07:55:41Z","title":"Neural Flow Samplers with Shortcut Models","summary":"  Sampling from unnormalized densities is a fundamental task across various\ndomains. Flow-based samplers generate samples by learning a velocity field that\nsatisfies the continuity equation, but this requires estimating the intractable\ntime derivative of the partition function. While importance sampling provides\nan approximation, it suffers from high variance. To mitigate this, we introduce\na velocity-driven Sequential Monte Carlo method combined with control variates\nto reduce variance. Additionally, we incorporate a shortcut model to improve\nefficiency by minimizing the number of sampling steps. Empirical results on\nboth synthetic datasets and $n$-body system targets validate the effectiveness\nof our approach.\n","authors":["Wuhao Chen","Zijing Ou","Yingzhen Li"],"pdf_url":"https://arxiv.org/pdf/2502.07337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07326v1","updated":"2025-02-11T07:43:46Z","published":"2025-02-11T07:43:46Z","title":"PICTS: A Novel Deep Reinforcement Learning Approach for Dynamic P-I\n  Control in Scanning Probe Microscopy","summary":"  We have developed a Parallel Integrated Control and Training System,\nleveraging the deep reinforcement learning to dynamically adjust the control\nstrategies in real time for scanning probe microscopy techniques.\n","authors":["Ziwei Wei","Shuming Wei","Qibin Zeng","Wanheng Lu","Huajun Liu","Kaiyang Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.07326v1.pdf","comment":"21 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.07325v1","updated":"2025-02-11T07:43:03Z","published":"2025-02-11T07:43:03Z","title":"Long-term simulation of physical and mechanical behaviors using\n  curriculum-transfer-learning based physics-informed neural networks","summary":"  This paper proposes a Curriculum-Transfer-Learning based physics-informed\nneural network (CTL-PINN) for long-term simulation of physical and mechanical\nbehaviors. The main innovation of CTL-PINN lies in decomposing long-term\nproblems into a sequence of short-term subproblems. Initially, the standard\nPINN is employed to solve the first sub-problem. As the simulation progresses,\nsubsequent time-domain problems are addressed using a curriculum learning\napproach that integrates information from previous steps. Furthermore, transfer\nlearning techniques are incorporated, allowing the model to effectively utilize\nprior training data and solve sequential time domain transfer problems.\nCTL-PINN combines the strengths of curriculum learning and transfer learning,\novercoming the limitations of standard PINNs, such as local optimization\nissues, and addressing the inaccuracies over extended time domains encountered\nin CL-PINN and the low computational efficiency of TL-PINN. The efficacy and\nrobustness of CTL-PINN are demonstrated through applications to nonlinear wave\npropagation, Kirchhoff plate dynamic response, and the hydrodynamic model of\nthe Three Gorges Reservoir Area, showcasing its superior capability in\naddressing long-term computational challenges.\n","authors":["Yuan Guo","Zhuojia Fu","Jian Min","Shiyu Lin","Xiaoting Liu","Youssef F. Rashed","Xiaoying Zhuang"],"pdf_url":"https://arxiv.org/pdf/2502.07325v1.pdf","comment":"31 pages, 18 figures"},{"id":"http://arxiv.org/abs/2502.07322v1","updated":"2025-02-11T07:42:09Z","published":"2025-02-11T07:42:09Z","title":"MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject\n  Batch Editing for LLMs","summary":"  As large language models continue to scale up, knowledge editing techniques\nthat modify models' internal knowledge without full retraining have gained\nsignificant attention. MEMIT, a prominent batch editing algorithm, stands out\nfor its capability to perform mass knowledge modifications. However, we uncover\na critical limitation that MEMIT's editing efficacy significantly deteriorates\nwhen processing batches containing multiple edits sharing the same subject. Our\nanalysis reveals that the root cause lies in MEMIT's key value modeling\nframework: When multiple facts with the same subject in a batch are modeled\nthrough MEMIT's key value mechanism, identical keys (derived from the shared\nsubject) are forced to represent different values (corresponding to different\nknowledge), resulting in updates conflicts during editing. Addressing this\nissue, we propose MEMIT-Merge, an enhanced approach that merges value\ncomputation processes for facts sharing the same subject, effectively resolving\nthe performance degradation in same-subject batch editing scenarios.\nExperimental results demonstrate that when MEMIT's edit success rate drops to\naround 50% at larger batch sizes, MEMIT-Merge maintains a success rate\nexceeding 90%, showcasing remarkable robustness to subject entity collisions.\n","authors":["Zilu Dong","Xiangqing Shen","Rui Xia"],"pdf_url":"https://arxiv.org/pdf/2502.07322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11394v3","updated":"2025-02-11T07:34:37Z","published":"2024-07-16T05:26:14Z","title":"DreamCatalyst: Fast and High-Quality 3D Editing via Controlling\n  Editability and Identity Preservation","summary":"  Score distillation sampling (SDS) has emerged as an effective framework in\ntext-driven 3D editing tasks, leveraging diffusion models for 3D-consistent\nediting. However, existing SDS-based 3D editing methods suffer from long\ntraining times and produce low-quality results. We identify that the root cause\nof this performance degradation is \\textit{their conflict with the sampling\ndynamics of diffusion models}. Addressing this conflict allows us to treat SDS\nas a diffusion reverse process for 3D editing via sampling from data space. In\ncontrast, existing methods naively distill the score function using diffusion\nmodels. From these insights, we propose DreamCatalyst, a novel framework that\nconsiders these sampling dynamics in the SDS framework. Specifically, we devise\nthe optimization process of our DreamCatalyst to approximate the diffusion\nreverse process in editing tasks, thereby aligning with diffusion sampling\ndynamics. As a result, DreamCatalyst successfully reduces training time and\nimproves editing quality. Our method offers two modes: (1) a fast mode that\nedits Neural Radiance Fields (NeRF) scenes approximately 23 times faster than\ncurrent state-of-the-art NeRF editing methods, and (2) a high-quality mode that\nproduces superior results about 8 times faster than these methods. Notably, our\nhigh-quality mode outperforms current state-of-the-art NeRF editing methods in\nterms of both speed and quality. DreamCatalyst also surpasses the\nstate-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing\nitself as an effective and model-agnostic 3D editing solution. See more\nextensive results on our project page: https://dream-catalyst.github.io.\n","authors":["Jiwook Kim","Seonho Lee","Jaeyo Shin","Jiho Choi","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2407.11394v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2408.00662v2","updated":"2025-02-11T07:32:30Z","published":"2024-08-01T15:58:05Z","title":"Aligning Multiple Knowledge Graphs in a Single Pass","summary":"  Entity alignment (EA) is to identify equivalent entities across different\nknowledge graphs (KGs), which can help fuse these KGs into a more comprehensive\none. Previous EA methods mainly focus on aligning a pair of KGs, and to the\nbest of our knowledge, no existing EA method considers aligning multiple (more\nthan two) KGs. To fill this research gap, in this work, we study a novel\nproblem of aligning multiple KGs and propose an effective framework named\nMultiEA to solve the problem. First, we embed the entities of all the candidate\nKGs into a common feature space by a shared KG encoder. Then, we explore three\nalignment strategies to minimize the distances among pre-aligned entities. In\nparticular, we propose an innovative inference enhancement technique to improve\nthe alignment performance by incorporating high-order similarities. Finally, to\nverify the effectiveness of MultiEA, we construct two new real-world benchmark\ndatasets and conduct extensive experiments on them. The results show that our\nMultiEA can effectively and efficiently align multiple KGs in a single pass. We\nrelease the source codes of MultiEA at: https://github.com/kepsail/MultiEA.\n","authors":["Yaming Yang","Zhe Wang","Ziyu Guan","Wei Zhao","Weigang Lu","Xinyan Huang","Jiangtao Cui","Xiaofei He"],"pdf_url":"https://arxiv.org/pdf/2408.00662v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01523v3","updated":"2025-02-11T07:31:03Z","published":"2024-12-02T14:16:03Z","title":"FlexSP: Accelerating Large Language Model Training via Flexible Sequence\n  Parallelism","summary":"  Extending the context length (i.e., the maximum supported sequence length) of\nLLMs is of paramount significance. To facilitate long context training of LLMs,\nsequence parallelism has emerged as an essential technique, which scatters each\ninput sequence across multiple devices and necessitates communication to\nprocess the sequence. In essence, existing sequence parallelism methods assume\nhomogeneous sequence lengths (i.e., all input sequences are equal in length)\nand therefore leverages a single, static scattering strategy for all input\nsequences. However, in reality, the sequence lengths in LLM training corpora\nexhibit substantial variability, often following a long-tail distribution,\nwhich leads to workload heterogeneity.\n  In this paper, we show that employing a single, static strategy results in\ninefficiency and resource under-utilization, highlighting the need for adaptive\napproaches to handle the heterogeneous workloads across sequences. To address\nthis, we propose a heterogeneity-adaptive sequence parallelism method. For each\ntraining step, our approach captures the variability in sequence lengths and\nassigns the optimal combination of scattering strategies based on workload\ncharacteristics. We model this problem as a linear programming optimization and\ndesign an efficient and effective solver to find the optimal solution.\nFurthermore, we implement our method in a high-performance system that supports\nadaptive parallelization in distributed LLM training. Experimental results\ndemonstrate that our system outperforms state-of-the-art training frameworks by\nup to 1.98x.\n","authors":["Yujie Wang","Shiju Wang","Shenhan Zhu","Fangcheng Fu","Xinyi Liu","Xuefeng Xiao","Huixia Li","Jiashi Li","Faming Wu","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2412.01523v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07319v1","updated":"2025-02-11T07:29:32Z","published":"2025-02-11T07:29:32Z","title":"Learnable Residual-based Latent Denoising in Semantic Communication","summary":"  A latent denoising semantic communication (SemCom) framework is proposed for\nrobust image transmission over noisy channels. By incorporating a learnable\nlatent denoiser into the receiver, the received signals are preprocessed to\neffectively remove the channel noise and recover the semantic information,\nthereby enhancing the quality of the decoded images. Specifically, a latent\ndenoising mapping is established by an iterative residual learning approach to\nimprove the denoising efficiency while ensuring stable performance. Moreover,\nchannel signal-to-noise ratio (SNR) is utilized to estimate and predict the\nlatent similarity score (SS) for conditional denoising, where the number of\ndenoising steps is adapted based on the predicted SS sequence, further reducing\nthe communication latency. Finally, simulations demonstrate that the proposed\nframework can effectively and efficiently remove the channel noise at various\nlevels and reconstruct visual-appealing images.\n","authors":["Mingkai Xu","Yongpeng Wu","Yuxuan Shi","Xiang-Gen Xia","Wenjun Zhang","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07319v1.pdf","comment":"This paper has been accepted by IEEE Wireless Communications Letters"},{"id":"http://arxiv.org/abs/2405.20759v3","updated":"2025-02-11T07:27:41Z","published":"2024-05-31T12:20:02Z","title":"Information Theoretic Text-to-Image Alignment","summary":"  Diffusion models for Text-to-Image (T2I) conditional generation have recently\nachieved tremendous success. Yet, aligning these models with user's intentions\nstill involves a laborious trial-and-error process, and this challenging\nalignment problem has attracted considerable attention from the research\ncommunity. In this work, instead of relying on fine-grained linguistic analyses\nof prompts, human annotation, or auxiliary vision-language models, we use\nMutual Information (MI) to guide model alignment. In brief, our method uses\nself-supervised fine-tuning and relies on a point-wise (MI) estimation between\nprompts and images to create a synthetic fine-tuning set for improving model\nalignment. Our analysis indicates that our method is superior to the\nstate-of-the-art, yet it only requires the pre-trained denoising network of the\nT2I model itself to estimate MI, and a simple fine-tuning strategy that\nimproves alignment while maintaining image quality. Code available at\nhttps://github.com/Chao0511/mitune.\n","authors":["Chao Wang","Giulio Franzese","Alessandro Finamore","Massimo Gallo","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2405.20759v3.pdf","comment":"to appear at ICLR25"},{"id":"http://arxiv.org/abs/2403.10348v3","updated":"2025-02-11T07:25:00Z","published":"2024-03-15T14:34:34Z","title":"Denoising Task Difficulty-based Curriculum for Training Diffusion Models","summary":"  Diffusion-based generative models have emerged as powerful tools in the realm\nof generative modeling. Despite extensive research on denoising across various\ntimesteps and noise levels, a conflict persists regarding the relative\ndifficulties of the denoising tasks. While various studies argue that lower\ntimesteps present more challenging tasks, others contend that higher timesteps\nare more difficult. To address this conflict, our study undertakes a\ncomprehensive examination of task difficulties, focusing on convergence\nbehavior and changes in relative entropy between consecutive probability\ndistributions across timesteps. Our observational study reveals that denoising\nat earlier timesteps poses challenges characterized by slower convergence and\nhigher relative entropy, indicating increased task difficulty at these lower\ntimesteps. Building on these observations, we introduce an easy-to-hard\nlearning scheme, drawing from curriculum learning, to enhance the training\nprocess of diffusion models. By organizing timesteps or noise levels into\nclusters and training models with ascending orders of difficulty, we facilitate\nan order-aware training regime, progressing from easier to harder denoising\ntasks, thereby deviating from the conventional approach of training diffusion\nmodels simultaneously across all timesteps. Our approach leads to improved\nperformance and faster convergence by leveraging benefits of curriculum\nlearning, while maintaining orthogonality with existing improvements in\ndiffusion training techniques. We validate these advantages through\ncomprehensive experiments in image generation tasks, including unconditional,\nclass-conditional, and text-to-image generation.\n","authors":["Jin-Young Kim","Hyojun Go","Soonwoo Kwon","Hyun-Gyoon Kim"],"pdf_url":"https://arxiv.org/pdf/2403.10348v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07312v1","updated":"2025-02-11T07:20:38Z","published":"2025-02-11T07:20:38Z","title":"OpenGrok: Enhancing SNS Data Processing with Distilled Knowledge and\n  Mask-like Mechanisms","summary":"  This report details Lumen Labs' novel approach to processing Social\nNetworking Service (SNS) data. We leverage knowledge distillation, specifically\na simple distillation method inspired by DeepSeek-R1's CoT acquisition,\ncombined with prompt hacking, to extract valuable training data from the Grok\nmodel. This data is then used to fine-tune a Phi-3-mini model, augmented with a\nmask-like mechanism specifically designed for handling the nuances of SNS data.\nOur method demonstrates state-of-the-art (SOTA) performance on several SNS data\nprocessing tasks, outperforming existing models like Grok, Phi-3, and GPT-4. We\nprovide a comprehensive analysis of our approach, including mathematical\nformulations, engineering details, ablation studies, and comparative\nevaluations.\n","authors":["Lumen AI","Zaozhuang No. 28 Middle School","Shihao Ji","Zihui Song","Fucheng Zhong","Jisen Jia","Zhaobo Wu","Zheyi Cao","Tianhao Xu"],"pdf_url":"https://arxiv.org/pdf/2502.07312v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2410.11271v2","updated":"2025-02-11T07:18:41Z","published":"2024-10-15T04:51:37Z","title":"Tackling Dimensional Collapse toward Comprehensive Universal Domain\n  Adaptation","summary":"  Universal Domain Adaptation (UniDA) addresses unsupervised domain adaptation\nwhere target classes may differ arbitrarily from source ones, except for a\nshared subset. An important approach, partial domain matching (PDM), aligns\nonly shared classes but struggles in extreme cases where many source classes\nare absent in the target domain, underperforming the most naive baseline that\ntrains on only source data. In this work, we identify that the failure of PDM\nfor extreme UniDA stems from dimensional collapse (DC) in target\nrepresentations. To address target DC, we propose to jointly leverage the\nalignment and uniformity techniques in modern self-supervised learning (SSL) on\nthe unlabeled target data to preserve the intrinsic structure of the learned\nrepresentations. Our experimental results confirm that SSL consistently\nadvances PDM and delivers new state-of-the-art results across a broader\nbenchmark of UniDA scenarios with different portions of shared classes,\nrepresenting a crucial step toward truly comprehensive UniDA.\n","authors":["Hung-Chieh Fang","Po-Yi Lu","Hsuan-Tien Lin"],"pdf_url":"https://arxiv.org/pdf/2410.11271v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07306v1","updated":"2025-02-11T07:09:37Z","published":"2025-02-11T07:09:37Z","title":"TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language\n  Navigation","summary":"  In this work, we propose a modular approach for the Vision-Language\nNavigation (VLN) task by decomposing the problem into four sub-modules that use\nstate-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs)\nin a zero-shot setting. Given navigation instruction in natural language, we\nfirst prompt LLM to extract the landmarks and the order in which they are\nvisited. Assuming the known model of the environment, we retrieve the top-k\nlocations of the last landmark and generate $k$ path hypotheses from the\nstarting location to the last landmark using the shortest path algorithm on the\ntopological map of the environment. Each path hypothesis is represented by a\nsequence of panoramas. We then use dynamic programming to compute the alignment\nscore between the sequence of panoramas and the sequence of landmark names,\nwhich match scores obtained from VLM. Finally, we compute the nDTW metric\nbetween the hypothesis that yields the highest alignment score to evaluate the\npath fidelity. We demonstrate superior performance compared to other approaches\nthat use joint semantic maps like VLMaps \\cite{vlmaps} on the complex\nR2R-Habitat \\cite{r2r} instruction dataset and quantify in detail the effect of\nvisual grounding on navigation performance.\n","authors":["Navid Rajabi","Jana Kosecka"],"pdf_url":"https://arxiv.org/pdf/2502.07306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06351v2","updated":"2025-02-11T07:06:40Z","published":"2025-02-10T11:00:24Z","title":"Calibrating LLMs with Information-Theoretic Evidential Deep Learning","summary":"  Fine-tuned large language models (LLMs) often exhibit overconfidence,\nparticularly when trained on small datasets, resulting in poor calibration and\ninaccurate uncertainty estimates. Evidential Deep Learning (EDL), an\nuncertainty-aware approach, enables uncertainty estimation in a single forward\npass, making it a promising method for calibrating fine-tuned LLMs. However,\ndespite its computational efficiency, EDL is prone to overfitting, as its\ntraining objective can result in overly concentrated probability distributions.\nTo mitigate this, we propose regularizing EDL by incorporating an information\nbottleneck (IB). Our approach IB-EDL suppresses spurious information in the\nevidence generated by the model and encourages truly predictive information to\ninfluence both the predictions and uncertainty estimates. Extensive experiments\nacross various fine-tuned LLMs and tasks demonstrate that IB-EDL outperforms\nboth existing EDL and non-EDL approaches. By improving the trustworthiness of\nLLMs, IB-EDL facilitates their broader adoption in domains requiring high\nlevels of confidence calibration. Code is available at\nhttps://github.com/sandylaker/ib-edl.\n","authors":["Yawei Li","David R√ºgamer","Bernd Bischl","Mina Rezaei"],"pdf_url":"https://arxiv.org/pdf/2502.06351v2.pdf","comment":"27 pages; 3 figures; accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.05407v2","updated":"2025-02-11T06:57:41Z","published":"2025-02-08T01:54:23Z","title":"The Complexity of Learning Sparse Superposed Features with Feedback","summary":"  The success of deep networks is crucially attributed to their ability to\ncapture latent features within a representation space. In this work, we\ninvestigate whether the underlying learned features of a model can be\nefficiently retrieved through feedback from an agent, such as a large language\nmodel (LLM), in the form of relative \\textit{triplet comparisons}. These\nfeatures may represent various constructs, including dictionaries in LLMs or\ncomponents of a covariance matrix of Mahalanobis distances. We analyze the\nfeedback complexity associated with learning a feature matrix in sparse\nsettings. Our results establish tight bounds when the agent is permitted to\nconstruct activations and demonstrate strong upper bounds in sparse scenarios\nwhen the agent's feedback is limited to distributional information. We validate\nour theoretical findings through experiments on two distinct applications:\nfeature recovery from Recursive Feature Machine-trained models and dictionary\nextraction from sparse autoencoders trained on Large Language Models.\n","authors":["Akash Kumar"],"pdf_url":"https://arxiv.org/pdf/2502.05407v2.pdf","comment":"41 pages, 20 figures"},{"id":"http://arxiv.org/abs/2502.07299v1","updated":"2025-02-11T06:53:59Z","published":"2025-02-11T06:53:59Z","title":"Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification","summary":"  The interactions between DNA, RNA, and proteins are fundamental to biological\nprocesses, as illustrated by the central dogma of molecular biology. While\nmodern biological pre-trained models have achieved great success in analyzing\nthese macromolecules individually, their interconnected nature remains\nunder-explored. In this paper, we follow the guidance of the central dogma to\nredesign both the data and model pipeline and offer a comprehensive framework,\nLife-Code, that spans different biological functions. As for data flow, we\npropose a unified pipeline to integrate multi-omics data by\nreverse-transcribing RNA and reverse-translating amino acids into\nnucleotide-based sequences. As for the model, we design a codon tokenizer and a\nhybrid long-sequence architecture to encode the interactions of both coding and\nnon-coding regions with masked modeling pre-training. To model the translation\nand folding process with coding sequences, Life-Code learns protein structures\nof the corresponding amino acids by knowledge distillation from off-the-shelf\nprotein language models. Such designs enable Life-Code to capture complex\ninteractions within genetic sequences, providing a more comprehensive\nunderstanding of multi-omics with the central dogma. Extensive Experiments show\nthat Life-Code achieves state-of-the-art performance on various tasks across\nthree omics, highlighting its potential for advancing multi-omics analysis and\ninterpretation.\n","authors":["Zicheng Liu","Siyuan Li","Zhiyuan Chen","Lei Xin","Fang Wu","Chang Yu","Qirong Yang","Yucheng Guo","Yujie Yang","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2502.07299v1.pdf","comment":"12 pages main text with 6 pages Appendix"},{"id":"http://arxiv.org/abs/2502.07297v1","updated":"2025-02-11T06:50:33Z","published":"2025-02-11T06:50:33Z","title":"Generation of Drug-Induced Cardiac Reactions towards Virtual Clinical\n  Trials","summary":"  Clinical trials are pivotal in cardiac drug development, yet they often fail\ndue to inadequate efficacy and unexpected safety issues, leading to significant\nfinancial losses. Using in-silico trials to replace a part of physical clinical\ntrials, e.g., leveraging advanced generative models to generate drug-influenced\nelectrocardiograms (ECGs), seems an effective method to reduce financial risk\nand potential harm to trial participants. While existing generative models have\ndemonstrated progress in ECG generation, they fall short in modeling drug\nreactions due to limited fidelity and inability to capture individualized drug\nresponse patterns. In this paper, we propose a Drug-Aware Diffusion Model\n(DADM), which could simulate individualized drug reactions while ensuring\nfidelity. To ensure fidelity, we construct a set of ordinary differential\nequations to provide external physical knowledge (EPK) of the realistic ECG\nmorphology. The EPK is used to adaptively constrain the morphology of the\ngenerated ECGs through a dynamic cross-attention (DCA) mechanism. Furthermore,\nwe propose an extension of ControlNet to incorporate demographic and drug data,\nsimulating individual drug reactions. We compare DADM with the other eight\nstate-of-the-art ECG generative models on two real-world databases covering 8\ntypes of drug regimens. The results demonstrate that DADM can more accurately\nsimulate drug-induced changes in ECGs, improving the accuracy by at least 5.79%\nand recall by 8%.\n","authors":["Qian Shao","Bang Du","Zepeng Li","Qiyuan Chen","Hongxia Xu","Jimeng Sun","Jian Wu","Jintai Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07297v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2402.04620v5","updated":"2025-02-11T06:48:43Z","published":"2024-02-07T07:07:02Z","title":"CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients","summary":"  The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndeveloped CataractBot. CataractBot answers cataract surgery related questions\ninstantly using an LLM to query a curated knowledge base, and provides\nexpert-verified responses asynchronously. It has multimodal and multilingual\ncapabilities. In an in-the-wild deployment study with 49 patients and\nattendants, 4 doctors, and 2 patient coordinators, CataractBot demonstrated\npotential, providing anytime accessibility, saving time, accommodating diverse\nliteracy levels, alleviating power differences, and adding a privacy layer\nbetween patients and doctors. Users reported that their trust in the system was\nestablished through expert verification. Broadly, our results could inform\nfuture work on designing expert-mediated LLM bots.\n","authors":["Pragnya Ramjee","Bhuvan Sachdeva","Satvik Golechha","Shreyas Kulkarni","Geeta Fulari","Kaushik Murali","Mohit Jain"],"pdf_url":"https://arxiv.org/pdf/2402.04620v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07032v4","updated":"2025-02-11T06:47:56Z","published":"2025-01-13T03:07:39Z","title":"PRKAN: Parameter-Reduced Kolmogorov-Arnold Networks","summary":"  Kolmogorov-Arnold Networks (KANs) represent an innovation in neural network\narchitectures, offering a compelling alternative to Multi-Layer Perceptrons\n(MLPs) in models such as Convolutional Neural Networks (CNNs), Recurrent Neural\nNetworks (RNNs), and Transformers. By advancing network design, KANs drive\ngroundbreaking research and enable transformative applications across various\nscientific domains involving neural networks. However, existing KANs often\nrequire significantly more parameters in their network layers than MLPs. To\naddress this limitation, this paper introduces PRKANs (Parameter-Reduced\nKolmogorov-Arnold Networks), which employ several methods to reduce the\nparameter count in KAN layers, making them comparable to MLP layers.\nExperimental results on the MNIST and Fashion-MNIST datasets demonstrate that\nPRKANs outperform several existing KANs, and their variant with attention\nmechanisms rivals the performance of MLPs, albeit with slightly longer training\ntimes. Furthermore, the study highlights the advantages of Gaussian Radial\nBasis Functions (GRBFs) and layer normalization in KAN designs. The repository\nfor this work is available at: https://github.com/hoangthangta/All-KAN.\n","authors":["Hoang-Thang Ta","Duy-Quy Thai","Anh Tran","Grigori Sidorov","Alexander Gelbukh"],"pdf_url":"https://arxiv.org/pdf/2501.07032v4.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2410.14615v2","updated":"2025-02-11T06:41:22Z","published":"2024-10-18T17:13:29Z","title":"Asymptotically Optimal Change Detection for Unnormalized Pre- and\n  Post-Change Distributions","summary":"  This paper addresses the problem of detecting changes when only unnormalized\npre- and post-change distributions are accessible. This situation happens in\nmany scenarios in physics such as in ferromagnetism, crystallography,\nmagneto-hydrodynamics, and thermodynamics, where the energy models are\ndifficult to normalize.\n  Our approach is based on the estimation of the Cumulative Sum (CUSUM)\nstatistics, which is known to produce optimal performance. We first present an\nintuitively appealing approximation method. Unfortunately, this produces a\nbiased estimator of the CUSUM statistics and may cause performance degradation.\nWe then propose the Log-Partition Approximation Cumulative Sum (LPA-CUSUM)\nalgorithm based on thermodynamic integration (TI) in order to estimate the\nlog-ratio of normalizing constants of pre- and post-change distributions. It is\nproved that this approach gives an unbiased estimate of the log-partition\nfunction and the CUSUM statistics, and leads to an asymptotically optimal\nperformance. Moreover, we derive a relationship between the required sample\nsize for thermodynamic integration and the desired detection delay performance,\noffering guidelines for practical parameter selection. Numerical studies are\nprovided demonstrating the efficacy of our approach.\n","authors":["Arman Adibi","Sanjeev Kulkarni","H. Vincent Poor","Taposh Banerjee","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2410.14615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07295v1","updated":"2025-02-11T06:36:20Z","published":"2025-02-11T06:36:20Z","title":"Treatment Effect Estimation for Exponential Family Outcomes using Neural\n  Networks with Targeted Regularization","summary":"  Neural Networks (NNs) have became a natural choice for treatment effect\nestimation due to their strong approximation capabilities. Nevertheless, how to\ndesign NN-based estimators with desirable properties, such as low bias and\ndoubly robustness, still remains a significant challenge. A common approach to\naddress this is targeted regularization, which modifies the objective function\nof NNs. However, existing works on targeted regularization are limited to\nGaussian-distributed outcomes, significantly restricting their applicability in\nreal-world scenarios. In this work, we aim to bridge this blank by extending\nthis framework to the boarder exponential family outcomes. Specifically, we\nfirst derive the von-Mises expansion of the Average Dose function of Canonical\nFunctions (ADCF), which inspires us how to construct a doubly robust estimator\nwith good properties. Based on this, we develop a NN-based estimator for ADCF\nby generalizing functional targeted regularization to exponential families, and\nprovide the corresponding theoretical convergence rate. Extensive experimental\nresults demonstrate the effectiveness of our proposed model.\n","authors":["Jiahong Li","Zeqin Yang","Jiayi Dan","Jixing Xu","Zhichao Zou","Peng Zhen","Jiecheng Guo"],"pdf_url":"https://arxiv.org/pdf/2502.07295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07293v1","updated":"2025-02-11T06:34:31Z","published":"2025-02-11T06:34:31Z","title":"Global Universal Scaling and Ultra-Small Parameterization in Machine\n  Learning Interatomic Potentials with Super-Linearity","summary":"  Using machine learning (ML) to construct interatomic interactions and thus\npotential energy surface (PES) has become a common strategy for materials\ndesign and simulations. However, those current models of machine learning\ninteratomic potential (MLIP) provide no relevant physical constrains, and thus\nmay owe intrinsic out-of-domain difficulty which underlies the challenges of\nmodel generalizability and physical scalability. Here, by incorporating\nphysics-informed Universal-Scaling law and nonlinearity-embedded interaction\nfunction, we develop a Super-linear MLIP with both Ultra-Small parameterization\nand greatly expanded expressive capability, named SUS2-MLIP. Due to the global\nscaling rooting in universal equation of state (UEOS), SUS2-MLIP not only has\nsignificantly-reduced parameters by decoupling the element space from\ncoordinate space, but also naturally outcomes the out-of-domain difficulty and\nendows the potentials with inherent generalizability and scalability even with\nrelatively small training dataset. The nonlinearity-enbeding transformation for\ninteraction function expands the expressive capability and make the potentials\nsuper-linear. The SUS2-MLIP outperforms the state-of-the-art MLIP models with\nits exceptional computational efficiency especially for multiple-element\nmaterials and physical scalability in property prediction. This work not only\npresents a highly-efficient universal MLIP model but also sheds light on\nincorporating physical constraints into artificial-intelligence-aided materials\nsimulation.\n","authors":["Yanxiao Hu","Ye Sheng","Jing Huang","Xiaoxin Xu","Yuyan Yang","Mingqiang Zhang","Yabei Wu","Caichao Ye","Jiong Yang","Wenqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06136v2","updated":"2025-02-11T06:30:25Z","published":"2025-02-10T03:55:09Z","title":"Graph Neural Networks at a Fraction","summary":"  Graph Neural Networks (GNNs) have emerged as powerful tools for learning\nrepresentations of graph-structured data. In addition to real-valued GNNs,\nquaternion GNNs also perform well on tasks on graph-structured data. With the\naim of reducing the energy footprint, we reduce the model size while\nmaintaining accuracy comparable to that of the original-sized GNNs. This paper\nintroduces Quaternion Message Passing Neural Networks (QMPNNs), a framework\nthat leverages quaternion space to compute node representations. Our approach\noffers a generalizable method for incorporating quaternion representations into\nGNN architectures at one-fourth of the original parameter count. Furthermore,\nwe present a novel perspective on Graph Lottery Tickets, redefining their\napplicability within the context of GNNs and QMPNNs. We specifically aim to\nfind the initialization lottery from the subnetwork of the GNNs that can\nachieve comparable performance to the original GNN upon training. Thereby\nreducing the trainable model parameters even further. To validate the\neffectiveness of our proposed QMPNN framework and LTH for both GNNs and QMPNNs,\nwe evaluate their performance on real-world datasets across three fundamental\ngraph-based tasks: node classification, link prediction, and graph\nclassification.\n","authors":["Rucha Bhalchandra Joshi","Sagar Prakash Barad","Nidhi Tiwari","Subhankar Mishra"],"pdf_url":"https://arxiv.org/pdf/2502.06136v2.pdf","comment":"12 pages, 2 figures, accepted at PAKKD 2025"},{"id":"http://arxiv.org/abs/2305.05642v3","updated":"2025-02-11T06:24:23Z","published":"2023-05-09T17:41:50Z","title":"A duality framework for analyzing random feature and two-layer neural\n  networks","summary":"  We consider the problem of learning functions within the\n$\\mathcal{F}_{p,\\pi}$ and Barron spaces, which play crucial roles in\nunderstanding random feature models (RFMs), two-layer neural networks, as well\nas kernel methods. Leveraging tools from information-based complexity (IBC), we\nestablish a dual equivalence between approximation and estimation, and then\napply it to study the learning of the preceding function spaces. The duality\nallows us to focus on the more tractable problem between approximation and\nestimation. To showcase the efficacy of our duality framework, we delve into\ntwo important but under-explored problems:\n  1) Random feature learning beyond kernel regime: We derive sharp bounds for\nlearning $\\mathcal{F}_{p,\\pi}$ using RFMs. Notably, the learning is efficient\nwithout the curse of dimensionality for $p>1$. This underscores the extended\napplicability of RFMs beyond the traditional kernel regime, since\n$\\mathcal{F}_{p,\\pi}$ with $p<2$ is strictly larger than the corresponding\nreproducing kernel Hilbert space (RKHS) where $p=2$.\n  2) The $L^\\infty$ learning of RKHS: We establish sharp, spectrum-dependent\ncharacterizations for the convergence of $L^\\infty$ learning error in both\nnoiseless and noisy settings. Surprisingly, we show that popular kernel ridge\nregression can achieve near-optimal performance in $L^\\infty$ learning, despite\nit primarily minimizing square loss.\n  To establish the aforementioned duality, we introduce a type of IBC, termed\n$I$-complexity, to measure the size of a function class. Notably,\n$I$-complexity offers a tight characterization of learning in noiseless\nsettings, yields lower bounds comparable to Le Cam's in noisy settings, and is\nversatile in deriving upper bounds. We believe that our duality framework holds\npotential for broad application in learning analysis across more scenarios.\n","authors":["Hongrui Chen","Jihao Long","Lei Wu"],"pdf_url":"https://arxiv.org/pdf/2305.05642v3.pdf","comment":"Accepted for publication in Annals of Statistics"},{"id":"http://arxiv.org/abs/2402.08193v7","updated":"2025-02-11T06:18:45Z","published":"2024-02-13T03:31:36Z","title":"Gaussian Ensemble Belief Propagation for Efficient Inference in\n  High-Dimensional Systems","summary":"  Efficient inference in high-dimensional models is a central challenge in\nmachine learning. We introduce the Gaussian Ensemble Belief Propagation (GEnBP)\nalgorithm, which combines the strengths of the Ensemble Kalman Filter (EnKF)\nand Gaussian Belief Propagation (GaBP) to address this challenge. GEnBP updates\nensembles of prior samples into posterior samples by passing low-rank local\nmessages over the edges of a graphical model, enabling efficient handling of\nhigh-dimensional states, parameters, and complex, noisy, black-box generation\nprocesses. By utilizing local message passing within a graphical model\nstructure, GEnBP effectively manages complex dependency structures and remains\ncomputationally efficient even when the ensemble size is much smaller than the\ninference dimension -- a common scenario in spatiotemporal modeling, image\nprocessing, and physical model inversion. We demonstrate that GEnBP can be\napplied to various problem structures, including data assimilation, system\nidentification, and hierarchical models, and show through experiments that it\noutperforms existing belief propagation methods in terms of accuracy and\ncomputational efficiency.\n  Supporting code is available at https://github.com/danmackinlay/GEnBP\n","authors":["Dan MacKinlay","Russell Tsuchida","Dan Pagendam","Petra Kuhnert"],"pdf_url":"https://arxiv.org/pdf/2402.08193v7.pdf","comment":"Under conference submission"},{"id":"http://arxiv.org/abs/2410.07616v2","updated":"2025-02-11T06:14:22Z","published":"2024-10-10T05:08:14Z","title":"The Plug-in Approach for Average-Reward and Discounted MDPs: Optimal\n  Sample Complexity Analysis","summary":"  We study the sample complexity of the plug-in approach for learning\n$\\varepsilon$-optimal policies in average-reward Markov decision processes\n(MDPs) with a generative model. The plug-in approach constructs a model\nestimate then computes an average-reward optimal policy in the estimated model.\nDespite representing arguably the simplest algorithm for this problem, the\nplug-in approach has never been theoretically analyzed. Unlike the more\nwell-studied discounted MDP reduction method, the plug-in approach requires no\nprior problem information or parameter tuning. Our results fill this gap and\naddress the limitations of prior approaches, as we show that the plug-in\napproach is optimal in several well-studied settings without using prior\nknowledge. Specifically it achieves the optimal diameter- and mixing-based\nsample complexities of $\\widetilde{O}\\left(SA \\frac{D}{\\varepsilon^2}\\right)$\nand $\\widetilde{O}\\left(SA \\frac{\\tau_{\\mathrm{unif}}}{\\varepsilon^2}\\right)$,\nrespectively, without knowledge of the diameter $D$ or uniform mixing time\n$\\tau_{\\mathrm{unif}}$. We also obtain span-based bounds for the plug-in\napproach, and complement them with algorithm-specific lower bounds suggesting\nthat they are unimprovable. Our results require novel techniques for analyzing\nlong-horizon problems which may be broadly useful and which also improve\nresults for the discounted plug-in approach, removing effective-horizon-related\nsample size restrictions and obtaining the first optimal complexity bounds for\nthe full range of sample sizes without reward perturbation.\n","authors":["Matthew Zurek","Yudong Chen"],"pdf_url":"https://arxiv.org/pdf/2410.07616v2.pdf","comment":"Accepted to 36th International Conference on Algorithmic Learning\n  Theory (ALT 2025)"},{"id":"http://arxiv.org/abs/2502.07285v1","updated":"2025-02-11T06:04:49Z","published":"2025-02-11T06:04:49Z","title":"Negative Dependence as a toolbox for machine learning : review and new\n  developments","summary":"  Negative dependence is becoming a key driver in advancing learning\ncapabilities beyond the limits of traditional independence. Recent developments\nhave evidenced support towards negatively dependent systems as a learning\nparadigm in a broad range of fundamental machine learning challenges including\noptimization, sampling, dimensionality reduction and sparse signal recovery,\noften surpassing the performance of current methods based on statistical\nindependence. The most popular negatively dependent model has been that of\ndeterminantal point processes (DPPs), which have their origins in quantum\ntheory. However, other models, such as perturbed lattice models, strongly\nRayleigh measures, zeros of random functions have gained salience in various\nlearning applications. In this article, we review this burgeoning field of\nresearch, as it has developed over the past two decades or so. We also present\nnew results on applications of DPPs to the parsimonious representation of\nneural networks. In the limited scope of the article, we mostly focus on\naspects of this area to which the authors contributed over the recent years,\nincluding applications to Monte Carlo methods, coresets and stochastic gradient\ndescent, stochastic networks, signal processing and connections to quantum\ncomputation. However, starting from basics of negative dependence for the\nuninitiated reader, extensive references are provided to a broad swath of\nrelated developments which could not be covered within our limited scope. While\nexisting works and reviews generally focus on specific negatively dependent\nmodels (e.g. DPPs), a notable feature of this article is that it addresses\nnegative dependence as a machine learning methodology as a whole. In this vein,\nit covers within its span an array of negatively dependent models and their\napplications well beyond DPPs, thereby putting forward a very general and\nrather unique perspective.\n","authors":["Hoang-Son Tran","Vladimir Petrovic","Remi Bardenet","Subhroshekhar Ghosh"],"pdf_url":"https://arxiv.org/pdf/2502.07285v1.pdf","comment":"Dedicated to the memory of Prof K.R. Parthasarathy: visionary, guru,\n  and scientist par excellence"},{"id":"http://arxiv.org/abs/2501.00135v3","updated":"2025-02-11T06:01:24Z","published":"2024-12-30T20:23:10Z","title":"GroverGPT: A Large Language Model with 8 Billion Parameters for Quantum\n  Searching","summary":"  Quantum computing is an exciting non-Von Neumann paradigm, offering provable\nspeedups over classical computing for specific problems. However, the practical\nlimits of classical simulatability for quantum circuits remain unclear,\nespecially with current noisy quantum devices. In this work, we explore the\npotential of leveraging Large Language Models (LLMs) to simulate the output of\na quantum Turing machine using Grover's quantum circuits, known to provide\nquadratic speedups over classical counterparts. To this end, we developed\nGroverGPT, a specialized model based on LLaMA's 8-billion-parameter\narchitecture, trained on over 15 trillion tokens. Unlike brute-force\nstate-vector simulations, which demand substantial computational resources,\nGroverGPT employs pattern recognition to approximate quantum search algorithms\nwithout explicitly representing quantum states. Analyzing 97K quantum search\ninstances, GroverGPT consistently outperformed OpenAI's GPT-4o (45\\% accuracy),\nachieving nearly 100\\% accuracy on 6- and 10-qubit datasets when trained on\n4-qubit or larger datasets. It also demonstrated strong generalization,\nsurpassing 95\\% accuracy for systems with over 20 qubits when trained on 3- to\n6-qubit data. Analysis indicates GroverGPT captures quantum features of\nGrover's search rather than classical patterns, supported by novel prompting\nstrategies to enhance performance. Although accuracy declines with increasing\nsystem size, these findings offer insights into the practical boundaries of\nclassical simulatability. This work suggests task-specific LLMs can surpass\ngeneral-purpose models like GPT-4o in quantum algorithm learning and serve as\npowerful tools for advancing quantum research.\n","authors":["Haoran Wang","Pingzhi Li","Min Chen","Jinglei Cheng","Junyu Liu","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2501.00135v3.pdf","comment":"12 pages including appendices. v2, v3: Add more experiments include\n  ablation tests. Fix the terminology about infidelity. Add more benchmarks\n  including Llama-3.2-3B and DeepSeek-v2-Lite"},{"id":"http://arxiv.org/abs/2502.07281v1","updated":"2025-02-11T05:55:27Z","published":"2025-02-11T05:55:27Z","title":"Supervised Contrastive Block Disentanglement","summary":"  Real-world datasets often combine data collected under different experimental\nconditions. This yields larger datasets, but also introduces spurious\ncorrelations that make it difficult to model the phenomena of interest. We\naddress this by learning two embeddings to independently represent the\nphenomena of interest and the spurious correlations. The embedding representing\nthe phenomena of interest is correlated with the target variable $y$, and is\ninvariant to the environment variable $e$. In contrast, the embedding\nrepresenting the spurious correlations is correlated with $e$. The invariance\nto $e$ is difficult to achieve on real-world datasets. Our primary contribution\nis an algorithm called Supervised Contrastive Block Disentanglement (SCBD) that\neffectively enforces this invariance. It is based purely on Supervised\nContrastive Learning, and applies to real-world data better than existing\napproaches. We empirically validate SCBD on two challenging problems. The first\nproblem is domain generalization, where we achieve strong performance on a\nsynthetic dataset, as well as on Camelyon17-WILDS. We introduce a single\nhyperparameter $\\alpha$ to control the degree of invariance to $e$. When we\nincrease $\\alpha$ to strengthen the degree of invariance, out-of-distribution\nperformance improves at the expense of in-distribution performance. The second\nproblem is batch correction, in which we apply SCBD to preserve biological\nsignal and remove inter-well batch effects when modeling single-cell\nperturbations from 26 million Optical Pooled Screening images.\n","authors":["Taro Makino","Ji Won Park","Natasa Tagasovska","Takamasa Kudo","Paula Coelho","Jan-Christian Huetter","Heming Yao","Burkhard Hoeckendorf","Ana Carolina Leote","Stephen Ra","David Richmond","Kyunghyun Cho","Aviv Regev","Romain Lopez"],"pdf_url":"https://arxiv.org/pdf/2502.07281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07280v1","updated":"2025-02-11T05:54:42Z","published":"2025-02-11T05:54:42Z","title":"MIGT: Memory Instance Gated Transformer Framework for Financial\n  Portfolio Management","summary":"  Deep reinforcement learning (DRL) has been applied in financial portfolio\nmanagement to improve returns in changing market conditions. However, unlike\nmost fields where DRL is widely used, the stock market is more volatile and\ndynamic as it is affected by several factors such as global events and investor\nsentiment. Therefore, it remains a challenge to construct a DRL-based portfolio\nmanagement framework with strong return capability, stable training, and\ngeneralization ability. This study introduces a new framework utilizing the\nMemory Instance Gated Transformer (MIGT) for effective portfolio management. By\nincorporating a novel Gated Instance Attention module, which combines a\ntransformer variant, instance normalization, and a Lite Gate Unit, our approach\naims to maximize investment returns while ensuring the learning process's\nstability and reducing outlier impacts. Tested on the Dow Jones Industrial\nAverage 30, our framework's performance is evaluated against fifteen other\nstrategies using key financial metrics like the cumulative return and\nrisk-return ratios (Sharpe, Sortino, and Omega ratios). The results highlight\nMIGT's advantage, showcasing at least a 9.75% improvement in cumulative returns\nand a minimum 2.36% increase in risk-return ratios over competing strategies,\nmarking a significant advancement in DRL for portfolio management.\n","authors":["Fengchen Gu","Angelos Stefanidis","√Ångel Garc√≠a-Fern√°ndez","Jionglong Su","Huakang Li"],"pdf_url":"https://arxiv.org/pdf/2502.07280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17323v3","updated":"2025-02-11T05:49:47Z","published":"2024-12-23T06:32:59Z","title":"xPatch: Dual-Stream Time Series Forecasting with Exponential\n  Seasonal-Trend Decomposition","summary":"  In recent years, the application of transformer-based models in time-series\nforecasting has received significant attention. While often demonstrating\npromising results, the transformer architecture encounters challenges in fully\nexploiting the temporal relations within time series data due to its attention\nmechanism. In this work, we design eXponential Patch (xPatch for short), a\nnovel dual-stream architecture that utilizes exponential decomposition.\nInspired by the classical exponential smoothing approaches, xPatch introduces\nthe innovative seasonal-trend exponential decomposition module. Additionally,\nwe propose a dual-flow architecture that consists of an MLP-based linear stream\nand a CNN-based non-linear stream. This model investigates the benefits of\nemploying patching and channel-independence techniques within a non-transformer\nmodel. Finally, we develop a robust arctangent loss function and a sigmoid\nlearning rate adjustment scheme, which prevent overfitting and boost\nforecasting performance. The code is available at the following repository:\nhttps://github.com/stitsyuk/xPatch.\n","authors":["Artyom Stitsyuk","Jaesik Choi"],"pdf_url":"https://arxiv.org/pdf/2412.17323v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07279v1","updated":"2025-02-11T05:48:51Z","published":"2025-02-11T05:48:51Z","title":"Exploratory Diffusion Policy for Unsupervised Reinforcement Learning","summary":"  Unsupervised reinforcement learning (RL) aims to pre-train agents by\nexploring states or skills in reward-free environments, facilitating the\nadaptation to downstream tasks. However, existing methods often overlook the\nfitting ability of pre-trained policies and struggle to handle the\nheterogeneous pre-training data, which are crucial for achieving efficient\nexploration and fast fine-tuning. To address this gap, we propose Exploratory\nDiffusion Policy (EDP), which leverages the strong expressive ability of\ndiffusion models to fit the explored data, both boosting exploration and\nobtaining an efficient initialization for downstream tasks. Specifically, we\nestimate the distribution of collected data in the replay buffer with the\ndiffusion policy and propose a score intrinsic reward, encouraging the agent to\nexplore unseen states. For fine-tuning the pre-trained diffusion policy on\ndownstream tasks, we provide both theoretical analyses and practical\nalgorithms, including an alternating method of Q function optimization and\ndiffusion policy distillation. Extensive experiments demonstrate the\neffectiveness of EDP in efficient exploration during pre-training and fast\nadaptation during fine-tuning.\n","authors":["Chengyang Ying","Huayu Chen","Xinning Zhou","Zhongkai Hao","Hang Su","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.07279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02583v2","updated":"2025-02-11T05:48:49Z","published":"2024-04-03T09:08:15Z","title":"Transformer-based Stagewise Decomposition for Large-Scale Multistage\n  Stochastic Optimization","summary":"  Solving large-scale multistage stochastic programming (MSP) problems poses a\nsignificant challenge as commonly used stagewise decomposition algorithms,\nincluding stochastic dual dynamic programming (SDDP), face growing time\ncomplexity as the subproblem size and problem count increase. Traditional\napproaches approximate the value functions as piecewise linear convex functions\nby incrementally accumulating subgradient cutting planes from the primal and\ndual solutions of stagewise subproblems. Recognizing these limitations, we\nintroduce TranSDDP, a novel Transformer-based stagewise decomposition\nalgorithm. This innovative approach leverages the structural advantages of the\nTransformer model, implementing a sequential method for integrating subgradient\ncutting planes to approximate the value function. Through our numerical\nexperiments, we affirm TranSDDP's effectiveness in addressing MSP problems. It\nefficiently generates a piecewise linear approximation for the value function,\nsignificantly reducing computation time while preserving solution quality, thus\nmarking a promising progression in the treatment of large-scale multistage\nstochastic programming problems.\n","authors":["Chanyeong Kim","Jongwoong Park","Hyunglip Bae","Woo Chang Kim"],"pdf_url":"https://arxiv.org/pdf/2404.02583v2.pdf","comment":"Accepted at ICML 2023 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2502.07276v1","updated":"2025-02-11T05:42:21Z","published":"2025-02-11T05:42:21Z","title":"Dataset Ownership Verification in Contrastive Pre-trained Models","summary":"  High-quality open-source datasets, which necessitate substantial efforts for\ncuration, has become the primary catalyst for the swift progress of deep\nlearning. Concurrently, protecting these datasets is paramount for the\nwell-being of the data owner. Dataset ownership verification emerges as a\ncrucial method in this domain, but existing approaches are often limited to\nsupervised models and cannot be directly extended to increasingly popular\nunsupervised pre-trained models. In this work, we propose the first dataset\nownership verification method tailored specifically for self-supervised\npre-trained models by contrastive learning. Its primary objective is to\nascertain whether a suspicious black-box backbone has been pre-trained on a\nspecific unlabeled dataset, aiding dataset owners in upholding their rights.\nThe proposed approach is motivated by our empirical insights that when models\nare trained with the target dataset, the unary and binary instance\nrelationships within the embedding space exhibit significant variations\ncompared to models trained without the target dataset. We validate the efficacy\nof this approach across multiple contrastive pre-trained models including\nSimCLR, BYOL, SimSiam, MOCO v3, and DINO. The results demonstrate that our\nmethod rejects the null hypothesis with a $p$-value markedly below $0.05$,\nsurpassing all previous methodologies. Our code is available at\nhttps://github.com/xieyc99/DOV4CL.\n","authors":["Yuechen Xie","Jie Song","Mengqi Xue","Haofei Zhang","Xingen Wang","Bingde Hu","Genlang Chen","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2502.07276v1.pdf","comment":"Accepted by ICLR2025"},{"id":"http://arxiv.org/abs/2502.07274v1","updated":"2025-02-11T05:40:52Z","published":"2025-02-11T05:40:52Z","title":"Cost-Efficient Continual Learning with Sufficient Exemplar Memory","summary":"  Continual learning (CL) research typically assumes highly constrained\nexemplar memory resources. However, in many real-world scenarios-especially in\nthe era of large foundation models-memory is abundant, while GPU computational\ncosts are the primary bottleneck. In this work, we investigate CL in a novel\nsetting where exemplar memory is ample (i.e., sufficient exemplar memory).\nUnlike prior methods designed for strict exemplar memory constraints, we\npropose a simple yet effective approach that directly operates in the model's\nweight space through a combination of weight resetting and averaging\ntechniques. Our method achieves state-of-the-art performance while reducing the\ncomputational cost to a quarter or third of existing methods. These findings\nchallenge conventional CL assumptions and provide a practical baseline for\ncomputationally efficient CL applications.\n","authors":["Dongkyu Cho","Taesup Moon","Rumi Chunara","Kyunghyun Cho","Sungmin Cha"],"pdf_url":"https://arxiv.org/pdf/2502.07274v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.07273v1","updated":"2025-02-11T05:40:42Z","published":"2025-02-11T05:40:42Z","title":"Variational Learning Induces Adaptive Label Smoothing","summary":"  We show that variational learning naturally induces an adaptive label\nsmoothing where label noise is specialized for each example. Such\nlabel-smoothing is useful to handle examples with labeling errors and\ndistribution shifts, but designing a good adaptivity strategy is not always\neasy. We propose to skip this step and simply use the natural adaptivity\ninduced during the optimization of a variational objective. We show empirical\nresults where a variational algorithm called IVON outperforms traditional label\nsmoothing and yields adaptivity strategies similar to those of an existing\napproach. By connecting Bayesian methods to label smoothing, our work provides\na new way to handle overconfident predictions.\n","authors":["Sin-Han Yang","Zhedong Liu","Gian Maria Marconi","Mohammad Emtiyaz Khan"],"pdf_url":"https://arxiv.org/pdf/2502.07273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07842v1","updated":"2025-02-11T05:32:14Z","published":"2025-02-11T05:32:14Z","title":"Column-wise Quantization of Weights and Partial Sums for Accurate and\n  Efficient Compute-In-Memory Accelerators","summary":"  Compute-in-memory (CIM) is an efficient method for implementing deep neural\nnetworks (DNNs) but suffers from substantial overhead from analog-to-digital\nconverters (ADCs), especially as ADC precision increases. Low-precision ADCs\ncan re- duce this overhead but introduce partial-sum quantization errors\ndegrading accuracy. Additionally, low-bit weight constraints, im- posed by cell\nlimitations and the need for multiple cells for higher- bit weights, present\nfurther challenges. While fine-grained partial- sum quantization has been\nstudied to lower ADC resolution effectively, weight granularity, which limits\noverall partial-sum quantized accuracy, remains underexplored. This work\naddresses these challenges by aligning weight and partial-sum quantization\ngranularities at the column-wise level. Our method improves accuracy while\nmaintaining dequantization overhead, simplifies training by removing two-stage\nprocesses, and ensures robustness to memory cell variations via independent\ncolumn-wise scale factors. We also propose an open-source CIM-oriented\nconvolution framework to handle fine-grained weights and partial-sums effi-\nciently, incorporating a novel tiling method and group convolution.\nExperimental results on ResNet-20 (CIFAR-10, CIFAR-100) and ResNet-18\n(ImageNet) show accuracy improvements of 0.99%, 2.69%, and 1.01%, respectively,\ncompared to the best-performing related works. Additionally, variation analysis\nreveals the robust- ness of our method against memory cell variations. These\nfindings highlight the effectiveness of our quantization scheme in enhancing\naccuracy and robustness while maintaining hardware efficiency in CIM-based DNN\nimplementations. Our code is available at\nhttps://github.com/jiyoonkm/ColumnQuant.\n","authors":["Jiyoon Kim","Kang Eun Jeon","Yulhwa Kim","Jong Hwan Ko"],"pdf_url":"https://arxiv.org/pdf/2502.07842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07266v1","updated":"2025-02-11T05:28:59Z","published":"2025-02-11T05:28:59Z","title":"When More is Less: Understanding Chain-of-Thought Length in LLMs","summary":"  Chain-of-thought (CoT) reasoning enhances the multi-step reasoning\ncapabilities of large language models (LLMs) by breaking complex tasks into\nsmaller, manageable sub-tasks. Researchers have been exploring ways to guide\nmodels to generate more complex CoT processes to improve the reasoning ability\nof LLMs, such as long CoT and the test-time scaling law. However, for most\nmodels and tasks, does an increase in CoT length consistently lead to improved\nreasoning accuracy? In this paper, we observe a nuanced relationship: as the\nnumber of reasoning steps increases, performance initially improves but\neventually decreases. To understand this phenomenon, we provide a piece of\nevidence that longer reasoning processes are increasingly susceptible to noise.\nWe theoretically prove the existence of an optimal CoT length and derive a\nscaling law for this optimal length based on model capability and task\ndifficulty. Inspired by our theory, we conduct experiments on both synthetic\nand real world datasets and propose Length-filtered Vote to alleviate the\neffects of excessively long or short CoTs. Our findings highlight the critical\nneed to calibrate CoT length to align with model capabilities and task demands,\noffering a principled framework for optimizing multi-step reasoning in LLMs.\n","authors":["Yuyang Wu","Yifei Wang","Tianqi Du","Stefanie Jegelka","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07265v1","updated":"2025-02-11T05:08:47Z","published":"2025-02-11T05:08:47Z","title":"Riemannian Proximal Sampler for High-accuracy Sampling on Manifolds","summary":"  We introduce the Riemannian Proximal Sampler, a method for sampling from\ndensities defined on Riemannian manifolds. The performance of this sampler\ncritically depends on two key oracles: the Manifold Brownian Increments (MBI)\noracle and the Riemannian Heat-kernel (RHK) oracle. We establish high-accuracy\nsampling guarantees for the Riemannian Proximal Sampler, showing that\ngenerating samples with $\\varepsilon$-accuracy requires\n$O(\\log(1/\\varepsilon))$ iterations in Kullback-Leibler divergence assuming\naccess to exact oracles and $O(\\log^2(1/\\varepsilon))$ iterations in the total\nvariation metric assuming access to sufficiently accurate inexact oracles.\nFurthermore, we present practical implementations of these oracles by\nleveraging heat-kernel truncation and Varadhan's asymptotics. In the latter\ncase, we interpret the Riemannian Proximal Sampler as a discretization of the\nentropy-regularized Riemannian Proximal Point Method on the associated\nWasserstein space. We provide preliminary numerical results that illustrate the\neffectiveness of the proposed methodology.\n","authors":["Yunrui Guan","Krishnakumar Balasubramanian","Shiqian Ma"],"pdf_url":"https://arxiv.org/pdf/2502.07265v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07259v1","updated":"2025-02-11T04:57:33Z","published":"2025-02-11T04:57:33Z","title":"Flat U-Net: An Efficient Ultralightweight Model for Solar Filament\n  Segmentation in Full-disk H$Œ±$ Images","summary":"  Solar filaments are one of the most prominent features observed on the Sun,\nand their evolutions are closely related to various solar activities, such as\nflares and coronal mass ejections. Real-time automated identification of solar\nfilaments is the most effective approach to managing large volumes of data.\nExisting models of filament identification are characterized by large parameter\nsizes and high computational costs, which limit their future applications in\nhighly integrated and intelligent ground-based and space-borne observation\ndevices. Consequently, the design of more lightweight models will facilitate\nthe advancement of intelligent observation equipment. In this study, we\nintroduce Flat U-Net, a novel and highly efficient ultralightweight model that\nincorporates simplified channel attention (SCA) and channel self-attention\n(CSA) convolutional blocks for the segmentation of solar filaments in full-disk\nH$\\alpha$ images. Feature information from each network layer is fully\nextracted to reconstruct interchannel feature representations. Each block\neffectively optimizes the channel features from the previous layer,\nsignificantly reducing parameters. The network architecture presents an elegant\nflattening, improving its efficiency, and simplifying the overall design.\nExperimental validation demonstrates that a model composed of pure SCAs\nachieves a precision of approximately 0.93, with dice similarity coefficient\n(DSC) and recall rates of 0.76 and 0.64, respectively, significantly\noutperforming the classical U-Net. Introducing a certain number of CSA blocks\nimproves the DSC and recall rates to 0.82 and 0.74, respectively, which\ndemonstrates a pronounced advantage, particularly concerning model weight size\nand detection effectiveness. The data set, models, and code are available as\nopen-source resources.\n","authors":["GaoFei Zhu","GangHua Lin","Xiao Yang","Cheng Zeng"],"pdf_url":"https://arxiv.org/pdf/2502.07259v1.pdf","comment":"15 pages, 5 figures, 3 tables, accepted for publication in ApJ"},{"id":"http://arxiv.org/abs/2502.04684v2","updated":"2025-02-11T04:42:11Z","published":"2025-02-07T06:16:31Z","title":"G2PDiffusion: Genotype-to-Phenotype Prediction with Diffusion Models","summary":"  Discovering the genotype-phenotype relationship is crucial for genetic\nengineering, which will facilitate advances in fields such as crop breeding,\nconservation biology, and personalized medicine. Current research usually\nfocuses on single species and small datasets due to limitations in phenotypic\ndata collection, especially for traits that require visual assessments or\nphysical measurements. Deciphering complex and composite phenotypes, such as\nmorphology, from genetic data at scale remains an open question. To break\nthrough traditional generic models that rely on simplified assumptions, this\npaper introduces G2PDiffusion, the first-of-its-kind diffusion model designed\nfor genotype-to-phenotype generation across multiple species. Specifically, we\nuse images to represent morphological phenotypes across species and redefine\nphenotype prediction as conditional image generation. To this end, this paper\nintroduces an environment-enhanced DNA sequence conditioner and trains a stable\ndiffusion model with a novel alignment method to improve genotype-to-phenotype\nconsistency. Extensive experiments demonstrate that our approach enhances\nphenotype prediction accuracy across species, capturing subtle genetic\nvariations that contribute to observable traits.\n","authors":["Mengdi Liu","Zhangyang Gao","Hong Chang","Stan Z. Li","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.04684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15882v2","updated":"2025-02-11T04:41:10Z","published":"2024-07-20T23:45:04Z","title":"Ensemble quantile-based deep learning framework for streamflow and flood\n  prediction in Australian catchments","summary":"  In recent years, climate extremes such as floods have created significant\nenvironmental and economic hazards for Australia. Deep learning methods have\nbeen promising for predicting extreme climate events; however, large flooding\nevents present a critical challenge due to factors such as model calibration\nand missing data. We present an ensemble quantile-based deep learning framework\nthat addresses large-scale streamflow forecasts using quantile regression for\nuncertainty projections in prediction. We evaluate selected univariate and\nmultivariate deep learning models and catchment strategies. Furthermore, we\nimplement a multistep time-series prediction model using the CAMELS dataset for\nselected catchments across Australia. The ensemble model employs a set of\nquantile deep learning models for streamflow determined by historical\nstreamflow data. We utilise the streamflow prediction and obtain flood\nprobability using flood frequency analysis and compare it with historical\nflooding events for selected catchments. Our results demonstrate notable\nefficacy and uncertainties in streamflow forecasts with varied catchment\nproperties. Our flood probability estimates show good accuracy in capturing the\nhistorical floods from the selected catchments. This underscores the potential\nfor our deep learning framework to revolutionise flood forecasting across\ndiverse regions and be implemented as an early warning system.\n","authors":["Rohitash Chandra","Arpit Kapoor","Siddharth Khedkar","Jim Ng","R. Willem Vervoort"],"pdf_url":"https://arxiv.org/pdf/2407.15882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07250v1","updated":"2025-02-11T04:34:53Z","published":"2025-02-11T04:34:53Z","title":"NARCE: A Mamba-Based Neural Algorithmic Reasoner Framework for Online\n  Complex Event Detection","summary":"  Current machine learning models excel in short-span perception tasks but\nstruggle to derive high-level insights from long-term observation, a capability\ncentral to understanding complex events (CEs). CEs, defined as sequences of\nshort-term atomic events (AEs) governed by spatiotemporal rules, are\nchallenging to detect online due to the need to extract meaningful patterns\nfrom long and noisy sensor data while ignoring irrelevant events. We\nhypothesize that state-based methods are well-suited for CE detection, as they\ncapture event progression through state transitions without requiring long-term\nmemory. Baseline experiments validate this, demonstrating that the state-space\nmodel Mamba outperforms existing architectures. However, Mamba's reliance on\nextensive labeled data, which are difficult to obtain, motivates our second\nhypothesis: decoupling CE rule learning from noisy sensor data can reduce data\nrequirements. To address this, we propose NARCE, a framework that combines\nNeural Algorithmic Reasoning (NAR) to split the task into two components: (i)\nlearning CE rules independently of sensor data using synthetic concept traces\ngenerated by LLMs and (ii) mapping sensor inputs to these rules via an adapter.\nOur results show that NARCE outperforms baselines in accuracy, generalization\nto unseen and longer sensor data, and data efficiency, significantly reducing\nannotation costs while advancing robust CE detection.\n","authors":["Liying Han","Gaofeng Dong","Xiaomin Ouyang","Lance Kaplan","Federico Cerutti","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2502.07250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07244v1","updated":"2025-02-11T04:24:43Z","published":"2025-02-11T04:24:43Z","title":"Linear Transformers as VAR Models: Aligning Autoregressive Attention\n  Mechanisms with Autoregressive Forecasting","summary":"  Autoregressive attention-based time series forecasting (TSF) has drawn\nincreasing interest, with mechanisms like linear attention sometimes\noutperforming vanilla attention. However, deeper Transformer architectures\nfrequently misalign with autoregressive objectives, obscuring the underlying\nVAR structure embedded within linear attention and hindering their ability to\ncapture the data generative processes in TSF. In this work, we first show that\na single linear attention layer can be interpreted as a dynamic vector\nautoregressive (VAR) structure. We then explain that existing multi-layer\nTransformers have structural mismatches with the autoregressive forecasting\nobjective, which impair interpretability and generalization ability. To address\nthis, we show that by rearranging the MLP, attention, and input-output flow,\nmulti-layer linear attention can also be aligned as a VAR model. Then, we\npropose Structural Aligned Mixture of VAR (SAMoVAR), a linear Transformer\nvariant that integrates interpretable dynamic VAR weights for multivariate TSF.\nBy aligning the Transformer architecture with autoregressive objectives,\nSAMoVAR delivers improved performance, interpretability, and computational\nefficiency, comparing to SOTA TSF models.\n","authors":["Jiecheng Lu","Shihao Yang"],"pdf_url":"https://arxiv.org/pdf/2502.07244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19272v3","updated":"2025-02-11T04:20:07Z","published":"2024-05-29T17:03:31Z","title":"Differentially Private Clustered Federated Learning","summary":"  Federated learning (FL), which is a decentralized machine learning (ML)\napproach, often incorporates differential privacy (DP) to provide rigorous data\nprivacy guarantees. Previous works attempted to address high structured data\nheterogeneity in vanilla FL settings through clustering clients (a.k.a\nclustered FL), but these methods remain sensitive and prone to errors, further\nexacerbated by the DP noise. This vulnerability makes the previous methods\ninappropriate for differentially private FL (DPFL) settings with structured\ndata heterogeneity. To address this gap, we propose an algorithm for\ndifferentially private clustered FL, which is robust to the DP noise in the\nsystem and identifies the underlying clients' clusters correctly. To this end,\nwe propose to cluster clients based on both their model updates and training\nloss values. Furthermore, for clustering clients' model updates at the end of\nthe first round, our proposed approach addresses the server's uncertainties by\nemploying large batch sizes as well as Gaussian Mixture Models (GMM) to reduce\nthe impact of DP and stochastic noise and avoid potential clustering errors.\nThis idea is efficient especially in privacy-sensitive scenarios with more DP\nnoise. We provide theoretical analysis to justify our approach and evaluate it\nacross diverse data distributions and privacy budgets. Our experimental results\nshow its effectiveness in addressing large structured data heterogeneity in\nDPFL.\n","authors":["Saber Malekmohammadi","Afaf Taik","Golnoosh Farnadi"],"pdf_url":"https://arxiv.org/pdf/2405.19272v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07237v1","updated":"2025-02-11T04:00:21Z","published":"2025-02-11T04:00:21Z","title":"DrugImproverGPT: A Large Language Model for Drug Optimization with\n  Fine-Tuning via Structured Policy Optimization","summary":"  Finetuning a Large Language Model (LLM) is crucial for generating results\ntowards specific objectives. This research delves into the realm of drug\noptimization and introduce a novel reinforcement learning algorithm to finetune\na drug optimization LLM-based generative model, enhancing the original drug\nacross target objectives, while retains the beneficial chemical properties of\nthe original drug. This work is comprised of two primary components: (1)\nDrugImprover: A framework tailored for improving robustness and efficiency in\ndrug optimization. It includes a LLM designed for drug optimization and a novel\nStructured Policy Optimization (SPO) algorithm, which is theoretically\ngrounded. This algorithm offers a unique perspective for fine-tuning the\nLLM-based generative model by aligning the improvement of the generated\nmolecule with the input molecule under desired objectives. (2) A dataset of 1\nmillion compounds, each with OEDOCK docking scores on 5 human proteins\nassociated with cancer cells and 24 binding sites from SARS-CoV-2 virus. We\nconduct a comprehensive evaluation of SPO and demonstrate its effectiveness in\nimproving the original drug across target properties. Our code and dataset will\nbe publicly available at: https://github.com/xuefeng-cs/DrugImproverGPT.\n","authors":["Xuefeng Liu","Songhao Jiang","Siyu Chen","Zhuoran Yang","Yuxin Chen","Ian Foster","Rick Stevens"],"pdf_url":"https://arxiv.org/pdf/2502.07237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00212v3","updated":"2025-02-11T03:52:52Z","published":"2025-01-31T23:01:48Z","title":"STP: Self-play LLM Theorem Provers with Iterative Conjecturing and\n  Proving","summary":"  A fundamental challenge in formal theorem proving by LLMs is the lack of\nhigh-quality training data. Although reinforcement learning or expert iteration\npartially mitigates this issue by alternating between LLM generating proofs and\nfinetuning them on correctly generated ones, performance quickly plateaus due\nto the scarcity of correct proofs (sparse rewards). To keep improving the\nmodels with limited data, we draw inspiration from mathematicians, who\ncontinuously develop new results, partly by proposing novel conjectures or\nexercises (which are often variants of known results) and attempting to solve\nthem. We design the Self-play Theorem Prover (STP) that simultaneously takes on\ntwo roles, conjecturer and prover, each providing training signals to the\nother. The conjecturer is trained iteratively on previously generated\nconjectures that are barely provable by the current prover, which incentivizes\nit to generate increasingly challenging conjectures over time. The prover\nattempts to prove the conjectures with standard expert iteration. We evaluate\nSTP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens\ngenerated during the training in Lean, STP proves 26.3% of the statements in\nthe LeanWorkbook dataset, doubling the previous best result of 13.2% achieved\nthrough expert iteration. The final model achieves state-of-the-art performance\namong whole-proof generation methods on miniF2F-test (61.7%, pass@3200),\nProofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@3200).\n","authors":["Kefan Dong","Tengyu Ma"],"pdf_url":"https://arxiv.org/pdf/2502.00212v3.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.07232v1","updated":"2025-02-11T03:48:40Z","published":"2025-02-11T03:48:40Z","title":"Simplifying Adversarially Robust PAC Learning with Tolerance","summary":"  Adversarially robust PAC learning has proved to be challenging, with the\ncurrently best known learners [Montasser et al., 2021a] relying on improper\nmethods based on intricate compression schemes, resulting in sample complexity\nexponential in the VC-dimension. A series of follow up work considered a\nslightly relaxed version of the problem called adversarially robust learning\nwith tolerance [Ashtiani et al., 2023, Bhattacharjee et al., 2023, Raman et\nal., 2024] and achieved better sample complexity in terms of the VC-dimension.\nHowever, those algorithms were either improper and complex, or required\nadditional assumptions on the hypothesis class H. We prove, for the first time,\nthe existence of a simpler learner that achieves a sample complexity linear in\nthe VC-dimension without requiring additional assumptions on H. Even though our\nlearner is improper, it is \"almost proper\" in the sense that it outputs a\nhypothesis that is \"similar\" to a hypothesis in H.\n  We also use the ideas from our algorithm to construct a semi-supervised\nlearner in the tolerant setting. This simple algorithm achieves comparable\nbounds to the previous (non-tolerant) semi-supervised algorithm of Attias et\nal. [2022a], but avoids the use of intricate subroutines from previous works,\nand is \"almost proper.\"\n","authors":["Hassan Ashtiani","Vinayak Pathak","Ruth Urner"],"pdf_url":"https://arxiv.org/pdf/2502.07232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11306v2","updated":"2025-02-11T03:38:57Z","published":"2024-08-21T03:21:52Z","title":"Are KANs Effective for Multivariate Time Series Forecasting?","summary":"  Multivariate time series forecasting is a crucial task that predicts the\nfuture states based on historical inputs. Related techniques have been\ndeveloping in parallel with the machine learning community, from early\nstatistical learning methods to current deep learning methods. Despite their\nsignificant advancements, existing methods continue to struggle with the\nchallenge of inadequate interpretability. The rise of the Kolmogorov-Arnold\nNetwork (KAN) provides a new perspective to solve this challenge, but current\nwork has not yet concluded whether KAN is effective in time series forecasting\ntasks. In this paper, we aim to evaluate the effectiveness of KANs in\ntime-series forecasting from the perspectives of performance, integrability,\nefficiency, and interpretability. To this end, we propose the Multi-layer\nMixture-of-KAN network (MMK), which achieves excellent performance while\nretaining KAN's ability to be transformed into a combination of symbolic\nfunctions. The core module of MMK is the mixture-of-KAN layer, which uses a\nmixture-of-experts structure to assign variables to best-matched KAN experts.\nThen, we explore some useful experimental strategies to deal with the issues in\nthe training stage. Finally, we compare MMK and various baselines on seven\ndatasets. Extensive experimental and visualization results demonstrate that\nKANs are effective in multivariate time series forecasting. Code is available\nat: https://github.com/2448845600/EasyTSF.\n","authors":["Xiao Han","Xinfeng Zhang","Yiling Wu","Zhenduo Zhang","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2408.11306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04442v3","updated":"2025-02-11T03:36:22Z","published":"2024-10-06T10:41:03Z","title":"TimeBridge: Non-Stationarity Matters for Long-term Time Series\n  Forecasting","summary":"  Non-stationarity poses significant challenges for multivariate time series\nforecasting due to the inherent short-term fluctuations and long-term trends\nthat can lead to spurious regressions or obscure essential long-term\nrelationships. Most existing methods either eliminate or retain\nnon-stationarity without adequately addressing its distinct impacts on\nshort-term and long-term modeling. Eliminating non-stationarity is essential\nfor avoiding spurious regressions and capturing local dependencies in\nshort-term modeling, while preserving it is crucial for revealing long-term\ncointegration across variates. In this paper, we propose TimeBridge, a novel\nframework designed to bridge the gap between non-stationarity and dependency\nmodeling in long-term time series forecasting. By segmenting input series into\nsmaller patches, TimeBridge applies Integrated Attention to mitigate short-term\nnon-stationarity and capture stable dependencies within each variate, while\nCointegrated Attention preserves non-stationarity to model long-term\ncointegration across variates. Extensive experiments show that TimeBridge\nconsistently achieves state-of-the-art performance in both short-term and\nlong-term forecasting. Additionally, TimeBridge demonstrates exceptional\nperformance in financial forecasting on the CSI 500 and S&P 500 indices,\nfurther validating its robustness and effectiveness. Code is available at\nhttps://github.com/Hank0626/TimeBridge.\n","authors":["Peiyuan Liu","Beiliang Wu","Yifan Hu","Naiqi Li","Tao Dai","Jigang Bao","Shu-tao Xia"],"pdf_url":"https://arxiv.org/pdf/2410.04442v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07222v1","updated":"2025-02-11T03:32:10Z","published":"2025-02-11T03:32:10Z","title":"A Memory Efficient Randomized Subspace Optimization Method for Training\n  Large Language Models","summary":"  The memory challenges associated with training Large Language Models (LLMs)\nhave become a critical concern, particularly when using the Adam optimizer. To\naddress this issue, numerous memory-efficient techniques have been proposed,\nwith GaLore standing out as a notable example designed to reduce the memory\nfootprint of optimizer states. However, these approaches do not alleviate the\nmemory burden imposed by activations, rendering them unsuitable for scenarios\ninvolving long context sequences or large mini-batches. Moreover, their\nconvergence properties are still not well-understood in the literature. In this\nwork, we introduce a Randomized Subspace Optimization framework for\npre-training and fine-tuning LLMs. Our approach decomposes the high-dimensional\ntraining problem into a series of lower-dimensional subproblems. At each\niteration, a random subspace is selected, and the parameters within that\nsubspace are optimized. This structured reduction in dimensionality allows our\nmethod to simultaneously reduce memory usage for both activations and optimizer\nstates. We establish comprehensive convergence guarantees and derive rates for\nvarious scenarios, accommodating different optimization strategies to solve the\nsubproblems. Extensive experiments validate the superior memory and\ncommunication efficiency of our method, achieving performance comparable to\nGaLore and Adam.\n","authors":["Yiming Chen","Yuan Zhang","Yin Liu","Kun Yuan","Zaiwen Wen"],"pdf_url":"https://arxiv.org/pdf/2502.07222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11587v2","updated":"2025-02-11T03:29:30Z","published":"2025-01-20T16:46:26Z","title":"Recurrent Diffusion for Large-Scale Parameter Generation","summary":"  Parameter generation has long struggled to match the scale of today large\nvision and language models, curbing its broader utility. In this paper, we\nintroduce Recurrent Diffusion for Large Scale Parameter Generation (RPG), a\nnovel framework that generates full neural network parameters up to hundreds of\nmillions on a single GPU. Our approach first partitions a networks parameters\ninto non-overlapping tokens, each corresponding to a distinct portion of the\nmodel. A recurrent mechanism then learns the inter token relationships,\nproducing prototypes which serve as conditions for a diffusion process that\nultimately synthesizes the full parameters. Across a spectrum of architectures\nand tasks including ResNets, ConvNeXts and ViTs on ImageNet 1K and COCO, and\neven LoRA based LLMs RPG achieves performance on par with fully trained\nnetworks while avoiding excessive memory overhead. Notably, it generalizes\nbeyond its training set to generate valid parameters for previously unseen\ntasks, highlighting its flexibility in dynamic and open ended scenarios. By\novercoming the longstanding memory and scalability barriers, RPG serves as a\ncritical advance in AI generating AI, potentially enabling efficient weight\ngeneration at scales previously deemed infeasible.\n","authors":["Kai Wang","Dongwen Tang","Wangbo Zhao","Konstantin Sch√ºrholt","Zhangyang Wang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2501.11587v2.pdf","comment":"Generating 200 million parameters in just minutes"},{"id":"http://arxiv.org/abs/2502.07218v1","updated":"2025-02-11T03:23:22Z","published":"2025-02-11T03:23:22Z","title":"LUNAR: LLM Unlearning via Neural Activation Redirection","summary":"  Large Language Models (LLMs) benefit from training on ever larger amounts of\ntextual data, but as a result, they increasingly incur the risk of leaking\nprivate information. The ability to selectively remove knowledge from LLMs is,\ntherefore, a highly desirable capability. In this paper, we propose LUNAR, a\nnovel unlearning methodology grounded in the Linear Representation Hypothesis.\nLUNAR operates by redirecting the representations of unlearned data to regions\nthat trigger the model's inherent ability to express its inability to answer.\nLUNAR achieves state-of-the-art unlearning performance while significantly\nenhancing the controllability of the unlearned model during inference.\nSpecifically, LUNAR achieves between 2.9x to 11.7x improvements on combined\n\"unlearning efficacy\" and \"model utility\" score (\"Deviation Score\") on the\nPISTOL dataset across various base models. We also demonstrate, through\nquantitative analysis and qualitative examples, LUNAR's superior\ncontrollability in generating coherent and contextually aware responses,\nmitigating undesired side effects of existing methods. Moreover, we demonstrate\nthat LUNAR is robust against white-box adversarial attacks and versatile in\nhandling real-world scenarios, such as processing sequential unlearning\nrequests.\n","authors":["William F. Shen","Xinchi Qiu","Meghdad Kurmanji","Alex Iacob","Lorenzo Sani","Yihong Chen","Nicola Cancedda","Nicholas D. Lane"],"pdf_url":"https://arxiv.org/pdf/2502.07218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07214v1","updated":"2025-02-11T03:16:08Z","published":"2025-02-11T03:16:08Z","title":"Pareto Optimal Algorithmic Recourse in Multi-cost Function","summary":"  In decision-making systems, algorithmic recourse aims to identify\nminimal-cost actions to alter an individual features, thereby obtaining a\ndesired outcome. This empowers individuals to understand, question, or alter\ndecisions that negatively affect them. However, due to the variety and\nsensitivity of system environments and individual personalities, quantifying\nthe cost of a single function is nearly impossible while considering multiple\ncriteria situations. Most current recourse mechanisms use gradient-based\nmethods that assume cost functions are differentiable, often not applicable in\nreal-world scenarios, resulting in sub-optimal solutions that compromise\nvarious criteria. These solutions are typically intractable and lack rigorous\ntheoretical foundations, raising concerns regarding interpretability,\nreliability, and transparency from the explainable AI (XAI) perspective.\n  To address these issues, this work proposes an algorithmic recourse framework\nthat handles non-differentiable and discrete multi-cost functions. By\nformulating recourse as a multi-objective optimization problem and assigning\nweights to different criteria based on their importance, our method identifies\nPareto optimal recourse recommendations. To demonstrate scalability, we\nincorporate the concept of epsilon-net, proving the ability to find\napproximated Pareto optimal actions. Experiments show the trade-off between\ndifferent criteria and the methods scalability in large graphs. Compared to\ncurrent heuristic practices, our approach provides a stronger theoretical\nfoundation and better aligns recourse suggestions with real-world requirements.\n","authors":["Wen-Ling Chen","Hong-Chang Huang","Kai-Hung Lin","Shang-Wei Hwang","Hao-Tsung Yang"],"pdf_url":"https://arxiv.org/pdf/2502.07214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07213v1","updated":"2025-02-11T03:12:08Z","published":"2025-02-11T03:12:08Z","title":"Evaluation for Regression Analyses on Evolving Data Streams","summary":"  The paper explores the challenges of regression analysis in evolving data\nstreams, an area that remains relatively underexplored compared to\nclassification. We propose a standardized evaluation process for regression and\nprediction interval tasks in streaming contexts. Additionally, we introduce an\ninnovative drift simulation strategy capable of synthesizing various drift\ntypes, including the less-studied incremental drift. Comprehensive experiments\nwith state-of-the-art methods, conducted under the proposed process, validate\nthe effectiveness and robustness of our approach.\n","authors":["Yibin Sun","Heitor Murilo Gomes","Bernhard Pfahringer","Albert Bifet"],"pdf_url":"https://arxiv.org/pdf/2502.07213v1.pdf","comment":"11 Pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.07211v1","updated":"2025-02-11T03:09:45Z","published":"2025-02-11T03:09:45Z","title":"Improve the Training Efficiency of DRL for Wireless Communication\n  Resource Allocation: The Role of Generative Diffusion Models","summary":"  Dynamic resource allocation in mobile wireless networks involves complex,\ntime-varying optimization problems, motivating the adoption of deep\nreinforcement learning (DRL). However, most existing works rely on pre-trained\npolicies, overlooking dynamic environmental changes that rapidly invalidate the\npolicies. Periodic retraining becomes inevitable but incurs prohibitive\ncomputational costs and energy consumption-critical concerns for\nresource-constrained wireless systems. We identify three root causes of\ninefficient retraining: high-dimensional state spaces, suboptimal action spaces\nexploration-exploitation trade-offs, and reward design limitations. To overcome\nthese limitations, we propose Diffusion-based Deep Reinforcement Learning\n(D2RL), which leverages generative diffusion models (GDMs) to holistically\nenhance all three DRL components. Iterative refinement process and distribution\nmodelling of GDMs enable (1) the generation of diverse state samples to improve\nenvironmental understanding, (2) balanced action space exploration to escape\nlocal optima, and (3) the design of discriminative reward functions that better\nevaluate action quality. Our framework operates in two modes: Mode I leverages\nGDMs to explore reward spaces and design discriminative reward functions that\nrigorously evaluate action quality, while Mode II synthesizes diverse state\nsamples to enhance environmental understanding and generalization. Extensive\nexperiments demonstrate that D2RL achieves faster convergence and reduced\ncomputational costs over conventional DRL methods for resource allocation in\nwireless communications while maintaining competitive policy performance. This\nwork underscores the transformative potential of GDMs in overcoming fundamental\nDRL training bottlenecks for wireless networks, paving the way for practical,\nreal-time deployments.\n","authors":["Xinren Zhang","Jiadong Yu"],"pdf_url":"https://arxiv.org/pdf/2502.07211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07209v1","updated":"2025-02-11T03:07:28Z","published":"2025-02-11T03:07:28Z","title":"Enhancing Physics-Informed Neural Networks Through Feature Engineering","summary":"  Physics-Informed Neural Networks (PINNs) seek to solve partial differential\nequations (PDEs) with deep learning. Mainstream approaches that deploy\nfully-connected multi-layer deep learning architectures require prolonged\ntraining to achieve even moderate accuracy, while recent work on feature\nengineering allows higher accuracy and faster convergence. This paper\nintroduces SAFE-NET, a Single-layered Adaptive Feature Engineering NETwork that\nachieves orders-of-magnitude lower errors with far fewer parameters than\nbaseline feature engineering methods. SAFE-NET returns to basic ideas in\nmachine learning, using Fourier features, a simplified single hidden layer\nnetwork architecture, and an effective optimizer that improves the conditioning\nof the PINN optimization problem. Numerical results show that SAFE-NET\nconverges faster and typically outperforms deeper networks and more complex\narchitectures. It consistently uses fewer parameters -- on average, 65% fewer\nthan the competing feature engineering methods -- while achieving comparable\naccuracy in less than 30% of the training epochs. Moreover, each SAFE-NET epoch\nis 95% faster than those of competing feature engineering approaches. These\nfindings challenge the prevailing belief that modern PINNs effectively learn\nfeatures in these scientific applications and highlight the efficiency gains\npossible through feature engineering.\n","authors":["Shaghayegh Fazliani","Zachary Frangella","Madeleine Udell"],"pdf_url":"https://arxiv.org/pdf/2502.07209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07207v1","updated":"2025-02-11T03:06:03Z","published":"2025-02-11T03:06:03Z","title":"A Study on the Importance of Features in Detecting Advanced Persistent\n  Threats Using Machine Learning","summary":"  Advanced Persistent Threats (APTs) pose a significant security risk to\norganizations and industries. These attacks often lead to severe data breaches\nand compromise the system for a long time. Mitigating these sophisticated\nattacks is highly challenging due to the stealthy and persistent nature of\nAPTs. Machine learning models are often employed to tackle this challenge by\nbringing automation and scalability to APT detection. Nevertheless, these\nintelligent methods are data-driven, and thus, highly affected by the quality\nand relevance of input data. This paper aims to analyze measurements considered\nwhen recording network traffic and conclude which features contribute more to\ndetecting APT samples. To do this, we study the features associated with\nvarious APT cases and determine their importance using a machine learning\nframework. To ensure the generalization of our findings, several feature\nselection techniques are employed and paired with different classifiers to\nevaluate their effectiveness. Our findings provide insights into how APT\ndetection can be enhanced in real-world scenarios.\n","authors":["Ehsan Hallaji","Roozbeh Razavi-Far","Mehrdad Saif"],"pdf_url":"https://arxiv.org/pdf/2502.07207v1.pdf","comment":"Accepted for publication in the 2024 International Conference on\n  Computational Science and Computational Intelligence (CSCI'24)"},{"id":"http://arxiv.org/abs/2502.07839v1","updated":"2025-02-11T03:01:05Z","published":"2025-02-11T03:01:05Z","title":"Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement\n  Learning","summary":"  With the increasing prevalence of autonomous vehicles (AVs), their\nvulnerability to various types of attacks has grown, presenting significant\nsecurity challenges. In this paper, we propose a reinforcement learning\n(RL)-based approach for designing optimal stealthy integrity attacks on AV\nactuators. We also analyze the limitations of state-of-the-art RL-based secure\ncontrollers developed to counter such attacks. Through extensive simulation\nexperiments, we demonstrate the effectiveness and efficiency of our proposed\nmethod.\n","authors":["Pengyu Wang","Jialu Li","Ling Shi"],"pdf_url":"https://arxiv.org/pdf/2502.07839v1.pdf","comment":"Accepted in 2024 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) Workshop"},{"id":"http://arxiv.org/abs/2502.01828v2","updated":"2025-02-11T03:00:12Z","published":"2025-02-03T21:11:02Z","title":"From Foresight to Forethought: VLM-In-the-Loop Policy Steering via\n  Latent Alignment","summary":"  While generative robot policies have demonstrated significant potential in\nlearning complex, multimodal behaviors from demonstrations, they still exhibit\ndiverse failures at deployment-time. Policy steering offers an elegant solution\nto reducing the chance of failure by using an external verifier to select from\nlow-level actions proposed by an imperfect generative policy. Here, one might\nhope to use a Vision Language Model (VLM) as a verifier, leveraging its\nopen-world reasoning capabilities. However, off-the-shelf VLMs struggle to\nunderstand the consequences of low-level robot actions as they are represented\nfundamentally differently than the text and images the VLM was trained on. In\nresponse, we propose FOREWARN, a novel framework to unlock the potential of\nVLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is\nto decouple the VLM's burden of predicting action outcomes (foresight) from\nevaluation (forethought). For foresight, we leverage a latent world model to\nimagine future latent states given diverse low-level action plans. For\nforethought, we align the VLM with these predicted latent states to reason\nabout the consequences of actions in its native representation--natural\nlanguage--and effectively filter proposed plans. We validate our framework\nacross diverse robotic manipulation tasks, demonstrating its ability to bridge\nrepresentational gaps and provide robust, generalizable policy steering. Videos\ncan be found on the project website: https://yilin-wu98.github.io/forewarn/.\n","authors":["Yilin Wu","Ran Tian","Gokul Swamy","Andrea Bajcsy"],"pdf_url":"https://arxiv.org/pdf/2502.01828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01932v2","updated":"2025-02-11T03:00:12Z","published":"2025-02-04T02:07:23Z","title":"VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining Motion\n  Control and Strategic Play","summary":"  Multi-agent reinforcement learning (MARL) has made significant progress,\nlargely fueled by the development of specialized testbeds that enable\nsystematic evaluation of algorithms in controlled yet challenging scenarios.\nHowever, existing testbeds often focus on purely virtual simulations or limited\nrobot morphologies such as robotic arms, quadrupeds, and humanoids, leaving\nhigh-mobility platforms with real-world physical constraints like drones\nunderexplored. To bridge this gap, we present VolleyBots, a new MARL testbed\nwhere multiple drones cooperate and compete in the sport of volleyball under\nphysical dynamics. VolleyBots features a turn-based interaction model under\nvolleyball rules, a hierarchical decision-making process that combines motion\ncontrol and strategic play, and a high-fidelity simulation for seamless\nsim-to-real transfer. We provide a comprehensive suite of tasks ranging from\nsingle-drone drills to multi-drone cooperative and competitive tasks,\naccompanied by baseline evaluations of representative MARL and game-theoretic\nalgorithms. Results in simulation show that while existing algorithms handle\nsimple tasks effectively, they encounter difficulty in complex tasks that\nrequire both low-level control and high-level strategy. We further demonstrate\nzero-shot deployment of a simulation-learned policy to real-world drones,\nhighlighting VolleyBots' potential to propel MARL research involving agile\nrobotic platforms. The project page is at\nhttps://sites.google.com/view/thu-volleybots/home.\n","authors":["Zelai Xu","Chao Yu","Ruize Zhang","Huining Yuan","Xiangmin Yi","Shilong Ji","Chuqi Wang","Wenhao Tang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.01932v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07205v1","updated":"2025-02-11T02:54:28Z","published":"2025-02-11T02:54:28Z","title":"VINP: Variational Bayesian Inference with Neural Speech Prior for Joint\n  ASR-Effective Speech Dereverberation and Blind RIR Identification","summary":"  Reverberant speech, denoting the speech signal degraded by the process of\nreverberation, contains crucial knowledge of both anechoic source speech and\nroom impulse response (RIR). This work proposes a variational Bayesian\ninference (VBI) framework with neural speech prior (VINP) for joint speech\ndereverberation and blind RIR identification. In VINP, a probabilistic signal\nmodel is constructed in the time-frequency (T-F) domain based on convolution\ntransfer function (CTF) approximation. For the first time, we propose using an\narbitrary discriminative dereverberation deep neural network (DNN) to predict\nthe prior distribution of anechoic speech within a probabilistic model. By\nintegrating both reverberant speech and the anechoic speech prior, VINP yields\nthe maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the\nanechoic speech spectrum and CTF filter, respectively. After simple\ntransformations, the waveforms of anechoic speech and RIR are estimated.\nMoreover, VINP is effective for automatic speech recognition (ASR) systems,\nwhich sets it apart from most deep learning (DL)-based single-channel\ndereverberation approaches. Experiments on single-channel speech\ndereverberation demonstrate that VINP reaches an advanced level in most metrics\nrelated to human perception and displays unquestionable state-of-the-art (SOTA)\nperformance in ASR-related metrics. For blind RIR identification, experiments\nindicate that VINP attains the SOTA level in blind estimation of reverberation\ntime at 60 dB (RT60) and direct-to-reverberation ratio (DRR). Codes and audio\nsamples are available online.\n","authors":["Pengyu Wang","Ying Fang","Xiaofei Li"],"pdf_url":"https://arxiv.org/pdf/2502.07205v1.pdf","comment":"Submitted to IEEE/ACM Trans. on TASLP"},{"id":"http://arxiv.org/abs/2502.07202v1","updated":"2025-02-11T02:51:42Z","published":"2025-02-11T02:51:42Z","title":"Monte Carlo Tree Diffusion for System 2 Planning","summary":"  Diffusion models have recently emerged as a powerful tool for planning.\nHowever, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally\nimproves with additional test-time computation (TTC), standard diffusion-based\nplanners offer only limited avenues for TTC scalability. In this paper, we\nintroduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates\nthe generative strength of diffusion models with the adaptive search\ncapabilities of MCTS. Our method reconceptualizes denoising as a\ntree-structured process, allowing partially denoised plans to be iteratively\nevaluated, pruned, and refined. By selectively expanding promising trajectories\nwhile retaining the flexibility to revisit and improve suboptimal branches,\nMCTD achieves the benefits of MCTS such as controlling exploration-exploitation\ntrade-offs within the diffusion framework. Empirical results on challenging\nlong-horizon tasks show that MCTD outperforms diffusion baselines, yielding\nhigher-quality solutions as TTC increases.\n","authors":["Jaesik Yoon","Hyeonseo Cho","Doojin Baek","Yoshua Bengio","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2502.07202v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.07199v1","updated":"2025-02-11T02:47:20Z","published":"2025-02-11T02:47:20Z","title":"Fixed-Confidence Best Arm Identification with Decreasing Variance","summary":"  We focus on the problem of best-arm identification in a stochastic multi-arm\nbandit with temporally decreasing variances for the arms' rewards. We model arm\nrewards as Gaussian random variables with fixed means and variances that\ndecrease with time. The cost incurred by the learner is modeled as a weighted\nsum of the time needed by the learner to identify the best arm, and the number\nof samples of arms collected by the learner before termination. Under this cost\nfunction, there is an incentive for the learner to not sample arms in all\nrounds, especially in the initial rounds. On the other hand, not sampling\nincreases the termination time of the learner, which also increases cost. This\ntrade-off necessitates new sampling strategies. We propose two policies. The\nfirst policy has an initial wait period with no sampling followed by continuous\nsampling. The second policy samples periodically and uses a weighted average of\nthe rewards observed to identify the best arm. We provide analytical guarantees\non the performance of both policies and supplement our theoretical results with\nsimulations which show that our polices outperform the state-of-the-art\npolicies for the classical best arm identification problem.\n","authors":["Tamojeet Roychowdhury","Kota Srinivas Reddy","Krishna P Jagannathan","Sharayu Moharir"],"pdf_url":"https://arxiv.org/pdf/2502.07199v1.pdf","comment":"6 pages, 2 figures, accepted in the National Conference on\n  Communications 2025"},{"id":"http://arxiv.org/abs/2502.07193v1","updated":"2025-02-11T02:36:01Z","published":"2025-02-11T02:36:01Z","title":"Provably Efficient RLHF Pipeline: A Unified View from Contextual Bandits","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a widely used approach\nfor aligning Large Language Models (LLMs) with human preferences. While recent\nadvancements have provided valuable insights into various stages and settings\nof RLHF, a comprehensive theoretical understanding of the entire RLHF pipeline\nremains lacking. Towards this end, we propose a unified framework for the RLHF\npipeline from the view of contextual bandits and provide provable efficiency\nguarantees. In particular, we decompose the RLHF process into two distinct\nstages: (post-)training and deployment, exploring both passive and active data\ncollection strategies during the training phase. By employing the Bradley-Terry\npreference model with a linearly parameterized reward function, we reformulate\nRLHF as a contextual preference bandit problem. We then develop novel\nalgorithms for each stage, demonstrating significant improvements over existing\napproaches in both statistical and computational efficiency. Finally, we apply\nour method to train and deploy Llama-3-8B-Instruct on the\nUltrafeedback-binarized dataset, and empirical results confirm the\neffectiveness of our approach.\n","authors":["Long-Fei Li","Yu-Yang Qian","Peng Zhao","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07189v1","updated":"2025-02-11T02:31:04Z","published":"2025-02-11T02:31:04Z","title":"Exploring Neural Network Pruning with Screening Methods","summary":"  Deep neural networks (DNNs) such as convolutional neural networks (CNNs) for\nvisual tasks, recurrent neural networks (RNNs) for sequence data, and\ntransformer models for rich linguistic or multimodal tasks, achieved\nunprecedented performance on a wide range of tasks. The impressive performance\nof modern DNNs is partially attributed to their sheer scale. The latest deep\nlearning models have tens to hundreds of millions of parameters which makes the\ninference processes resource-intensive. The high computational complexity of\nthese networks prevents their deployment on resource-limited devices such as\nmobile platforms, IoT devices, and edge computing systems because these devices\nrequire energy-efficient and real-time processing capabilities. This paper\nproposes and evaluates a network pruning framework that eliminates\nnon-essential parameters based on a statistical analysis of network component\nsignificance across classification categories. The proposed method uses\nscreening methods coupled with a weighted scheme to assess connection and\nchannel contributions for unstructured and structured pruning which allows for\nthe elimination of unnecessary network elements without significantly degrading\nmodel performance. Extensive experimental validation on real-world vision\ndatasets for both fully connected neural networks (FNNs) and CNNs has shown\nthat the proposed framework produces competitive lean networks compared to the\noriginal networks. Moreover, the proposed framework outperforms state-of-art\nnetwork pruning methods in two out of three cases.\n","authors":["Mingyuan Wang","Yangzi Guo","Sida Liu","Yanwen Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.07189v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2502.07187v1","updated":"2025-02-11T02:28:35Z","published":"2025-02-11T02:28:35Z","title":"Local Regularizers Are Not Transductive Learners","summary":"  We partly resolve an open question raised by Asilis et al. (COLT 2024):\nwhether the algorithmic template of local regularization -- an intriguing\ngeneralization of explicit regularization, a.k.a. structural risk minimization\n-- suffices to learn all learnable multiclass problems. Specifically, we\nprovide a negative answer to this question in the transductive model of\nlearning. We exhibit a multiclass classification problem which is learnable in\nboth the transductive and PAC models, yet cannot be learned transductively by\nany local regularizer. The corresponding hypothesis class, and our proof, are\nbased on principles from cryptographic secret sharing. We outline challenges in\nextending our negative result to the PAC model, leaving open the tantalizing\npossibility of a PAC/transductive separation with respect to local\nregularization.\n","authors":["Sky Jafar","Julian Asilis","Shaddin Dughmi"],"pdf_url":"https://arxiv.org/pdf/2502.07187v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2502.07186v1","updated":"2025-02-11T02:25:44Z","published":"2025-02-11T02:25:44Z","title":"Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs","summary":"  Zero-shot LLMs are now also used for textual classification tasks, e.g.,\nsentiment/emotion detection of a given input as a sentence/article. However,\ntheir performance can be suboptimal in such data annotation tasks. We introduce\na novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's\nconfidence for its classification of an input by leveraging Metamorphic\nRelations (MRs). The MRs generate semantically equivalent yet textually mutated\nversions of the input. Following the principles of Metamorphic Testing (MT),\nthe mutated versions are expected to have annotation labels similar to the\ninput. By analyzing the consistency of LLM responses across these variations,\nPCS computes a confidence score based on the frequency of predicted labels. PCS\ncan be used both for single LLM and multiple LLM settings (e.g., majority\nvoting). We introduce an algorithm Perceived Differential Evolution (PDE) that\ndetermines the optimal weights assigned to the MRs and the LLMs for a\nclassification task. Empirical evaluation shows PCS significantly improves\nzero-shot accuracy for Llama-3-8B-Instruct (4.96%) and Mistral-7B-Instruct-v0.3\n(10.52%), with Gemma-2-9b-it showing a 9.39% gain. When combining all three\nmodels, PCS significantly outperforms majority voting by 7.75%.\n","authors":["Sina Salimian","Gias Uddin","Most Husne Jahan","Shaina Raza"],"pdf_url":"https://arxiv.org/pdf/2502.07186v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11230v2","updated":"2025-02-11T02:17:24Z","published":"2024-06-17T05:54:06Z","title":"Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of\n  Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) have shown significant promise in\nvarious applications, leading to broad interest from researchers and\npractitioners alike. However, a comprehensive evaluation of their long-context\ncapabilities remains underexplored. To address these gaps, we introduce the\nMultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to\nassess the long-context capabilities of MLLMs. Besides multi-image input, we\nemploy image stitching to further increase the input context length, and\ndevelop a protocol to automatically generate labels for sub-image level\nretrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their\ncapability to locate a target sub-image (needle) within a set of images\n(haystack) based on textual instructions and descriptions of image contents.\nThis setup necessitates an advanced understanding of extensive visual contexts\nand effective information retrieval within long-context image inputs. With this\nbenchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and\nopen-source models. The findings reveal that GPT-4o consistently surpasses\nother models in long-context scenarios, but suffers from hallucination problems\nin negative samples, i.e., when needles are not in the haystacks. Our\ncomprehensive long-context evaluation of MLLMs also sheds lights on the\nconsiderable performance gap between API-based and open-source models. All the\ncode, data, and instructions required to reproduce the main results are\navailable at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.\n","authors":["Hengyi Wang","Haizhou Shi","Shiwei Tan","Weiyi Qin","Wenyuan Wang","Tunyu Zhang","Akshay Nambi","Tanuja Ganu","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2406.11230v2.pdf","comment":"Accepted at NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2502.07837v1","updated":"2025-02-11T02:16:59Z","published":"2025-02-11T02:16:59Z","title":"RoboBERT: An End-to-end Multimodal Robotic Manipulation Model","summary":"  Embodied intelligence integrates multiple modalities, enabling agents to\nunderstand images, language, and actions simultaneously. However, existing\nmodels always depend on additional datasets or extensive pre-training to\nmaximize performance improvements, consuming abundant training time and\nexpensive hardware cost. To tackle this issue, we present RoboBERT, a novel\nend-to-end robotic manipulation model integrated with a unique training\nstrategy. This model utilizes a CNN-based diffusion policy, enhancing and\nstabilizing the effectiveness of this model by separating training processes\nfor different modalities. It also underscores the importance of data\naugmentation, verifying various techniques to significantly boost performance.\nUnlike models that depend on extra data or large foundation models, RoboBERT\nachieves a highly competitive success rate while using only language-labeled\nexpert demonstrations and maintaining a relatively smaller model size.\nSpecifically, RoboBERT achieves an average length of 4.52 on the CALVIN\nbenchmark for \\(ABCD \\rightarrow D\\) task, setting a new state-of-the-art\n(SOTA) record. Furthermore, when tested on a real robot, the model demonstrates\nsuperior performance, achieving a higher success rate than other methods\ntrained with the same data. We propose that these concepts and methodologies of\nRoboBERT demonstrate extensive versatility and compatibility, contributing\nsignificantly to the development of lightweight multimodal robotic models. The\ncode can be accessed on https://github.com/PeterWangsicheng/RoboBERT\n","authors":["Sicheng Wang","Jianhua Shan","Jianwei Zhang","Haozhang Gao","Hailiang Han","Yipeng Chen","Kang Wei","Chengkun Zhang","Kairos Wong","Jie Zhao","Lei Zhao","Bin Fang"],"pdf_url":"https://arxiv.org/pdf/2502.07837v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07181v1","updated":"2025-02-11T02:12:29Z","published":"2025-02-11T02:12:29Z","title":"Tab2Visual: Overcoming Limited Data in Tabular Data Classification Using\n  Deep Learning with Visual Representations","summary":"  This research addresses the challenge of limited data in tabular data\nclassification, particularly prevalent in domains with constraints like\nhealthcare. We propose Tab2Visual, a novel approach that transforms\nheterogeneous tabular data into visual representations, enabling the\napplication of powerful deep learning models. Tab2Visual effectively addresses\ndata scarcity by incorporating novel image augmentation techniques and\nfacilitating transfer learning. We extensively evaluate the proposed approach\non diverse tabular datasets, comparing its performance against a wide range of\nmachine learning algorithms, including classical methods, tree-based ensembles,\nand state-of-the-art deep learning models specifically designed for tabular\ndata. We also perform an in-depth analysis of factors influencing Tab2Visual's\nperformance. Our experimental results demonstrate that Tab2Visual outperforms\nother methods in classification problems with limited tabular data.\n","authors":["Ahmed Mamdouh","Moumen El-Melegy","Samia Ali","Ron Kikinis"],"pdf_url":"https://arxiv.org/pdf/2502.07181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09851v2","updated":"2025-02-11T02:11:22Z","published":"2024-11-15T00:09:37Z","title":"SymbolFit: Automatic Parametric Modeling with Symbolic Regression","summary":"  We introduce SymbolFit, a framework that automates parametric modeling by\nusing symbolic regression to perform a machine-search for functions that fit\nthe data, while simultaneously providing uncertainty estimates in a single run.\nTraditionally, constructing a parametric model to accurately describe binned\ndata has been a manual and iterative process, requiring an adequate functional\nform to be determined before the fit can be performed. The main challenge\narises when the appropriate functional forms cannot be derived from first\nprinciples, especially when there is no underlying true closed-form function\nfor the distribution. In this work, we address this problem by utilizing\nsymbolic regression, a machine learning technique that explores a vast space of\ncandidate functions without needing a predefined functional form, treating the\nfunctional form itself as a trainable parameter. Our approach is demonstrated\nin data analysis applications in high-energy physics experiments at the CERN\nLarge Hadron Collider (LHC). We demonstrate its effectiveness and efficiency\nusing five real proton-proton collision datasets from new physics searches at\nthe LHC, namely the background modeling in resonance searches for high-mass\ndijet, trijet, paired-dijet, diphoton, and dimuon events. We also validate the\nframework using several toy datasets with one and more variables.\n","authors":["Ho Fung Tsoi","Dylan Rankin","Cecile Caillol","Miles Cranmer","Sridhara Dasu","Javier Duarte","Philip Harris","Elliot Lipeles","Vladimir Loncar"],"pdf_url":"https://arxiv.org/pdf/2411.09851v2.pdf","comment":"53 pages, 35 figures. Under review. The API can be used\n  out-of-the-box and is available at https://github.com/hftsoi/symbolfit"},{"id":"http://arxiv.org/abs/2502.07176v1","updated":"2025-02-11T01:59:46Z","published":"2025-02-11T01:59:46Z","title":"MatrixKAN: Parallelized Kolmogorov-Arnold Network","summary":"  Kolmogorov-Arnold Networks (KAN) are a new class of neural network\narchitecture representing a promising alternative to the Multilayer Perceptron\n(MLP), demonstrating improved expressiveness and interpretability. However,\nKANs suffer from slow training and inference speeds relative to MLPs due in\npart to the recursive nature of the underlying B-spline calculations. This\nissue is particularly apparent with respect to KANs utilizing high-degree\nB-splines, as the number of required non-parallelizable recursions is\nproportional to B-spline degree. We solve this issue by proposing MatrixKAN, a\nnovel optimization that parallelizes B-spline calculations with matrix\nrepresentation and operations, thus significantly improving effective\ncomputation time for models utilizing high-degree B-splines. In this paper, we\ndemonstrate the superior scaling of MatrixKAN's computation time relative to\nB-spline degree. Further, our experiments demonstrate speedups of approximately\n40x relative to KAN, with significant additional speedup potential for larger\ndatasets or higher spline degrees.\n","authors":["Cale Coffman","Lizhong Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09302v2","updated":"2025-02-11T01:46:35Z","published":"2024-10-11T23:29:20Z","title":"Enhancing Multi-Step Reasoning Abilities of Language Models through\n  Direct Q-Function Optimization","summary":"  Reinforcement Learning (RL) plays a crucial role in aligning large language\nmodels (LLMs) with human preferences and improving their ability to perform\ncomplex tasks. However, current approaches either require significant\ncomputational resources due to the use of multiple models and extensive online\nsampling for training (e.g., PPO) or are framed as bandit problems (e.g., DPO,\nDRO), which often struggle with multi-step reasoning tasks, such as math\nproblem solving and complex reasoning that involve long chains of thought. To\novercome these limitations, we introduce Direct Q-function Optimization (DQO),\nwhich formulates the response generation process as a Markov Decision Process\n(MDP) and utilizes the soft actor-critic (SAC) framework to optimize a\nQ-function directly parameterized by the language model. The MDP formulation of\nDQO offers structural advantages over bandit-based methods, enabling more\neffective process supervision. Experimental results on two math problem-solving\ndatasets, GSM8K and MATH, demonstrate that DQO outperforms previous methods,\nestablishing it as a promising offline reinforcement learning approach for\naligning language models.\n","authors":["Kaixuan Ji","Guanlin Liu","Ning Dai","Qingping Yang","Renjie Zheng","Zheng Wu","Chen Dun","Quanquan Gu","Lin Yan"],"pdf_url":"https://arxiv.org/pdf/2410.09302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07836v1","updated":"2025-02-11T01:44:51Z","published":"2025-02-11T01:44:51Z","title":"Advancing Precision Oncology Through Modeling of Longitudinal and\n  Multimodal Data","summary":"  Cancer evolves continuously over time through a complex interplay of genetic,\nepigenetic, microenvironmental, and phenotypic changes. This dynamic behavior\ndrives uncontrolled cell growth, metastasis, immune evasion, and therapy\nresistance, posing challenges for effective monitoring and treatment. However,\ntoday's data-driven research in oncology has primarily focused on\ncross-sectional analysis using data from a single modality, limiting the\nability to fully characterize and interpret the disease's dynamic\nheterogeneity. Advances in multiscale data collection and computational methods\nnow enable the discovery of longitudinal multimodal biomarkers for precision\noncology. Longitudinal data reveal patterns of disease progression and\ntreatment response that are not evident from single-timepoint data, enabling\ntimely abnormality detection and dynamic treatment adaptation. Multimodal data\nintegration offers complementary information from diverse sources for more\nprecise risk assessment and targeting of cancer therapy. In this review, we\nsurvey methods of longitudinal and multimodal modeling, highlighting their\nsynergy in providing multifaceted insights for personalized care tailored to\nthe unique characteristics of a patient's cancer. We summarize the current\nchallenges and future directions of longitudinal multimodal analysis in\nadvancing precision oncology.\n","authors":["Luoting Zhuang","Stephen H. Park","Steven J. Skates","Ashley E. Prosper","Denise R. Aberle","William Hsu"],"pdf_url":"https://arxiv.org/pdf/2502.07836v1.pdf","comment":"This work has been submitted to the IEEE RBME for potential\n  publication"},{"id":"http://arxiv.org/abs/2405.05521v2","updated":"2025-02-11T01:43:34Z","published":"2024-05-09T03:19:20Z","title":"Machine Learning for Scalable and Optimal Load Shedding Under Power\n  System Contingency","summary":"  Prompt and effective corrective actions in response to unexpected\ncontingencies are crucial for improving power system resilience and preventing\ncascading blackouts. The optimal load shedding (OLS) accounting for network\nlimits has the potential to address the diverse system-wide impacts of\ncontingency scenarios as compared to traditional local schemes. However, due to\nthe fast cascading propagation of initial contingencies, real-time OLS\nsolutions are challenging to attain in large systems with high computation and\ncommunication needs. In this paper, we propose a decentralized design that\nleverages offline training of a neural network (NN) model for individual load\ncenters to autonomously construct the OLS solutions from locally available\nmeasurements. Our learning-for-OLS approach can greatly reduce the computation\nand communication needs during online emergency responses, thus preventing the\ncascading propagation of contingencies for enhanced power grid resilience.\nNumerical studies on both the IEEE 118-bus system and a synthetic Texas\n2000-bus system have demonstrated the efficiency and effectiveness of our\nscalable OLS learning design for timely power system emergency operations.\n","authors":["Yuqi Zhou","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2405.05521v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19214v6","updated":"2025-02-11T01:38:53Z","published":"2024-09-28T02:45:14Z","title":"Group & Reweight: A Novel Cost-Sensitive Approach to Mitigating Class\n  Imbalance in Network Traffic Classification","summary":"  Internet services have led to the eruption of network traffic, and machine\nlearning on these Internet data has become an indispensable tool, especially\nwhen the application is risk-sensitive. This paper focuses on network traffic\nclassification in the presence of severe class imbalance. Such a distributional\ntrait mostly drifts the optimal decision boundary and results in an\nunsatisfactory solution. This raises safety concerns in the network traffic\nfield when previous class imbalance methods hardly deal with numerous minority\nmalicious classes. To alleviate these effects, we design a group & reweight\nstrategy for alleviating class imbalance. Inspired by the group\ndistributionally optimization framework, our approach heuristically clusters\nclasses into groups, iteratively updates the non-parametric weights for\nseparate classes, and optimizes the learning model by minimizing reweighted\nlosses. We theoretically interpret the optimization process from a Stackelberg\ngame and perform extensive experiments on typical benchmarks. Results show that\nour approach can not only suppress the negative effect of class imbalance but\nalso improve the comprehensive performance in prediction.\n","authors":["Wumei Du","Dong Liang","Yiqin Lv","Xingxing Liang","Guanlin Wu","Qi Wang","Zheng Xie"],"pdf_url":"https://arxiv.org/pdf/2409.19214v6.pdf","comment":"21 pages, 10 figures, 7 tables"},{"id":"http://arxiv.org/abs/2502.07171v1","updated":"2025-02-11T01:33:35Z","published":"2025-02-11T01:33:35Z","title":"Enhancing Robustness Of Digital Shadow For CO2 Storage Monitoring With\n  Augmented Rock Physics Modeling","summary":"  To meet climate targets, the IPCC underscores the necessity of technologies\ncapable of removing gigatonnes of CO2 annually, with Geological Carbon Storage\n(GCS) playing a central role. GCS involves capturing CO2 and injecting it into\ndeep geological formations for long-term storage, requiring precise monitoring\nto ensure containment and prevent leakage. Time-lapse seismic imaging is\nessential for tracking CO2 migration but often struggles to capture the\ncomplexities of multi-phase subsurface flow. Digital Shadows (DS), leveraging\nmachine learning-driven data assimilation techniques such as nonlinear Bayesian\nfiltering and generative AI, provide a more detailed, uncertainty-aware\nmonitoring approach. By incorporating uncertainties in reservoir properties, DS\nframeworks improve CO2 migration forecasts, reducing risks in GCS operations.\nHowever, data assimilation depends on assumptions regarding reservoir\nproperties, rock physics models, and initial conditions, which, if inaccurate,\ncan compromise prediction reliability. This study demonstrates that augmenting\nforecast ensembles with diverse rock physics models mitigates the impact of\nincorrect assumptions and improves predictive accuracy, particularly in\ndifferentiating uniform versus patchy saturation models.\n","authors":["Abhinav Prakash Gahlot","Felix J. Herrmann"],"pdf_url":"https://arxiv.org/pdf/2502.07171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07169v1","updated":"2025-02-11T01:25:57Z","published":"2025-02-11T01:25:57Z","title":"Advancing Geological Carbon Storage Monitoring With 3d Digital Shadow\n  Technology","summary":"  Geological Carbon Storage (GCS) is a key technology for achieving global\nclimate goals by capturing and storing CO2 in deep geological formations. Its\neffectiveness and safety rely on accurate monitoring of subsurface CO2\nmigration using advanced time-lapse seismic imaging. A Digital Shadow framework\nintegrates field data, including seismic and borehole measurements, to track\nCO2 saturation over time. Machine learning-assisted data assimilation\ntechniques, such as generative AI and nonlinear ensemble Bayesian filtering,\nupdate a digital model of the CO2 plume while incorporating uncertainties in\nreservoir properties. Compared to 2D approaches, 3D monitoring enhances the\nspatial accuracy of GCS assessments, capturing the full extent of CO2\nmigration. This study extends the uncertainty-aware 2D Digital Shadow framework\nby incorporating 3D seismic imaging and reservoir modeling, improving\ndecision-making and risk mitigation in CO2 storage projects.\n","authors":["Abhinav Prakash Gahlot","Rafael Orozco","Felix J. Herrmann"],"pdf_url":"https://arxiv.org/pdf/2502.07169v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07166v1","updated":"2025-02-11T01:20:32Z","published":"2025-02-11T01:20:32Z","title":"Bayesian Optimization for Building Social-Influence-Free Consensus","summary":"  We introduce Social Bayesian Optimization (SBO), a vote-efficient algorithm\nfor consensus-building in collective decision-making. In contrast to\nsingle-agent scenarios, collective decision-making encompasses group dynamics\nthat may distort agents' preference feedback, thereby impeding their capacity\nto achieve a social-influence-free consensus -- the most preferable decision\nbased on the aggregated agent utilities. We demonstrate that under mild\nrationality axioms, reaching social-influence-free consensus using noisy\nfeedback alone is impossible. To address this, SBO employs a dual voting\nsystem: cheap but noisy public votes (e.g., show of hands in a meeting), and\nmore accurate, though expensive, private votes (e.g., one-to-one interview). We\nmodel social influence using an unknown social graph and leverage the dual\nvoting system to efficiently learn this graph. Our theoretical findigns show\nthat social graph estimation converges faster than the black-box estimation of\nagents' utilities, allowing us to reduce reliance on costly private votes early\nin the process. This enables efficient consensus-building primarily through\nnoisy public votes, which are debiased based on the estimated social graph to\ninfer social-influence-free feedback. We validate the efficacy of SBO across\nmultiple real-world applications, including thermal comfort, team building,\ntravel negotiation, and energy trading collaboration.\n","authors":["Masaki Adachi","Siu Lun Chau","Wenjie Xu","Anurag Singh","Michael A. Osborne","Krikamol Muandet"],"pdf_url":"https://arxiv.org/pdf/2502.07166v1.pdf","comment":"50 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.07164v1","updated":"2025-02-11T01:03:33Z","published":"2025-02-11T01:03:33Z","title":"Does Training on Synthetic Data Make Models Less Robust?","summary":"  An increasingly common practice is to train large language models (LLMs)\nusing synthetic data. Often this synthetic data is produced by the same or\nsimilar LLMs as those it is being used to train. This raises the question of\nwhether the synthetic data might in fact exacerbate certain \"blindspots\" by\nreinforcing heuristics that the LLM already encodes. In this paper, we conduct\nsimulated experiments on the natural language inference (NLI) task with\nLlama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted\nevaluation set designed to measure the presence of specific heuristic\nstrategies for NLI, as our \"blindspot\" task. Our goal is to determine whether\nperformance disparities between the general and blind spot tasks emerge. Our\nresults indicate that synthetic data does not reinforce blindspots in the way\nwe expected. Specifically, we see that, while fine-tuning with synthetic data\ndoesn't necessarily reduce the use of the heuristic, it also does not make it\nworse as we hypothesized.\n","authors":["Lingze Zhang","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2502.07164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02322v2","updated":"2025-02-11T00:53:58Z","published":"2024-09-03T22:31:57Z","title":"TimeDiT: General-purpose Diffusion Transformers for Time Series\n  Foundation Model","summary":"  Foundation models, particularly Large Language Models (LLMs), have\nrevolutionized text and video processing, yet time series data presents\ndistinct challenges for such approaches due to domain-specific features such as\nmissing values, multi-resolution characteristics, etc. Furthermore, the\nde-facto autoregressive transformers tend to learn deterministic temporal\ndependencies within pre-trained data while overlooking inherent uncertainties\nand lacking integration of physical constraints. In this paper, we introduce\nTimeDiT, a diffusion transformer model that synergistically combines\ntransformer-based temporal dependency learning with diffusion-based\nprobabilistic sampling. TimeDiT employs a unified masking mechanism to\nharmonize the training and inference process across diverse tasks while\nintroducing a theoretically grounded, finetuning-free model editing strategy\nthat enables flexible integration of external knowledge during sampling.\nAcknowledging the challenges of unifying multiple downstream tasks under a\nsingle model, our systematic evaluation demonstrates TimeDiT's effectiveness\nboth in fundamental tasks, i.e., forecasting and imputation, through\nzero-shot/fine-tuning; and in domain tasks, i.e., multi-resolution forecasting,\nanomaly detection, and data generation, establishing it as a\n\\textit{proto-foundation model} that bridges the gap between general-purpose\nand domain-specific models.\n","authors":["Defu Cao","Wen Ye","Yizhou Zhang","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2409.02322v2.pdf","comment":"31 Pages, 11 Figures, 22 Tables. First present at ICML 2024 Workshop\n  on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2502.07158v1","updated":"2025-02-11T00:53:36Z","published":"2025-02-11T00:53:36Z","title":"Early Risk Prediction of Pediatric Cardiac Arrest from Electronic Health\n  Records via Multimodal Fused Transformer","summary":"  Early prediction of pediatric cardiac arrest (CA) is critical for timely\nintervention in high-risk intensive care settings. We introduce PedCA-FT, a\nnovel transformer-based framework that fuses tabular view of EHR with the\nderived textual view of EHR to fully unleash the interactions of\nhigh-dimensional risk factors and their dynamics. By employing dedicated\ntransformer modules for each modality view, PedCA-FT captures complex temporal\nand contextual patterns to produce robust CA risk estimates. Evaluated on a\ncurated pediatric cohort from the CHOA-CICU database, our approach outperforms\nten other artificial intelligence models across five key performance metrics\nand identifies clinically meaningful risk factors. These findings underscore\nthe potential of multimodal fusion techniques to enhance early CA detection and\nimprove patient care.\n","authors":["Jiaying Lu","Stephanie R. Brown","Songyuan Liu","Shifan Zhao","Kejun Dong","Del Bold","Michael Fundora","Alaa Aljiffry","Alex Fedorov","Jocelyn Grunwell","Xiao Hu"],"pdf_url":"https://arxiv.org/pdf/2502.07158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07834v1","updated":"2025-02-11T00:53:15Z","published":"2025-02-11T00:53:15Z","title":"MEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for\n  Fully-Utilized In-Memory Computing Architectures","summary":"  The implementation of Hyperdimensional Computing (HDC) on In-Memory Computing\n(IMC) architectures faces significant challenges due to the mismatch between\nhighdimensional vectors and IMC array sizes, leading to inefficient memory\nutilization and increased computation cycles. This paper presents MEMHD, a\nMemory-Efficient Multi-centroid HDC framework designed to address these\nchallenges. MEMHD introduces a clustering-based initialization method and\nquantization aware iterative learning for multi-centroid associative memory.\nThrough these approaches and its overall architecture, MEMHD achieves a\nsignificant reduction in memory requirements while maintaining or improving\nclassification accuracy. Our approach achieves full utilization of IMC arrays\nand enables one-shot (or few-shot) associative search. Experimental results\ndemonstrate that MEMHD outperforms state-of-the-art binary HDC models,\nachieving up to 13.69% higher accuracy with the same memory usage, or 13.25x\nmore memory efficiency at the same accuracy level. Moreover, MEMHD reduces\ncomputation cycles by up to 80x and array usage by up to 71x compared to\nbaseline IMC mapping methods when mapped to 128x128 IMC arrays, while\nsignificantly improving energy and computation cycle efficiency.\n","authors":["Do Yeong Kang","Yeong Hwan Oh","Chanwook Hwang","Jinhee Kim","Kang Eun Jeon","Jong Hwan Ko"],"pdf_url":"https://arxiv.org/pdf/2502.07834v1.pdf","comment":"Accepted to appear at DATE 2025"},{"id":"http://arxiv.org/abs/2502.07154v1","updated":"2025-02-11T00:33:31Z","published":"2025-02-11T00:33:31Z","title":"Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting\n  Confidence Improves Mathematical Reasoning","summary":"  Recent progress in large language models (LLMs) highlights the power of\nscaling test-time compute to achieve strong performance on complex tasks, such\nas mathematical reasoning and code generation. This raises a critical question:\nhow should model training be modified to optimize performance under a\nsubsequent test-time compute strategy and budget? To explore this, we focus on\npass@N, a simple test-time strategy that searches for a correct answer in $N$\nindependent samples. We show, surprisingly, that training with cross-entropy\n(CE) loss can be ${\\it misaligned}$ with pass@N in that pass@N accuracy ${\\it\ndecreases}$ with longer training. We explain the origins of this misalignment\nin terms of model overconfidence induced by CE, and experimentally verify our\nprediction of overconfidence as an impediment to scaling test-time compute via\npass@N. Furthermore we suggest a principled, modified training loss that is\nbetter aligned to pass@N by limiting model confidence and rescuing pass@N test\nperformance. Our algorithm demonstrates improved mathematical reasoning on MATH\nand MiniF2F benchmarks under several scenarios: (1) providing answers to math\nquestions; and (2) proving theorems by searching over proof trees of varying\nshapes. Overall our work underscores the importance of co-designing two\ntraditionally separate phases of LLM development: training-time protocols and\ntest-time search and reasoning strategies.\n","authors":["Feng Chen","Allan Raventos","Nan Cheng","Surya Ganguli","Shaul Druckmann"],"pdf_url":"https://arxiv.org/pdf/2502.07154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07153v1","updated":"2025-02-11T00:29:55Z","published":"2025-02-11T00:29:55Z","title":"Feature Importance Depends on Properties of the Data: Towards Choosing\n  the Correct Explanations for Your Data and Decision Trees based Models","summary":"  In order to ensure the reliability of the explanations of machine learning\nmodels, it is crucial to establish their advantages and limits and in which\ncase each of these methods outperform. However, the current understanding of\nwhen and how each method of explanation can be used is insufficient. To fill\nthis gap, we perform a comprehensive empirical evaluation by synthesizing\nmultiple datasets with the desired properties. Our main objective is to assess\nthe quality of feature importance estimates provided by local explanation\nmethods, which are used to explain predictions made by decision tree-based\nmodels. By analyzing the results obtained from synthetic datasets as well as\npublicly available binary classification datasets, we observe notable\ndisparities in the magnitude and sign of the feature importance estimates\ngenerated by these methods. Moreover, we find that these estimates are\nsensitive to specific properties present in the data. Although some model\nhyper-parameters do not significantly influence feature importance assignment,\nit is important to recognize that each method of explanation has limitations in\nspecific contexts. Our assessment highlights these limitations and provides\nvaluable insight into the suitability and reliability of different explanatory\nmethods in various scenarios.\n","authors":["C√©lia Wafa Ayad","Thomas Bonnier","Benjamin Bosch","Sonali Parbhoo","Jesse Read"],"pdf_url":"https://arxiv.org/pdf/2502.07153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07151v1","updated":"2025-02-11T00:28:24Z","published":"2025-02-11T00:28:24Z","title":"Conditional Distribution Quantization in Machine Learning","summary":"  Conditional expectation \\mathbb{E}(Y \\mid X) often fails to capture the\ncomplexity of multimodal conditional distributions \\mathcal{L}(Y \\mid X). To\naddress this, we propose using n-point conditional quantizations--functional\nmappings of X that are learnable via gradient descent--to approximate\n\\mathcal{L}(Y \\mid X). This approach adapts Competitive Learning Vector\nQuantization (CLVQ), tailored for conditional distributions. It goes beyond\nsingle-valued predictions by providing multiple representative points that\nbetter reflect multimodal structures. It enables the approximation of the true\nconditional law in the Wasserstein distance. The resulting framework is\ntheoretically grounded and useful for uncertainty quantification and multimodal\ndata generation tasks. For example, in computer vision inpainting tasks,\nmultiple plausible reconstructions may exist for the same partially observed\ninput image X. We demonstrate the effectiveness of our approach through\nexperiments on synthetic and real-world datasets.\n","authors":["Blaise Delattre","Sylvain Delattre","Alexandre V√©rine","Alexandre Allauzen"],"pdf_url":"https://arxiv.org/pdf/2502.07151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16374v2","updated":"2025-02-11T00:26:45Z","published":"2025-01-23T06:20:33Z","title":"SAFR: Neuron Redistribution for Interpretability","summary":"  Superposition refers to encoding representations of multiple features within\na single neuron, which is common in deep neural networks. This property allows\nneurons to combine and represent multiple features, enabling the model to\ncapture intricate information and handle complex tasks. Despite promising\nperformance, the model's interpretability has been diminished. This paper\npresents a novel approach to enhance model interpretability by regularizing\nfeature superposition. We introduce SAFR, which simply applies regularizations\nto the loss function to promote monosemantic representations for important\ntokens while encouraging polysemanticity for correlated token pairs, where\nimportant tokens and correlated token pairs are identified via VMASK and\nattention weights respectively. We evaluate SAFR with a transformer model on\ntwo classification tasks. Experiments demonstrate the effectiveness of SAFR in\nimproving model interpretability without compromising prediction performance.\nBesides, SAFR provides explanations by visualizing the neuron allocation within\nthe intermediate layers.\n","authors":["Ruidi Chang","Chunyuan Deng","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2501.16374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07832v1","updated":"2025-02-11T00:21:40Z","published":"2025-02-11T00:21:40Z","title":"SHARP: Accelerating Language Model Inference by SHaring Adjacent layers\n  with Recovery Parameters","summary":"  While Large language models (LLMs) have advanced natural language processing\ntasks, their growing computational and memory demands make deployment on\nresource-constrained devices like mobile phones increasingly challenging. In\nthis paper, we propose SHARP (SHaring Adjacent Layers with Recovery\nParameters), a novel approach to accelerate LLM inference by sharing parameters\nacross adjacent layers, thus reducing memory load overhead, while introducing\nlow-rank recovery parameters to maintain performance. Inspired by observations\nthat consecutive layers have similar outputs, SHARP employs a two-stage\nrecovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT).\nThe SLW stage aligns the outputs of the shared layers using L_2 loss, providing\na good initialization for the following SFT stage to further restore the model\nperformance. Extensive experiments demonstrate that SHARP can recover the\nmodel's perplexity on various in-distribution tasks using no more than 50k\nfine-tuning data while reducing the number of stored MLP parameters by 38% to\n65%. We also conduct several ablation studies of SHARP and show that replacing\nlayers towards the later parts of the model yields better performance\nretention, and that different recovery parameterizations perform similarly when\nparameter counts are matched. Furthermore, SHARP saves 42.8% in model storage\nand reduces the total inference time by 42.2% compared to the original\nLlama2-7b model on mobile devices. Our results highlight SHARP as an efficient\nsolution for reducing inference costs in deploying LLMs without the need for\npretraining-scale resources.\n","authors":["Yiping Wang","Hanxian Huang","Yifang Chen","Jishen Zhao","Simon Shaolei Du","Yuandong Tian"],"pdf_url":"https://arxiv.org/pdf/2502.07832v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2502.02917v2","updated":"2025-02-11T00:20:37Z","published":"2025-02-05T06:26:49Z","title":"Interactive Symbolic Regression through Offline Reinforcement Learning:\n  A Co-Design Framework","summary":"  Symbolic Regression (SR) holds great potential for uncovering underlying\nmathematical and physical relationships from observed data. However, the vast\ncombinatorial space of possible expressions poses significant challenges for\nboth online search methods and pre-trained transformer models. Additionally,\ncurrent state-of-the-art approaches typically do not consider the integration\nof domain experts' prior knowledge and do not support iterative interactions\nwith the model during the equation discovery process. To address these\nchallenges, we propose the Symbolic Q-network (Sym-Q), an advanced interactive\nframework for large-scale symbolic regression. Unlike previous large-scale\ntransformer-based SR approaches, Sym-Q leverages reinforcement learning without\nrelying on a transformer-based decoder. This formulation allows the agent to\nlearn through offline reinforcement learning using any type of tree encoder,\nenabling more efficient training and inference. Furthermore, we propose a\nco-design mechanism, where the reinforcement learning-based Sym-Q facilitates\neffective interaction with domain experts at any stage of the equation\ndiscovery process. Users can dynamically modify generated nodes of the\nexpression, collaborating with the agent to tailor the mathematical expression\nto best fit the problem and align with the assumed physical laws, particularly\nwhen there is prior partial knowledge of the expected behavior. Our experiments\ndemonstrate that the pre-trained Sym-Q surpasses existing SR algorithms on the\nchallenging SSDNC benchmark. Moreover, we experimentally show on real-world\ncases that its performance can be further enhanced by the interactive co-design\nmechanism, with Sym-Q achieving greater performance gains than other\nstate-of-the-art models. Our reproducible code is available at\nhttps://github.com/EPFL-IMOS/Sym-Q.\n","authors":["Yuan Tian","Wenqi Zhou","Michele Viscione","Hao Dong","David Kammer","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2502.02917v2.pdf","comment":"This work should not be a new submission but instead should be an\n  update to my existing article, arXiv:2402.05306"},{"id":"http://arxiv.org/abs/2502.02014v2","updated":"2025-02-11T00:19:47Z","published":"2025-02-04T05:04:15Z","title":"Analytical Lyapunov Function Discovery: An RL-based Generative Approach","summary":"  Despite advances in learning-based methods, finding valid Lyapunov functions\nfor nonlinear dynamical systems remains challenging. Current neural network\napproaches face two main issues: challenges in scalable verification and\nlimited interpretability. To address these, we propose an end-to-end framework\nusing transformers to construct analytical Lyapunov functions (local), which\nsimplifies formal verification, enhances interpretability, and provides\nvaluable insights for control engineers. Our framework consists of a\ntransformer-based trainer that generates candidate Lyapunov functions and a\nfalsifier that verifies candidate expressions and refines the model via\nrisk-seeking policy gradient. Unlike Alfarano et al. (2024), which utilizes\npre-training and seeks global Lyapunov functions for low-dimensional systems,\nour model is trained from scratch via reinforcement learning (RL) and succeeds\nin finding local Lyapunov functions for high-dimensional and non-polynomial\nsystems. Given the analytical nature of the candidates, we employ efficient\noptimization methods for falsification during training and formal verification\ntools for the final verification. We demonstrate the efficiency of our approach\non a range of nonlinear dynamical systems with up to ten dimensions and show\nthat it can discover Lyapunov functions not previously identified in the\ncontrol literature.\n","authors":["Haohan Zou","Jie Feng","Hao Zhao","Yuanyuan Shi"],"pdf_url":"https://arxiv.org/pdf/2502.02014v2.pdf","comment":"26 pages (8+18), preprint for discussion. Haohan and Jie contribute\n  equally"},{"id":"http://arxiv.org/abs/2502.07141v1","updated":"2025-02-11T00:12:04Z","published":"2025-02-11T00:12:04Z","title":"Small steps no more: Global convergence of stochastic gradient bandits\n  for arbitrary learning rates","summary":"  We provide a new understanding of the stochastic gradient bandit algorithm by\nshowing that it converges to a globally optimal policy almost surely using\n\\emph{any} constant learning rate. This result demonstrates that the stochastic\ngradient algorithm continues to balance exploration and exploitation\nappropriately even in scenarios where standard smoothness and noise control\nassumptions break down. The proofs are based on novel findings about action\nsampling rates and the relationship between cumulative progress and noise, and\nextend the current understanding of how simple stochastic gradient methods\nbehave in bandit settings.\n","authors":["Jincheng Mei","Bo Dai","Alekh Agarwal","Sharan Vaswani","Anant Raj","Csaba Szepesvari","Dale Schuurmans"],"pdf_url":"https://arxiv.org/pdf/2502.07141v1.pdf","comment":"Updated version for a paper published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2502.07830v1","updated":"2025-02-11T00:11:13Z","published":"2025-02-11T00:11:13Z","title":"Captured by Captions: On Memorization and its Mitigation in CLIP Models","summary":"  Multi-modal models, such as CLIP, have demonstrated strong performance in\naligning visual and textual representations, excelling in tasks like image\nretrieval and zero-shot classification. Despite this success, the mechanisms by\nwhich these models utilize training data, particularly the role of\nmemorization, remain unclear. In uni-modal models, both supervised and\nself-supervised, memorization has been shown to be essential for\ngeneralization. However, it is not well understood how these findings would\napply to CLIP, which incorporates elements from both supervised learning via\ncaptions that provide a supervisory signal similar to labels, and from\nself-supervised learning via the contrastive objective. To bridge this gap in\nunderstanding, we propose a formal definition of memorization in CLIP (CLIPMem)\nand use it to quantify memorization in CLIP models. Our results indicate that\nCLIP's memorization behavior falls between the supervised and self-supervised\nparadigms, with \"mis-captioned\" samples exhibiting highest levels of\nmemorization. Additionally, we find that the text encoder contributes more to\nmemorization than the image encoder, suggesting that mitigation strategies\nshould focus on the text domain. Building on these insights, we propose\nmultiple strategies to reduce memorization while at the same time improving\nutility--something that had not been shown before for traditional learning\nparadigms where reducing memorization typically results in utility decrease.\n","authors":["Wenhao Wang","Adam Dziedzic","Grace C. Kim","Michael Backes","Franziska Boenisch"],"pdf_url":"https://arxiv.org/pdf/2502.07830v1.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07139v1","updated":"2025-02-11T00:09:45Z","published":"2025-02-11T00:09:45Z","title":"Language-TPP: Integrating Temporal Point Processes with Language Models\n  for Event Analysis","summary":"  Temporal Point Processes (TPPs) have been widely used for event sequence\nmodeling, but they often struggle to incorporate rich textual event\ndescriptions effectively. Conversely, while Large Language Models (LLMs) have\nbeen shown remarkable capabilities in processing textual data, they lack\nmechanisms for handling temporal dynamics. To bridge this gap, we introduce\nLanguage-TPP, a unified framework that integrates TPPs with LLMs for enhanced\nevent sequence modeling. Language-TPP introduces a novel temporal encoding\nmechanism that converts continuous time intervals into specialized byte-tokens,\nenabling seamless integration with standard LLM architectures. This approach\nallows Language-TPP to achieve state-of-the-art performance across multiple TPP\ntasks, including event time prediction, type prediction, and intensity\nestimation, on five datasets. Additionally, we demonstrate that incorporating\ntemporal information significantly improves the quality of generated event\ndescriptions.\n","authors":["Quyu Kong","Yixuan Zhang","Yang Liu","Panrong Tong","Enqi Liu","Feng Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07138v1","updated":"2025-02-11T00:07:40Z","published":"2025-02-11T00:07:40Z","title":"Towards a Robust Framework for Multimodal Hate Detection: A Study on\n  Video vs. Image-based Content","summary":"  Social media platforms enable the propagation of hateful content across\ndifferent modalities such as textual, auditory, and visual, necessitating\neffective detection methods. While recent approaches have shown promise in\nhandling individual modalities, their effectiveness across different modality\ncombinations remains unexplored. This paper presents a systematic analysis of\nfusion-based approaches for multimodal hate detection, focusing on their\nperformance across video and image-based content. Our comprehensive evaluation\nreveals significant modality-specific limitations: while simple embedding\nfusion achieves state-of-the-art performance on video content (HateMM dataset)\nwith a 9.9% points F1-score improvement, it struggles with complex image-text\nrelationships in memes (Hateful Memes dataset). Through detailed ablation\nstudies and error analysis, we demonstrate how current fusion approaches fail\nto capture nuanced cross-modal interactions, particularly in cases involving\nbenign confounders. Our findings provide crucial insights for developing more\nrobust hate detection systems and highlight the need for modality-specific\narchitectural considerations. The code is available at\nhttps://github.com/gak97/Video-vs-Meme-Hate.\n","authors":["Girish A. Koushik","Diptesh Kanojia","Helen Treharne"],"pdf_url":"https://arxiv.org/pdf/2502.07138v1.pdf","comment":"Accepted to the MM4SG Workshop at the WebConf 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.00950v3","updated":"2025-02-11T18:24:41Z","published":"2025-02-02T23:04:55Z","title":"Fast Audio Codec Identification Using Overlapping LCS","summary":"  Audio data are widely exchanged over telecommunications networks. Due to the\nlimitations of network resources, these data are typically compressed before\ntransmission. Various methods are available for compressing audio data. To\naccess such audio information, it is first necessary to identify the codec used\nfor compression. One of the most effective approaches for audio codec\nidentification involves analyzing the content of received packets. In these\nmethods, statistical features extracted from the packets are utilized to\ndetermine the codec employed. This paper proposes a novel method for audio\ncodec classification based on features derived from the overlapped longest\ncommon sub-string and sub-sequence (LCS). The simulation results, which\nachieved an accuracy of 97% for 8 KB packets, demonstrate the superiority of\nthe proposed method over conventional approaches. This method divides each 8 KB\npacket into fifteen 1 KB packets with a 50% overlap. The results indicate that\nthis division has no significant impact on the simulation outcomes, while\nsignificantly speeding up the feature extraction, being eight times faster than\nthe traditional method for extracting LCS features.\n","authors":["Farzane Jafari"],"pdf_url":"https://arxiv.org/pdf/2502.00950v3.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.07711v1","updated":"2025-02-11T17:10:31Z","published":"2025-02-11T17:10:31Z","title":"RenderBox: Expressive Performance Rendering with Text Control","summary":"  Expressive music performance rendering involves interpreting symbolic scores\nwith variations in timing, dynamics, articulation, and instrument-specific\ntechniques, resulting in performances that capture musical can emotional\nintent. We introduce RenderBox, a unified framework for text-and-score\ncontrolled audio performance generation across multiple instruments, applying\ncoarse-level controls through natural language descriptions and granular-level\ncontrols using music scores. Based on a diffusion transformer architecture and\ncross-attention joint conditioning, we propose a curriculum-based paradigm that\ntrains from plain synthesis to expressive performance, gradually incorporating\ncontrollable factors such as speed, mistakes, and style diversity.\n  RenderBox achieves high performance compared to baseline models across key\nmetrics such as FAD and CLAP, and also tempo and pitch accuracy under different\nprompting tasks. Subjective evaluation further demonstrates that RenderBox is\nable to generate controllable expressive performances that sound natural and\nmusically engaging, aligning well with prompts and intent.\n","authors":["Huan Zhang","Akira Maezawa","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2502.07711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07411v1","updated":"2025-02-11T09:45:06Z","published":"2025-02-11T09:45:06Z","title":"EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering","summary":"  We introduce EgoTextVQA, a novel and rigorously constructed benchmark for\negocentric QA assistance involving scene text. EgoTextVQA contains 1.5K\nego-view videos and 7K scene-text aware questions that reflect real-user needs\nin outdoor driving and indoor house-keeping activities. The questions are\ndesigned to elicit identification and reasoning on scene text in an egocentric\nand dynamic environment. With EgoTextVQA, we comprehensively evaluate 10\nprominent multimodal large language models. Currently, all models struggle, and\nthe best results (Gemini 1.5 Pro) are around 33% accuracy, highlighting the\nsevere deficiency of these techniques in egocentric QA assistance. Our further\ninvestigations suggest that precise temporal grounding and multi-frame\nreasoning, along with high resolution and auxiliary scene-text inputs, are key\nfor better performance. With thorough analyses and heuristic suggestions, we\nhope EgoTextVQA can serve as a solid testbed for research in egocentric\nscene-text QA assistance.\n","authors":["Sheng Zhou","Junbin Xiao","Qingyun Li","Yicong Li","Xun Yang","Dan Guo","Meng Wang","Tat-Seng Chua","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2502.07411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07160v1","updated":"2025-02-11T00:56:44Z","published":"2025-02-11T00:56:44Z","title":"HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates","summary":"  Image compression under ultra-low bitrates remains challenging for both\nconventional learned image compression (LIC) and generative vector-quantized\n(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy\nquantization, while generative VQ modeling gives poor fidelity due to the\nmismatch between learned generative priors and specific inputs. In this work,\nwe propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream\nframework that utilizes both generative VQ-modeling and diffusion models, as\nwell as conventional LIC, to achieve both high fidelity and high perceptual\nquality. Different from previous hybrid methods that directly use pre-trained\nLIC models to generate low-quality fidelity-preserving information from heavily\nquantized latent, we use diffusion models to extract high-quality complimentary\nfidelity information from the ground-truth input, which can enhance the system\nperformance in several aspects: improving indices map prediction, enhancing the\nfidelity-preserving output of the LIC stream, and refining conditioned image\nreconstruction with VQ-latent correction. In addition, our diffusion model is\nbased on a dense representative vector (DRV), which is lightweight with very\nsimple sampling schedulers. Extensive experiments demonstrate that our\nHDCompression outperforms the previous conventional LIC, generative\nVQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative\nvisualization, providing balanced robust compression performance at ultra-low\nbitrates.\n","authors":["Lei Lu","Yize Li","Yanzhi Wang","Wei Wang","Wei Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.07160v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2404.03161v2","updated":"2025-02-11T00:45:46Z","published":"2024-04-04T02:22:37Z","title":"BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro\n  QR Codes","summary":"  This paper introduces BioVL-QR, a biochemical vision-and-language dataset\ncomprising 23 egocentric experiment videos, corresponding protocols, and\nvision-and-language alignments. A major challenge in understanding biochemical\nvideos is detecting equipment, reagents, and containers because of the\ncluttered environment and indistinguishable objects. Previous studies assumed\nmanual object annotation, which is costly and time-consuming. To address the\nissue, we focus on Micro QR Codes. However, detecting objects using only Micro\nQR Codes is still difficult due to blur and occlusion caused by object\nmanipulation. To overcome this, we propose an object labeling method combining\na Micro QR Code detector with an off-the-shelf hand object detector. As an\napplication of the method and BioVL-QR, we tackled the task of localizing the\nprocedural steps in an instructional video. The experimental results show that\nusing Micro QR Codes and our method improves biochemical video understanding.\nData and code are available through https://nishi10mo.github.io/BioVL-QR/\n","authors":["Tomohiro Nishimoto","Taichi Nishimura","Koki Yamamoto","Keisuke Shirai","Hirotaka Kameko","Yuto Haneji","Tomoya Yoshida","Keiya Kajimura","Taiyu Cui","Chihiro Nishiwaki","Eriko Daikoku","Natsuko Okuda","Fumihito Ono","Shinsuke Mori"],"pdf_url":"https://arxiv.org/pdf/2404.03161v2.pdf","comment":"6 pages"}]},"2025-02-10T00:00:00Z":{"Machine Learning":[{"id":"http://arxiv.org/abs/2502.07135v1","updated":"2025-02-10T23:56:08Z","published":"2025-02-10T23:56:08Z","title":"One-Shot Learning for k-SAT","summary":"  Consider a $k$-SAT formula $\\Phi$ where every variable appears at most $d$\ntimes, and let $\\sigma$ be a satisfying assignment of $\\Phi$ sampled\nproportionally to $e^{\\beta m(\\sigma)}$ where $m(\\sigma)$ is the number of\nvariables set to true and $\\beta$ is a real parameter. Given $\\Phi$ and\n$\\sigma$, can we learn the value of $\\beta$ efficiently?\n  This problem falls into a recent line of works about single-sample\n(\"one-shot\") learning of Markov random fields. The $k$-SAT setting we consider\nhere was recently studied by Galanis, Kandiros, and Kalavasis (SODA'24) where\nthey showed that single-sample learning is possible when roughly $d\\leq\n2^{k/6.45}$ and impossible when $d\\geq (k+1) 2^{k-1}$. Crucially, for their\nimpossibility results they used the existence of unsatisfiable instances which,\naside from the gap in $d$, left open the question of whether the feasibility\nthreshold for one-shot learning is dictated by the satisfiability threshold of\n$k$-SAT formulas of bounded degree.\n  Our main contribution is to answer this question negatively. We show that\none-shot learning for $k$-SAT is infeasible well below the satisfiability\nthreshold; in fact, we obtain impossibility results for degrees $d$ as low as\n$k^2$ when $\\beta$ is sufficiently large, and bootstrap this to small values of\n$\\beta$ when $d$ scales exponentially with $k$, via a probabilistic\nconstruction. On the positive side, we simplify the analysis of the learning\nalgorithm and obtain significantly stronger bounds on $d$ in terms of $\\beta$.\nIn particular, for the uniform case $\\beta\\rightarrow 0$ that has been studied\nextensively in the sampling literature, our analysis shows that learning is\npossible under the condition $d\\lesssim 2^{k/2}$. This is nearly optimal (up to\nconstant factors) in the sense that it is known that sampling a\nuniformly-distributed satisfying assignment is NP-hard for $d\\gtrsim 2^{k/2}$.\n","authors":["Andreas Galanis","Leslie Ann Goldberg","Xusheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.07135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07130v1","updated":"2025-02-10T23:49:06Z","published":"2025-02-10T23:49:06Z","title":"Unconstrained Body Recognition at Altitude and Range: Comparing Four\n  Approaches","summary":"  This study presents an investigation of four distinct approaches to long-term\nperson identification using body shape. Unlike short-term re-identification\nsystems that rely on temporary features (e.g., clothing), we focus on learning\npersistent body shape characteristics that remain stable over time. We\nintroduce a body identification model based on a Vision Transformer (ViT) (Body\nIdentification from Diverse Datasets, BIDDS) and on a Swin-ViT model\n(Swin-BIDDS). We also expand on previous approaches based on the Linguistic and\nNon-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with\nimproved training. All models are trained on a large and diverse dataset of\nover 1.9 million images of approximately 5k identities across 9 databases.\nPerformance was evaluated on standard re-identification benchmark datasets\n(MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that\nincludes images at a distance (from close-range to 1000m), at altitude (from an\nunmanned aerial vehicle, UAV), and with clothing change. A comparative analysis\nacross these models provides insights into how different backbone architectures\nand input image sizes impact long-term body identification performance across\nreal-world conditions.\n","authors":["Blake A Myers","Matthew Q Hill","Veda Nandan Gandi","Thomas M Metz","Alice J O'Toole"],"pdf_url":"https://arxiv.org/pdf/2502.07130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07129v1","updated":"2025-02-10T23:48:10Z","published":"2025-02-10T23:48:10Z","title":"Fourier-enhanced Neural Networks For Systems Biology Applications","summary":"  In the field of systems biology, differential equations are commonly used to\nmodel biological systems, but solving them for large-scale and complex systems\ncan be computationally expensive. Recently, the integration of machine learning\nand mathematical modeling has offered new opportunities for scientific\ndiscoveries in biology and health. The emerging physics-informed neural network\n(PINN) has been proposed as a solution to this problem. However, PINN can be\ncomputationally expensive and unreliable for complex biological systems. To\naddress these issues, we propose the Fourier-enhanced Neural Networks for\nsystems biology (SB-FNN). SB-FNN uses an embedded Fourier neural network with\nan adaptive activation function and a cyclic penalty function to optimize the\nprediction of biological dynamics, particularly for biological systems that\nexhibit oscillatory patterns. Experimental results demonstrate that SB-FNN\nachieves better performance and is more efficient than PINN for handling\ncomplex biological models. Experimental results on cellular and population\nmodels demonstrate that SB-FNN outperforms PINN in both accuracy and\nefficiency, making it a promising alternative approach for handling complex\nbiological models. The proposed method achieved better performance on six\nbiological models and is expected to replace PINN as the most advanced method\nin systems biology.\n","authors":["Enze Xu","Minghan Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17455v2","updated":"2025-02-10T23:30:12Z","published":"2024-12-23T10:21:38Z","title":"Learning from Summarized Data: Gaussian Process Regression with Sample\n  Quasi-Likelihood","summary":"  Gaussian process regression is a powerful Bayesian nonlinear regression\nmethod. Recent research has enabled the capture of many types of observations\nusing non-Gaussian likelihoods. To deal with various tasks in spatial modeling,\nwe benefit from this development. Difficulties still arise when we can only\naccess summarized data consisting of representative features, summary\nstatistics, and data point counts. Such situations frequently occur primarily\ndue to concerns about confidentiality and management costs associated with\nspatial data. This study tackles learning and inference using only summarized\ndata within the framework of Gaussian process regression. To address this\nchallenge, we analyze the approximation errors in the marginal likelihood and\nposterior distribution that arise from utilizing representative features. We\nalso introduce the concept of sample quasi-likelihood, which facilitates\nlearning and inference using only summarized data. Non-Gaussian likelihoods\nsatisfying certain assumptions can be captured by specifying a variance\nfunction that characterizes a sample quasi-likelihood function. Theoretical and\nexperimental results demonstrate that the approximation performance is\ninfluenced by the granularity of summarized data relative to the length scale\nof covariance functions. Experiments on a real-world dataset highlight the\npracticality of our method for spatial modeling.\n","authors":["Yuta Shikuri"],"pdf_url":"https://arxiv.org/pdf/2412.17455v2.pdf","comment":"19 pages, 4 figures, 5 tables, AAAI2025"},{"id":"http://arxiv.org/abs/2402.06529v4","updated":"2025-02-10T23:28:39Z","published":"2024-02-09T16:40:59Z","title":"Introspective Planning: Aligning Robots' Uncertainty with Inherent Task\n  Ambiguity","summary":"  Large language models (LLMs) exhibit advanced reasoning skills, enabling\nrobots to comprehend natural language instructions and strategically plan\nhigh-level actions through proper grounding. However, LLM hallucination may\nresult in robots confidently executing plans that are misaligned with user\ngoals or even unsafe in critical scenarios. Additionally, inherent ambiguity in\nnatural language instructions can introduce uncertainty into the LLM's\nreasoning and planning processes.We propose introspective planning, a\nsystematic approach that align LLM's uncertainty with the inherent ambiguity of\nthe task. Our approach constructs a knowledge base containing introspective\nreasoning examples as post-hoc rationalizations of human-selected safe and\ncompliant plans, which are retrieved during deployment. Evaluations on three\ntasks, including a newly introduced safe mobile manipulation benchmark,\ndemonstrate that introspection substantially improves both compliance and\nsafety over state-of-the-art LLM-based planning methods. Furthermore, we\nempirically show that introspective planning, in combination with conformal\nprediction, achieves tighter confidence bounds, maintaining statistical success\nguarantees while minimizing unnecessary user clarification requests. The\nwebpage and code are accessible at https://introplan.github.io.\n","authors":["Kaiqu Liang","Zixu Zhang","Jaime Fern√°ndez Fisac"],"pdf_url":"https://arxiv.org/pdf/2402.06529v4.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2411.09856v3","updated":"2025-02-10T23:22:48Z","published":"2024-11-15T00:31:45Z","title":"InvestESG: A multi-agent reinforcement learning benchmark for studying\n  climate investment as a social dilemma","summary":"  InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark\ndesigned to study the impact of Environmental, Social, and Governance (ESG)\ndisclosure mandates on corporate climate investments. The benchmark models an\nintertemporal social dilemma where companies balance short-term profit losses\nfrom climate mitigation efforts and long-term benefits from reducing climate\nrisk, while ESG-conscious investors attempt to influence corporate behavior\nthrough their investment decisions. Companies allocate capital across\nmitigation, greenwashing, and resilience, with varying strategies influencing\nclimate outcomes and investor preferences. We are releasing open-source\nversions of InvestESG in both PyTorch and JAX, which enable scalable and\nhardware-accelerated simulations for investigating competing incentives in\nmitigate climate change. Our experiments show that without ESG-conscious\ninvestors with sufficient capital, corporate mitigation efforts remain limited\nunder the disclosure mandate. However, when a critical mass of investors\nprioritizes ESG, corporate cooperation increases, which in turn reduces climate\nrisks and enhances long-term financial stability. Additionally, providing more\ninformation about global climate risks encourages companies to invest more in\nmitigation, even without investor involvement. Our findings align with\nempirical research using real-world data, highlighting MARL's potential to\ninform policy by providing insights into large-scale socio-economic challenges\nthrough efficient testing of alternative policy and market designs.\n","authors":["Xiaoxuan Hou","Jiayi Yuan","Joel Z. Leibo","Natasha Jaques"],"pdf_url":"https://arxiv.org/pdf/2411.09856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08074v2","updated":"2025-02-10T23:21:18Z","published":"2024-10-10T16:10:27Z","title":"Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion\n  Models","summary":"  Text-to-image diffusion models rely on massive, web-scale datasets. Training\nthem from scratch is computationally expensive, and as a result, developers\noften prefer to make incremental updates to existing models. These updates\noften compose fine-tuning steps (to learn new concepts or improve model\nperformance) with \"unlearning\" steps (to \"forget\" existing concepts, such as\ncopyrighted works or explicit content). In this work, we demonstrate a critical\nand previously unknown vulnerability that arises in this paradigm: even under\nbenign, non-adversarial conditions, fine-tuning a text-to-image diffusion model\non seemingly unrelated images can cause it to \"relearn\" concepts that were\npreviously \"unlearned.\" We comprehensively investigate the causes and scope of\nthis phenomenon, which we term concept resurgence, by performing a series of\nexperiments which compose \"concept unlearning\" with subsequent fine-tuning of\nStable Diffusion v1.4 and Stable Diffusion v2.1. Our findings underscore the\nfragility of composing incremental model updates, and raise serious new\nconcerns about current approaches to ensuring the safety and alignment of\ntext-to-image diffusion models.\n","authors":["Vinith M. Suriyakumar","Rohan Alur","Ayush Sekhari","Manish Raghavan","Ashia C. Wilson"],"pdf_url":"https://arxiv.org/pdf/2410.08074v2.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2502.07119v1","updated":"2025-02-10T23:20:59Z","published":"2025-02-10T23:20:59Z","title":"SAFE: Self-Supervised Anomaly Detection Framework for Intrusion\n  Detection","summary":"  The proliferation of IoT devices has significantly increased network\nvulnerabilities, creating an urgent need for effective Intrusion Detection\nSystems (IDS). Machine Learning-based IDS (ML-IDS) offer advanced detection\ncapabilities but rely on labeled attack data, which limits their ability to\nidentify unknown threats. Self-Supervised Learning (SSL) presents a promising\nsolution by using only normal data to detect patterns and anomalies. This paper\nintroduces SAFE, a novel framework that transforms tabular network intrusion\ndata into an image-like format, enabling Masked Autoencoders (MAEs) to learn\nrobust representations of network behavior. The features extracted by the MAEs\nare then incorporated into a lightweight novelty detector, enhancing the\neffectiveness of anomaly detection. Experimental results demonstrate that SAFE\noutperforms the state-of-the-art anomaly detection method, Scale Learning-based\nDeep Anomaly Detection method (SLAD), by up to 26.2% and surpasses the\nstate-of-the-art SSL-based network intrusion detection approach, Anomal-E, by\nup to 23.5% in F1-score.\n","authors":["Elvin Li","Zhengli Shang","Onat Gungor","Tajana Rosing"],"pdf_url":"https://arxiv.org/pdf/2502.07119v1.pdf","comment":"Accepted by the AAAI-25 Workshop on Artificial Intelligence for Cyber\n  Security (AICS)"},{"id":"http://arxiv.org/abs/2410.06303v2","updated":"2025-02-10T23:16:02Z","published":"2024-10-08T19:25:07Z","title":"Compositional Risk Minimization","summary":"  Compositional generalization is a crucial step towards developing\ndata-efficient intelligent machines that generalize in human-like ways. In this\nwork, we tackle a challenging form of distribution shift, termed compositional\nshift, where some attribute combinations are completely absent at training but\npresent in the test distribution. This shift tests the model's ability to\ngeneralize compositionally to novel attribute combinations in discriminative\ntasks. We model the data with flexible additive energy distributions, where\neach energy term represents an attribute, and derive a simple alternative to\nempirical risk minimization termed compositional risk minimization (CRM). We\nfirst train an additive energy classifier to predict the multiple attributes\nand then adjust this classifier to tackle compositional shifts. We provide an\nextensive theoretical analysis of CRM, where we show that our proposal\nextrapolates to special affine hulls of seen attribute combinations. Empirical\nevaluations on benchmark datasets confirms the improved robustness of CRM\ncompared to other methods from the literature designed to tackle various forms\nof subpopulation shifts.\n","authors":["Divyat Mahajan","Mohammad Pezeshki","Charles Arnal","Ioannis Mitliagkas","Kartik Ahuja","Pascal Vincent"],"pdf_url":"https://arxiv.org/pdf/2410.06303v2.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2502.07117v1","updated":"2025-02-10T23:14:09Z","published":"2025-02-10T23:14:09Z","title":"Choroidal image analysis for OCT image sequences with applications in\n  systemic health","summary":"  The choroid, a highly vascular layer behind the retina, is an extension of\nthe central nervous system and has parallels with the renal cortex, with blood\nflow far exceeding that of the brain and kidney. Thus, there has been growing\ninterest of choroidal blood flow reflecting physiological status of systemic\ndisease. Optical coherence tomography (OCT) enables high-resolution imaging of\nthe choroid, but conventional analysis methods remain manual or semi-automatic,\nlimiting reproducibility, standardisation and clinical utility. In this thesis,\nI develop several new methods to analyse the choroid in OCT image sequences,\nwith each successive method improving on its predecessors. I first develop two\nsemi-automatic approaches for choroid region (Gaussian Process Edge Tracing,\nGPET) and vessel (Multi-scale Median Cut Quantisation, MMCQ) analysis, which\nimprove on manual approaches but remain user-dependent. To address this, I\nintroduce DeepGPET, a deep learning-based region segmentation method which\nimproves on execution time, reproducibility, and end-user accessibility, but\nlacks choroid vessel analysis and automatic feature measurement. Improving on\nthis, I developed Choroidalyzer, a deep learning-based pipeline to segment the\nchoroidal space and vessels and generate fully automatic, clinically meaningful\nand reproducible choroidal features. I provide rigorous evaluation of these\nfour approaches and consider their potential clinical value in three\napplications into systemic health: OCTANE, assessing choroidal changes in renal\ntransplant recipients and donors; PREVENT, exploring choroidal associations\nwith Alzheimer's risk factors at mid-life; D-RISCii, assessing choroidal\nvariation and feasibility of OCT in critical care. In short, this thesis\ncontributes many open-source tools for standardised choroidal measurement and\nhighlights the choroid's potential as a biomarker in systemic health.\n","authors":["Jamie Burke"],"pdf_url":"https://arxiv.org/pdf/2502.07117v1.pdf","comment":"PhD thesis toward a doctorate degree at the University of Edinburgh.\n  PhD funded by the Medical Research Council (grant MR/N013166/1). Reviewed and\n  examined by Dr. Roly Megaw (internal) and Prof. Pearse Keane (external) in\n  December 2024 and ratified in the same month by the university. Official\n  record found here: https://era.ed.ac.uk/handle/1842/42956"},{"id":"http://arxiv.org/abs/2405.13180v2","updated":"2025-02-10T23:12:13Z","published":"2024-05-21T20:06:12Z","title":"Data Assimilation with Machine Learning Surrogate Models: A Case Study\n  with FourCastNet","summary":"  Modern data-driven surrogate models for weather forecasting provide accurate\nshort-term predictions but inaccurate and nonphysical long-term forecasts. This\npaper investigates online weather prediction using machine learning surrogates\nsupplemented with partial and noisy observations. We empirically demonstrate\nand theoretically justify that, despite the long-time instability of the\nsurrogates and the sparsity of the observations, filtering estimates can remain\naccurate in the long-time horizon. As a case study, we integrate FourCastNet, a\nweather surrogate model, within a variational data assimilation framework using\npartial, noisy ERA5 data. Our results show that filtering estimates remain\naccurate over a year-long assimilation window and provide effective initial\nconditions for forecasting tasks, including extreme event prediction.\n","authors":["Melissa Adrian","Daniel Sanz-Alonso","Rebecca Willett"],"pdf_url":"https://arxiv.org/pdf/2405.13180v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07114v1","updated":"2025-02-10T23:11:02Z","published":"2025-02-10T23:11:02Z","title":"Online Covariance Matrix Estimation in Sketched Newton Methods","summary":"  Given the ubiquity of streaming data, online algorithms have been widely used\nfor parameter estimation, with second-order methods particularly standing out\nfor their efficiency and robustness. In this paper, we study an online sketched\nNewton method that leverages a randomized sketching technique to perform an\napproximate Newton step in each iteration, thereby eliminating the\ncomputational bottleneck of second-order methods. While existing studies have\nestablished the asymptotic normality of sketched Newton methods, a consistent\nestimator of the limiting covariance matrix remains an open problem. We propose\na fully online covariance matrix estimator that is constructed entirely from\nthe Newton iterates and requires no matrix factorization. Compared to\ncovariance estimators for first-order online methods, our estimator for\nsecond-order methods is batch-free. We establish the consistency and\nconvergence rate of our estimator, and coupled with asymptotic normality\nresults, we can then perform online statistical inference for the model\nparameters based on sketched Newton methods. We also discuss the extension of\nour estimator to constrained problems, and demonstrate its superior performance\non regression problems as well as benchmark problems in the CUTEst set.\n","authors":["Wei Kuang","Mihai Anitescu","Sen Na"],"pdf_url":"https://arxiv.org/pdf/2502.07114v1.pdf","comment":"52 pages, 2 figures, 7 tables"},{"id":"http://arxiv.org/abs/2502.07111v1","updated":"2025-02-10T23:09:12Z","published":"2025-02-10T23:09:12Z","title":"Likelihood-Free Estimation for Spatiotemporal Hawkes processes with\n  missing data and application to predictive policing","summary":"  With the growing use of AI technology, many police departments use\nforecasting software to predict probable crime hotspots and allocate patrolling\nresources effectively for crime prevention. The clustered nature of crime data\nmakes self-exciting Hawkes processes a popular modeling choice. However, one\nsignificant challenge in fitting such models is the inherent missingness in\ncrime data due to non-reporting, which can bias the estimated parameters of the\npredictive model, leading to inaccurate downstream hotspot forecasts, often\nresulting in over or under-policing in various communities, especially the\nvulnerable ones. Our work introduces a Wasserstein Generative Adversarial\nNetworks (WGAN) driven likelihood-free approach to account for unreported\ncrimes in Spatiotemporal Hawkes models. We demonstrate through empirical\nanalysis how this methodology improves the accuracy of parametric estimation in\nthe presence of data missingness, leading to more reliable and efficient\npolicing strategies.\n","authors":["Pramit Das","Moulinath Banerjee","Yuekai Sun"],"pdf_url":"https://arxiv.org/pdf/2502.07111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00133v5","updated":"2025-02-10T23:08:02Z","published":"2022-11-30T21:56:09Z","title":"Universal Neural Optimal Transport","summary":"  Optimal Transport (OT) problems are a cornerstone of many applications, but\nsolving them is computationally expensive. To address this problem, we propose\nUNOT (Universal Neural Optimal Transport), a novel framework capable of\naccurately predicting (entropic) OT distances and plans between discrete\nmeasures of variable resolution for a given cost function. UNOT builds on\nFourier Neural Operators, a universal class of neural networks that map between\nfunction spaces and that are discretization-invariant, which enables our\nnetwork to process measures of varying sizes. The network is trained\nadversarially using a second, generating network and a self-supervised\nbootstrapping loss. We theoretically justify the use of FNOs, prove that our\ngenerator is universal, and that minimizing the bootstrapping loss provably\nminimizes the ground truth loss. Through extensive experiments, we show that\nour network not only accurately predicts optimal transport distances and plans\nacross a wide range of datasets, but also captures the geometry of the\nWasserstein space correctly. Furthermore, we show that our network can be used\nas a state-of-the-art initialization for the Sinkhorn algorithm, significantly\noutperforming existing approaches.\n","authors":["Jonathan Geuter","Gregor Kornhardt","Ingimar Tomasson","Vaios Laschos"],"pdf_url":"https://arxiv.org/pdf/2212.00133v5.pdf","comment":"30 pages, 16 figures"},{"id":"http://arxiv.org/abs/2409.07131v2","updated":"2025-02-10T23:07:49Z","published":"2024-09-11T09:27:50Z","title":"Reranking Laws for Language Generation: A Communication-Theoretic\n  Perspective","summary":"  To ensure large language models (LLMs) are used safely, one must reduce their\npropensity to hallucinate or to generate unacceptable answers. A simple and\noften used strategy is to first let the LLM generate multiple hypotheses and\nthen employ a reranker to choose the best one. In this paper, we draw a\nparallel between this strategy and the use of redundancy to decrease the error\nrate in noisy communication channels. We conceptualize the generator as a\nsender transmitting multiple descriptions of a message through parallel noisy\nchannels. The receiver decodes the message by ranking the (potentially\ncorrupted) descriptions and selecting the one found to be most reliable. We\nprovide conditions under which this protocol is asymptotically error-free\n(i.e., yields an acceptable answer almost surely) even in scenarios where the\nreranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the\nchannel distributions are statistically dependent. We use our framework to\nobtain reranking laws which we validate empirically on two real-world tasks\nusing LLMs: text-to-code generation with DeepSeek-Coder 7B and machine\ntranslation of medical data with TowerInstruct 13B.\n","authors":["Ant√≥nio Farinhas","Haau-Sing Li","Andr√© F. T. Martins"],"pdf_url":"https://arxiv.org/pdf/2409.07131v2.pdf","comment":"NeurIPS 2024 (spotlight)"},{"id":"http://arxiv.org/abs/2502.07109v1","updated":"2025-02-10T23:06:10Z","published":"2025-02-10T23:06:10Z","title":"Game of Coding With an Unknown Adversary","summary":"  Motivated by emerging decentralized applications, the \\emph{game of coding}\nframework has been recently introduced to address scenarios where the\nadversary's control over coded symbols surpasses the fundamental limits of\ntraditional coding theory. Still, the reward mechanism available in\ndecentralized systems, motivates the adversary to act rationally. While the\ndecoder, as the data collector (DC), has an acceptance and rejection mechanism,\nfollowed by an estimation module, the adversary aims to maximize its utility,\nas an increasing function of (1) the chance of acceptance (to increase the\nreward), and (2) estimation error. On the other hand, the decoder also adjusts\nits acceptance rule to maximize its own utility, as (1) an increasing function\nof the chance of acceptance (to keep the system functional), (2) decreasing\nfunction of the estimation error. Prior works within this framework rely on the\nassumption that the game is complete, that is, both the DC and the adversary\nare fully aware of each other's utility functions. However, in practice, the\ndecoder is often unaware of the utility of the adversary. To address this\nlimitation, we develop an algorithm enabling the DC to commit to a strategy\nthat achieves within the vicinity of the equilibrium, without knowledge of the\nadversary's utility function. Our approach builds on an observation that at the\nequilibrium, the relationship between the probability of acceptance and the\nmean squared error (MSE) follows a predetermined curve independent of the\nspecific utility functions of the players. By exploiting this invariant\nrelationship, the DC can iteratively refine its strategy based on observable\nparameters, converging to a near-optimal solution. We provide theoretical\nguarantees on sample complexity and accuracy of the proposed scheme.\n","authors":["Hanzaleh Akbarinodehi","Parsa Moradi","Mohammad Ali Maddah-Ali"],"pdf_url":"https://arxiv.org/pdf/2502.07109v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22944v3","updated":"2025-02-10T23:03:19Z","published":"2024-10-30T12:01:48Z","title":"Focus On This, Not That! Steering LLMs With Adaptive Feature\n  Specification","summary":"  Despite the success of Instruction Tuning (IT) in training large language\nmodels (LLMs) to perform arbitrary user-specified tasks, these models often\nstill leverage spurious or biased features learned from their training data,\nleading to undesired behaviours when deploying them in new contexts. In this\nwork, we introduce Focus Instruction Tuning (FIT), which trains LLMs to\ncondition their responses by focusing on specific features whilst ignoring\nothers, leading to different behaviours based on what features are specified.\nAcross several experimental settings, we show that focus-tuned models can be\nadaptively steered by focusing on different features at inference-time: for\ninstance, robustness can be improved by focusing on task-causal features and\nignoring spurious features, and social bias can be mitigated by ignoring\ndemographic categories. Furthermore, FIT can steer behaviour in new contexts,\ngeneralising under distribution shift and to new unseen features at inference\ntime, and thereby facilitating more robust, fair, and controllable LLM\napplications in real-world environments.\n","authors":["Tom A. Lamb","Adam Davies","Alasdair Paren","Philip H. S. Torr","Francesco Pinto"],"pdf_url":"https://arxiv.org/pdf/2410.22944v3.pdf","comment":"32pages, 17 figures"},{"id":"http://arxiv.org/abs/2410.08069v2","updated":"2025-02-10T22:59:07Z","published":"2024-10-10T16:02:39Z","title":"Unlearning-based Neural Interpretations","summary":"  Gradient-based interpretations often require an anchor point of comparison to\navoid saturation in computing feature importance. We show that current\nbaselines defined using static functions--constant mapping, averaging or\nblurring--inject harmful colour, texture or frequency assumptions that deviate\nfrom model behaviour. This leads to accumulation of irregular gradients,\nresulting in attribution maps that are biased, fragile and manipulable.\nDeparting from the static approach, we propose UNI to compute an (un)learnable,\ndebiased and adaptive baseline by perturbing the input towards an unlearning\ndirection of steepest ascent. Our method discovers reliable baselines and\nsucceeds in erasing salient features, which in turn locally smooths the\nhigh-curvature decision boundaries. Our analyses point to unlearning as a\npromising avenue for generating faithful, efficient and robust interpretations.\n","authors":["Ching Lam Choi","Alexandre Duplessis","Serge Belongie"],"pdf_url":"https://arxiv.org/pdf/2410.08069v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2407.18811v2","updated":"2025-02-10T22:55:58Z","published":"2024-07-26T15:20:42Z","title":"Interpreting artificial neural networks to detect genome-wide\n  association signals for complex traits","summary":"  Investigating the genetic architecture of complex diseases is challenging due\nto the multifactorial and interactive landscape of genomic and environmental\ninfluences. Although genome-wide association studies (GWAS) have identified\nthousands of variants for multiple complex traits, conventional statistical\napproaches can be limited by simplified assumptions such as linearity and lack\nof epistasis in models. In this work, we trained artificial neural networks to\npredict complex traits using both simulated and real genotype-phenotype\ndatasets. We extracted feature importance scores via different post hoc\ninterpretability methods to identify potentially associated loci (PAL) for the\ntarget phenotype and devised an approach for obtaining p-values for the\ndetected PAL. Simulations with various parameters demonstrated that associated\nloci can be detected with good precision using strict selection criteria. By\napplying our approach to the schizophrenia cohort in the Estonian Biobank, we\ndetected multiple loci associated with this highly polygenic and heritable\ndisorder. There was significant concordance between PAL and loci previously\nassociated with schizophrenia and bipolar disorder, with enrichment analyses of\ngenes within the identified PAL predominantly highlighting terms related to\nbrain morphology and function. With advancements in model optimization and\nuncertainty quantification, artificial neural networks have the potential to\nenhance the identification of genomic loci associated with complex diseases,\noffering a more comprehensive approach for GWAS and serving as initial\nscreening tools for subsequent functional studies.\n","authors":["Burak Yelmen","Maris Alver","Merve Nur G√ºler","Estonian Biobank Research Team","Flora Jay","Lili Milani"],"pdf_url":"https://arxiv.org/pdf/2407.18811v2.pdf","comment":"18 pages, 3 main figures, 1 main table. Extensive changes from the\n  previous version including new methodology for obtaining statistical\n  significance and extended discussion"},{"id":"http://arxiv.org/abs/2502.04649v2","updated":"2025-02-10T22:51:41Z","published":"2025-02-07T04:18:56Z","title":"End-to-End Learning Framework for Solving Non-Markovian Optimal Control","summary":"  Integer-order calculus often falls short in capturing the long-range\ndependencies and memory effects found in many real-world processes. Fractional\ncalculus addresses these gaps via fractional-order integrals and derivatives,\nbut fractional-order dynamical systems pose substantial challenges in system\nidentification and optimal control due to the lack of standard control\nmethodologies. In this paper, we theoretically derive the optimal control via\n\\textit{linear quadratic regulator} (LQR) for \\textit{fractional-order linear\ntime-invariant }(FOLTI) systems and develop an end-to-end deep learning\nframework based on this theoretical foundation. Our approach establishes a\nrigorous mathematical model, derives analytical solutions, and incorporates\ndeep learning to achieve data-driven optimal control of FOLTI systems. Our key\ncontributions include: (i) proposing an innovative system identification method\ncontrol strategy for FOLTI systems, (ii) developing the first end-to-end\ndata-driven learning framework, \\textbf{F}ractional-\\textbf{O}rder\n\\textbf{L}earning for \\textbf{O}ptimal \\textbf{C}ontrol (FOLOC), that learns\ncontrol policies from observed trajectories, and (iii) deriving a theoretical\nanalysis of sample complexity to quantify the number of samples required for\naccurate optimal control in complex real-world problems. Experimental results\nindicate that our method accurately approximates fractional-order system\nbehaviors without relying on Gaussian noise assumptions, pointing to promising\navenues for advanced optimal control.\n","authors":["Xiaole Zhang","Peiyu Zhang","Xiongye Xiao","Shixuan Li","Vasileios Tzoumas","Vijay Gupta","Paul Bogdan"],"pdf_url":"https://arxiv.org/pdf/2502.04649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10450v2","updated":"2025-02-10T22:39:17Z","published":"2024-12-11T23:55:38Z","title":"Regional Weather Variable Predictions by Machine Learning with\n  Near-Surface Observational and Atmospheric Numerical Data","summary":"  Accurate and timely regional weather prediction is vital for sectors\ndependent on weather-related decisions. Traditional prediction methods, based\non atmospheric equations, often struggle with coarse temporal resolutions and\ninaccuracies. This paper presents a novel machine learning (ML) model, called\nMiMa (short for Micro-Macro), that integrates both near-surface observational\ndata from Kentucky Mesonet stations (collected every five minutes, known as\nMicro data) and hourly atmospheric numerical outputs (termed as Macro data) for\nfine-resolution weather forecasting. The MiMa model employs an encoder-decoder\ntransformer structure, with two encoders for processing multivariate data from\nboth datasets and a decoder for forecasting weather variables over short time\nhorizons. Each instance of the MiMa model, called a modelet, predicts the\nvalues of a specific weather parameter at an individual Mesonet station. The\napproach is extended with Re-MiMa modelets, which are designed to predict\nweather variables at ungauged locations by training on multivariate data from a\nfew representative stations in a region, tagged with their elevations. Re-MiMa\n(short for Regional-MiMa) can provide highly accurate predictions across an\nentire region, even in areas without observational stations. Experimental\nresults show that MiMa significantly outperforms current models, with Re-MiMa\noffering precise short-term forecasts for ungauged locations, marking a\nsignificant advancement in weather forecasting accuracy and applicability.\n","authors":["Yihe Zhang","Bryce Turney","Purushottam Sigdel","Xu Yuan","Eric Rappin","Adrian Lago","Sytske Kimball","Li Chen","Paul Darby","Lu Peng","Sercan Aygun","Yazhou Tu","M. Hassan Najafi","Nian-Feng Tzeng"],"pdf_url":"https://arxiv.org/pdf/2412.10450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19223v5","updated":"2025-02-10T22:34:34Z","published":"2024-11-28T15:48:02Z","title":"On the Unknowable Limits to Prediction","summary":"  We propose a rigorous decomposition of predictive error, highlighting that\nnot all 'irreducible' error is genuinely immutable. Many domains stand to\nbenefit from iterative enhancements in measurement, construct validity, and\nmodeling. Our approach demonstrates how apparently 'unpredictable' outcomes can\nbecome more tractable with improved data (across both target and features) and\nrefined algorithms. By distinguishing aleatoric from epistemic error, we\ndelineate how accuracy may asymptotically improve--though inherent\nstochasticity may remain--and offer a robust framework for advancing\ncomputational research.\n","authors":["Jiani Yan","Charles Rahal"],"pdf_url":"https://arxiv.org/pdf/2411.19223v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07090v1","updated":"2025-02-10T22:30:35Z","published":"2025-02-10T22:30:35Z","title":"Generative Distribution Prediction: A Unified Approach to Multimodal\n  Learning","summary":"  Accurate prediction with multimodal data-encompassing tabular, textual, and\nvisual inputs or outputs-is fundamental to advancing analytics in diverse\napplication domains. Traditional approaches often struggle to integrate\nheterogeneous data types while maintaining high predictive accuracy. We\nintroduce Generative Distribution Prediction (GDP), a novel framework that\nleverages multimodal synthetic data generation-such as conditional diffusion\nmodels-to enhance predictive performance across structured and unstructured\nmodalities. GDP is model-agnostic, compatible with any high-fidelity generative\nmodel, and supports transfer learning for domain adaptation. We establish a\nrigorous theoretical foundation for GDP, providing statistical guarantees on\nits predictive accuracy when using diffusion models as the generative backbone.\nBy estimating the data-generating distribution and adapting to various loss\nfunctions for risk minimization, GDP enables accurate point predictions across\nmultimodal settings. We empirically validate GDP on four supervised learning\ntasks-tabular data prediction, question answering, image captioning, and\nadaptive quantile regression-demonstrating its versatility and effectiveness\nacross diverse domains.\n","authors":["Xinyu Tian","Xiaotong Shen"],"pdf_url":"https://arxiv.org/pdf/2502.07090v1.pdf","comment":"31 pages 4 figures"},{"id":"http://arxiv.org/abs/2502.07087v1","updated":"2025-02-10T22:27:02Z","published":"2025-02-10T22:27:02Z","title":"Evaluating the Systematic Reasoning Abilities of Large Language Models\n  through Graph Coloring","summary":"  Contemporary large language models are powerful problem-solving tools, but\nthey exhibit weaknesses in their reasoning abilities which ongoing research\nseeks to mitigate. We investigate graph coloring as a means of evaluating an\nLLM's capacities for systematic step-by-step reasoning and possibility space\nexploration, as well as effects of semantic problem framing. We test Claude 3.5\nSonnet, Llama 3.1 405B, Gemini 1.5 Pro, GPT-4o, o1-mini, and DeepSeek-R1 on a\ndataset of $k$-coloring problems with $2 \\leq k \\leq 4$ and vertex count $4\n\\leq n \\leq 8$, using partial algorithmic solvers to further categorize\nproblems by difficulty. In addition to substantial but varying framing effects,\nwe find that all models except o1-mini and R1 exhibit $>60\\%$ error rates on\ndifficult problem types in all frames ($>15\\%$ for o1-mini and $>10\\%$ for R1),\nand no model achieves perfect accuracy even in the simple domain of 2-coloring\n4-vertex graphs. Our results highlight both the considerable recent progress in\nLLM systematic reasoning and the limits of its reliability, especially in\nrelation to increasing computational costs. We expect that more complex graph\ncoloring problems, and procedural generation of arbitrary-complexity reasoning\nproblems more broadly, offer further untapped potential for LLM benchmarking.\n","authors":["Alex Heyman","Joel Zylberberg"],"pdf_url":"https://arxiv.org/pdf/2502.07087v1.pdf","comment":"23 pages (8 excluding references and appendices); 8 figures (3\n  excluding appendices)"},{"id":"http://arxiv.org/abs/2402.17902v2","updated":"2025-02-10T22:23:56Z","published":"2024-02-27T21:42:18Z","title":"SequentialAttention++ for Block Sparsification: Differentiable Pruning\n  Meets Combinatorial Optimization","summary":"  Neural network pruning is a key technique towards engineering large yet\nscalable, interpretable, and generalizable models. Prior work on the subject\nhas developed largely along two orthogonal directions: (1) differentiable\npruning for efficiently and accurately scoring the importance of parameters,\nand (2) combinatorial optimization for efficiently searching over the space of\nsparse models. We unite the two approaches, both theoretically and empirically,\nto produce a coherent framework for structured neural network pruning in which\ndifferentiable pruning guides combinatorial optimization algorithms to select\nthe most important sparse set of parameters. Theoretically, we show how many\nexisting differentiable pruning techniques can be understood as nonconvex\nregularization for group sparse optimization, and prove that for a wide class\nof nonconvex regularizers, the global optimum is unique, group-sparse, and\nprovably yields an approximate solution to a sparse convex optimization\nproblem. The resulting algorithm that we propose, SequentialAttention++,\nadvances the state of the art in large-scale neural network block-wise pruning\ntasks on the ImageNet and Criteo datasets.\n","authors":["Taisuke Yasuda","Kyriakos Axiotis","Gang Fu","MohammadHossein Bateni","Vahab Mirrokni"],"pdf_url":"https://arxiv.org/pdf/2402.17902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07081v1","updated":"2025-02-10T22:19:08Z","published":"2025-02-10T22:19:08Z","title":"Fast Clustering of Categorical Big Data","summary":"  The K-Modes algorithm, developed for clustering categorical data, is of high\nalgorithmic simplicity but suffers from unreliable performances in clustering\nquality and clustering efficiency, both heavily influenced by the choice of\ninitial cluster centers. In this paper, we investigate Bisecting K-Modes\n(BK-Modes), a successive bisecting process to find clusters, in examining how\ngood the cluster centers out of the bisecting process will be when used as\ninitial centers for the K-Modes. The BK-Modes works by splitting a dataset into\nmultiple clusters iteratively with one cluster being chosen and bisected into\ntwo clusters in each iteration. We use the sum of distances of data to their\ncluster centers as the selection metric to choose a cluster to be bisected in\neach iteration. This iterative process stops when K clusters are produced. The\ncenters of these K clusters are then used as the initial cluster centers for\nthe K-Modes. Experimental studies of the BK-Modes were carried out and were\ncompared against the K-Modes with multiple sets of initial cluster centers as\nwell as the best of the existing methods we found so far in our survey.\nExperimental results indicated good performances of BK-Modes both in the\nclustering quality and efficiency for large datasets.\n","authors":["Bipana Thapaliya","Yu Zhuang"],"pdf_url":"https://arxiv.org/pdf/2502.07081v1.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.07064v1","updated":"2025-02-10T21:57:00Z","published":"2025-02-10T21:57:00Z","title":"Contextual Thompson Sampling via Generation of Missing Data","summary":"  We introduce a framework for Thompson sampling contextual bandit algorithms,\nin which the algorithm's ability to quantify uncertainty and make decisions\ndepends on the quality of a generative model that is learned offline. Instead\nof viewing uncertainty in the environment as arising from unobservable latent\nparameters, our algorithm treats uncertainty as stemming from missing, but\npotentially observable, future outcomes. If these future outcomes were all\nobserved, one could simply make decisions using an \"oracle\" policy fit on the\ncomplete dataset. Inspired by this conceptualization, at each decision-time,\nour algorithm uses a generative model to probabilistically impute missing\nfuture outcomes, fits a policy using the imputed complete dataset, and uses\nthat policy to select the next action. We formally show that this algorithm is\na generative formulation of Thompson Sampling and prove a state-of-the-art\nregret bound for it. Notably, our regret bound i) depends on the probabilistic\ngenerative model only through the quality of its offline prediction loss, and\nii) applies to any method of fitting the \"oracle\" policy, which easily allows\none to adapt Thompson sampling to decision-making settings with fairness and/or\nresource constraints.\n","authors":["Kelly W. Zhang","Tiffany Tianhui Cai","Hongseok Namkoong","Daniel Russo"],"pdf_url":"https://arxiv.org/pdf/2502.07064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07059v1","updated":"2025-02-10T21:51:02Z","published":"2025-02-10T21:51:02Z","title":"Federated Continual Learning: Concepts, Challenges, and Solutions","summary":"  Federated Continual Learning (FCL) has emerged as a robust solution for\ncollaborative model training in dynamic environments, where data samples are\ncontinuously generated and distributed across multiple devices. This survey\nprovides a comprehensive review of FCL, focusing on key challenges such as\nheterogeneity, model stability, communication overhead, and privacy\npreservation. We explore various forms of heterogeneity and their impact on\nmodel performance. Solutions to non-IID data, resource-constrained platforms,\nand personalized learning are reviewed in an effort to show the complexities of\nhandling heterogeneous data distributions. Next, we review techniques for\nensuring model stability and avoiding catastrophic forgetting, which are\ncritical in non-stationary environments. Privacy-preserving techniques are\nanother aspect of FCL that have been reviewed in this work. This survey has\nintegrated insights from federated learning and continual learning to present\nstrategies for improving the efficacy and scalability of FCL systems, making it\napplicable to a wide range of real-world scenarios.\n","authors":["Parisa Hamedi","Roozbeh Razavi-Far","Ehsan Hallaji"],"pdf_url":"https://arxiv.org/pdf/2502.07059v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07056v1","updated":"2025-02-10T21:46:54Z","published":"2025-02-10T21:46:54Z","title":"Autonomous Deep Agent","summary":"  This technical brief introduces Deep Agent, an advanced autonomous AI system\ndesigned to manage complex multi-phase tasks through a novel hierarchical task\nmanagement architecture. The system's foundation is built on our Hierarchical\nTask DAG (HTDAG) framework, which dynamically decomposes high-level objectives\ninto manageable sub-tasks while rigorously maintaining dependencies and\nexecution coherence. Deep Agent advances beyond traditional agent systems\nthrough three key innovations: First, it implements a recursive two-stage\nplanner-executor architecture that enables continuous task refinement and\nadaptation as circumstances change. Second, it features an Autonomous API &\nTool Creation (AATC) system that automatically generates reusable components\nfrom UI interactions, substantially reducing operational costs for similar\ntasks. Third, it incorporates Prompt Tweaking Engine and Autonomous Prompt\nFeedback Learning components that optimize Large Language Model prompts for\nspecific scenarios, enhancing both inference accuracy and operational\nstability. These components are integrated to form a service infrastructure\nthat manages user contexts, handles complex task dependencies, and orchestrates\nend-to-end agentic workflow execution. Through this sophisticated architecture,\nDeep Agent establishes a novel paradigm in self-governing AI systems,\ndemonstrating robust capability to independently handle intricate, multi-step\ntasks while maintaining consistent efficiency and reliability through\ncontinuous self-optimization.\n","authors":["Amy Yu","Erik Lebedev","Lincoln Everett","Xiaoxin Chen","Terry Chen"],"pdf_url":"https://arxiv.org/pdf/2502.07056v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.18787v3","updated":"2025-02-10T21:46:15Z","published":"2023-11-30T18:37:15Z","title":"Communication-Efficient Federated Optimization over Semi-Decentralized\n  Networks","summary":"  In large-scale federated and decentralized learning, communication efficiency\nis one of the most challenging bottlenecks. While gossip communication -- where\nagents can exchange information with their connected neighbors -- is more\ncost-effective than communicating with the remote server, it often requires a\ngreater number of communication rounds, especially for large and sparse\nnetworks. To tackle the trade-off, we examine the communication efficiency\nunder a semi-decentralized communication protocol, in which agents can perform\nboth agent-to-agent and agent-to-server communication in a probabilistic\nmanner. We design a tailored communication-efficient algorithm over\nsemi-decentralized networks, referred to as PISCO, which inherits the\nrobustness to data heterogeneity thanks to gradient tracking and allows\nmultiple local updates for saving communication. We establish the convergence\nrate of PISCO for nonconvex problems and show that PISCO enjoys a linear\nspeedup in terms of the number of agents and local updates. Our numerical\nresults highlight the superior communication efficiency of PISCO and its\nresilience to data heterogeneity and various network topologies.\n","authors":["He Wang","Yuejie Chi"],"pdf_url":"https://arxiv.org/pdf/2311.18787v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06949v2","updated":"2025-02-10T21:34:00Z","published":"2024-12-09T19:53:13Z","title":"Bridging Conversational and Collaborative Signals for Conversational\n  Recommendation","summary":"  Conversational recommendation systems (CRS) leverage contextual information\nfrom conversations to generate recommendations but often struggle due to a lack\nof collaborative filtering (CF) signals, which capture user-item interaction\npatterns essential for accurate recommendations. We introduce Reddit-ML32M, a\ndataset that links Reddit conversations with interactions on MovieLens 32M, to\nenrich item representations by leveraging collaborative knowledge and\naddressing interaction sparsity in conversational datasets. We propose an\nLLM-based framework that uses Reddit-ML32M to align LLM-generated\nrecommendations with CF embeddings, refining rankings for better performance.\nWe evaluate our framework against three sets of baselines: CF-based\nrecommenders using only interactions from CRS tasks, traditional CRS models,\nand LLM-based methods relying on conversational context without item\nrepresentations. Our approach achieves consistent improvements, including a\n12.32% increase in Hit Rate and a 9.9% improvement in NDCG, outperforming the\nbest-performing baseline that relies on conversational context but lacks\ncollaborative item representations.\n","authors":["Ahmad Bin Rabiah","Nafis Sadeq","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2412.06949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07046v1","updated":"2025-02-10T21:28:15Z","published":"2025-02-10T21:28:15Z","title":"SnipGen: A Mining Repository Framework for Evaluating LLMs for Code","summary":"  Language Models (LLMs), such as transformer-based neural networks trained on\nbillions of parameters, have become increasingly prevalent in software\nengineering (SE). These models, trained on extensive datasets that include code\nrepositories, exhibit remarkable capabilities for SE tasks. However, evaluating\ntheir effectiveness poses significant challenges, primarily due to the\npotential overlap between the datasets used for training and those employed for\nevaluation. To address this issue, we introduce SnipGen, a comprehensive\nrepository mining framework designed to leverage prompt engineering across\nvarious downstream tasks for code generation. SnipGen aims to mitigate data\ncontamination by generating robust testbeds and crafting tailored data points\nto assist researchers and practitioners in evaluating LLMs for code-related\ntasks. In our exploratory study, SnipGen mined approximately 227K data points\nfrom 338K recent code changes in GitHub commits, focusing on method-level\ngranularity. SnipGen features a collection of prompt templates that can be\ncombined to create a Chain-of-Thought-like sequence of prompts, enabling a\nnuanced assessment of LLMs' code generation quality. By providing the mining\ntool, the methodology, and the dataset, SnipGen empowers researchers and\npractitioners to rigorously evaluate and interpret LLMs' performance in\nsoftware engineering contexts.\n","authors":["Daniel Rodriguez-Cardenas","Alejandro Velasco","Denys Poshyvany"],"pdf_url":"https://arxiv.org/pdf/2502.07046v1.pdf","comment":"5 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.20245v2","updated":"2025-02-10T21:17:54Z","published":"2024-10-26T18:21:44Z","title":"Improving Model Evaluation using SMART Filtering of Benchmark Datasets","summary":"  One of the most challenging problems facing NLP today is evaluation. Some of\nthe most pressing issues pertain to benchmark saturation, data contamination,\nand diversity in the quality of test examples. To address these concerns, we\npropose Selection Methodology for Accurate, Reduced, and Targeted (SMART)\nfiltering, a novel approach to select a high-quality subset of examples from\nexisting benchmark datasets by systematically removing less informative and\nless challenging examples. Our approach applies three filtering criteria,\nremoving (i) easy examples, (ii) data-contaminated examples, and (iii) examples\nthat are similar to each other based on distance in an embedding space. We\ndemonstrate the effectiveness of SMART on three multiple choice QA datasets,\nwhere our methodology increases efficiency by reducing dataset size by 48\\% on\naverage, while increasing Pearson correlation with rankings from ChatBot Arena,\na more open-ended human evaluation setting. Our method enables us to be more\nefficient, whether using SMART to make new benchmarks more challenging or to\nrevitalize older datasets, while still preserving the relative model rankings.\n","authors":["Vipul Gupta","Candace Ross","David Pantoja","Rebecca J. Passonneau","Megan Ung","Adina Williams"],"pdf_url":"https://arxiv.org/pdf/2410.20245v2.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.18754v2","updated":"2025-02-10T21:17:29Z","published":"2024-05-29T04:39:24Z","title":"GIST: Greedy Independent Set Thresholding for Diverse Data Summarization","summary":"  We introduce a novel subset selection problem called min-distance\ndiversification with monotone submodular utility ($\\textsf{MDMS}$), which has a\nwide variety of applications in machine learning, e.g., data sampling and\nfeature selection. Given a set of points in a metric space, the goal of\n$\\textsf{MDMS}$ is to maximize an objective function combining a monotone\nsubmodular utility term and a min-distance diversity term between any pair of\nselected points, subject to a cardinality constraint. We propose the\n$\\texttt{GIST}$ algorithm, which achieves a $\\frac{1}{2}$-approximation\nguarantee for $\\textsf{MDMS}$ by approximating a series of maximum independent\nset problems with a bicriteria greedy algorithm. We also prove that it is\nNP-hard to approximate to within a factor of $0.5584$. Finally, we demonstrate\nthat $\\texttt{GIST}$ outperforms existing benchmarks for on a real-world image\nclassification task that studies single-shot subset selection for ImageNet.\n","authors":["Matthew Fahrbach","Srikumar Ramalingam","Morteza Zadimoghaddam","Sara Ahmadian","Gui Citovsky","Giulia DeSalvo"],"pdf_url":"https://arxiv.org/pdf/2405.18754v2.pdf","comment":"19 pages, 3 figures"},{"id":"http://arxiv.org/abs/2501.08617v2","updated":"2025-02-10T21:17:01Z","published":"2025-01-15T06:33:15Z","title":"RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation","summary":"  While Reinforcement Learning from Human Feedback (RLHF) has shown promise in\naligning generative AI, we present empirical evidence that it can also cause\nsevere, systematic misalignment. We hypothesize that this stems from evaluator\nfeedback depending on downstream outcome predictions (foresight) that can be\ninfluenced by the AI's output, inducing Goodhart's law dynamics. Conversely,\nour theoretical analysis shows that conditioning evaluator feedback on\ndownstream observations (hindsight) inhibits this effect by decoupling the\nalignment signal from potentially compromised predictions-crucially, the result\nholds even if the observed outcomes are sampled from the AI's own world model.\nBuilding on this insight, we introduce Reinforcement Learning from Hindsight\nSimulation (RLHS), which presents plausible simulated outcomes to evaluators\nbefore eliciting feedback. We demonstrate RLHS on online (PPO) and offline\n(DPO) large language model fine-tuning, obtaining superior alignment over RLHF\nin controlled consultancy-type experiments and user studies. We evaluate\npost-hoc on the TruthfulQA benchmark and find that, even after single-task\nfine-tuning, both RLHF misalignment and RLHS alignment carry over to\nsubstantially different settings.\n","authors":["Kaiqu Liang","Haimin Hu","Ryan Liu","Thomas L. Griffiths","Jaime Fern√°ndez Fisac"],"pdf_url":"https://arxiv.org/pdf/2501.08617v2.pdf","comment":"24 pages, 18 figures"},{"id":"http://arxiv.org/abs/2502.07039v1","updated":"2025-02-10T21:09:19Z","published":"2025-02-10T21:09:19Z","title":"Boosting of Classification Models with Human-in-the-Loop Computational\n  Visual Knowledge Discovery","summary":"  High-risk artificial intelligence and machine learning classification tasks,\nsuch as healthcare diagnosis, require accurate and interpretable prediction\nmodels. However, classifier algorithms typically sacrifice individual\ncase-accuracy for overall model accuracy, limiting analysis of class overlap\nareas regardless of task significance. The Adaptive Boosting meta-algorithm,\nwhich won the 2003 G\\\"odel Prize, analytically assigns higher weights to\nmisclassified cases to reclassify. However, it relies on weaker base\nclassifiers that are iteratively strengthened, limiting improvements from base\nclassifiers. Combining visual and computational approaches enables selecting\nstronger base classifiers before boosting. This paper proposes moving boosting\nmethodology from focusing on only misclassified cases to all cases in the class\noverlap areas using Computational and Interactive Visual Learning (CIVL) with a\nHuman-in-the-Loop. It builds classifiers in lossless visualizations integrating\nhuman domain expertise and visual insights. A Divide and Classify process\nsplits cases to simple and complex, classifying these individually through\ncomputational analysis and data visualization with lossless visualization\nspaces of Parallel Coordinates or other General Line Coordinates. After finding\npure and overlap class areas simple cases in pure areas are classified,\ngenerating interpretable sub-models like decision rules in Propositional and\nFirst-order Logics. Only multidimensional cases in the overlap areas are\nlosslessly visualized simplifying end-user cognitive tasks to identify\ndifficult case patterns, including engineering features to form new\nclassifiable patterns. Demonstration shows a perfectly accurate and losslessly\ninterpretable model of the Iris dataset, and simulated data shows generalized\nbenefits to accuracy and interpretability of models, increasing end-user\nconfidence in discovered models.\n","authors":["Alice Williams","Boris Kovalerchuk"],"pdf_url":"https://arxiv.org/pdf/2502.07039v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2502.07036v1","updated":"2025-02-10T21:03:24Z","published":"2025-02-10T21:03:24Z","title":"Automated Consistency Analysis of LLMs","summary":"  Generative AI (Gen AI) with large language models (LLMs) are being widely\nadopted across the industry, academia and government. Cybersecurity is one of\nthe key sectors where LLMs can be and/or are already being used. There are a\nnumber of problems that inhibit the adoption of trustworthy Gen AI and LLMs in\ncybersecurity and such other critical areas. One of the key challenge to the\ntrustworthiness and reliability of LLMs is: how consistent an LLM is in its\nresponses?\n  In this paper, we have analyzed and developed a formal definition of\nconsistency of responses of LLMs. We have formally defined what is consistency\nof responses and then develop a framework for consistency evaluation. The paper\nproposes two approaches to validate consistency: self-validation, and\nvalidation across multiple LLMs. We have carried out extensive experiments for\nseveral LLMs such as GPT4oMini, GPT3.5, Gemini, Cohere, and Llama3, on a\nsecurity benchmark consisting of several cybersecurity questions: informational\nand situational. Our experiments corroborate the fact that even though these\nLLMs are being considered and/or already being used for several cybersecurity\ntasks today, they are often inconsistent in their responses, and thus are\nuntrustworthy and unreliable for cybersecurity.\n","authors":["Aditya Patwardhan","Vivek Vaidya","Ashish Kundu"],"pdf_url":"https://arxiv.org/pdf/2502.07036v1.pdf","comment":"10 pages, 12 figures, 3 tables, 3 algorithms"},{"id":"http://arxiv.org/abs/2408.05636v4","updated":"2025-02-10T20:51:18Z","published":"2024-08-10T21:24:25Z","title":"Speculative Diffusion Decoding: Accelerating Language Generation through\n  Diffusion","summary":"  Speculative decoding has emerged as a widely adopted method to accelerate\nlarge language model inference without sacrificing the quality of the model\noutputs. While this technique has facilitated notable speed improvements by\nenabling parallel sequence verification, its efficiency remains inherently\nlimited by the reliance on incremental token generation in existing draft\nmodels. To overcome this limitation, this paper proposes an adaptation of\nspeculative decoding which uses discrete diffusion models to generate draft\nsequences. This allows parallelization of both the drafting and verification\nsteps, providing significant speedups to the inference process. Our proposed\napproach, $\\textit{Speculative Diffusion Decoding (SpecDiff)}$, is validated on\nstandard language generation benchmarks and empirically demonstrated to provide\nup to 7.2x speedups over standard generation processes and up to 1.75x speedups\nover existing speculative decoding approaches.\n","authors":["Jacob K Christopher","Brian R Bartoldson","Tal Ben-Nun","Michael Cardei","Bhavya Kailkhura","Ferdinando Fioretto"],"pdf_url":"https://arxiv.org/pdf/2408.05636v4.pdf","comment":"Published at the 2025 Annual Conference of the Nations of the\n  Americas Chapter of the Association for Computational Linguistics (NAACL\n  2025)"},{"id":"http://arxiv.org/abs/2502.07030v1","updated":"2025-02-10T20:50:12Z","published":"2025-02-10T20:50:12Z","title":"PrismAvatar: Real-time animated 3D neural head avatars on edge devices","summary":"  We present PrismAvatar: a 3D head avatar model which is designed specifically\nto enable real-time animation and rendering on resource-constrained edge\ndevices, while still enjoying the benefits of neural volumetric rendering at\ntraining time. By integrating a rigged prism lattice with a 3D morphable head\nmodel, we use a hybrid rendering model to simultaneously reconstruct a\nmesh-based head and a deformable NeRF model for regions not represented by the\n3DMM. We then distill the deformable NeRF into a rigged mesh and neural\ntextures, which can be animated and rendered efficiently within the constraints\nof the traditional triangle rendering pipeline. In addition to running at 60\nfps with low memory usage on mobile devices, we find that our trained models\nhave comparable quality to state-of-the-art 3D avatar models on desktop\ndevices.\n","authors":["Prashant Raina","Felix Taubner","Mathieu Tuli","Eu Wern Teh","Kevin Ferreira"],"pdf_url":"https://arxiv.org/pdf/2502.07030v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.07029v1","updated":"2025-02-10T20:46:42Z","published":"2025-02-10T20:46:42Z","title":"Leveraging Allophony in Self-Supervised Speech Models for Atypical\n  Pronunciation Assessment","summary":"  Allophony refers to the variation in the phonetic realization of a phoneme\nbased on its phonetic environment. Modeling allophones is crucial for atypical\npronunciation assessment, which involves distinguishing atypical from typical\npronunciations. However, recent phoneme classifier-based approaches often\nsimplify this by treating various realizations as a single phoneme, bypassing\nthe complexity of modeling allophonic variation. Motivated by the acoustic\nmodeling capabilities of frozen self-supervised speech model (S3M) features, we\npropose MixGoP, a novel approach that leverages Gaussian mixture models to\nmodel phoneme distributions with multiple subclusters. Our experiments show\nthat MixGoP achieves state-of-the-art performance across four out of five\ndatasets, including dysarthric and non-native speech. Our analysis further\nsuggests that S3M features capture allophonic variation more effectively than\nMFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP\nwith S3M features.\n","authors":["Kwanghee Choi","Eunjung Yeo","Kalvin Chang","Shinji Watanabe","David Mortensen"],"pdf_url":"https://arxiv.org/pdf/2502.07029v1.pdf","comment":"Accepted to NAACL 2025. Codebase available at\n  https://github.com/juice500ml/acoustic-units-for-ood"},{"id":"http://arxiv.org/abs/2305.16474v3","updated":"2025-02-10T20:39:55Z","published":"2023-05-25T21:07:20Z","title":"FairDP: Certified Fairness with Differential Privacy","summary":"  This paper introduces FairDP, a novel training mechanism designed to provide\ngroup fairness certification for the trained model's decisions, along with a\ndifferential privacy (DP) guarantee to protect training data. The key idea of\nFairDP is to train models for distinct individual groups independently, add\nnoise to each group's gradient for data privacy protection, and progressively\nintegrate knowledge from group models to formulate a comprehensive model that\nbalances privacy, utility, and fairness in downstream tasks. By doing so,\nFairDP ensures equal contribution from each group while gaining control over\nthe amount of DP-preserving noise added to each group's contribution. To\nprovide fairness certification, FairDP leverages the DP-preserving noise to\nstatistically quantify and bound fairness metrics. An extensive theoretical and\nempirical analysis using benchmark datasets validates the efficacy of FairDP\nand improved trade-offs between model utility, privacy, and fairness compared\nwith existing methods. Our empirical results indicate that FairDP can improve\nfairness metrics by more than 65% on average while attaining marginal utility\ndrop (less than 4% on average) under a rigorous DP-preservation across\nbenchmark datasets compared with existing baselines.\n","authors":["Khang Tran","Ferdinando Fioretto","Issa Khalil","My T. Thai","Linh Thi Xuan Phan NhatHai Phan"],"pdf_url":"https://arxiv.org/pdf/2305.16474v3.pdf","comment":"Accepted at 3rd IEEE Conference on Secure and Trustworthy Machine\n  Learning"},{"id":"http://arxiv.org/abs/2502.07026v1","updated":"2025-02-10T20:38:53Z","published":"2025-02-10T20:38:53Z","title":"Machine Learning for Everyone: Simplifying Healthcare Analytics with\n  BigQuery ML","summary":"  Machine learning (ML) is transforming healthcare by enabling predictive\nanalytics, personalized treatments, and improved patient outcomes. However,\ntraditional ML workflows require specialized skills, infrastructure, and\nresources, limiting accessibility for many healthcare professionals. This paper\nexplores how Google Cloud's BigQuery ML simplifies the development and\ndeployment of ML models using SQL, reducing technical barriers. Through a case\nstudy on diabetes prediction using the Diabetes Health Indicators Dataset, we\nevaluate three predictive models: Logistic Regression, Boosted Tree, and Deep\nNeural Network (DNN). Our results demonstrate that the Boosted Tree model\nachieves the highest performance, making it highly effective for diabetes\nprediction. This study highlights BigQuery ML's role in democratizing machine\nlearning by providing a scalable, efficient, and accessible solution for\nhealthcare analytics.\n","authors":["Mohammad Amir Salari","Bahareh Rahmani"],"pdf_url":"https://arxiv.org/pdf/2502.07026v1.pdf","comment":"Focus: Artificial Intelligence, Healthcare analytics, cloud\n  computing, BigQuery ML"},{"id":"http://arxiv.org/abs/2502.07025v1","updated":"2025-02-10T20:35:28Z","published":"2025-02-10T20:35:28Z","title":"Detecting Neurodegenerative Diseases using Frame-Level Handwriting\n  Embeddings","summary":"  In this study, we explored the use of spectrograms to represent handwriting\nsignals for assessing neurodegenerative diseases, including 42 healthy controls\n(CTL), 35 subjects with Parkinson's Disease (PD), 21 with Alzheimer's Disease\n(AD), and 15 with Parkinson's Disease Mimics (PDM). We applied CNN and\nCNN-BLSTM models for binary classification using both multi-channel fixed-size\nand frame-based spectrograms. Our results showed that handwriting tasks and\nspectrogram channel combinations significantly impacted classification\nperformance. The highest F1-score (89.8%) was achieved for AD vs. CTL, while PD\nvs. CTL reached 74.5%, and PD vs. PDM scored 77.97%. CNN consistently\noutperformed CNN-BLSTM. Different sliding window lengths were tested for\nconstructing frame-based spectrograms. A 1-second window worked best for AD,\nlonger windows improved PD classification, and window length had little effect\non PD vs. PDM.\n","authors":["Sarah Laouedj","Yuzhe Wang","Jesus Villalba","Thomas Thebaud","Laureano Moro-Velazquez","Najim Dehak"],"pdf_url":"https://arxiv.org/pdf/2502.07025v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07022v1","updated":"2025-02-10T20:30:32Z","published":"2025-02-10T20:30:32Z","title":"AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in\n  Corporate Statements","summary":"  Despite over a decade of legislative efforts to address modern slavery in the\nsupply chains of large corporations, the effectiveness of government oversight\nremains hampered by the challenge of scrutinizing thousands of statements\nannually. While Large Language Models (LLMs) can be considered a well\nestablished solution for the automatic analysis and summarization of documents,\nrecognizing concrete modern slavery countermeasures taken by companies and\ndifferentiating those from vague claims remains a challenging task. To help\nevaluate and fine-tune LLMs for the assessment of corporate statements, we\nintroduce a dataset composed of 5,731 modern slavery statements taken from the\nAustralian Modern Slavery Register and annotated at the sentence level. This\npaper details the construction steps for the dataset that include the careful\ndesign of annotation specifications, the selection and preprocessing of\nstatements, and the creation of high-quality annotation subsets for effective\nmodel evaluations. To demonstrate our dataset's utility, we propose a machine\nlearning methodology for the detection of sentences relevant to mandatory\nreporting requirements set by the Australian Modern Slavery Act. We then follow\nthis methodology to benchmark modern language models under zero-shot and\nsupervised learning settings.\n","authors":["Adriana Eufrosiana Bora","Pierre-Luc St-Charles","Mirko Bronzi","Ars√®ne Fansi Tchango","Bruno Rousseau","Kerrie Mengersen"],"pdf_url":"https://arxiv.org/pdf/2502.07022v1.pdf","comment":"Camera ready. ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07021v1","updated":"2025-02-10T20:29:57Z","published":"2025-02-10T20:29:57Z","title":"Federated Sinkhorn","summary":"  In this work we investigate the potential of solving the discrete Optimal\nTransport (OT) problem with entropy regularization in a federated learning\nsetting. Recall that the celebrated Sinkhorn algorithm transforms the classical\nOT linear program into strongly convex constrained optimization, facilitating\nfirst order methods for otherwise intractably large problems. A common\ncontemporary setting that remains an open problem as far as the application of\nSinkhorn is the presence of data spread across clients with distributed\ninter-communication, either due to clients whose privacy is a concern, or\nsimply by necessity of processing and memory hardware limitations. In this work\nwe investigate various natural procedures, which we refer to as Federated\nSinkhorn, that handle distributed environments where data is partitioned across\nmultiple clients. We formulate the problem as minimizing the transport cost\nwith an entropy regularization term, subject to marginal constraints, where\nblock components of the source and target distribution vectors are locally\nknown to clients corresponding to each block. We consider both synchronous and\nasynchronous variants as well as all-to-all and server-client communication\ntopology protocols. Each procedure allows clients to compute local operations\non their data partition while periodically exchanging information with others.\nWe provide theoretical guarantees on convergence for the different variants\nunder different possible conditions. We empirically demonstrate the algorithms\nperformance on synthetic datasets and a real-world financial risk assessment\napplication. The investigation highlights the subtle tradeoffs associated with\ncomputation and communication time in different settings and how they depend on\nproblem size and sparsity.\n","authors":["Jeremy Kulcsar","Vyacheslav Kungurtsev","Georgios Korpas","Giulio Giaconi","William Shoosmith"],"pdf_url":"https://arxiv.org/pdf/2502.07021v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07829v1","updated":"2025-02-10T20:25:11Z","published":"2025-02-10T20:25:11Z","title":"Preference Alignment on Diffusion Model: A Comprehensive Survey for\n  Image Generation and Editing","summary":"  The integration of preference alignment with diffusion models (DMs) has\nemerged as a transformative approach to enhance image generation and editing\ncapabilities. Although integrating diffusion models with preference alignment\nstrategies poses significant challenges for novices at this intersection,\ncomprehensive and systematic reviews of this subject are still notably lacking.\nTo bridge this gap, this paper extensively surveys preference alignment with\ndiffusion models in image generation and editing. First, we systematically\nreview cutting-edge optimization techniques such as reinforcement learning with\nhuman feedback (RLHF), direct preference optimization (DPO), and others,\nhighlighting their pivotal role in aligning preferences with DMs. Then, we\nthoroughly explore the applications of aligning preferences with DMs in\nautonomous driving, medical imaging, robotics, and more. Finally, we\ncomprehensively discuss the challenges of preference alignment with DMs. To our\nknowledge, this is the first survey centered on preference alignment with DMs,\nproviding insights to drive future innovation in this dynamic area.\n","authors":["Sihao Wu","Xiaonan Si","Chi Xing","Jianhong Wang","Gaojie Jin","Guangliang Cheng","Lijun Zhang","Xiaowei Huang"],"pdf_url":"https://arxiv.org/pdf/2502.07829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07016v1","updated":"2025-02-10T20:22:02Z","published":"2025-02-10T20:22:02Z","title":"Confidence Intervals for Evaluation of Data Mining","summary":"  In data mining, when binary prediction rules are used to predict a binary\noutcome, many performance measures are used in a vast array of literature for\nthe purposes of evaluation and comparison. Some examples include classification\naccuracy, precision, recall, F measures, and Jaccard index. Typically, these\nperformance measures are only approximately estimated from a finite dataset,\nwhich may lead to findings that are not statistically significant. In order to\nproperly quantify such statistical uncertainty, it is important to provide\nconfidence intervals associated with these estimated performance measures. We\nconsider statistical inference about general performance measures used in data\nmining, with both individual and joint confidence intervals. These confidence\nintervals are based on asymptotic normal approximations and can be computed\nfast, without needs to do bootstrap resampling. We study the finite sample\ncoverage probabilities for these confidence intervals and also propose a\n`blurring correction' on the variance to improve the finite sample performance.\nThis 'blurring correction' generalizes the plus-four method from binomial\nproportion to general performance measures used in data mining. Our framework\nallows multiple performance measures of multiple classification rules to be\ninferred simultaneously for comparisons.\n","authors":["Zheng Yuan","Wenxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2502.07016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11582v3","updated":"2025-02-10T20:19:38Z","published":"2023-12-18T11:37:19Z","title":"Shapley-PC: Constraint-based Causal Structure Learning with a Shapley\n  Inspired Framework","summary":"  Causal Structure Learning (CSL), also referred to as causal discovery,\namounts to extracting causal relations among variables in data. CSL enables the\nestimation of causal effects from observational data alone, avoiding the need\nto perform real life experiments. Constraint-based CSL leverages conditional\nindependence tests to perform causal discovery. We propose Shapley-PC, a novel\nmethod to improve constraint-based CSL algorithms by using Shapley values over\nthe possible conditioning sets, to decide which variables are responsible for\nthe observed conditional (in)dependences. We prove soundness, completeness and\nasymptotic consistency of Shapley-PC and run a simulation study showing that\nour proposed algorithm is superior to existing versions of PC.\n","authors":["Fabrizio Russo","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2312.11582v3.pdf","comment":"Accepted for CLeaR 2025 - 47 pages (with appendix)"},{"id":"http://arxiv.org/abs/2502.07011v1","updated":"2025-02-10T20:15:43Z","published":"2025-02-10T20:15:43Z","title":"DROP: Poison Dilution via Knowledge Distillation for Federated Learning","summary":"  Federated Learning is vulnerable to adversarial manipulation, where malicious\nclients can inject poisoned updates to influence the global model's behavior.\nWhile existing defense mechanisms have made notable progress, they fail to\nprotect against adversaries that aim to induce targeted backdoors under\ndifferent learning and attack configurations. To address this limitation, we\nintroduce DROP (Distillation-based Reduction Of Poisoning), a novel defense\nmechanism that combines clustering and activity-tracking techniques with\nextraction of benign behavior from clients via knowledge distillation to tackle\nstealthy adversaries that manipulate low data poisoning rates and diverse\nmalicious client ratios within the federation. Through extensive\nexperimentation, our approach demonstrates superior robustness compared to\nexisting defenses across a wide range of learning configurations. Finally, we\nevaluate existing defenses and our method under the challenging setting of\nnon-IID client data distribution and highlight the challenges of designing a\nresilient FL defense in this setting.\n","authors":["Georgios Syros","Anshuman Suri","Farinaz Koushanfar","Cristina Nita-Rotaru","Alina Oprea"],"pdf_url":"https://arxiv.org/pdf/2502.07011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02762v2","updated":"2025-02-10T20:13:31Z","published":"2024-10-03T17:59:57Z","title":"Interpreting and Editing Vision-Language Representations to Mitigate\n  Hallucinations","summary":"  We investigate the internal representations of vision-language models (VLMs)\nto address hallucinations, a persistent challenge despite advances in model\nsize and training. We project VLMs' internal image representations to their\nlanguage vocabulary and observe more confident output probabilities on real\nobjects than hallucinated objects. We additionally use these output\nprobabilities to spatially localize real objects. Building on this approach, we\nintroduce a knowledge erasure algorithm that removes hallucinations by linearly\northogonalizing image features with respect to hallucinated object features. We\nshow that targeted edits to a model's latent representations can reduce\nhallucinations by up to 25.7% on the COCO2014 dataset while preserving\nperformance. Our findings demonstrate how a deeper understanding of VLMs'\nlatent representations can enhance reliability and enable novel capabilities,\nsuch as zero-shot segmentation.\n","authors":["Nick Jiang","Anish Kachinthaya","Suzie Petryk","Yossi Gandelsman"],"pdf_url":"https://arxiv.org/pdf/2410.02762v2.pdf","comment":"Accepted to ICLR '25. Project page:\n  http://anishk23733.github.io/vl-interp/. V2 added more experiments in\n  appendix"},{"id":"http://arxiv.org/abs/2405.09806v4","updated":"2025-02-10T20:00:24Z","published":"2024-05-16T04:28:44Z","title":"MediSyn: A Generalist Text-Guided Latent Diffusion Model For Diverse\n  Medical Image Synthesis","summary":"  Deep learning algorithms require extensive data to achieve robust\nperformance. However, data availability is often restricted in the medical\ndomain due to patient privacy concerns. Synthetic data presents a possible\nsolution to these challenges. Recently, image generative models have found\nincreasing use for medical applications but are often designed for singular\nmedical specialties and imaging modalities, thus limiting their broader\nutility. To address this, we introduce MediSyn: a text-guided, latent diffusion\nmodel capable of generating synthetic images from 6 medical specialties and 10\nimage types. The synthetic images are validated by expert clinicians for\nalignment with their corresponding text prompts. Furthermore, a direct\ncomparison of the synthetic images against the real images confirms that our\nmodel synthesizes novel images and, crucially, may preserve patient privacy.\nFinally, classifiers trained on a mixture of synthetic and real data achieve\nsimilar performance to those trained on twice the amount of real data. Our\nfindings highlight the immense potential for generalist image generative models\nto accelerate algorithmic research and development in medicine.\n","authors":["Joseph Cho","Mrudang Mathur","Cyril Zakka","Dhamanpreet Kaur","Matthew Leipzig","Alex Dalal","Aravind Krishnan","Eubee Koo","Karen Wai","Cindy S. Zhao","Rohan Shad","Robyn Fong","Ross Wightman","Akshay Chaudhari","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2405.09806v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07827v1","updated":"2025-02-10T19:59:31Z","published":"2025-02-10T19:59:31Z","title":"Implicit Language Models are RNNs: Balancing Parallelization and\n  Expressivity","summary":"  State-space models (SSMs) and transformers dominate the language modeling\nlandscape. However, they are constrained to a lower computational complexity\nthan classical recurrent neural networks (RNNs), limiting their expressivity.\nIn contrast, RNNs lack parallelization during training, raising fundamental\nquestions about the trade off between parallelization and expressivity. We\npropose implicit SSMs, which iterate a transformation until convergence to a\nfixed point. Theoretically, we show that implicit SSMs implement the non-linear\nstate-transitions of RNNs. Empirically, we find that only approximate\nfixed-point convergence suffices, enabling the design of a scalable training\ncurriculum that largely retains parallelization, with full convergence required\nonly for a small subset of tokens. Our approach demonstrates superior\nstate-tracking capabilities on regular languages, surpassing transformers and\nSSMs. We further scale implicit SSMs to natural language reasoning tasks and\npretraining of large-scale language models up to 1.3B parameters on 207B tokens\n- representing, to our knowledge, the largest implicit model trained to date.\nNotably, our implicit models outperform their explicit counterparts on standard\nbenchmarks.\n","authors":["Mark Sch√∂ne","Babak Rahmani","Heiner Kremer","Fabian Falck","Hitesh Ballani","Jannes Gladrow"],"pdf_url":"https://arxiv.org/pdf/2502.07827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04430v2","updated":"2025-02-10T19:55:17Z","published":"2024-11-07T04:52:18Z","title":"Towards Unifying Interpretability and Control: Evaluation via\n  Intervention","summary":"  With the growing complexity and capability of large language models, a need\nto understand model reasoning has emerged, often motivated by an underlying\ngoal of controlling and aligning models. While numerous interpretability and\nsteering methods have been proposed as solutions, they are typically designed\neither for understanding or for control, seldom addressing both. Additionally,\nthe lack of standardized applications, motivations, and evaluation metrics\nmakes it difficult to assess methods' practical utility and efficacy. To\naddress the aforementioned issues, we argue that intervention is a fundamental\ngoal of interpretability and introduce success criteria to evaluate how well\nmethods can control model behavior through interventions. To evaluate existing\nmethods for this ability, we unify and extend four popular interpretability\nmethods-sparse autoencoders, logit lens, tuned lens, and probing-into an\nabstract encoder-decoder framework, enabling interventions on interpretable\nfeatures that can be mapped back to latent representations to control model\noutputs. We introduce two new evaluation metrics: intervention success rate and\ncoherence-intervention tradeoff, designed to measure the accuracy of\nexplanations and their utility in controlling model behavior. Our findings\nreveal that (1) while current methods allow for intervention, their\neffectiveness is inconsistent across features and models, (2) lens-based\nmethods outperform SAEs and probes in achieving simple, concrete interventions,\nand (3) mechanistic interventions often compromise model coherence,\nunderperforming simpler alternatives, such as prompting, and highlighting a\ncritical shortcoming of current interpretability approaches in applications\nrequiring control.\n","authors":["Usha Bhalla","Suraj Srinivas","Asma Ghandeharioun","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2411.04430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07001v1","updated":"2025-02-10T19:53:46Z","published":"2025-02-10T19:53:46Z","title":"From Image to Video: An Empirical Study of Diffusion Representations","summary":"  Diffusion models have revolutionized generative modeling, enabling\nunprecedented realism in image and video synthesis. This success has sparked\ninterest in leveraging their representations for visual understanding tasks.\nWhile recent works have explored this potential for image generation, the\nvisual understanding capabilities of video diffusion models remain largely\nuncharted. To address this gap, we systematically compare the same model\narchitecture trained for video versus image generation, analyzing the\nperformance of their latent representations on various downstream tasks\nincluding image classification, action recognition, depth estimation, and\ntracking. Results show that video diffusion models consistently outperform\ntheir image counterparts, though we find a striking range in the extent of this\nsuperiority. We further analyze features extracted from different layers and\nwith varying noise levels, as well as the effect of model size and training\nbudget on representation and generation quality. This work marks the first\ndirect comparison of video and image diffusion objectives for visual\nunderstanding, offering insights into the role of temporal information in\nrepresentation learning.\n","authors":["Pedro V√©lez","Luisa F. Polan√≠a","Yi Yang","Chuhan Zhang","Rishab Kabra","Anurag Arnab","Mehdi S. M. Sajjadi"],"pdf_url":"https://arxiv.org/pdf/2502.07001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22564v2","updated":"2025-02-10T19:52:40Z","published":"2024-10-29T22:09:31Z","title":"Vertical Federated Learning with Missing Features During Training and\n  Inference","summary":"  Vertical federated learning trains models from feature-partitioned datasets\nacross multiple clients, who collaborate without sharing their local data.\nStandard approaches assume that all feature partitions are available during\nboth training and inference. Yet, in practice, this assumption rarely holds, as\nfor many samples only a subset of the clients observe their partition. However,\nnot utilizing incomplete samples during training harms generalization, and not\nsupporting them during inference limits the utility of the model. Moreover, if\nany client leaves the federation after training, its partition becomes\nunavailable, rendering the learned model unusable. Missing feature blocks are\ntherefore a key challenge limiting the applicability of vertical federated\nlearning in real-world scenarios. To address this, we propose LASER-VFL, a\nvertical federated learning method for efficient training and inference of\nsplit neural network-based models that is capable of handling arbitrary sets of\npartitions. Our approach is simple yet effective, relying on the sharing of\nmodel parameters and on task-sampling to train a family of predictors. We show\nthat LASER-VFL achieves a $\\mathcal{O}({1}/{\\sqrt{T}})$ convergence rate for\nnonconvex objectives and, under the Polyak-{\\L}ojasiewicz inequality, it\nachieves linear convergence to a neighborhood of the optimum. Numerical\nexperiments show improved performance of LASER-VFL over the baselines.\nRemarkably, this is the case even in the absence of missing features. For\nexample, for CIFAR-100, we see an improvement in accuracy of $18.2\\%$ when each\nof four feature blocks is observed with a probability of 0.5 and of $7.4\\%$\nwhen all features are observed. The code for this work is available at\nhttps://github.com/Valdeira/LASER-VFL.\n","authors":["Pedro Valdeira","Shiqiang Wang","Yuejie Chi"],"pdf_url":"https://arxiv.org/pdf/2410.22564v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.06999v1","updated":"2025-02-10T19:49:54Z","published":"2025-02-10T19:49:54Z","title":"Outsourced diffusion sampling: Efficient posterior inference in latent\n  spaces of generative models","summary":"  Any well-behaved generative model over a variable $\\mathbf{x}$ can be\nexpressed as a deterministic transformation of an exogenous ('outsourced')\nGaussian noise variable $\\mathbf{z}$: $\\mathbf{x}=f_\\theta(\\mathbf{z})$. In\nsuch a model (e.g., a VAE, GAN, or continuous-time flow-based model), sampling\nof the target variable $\\mathbf{x} \\sim p_\\theta(\\mathbf{x})$ is\nstraightforward, but sampling from a posterior distribution of the form\n$p(\\mathbf{x}\\mid\\mathbf{y}) \\propto\np_\\theta(\\mathbf{x})r(\\mathbf{x},\\mathbf{y})$, where $r$ is a constraint\nfunction depending on an auxiliary variable $\\mathbf{y}$, is generally\nintractable. We propose to amortize the cost of sampling from such posterior\ndistributions with diffusion models that sample a distribution in the noise\nspace ($\\mathbf{z}$). These diffusion samplers are trained by reinforcement\nlearning algorithms to enforce that the transformed samples\n$f_\\theta(\\mathbf{z})$ are distributed according to the posterior in the data\nspace ($\\mathbf{x}$). For many models and constraints of interest, the\nposterior in the noise space is smoother than the posterior in the data space,\nmaking it more amenable to such amortized inference. Our method enables\nconditional sampling under unconditional GAN, (H)VAE, and flow-based priors,\ncomparing favorably both with current amortized and non-amortized inference\nmethods. We demonstrate the proposed outsourced diffusion sampling in several\nexperiments with large pretrained prior models: conditional image generation,\nreinforcement learning with human feedback, and protein structure generation.\n","authors":["Siddarth Venkatraman","Mohsin Hasan","Minsu Kim","Luca Scimeca","Marcin Sendera","Yoshua Bengio","Glen Berseth","Nikolay Malkin"],"pdf_url":"https://arxiv.org/pdf/2502.06999v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06995v1","updated":"2025-02-10T19:42:54Z","published":"2025-02-10T19:42:54Z","title":"Epistemic Uncertainty in Conformal Scores: A Unified Approach","summary":"  Conformal prediction methods create prediction bands with distribution-free\nguarantees but do not explicitly capture epistemic uncertainty, which can lead\nto overconfident predictions in data-sparse regions. Although recent conformal\nscores have been developed to address this limitation, they are typically\ndesigned for specific tasks, such as regression or quantile regression.\nMoreover, they rely on particular modeling choices for epistemic uncertainty,\nrestricting their applicability. We introduce $\\texttt{EPICSCORE}$, a\nmodel-agnostic approach that enhances any conformal score by explicitly\nintegrating epistemic uncertainty. Leveraging Bayesian techniques such as\nGaussian Processes, Monte Carlo Dropout, or Bayesian Additive Regression Trees,\n$\\texttt{EPICSCORE}$ adaptively expands predictive intervals in regions with\nlimited data while maintaining compact intervals where data is abundant. As\nwith any conformal method, it preserves finite-sample marginal coverage.\nAdditionally, it also achieves asymptotic conditional coverage. Experiments\ndemonstrate its good performance compared to existing methods. Designed for\ncompatibility with any Bayesian model, but equipped with distribution-free\nguarantees, $\\texttt{EPICSCORE}$ provides a general-purpose framework for\nuncertainty quantification in prediction problems.\n","authors":["Luben M. C. Cabezas","Vagner S. Santos","Thiago R. Ramos","Rafael Izbicki"],"pdf_url":"https://arxiv.org/pdf/2502.06995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00397v3","updated":"2025-02-10T19:33:03Z","published":"2024-06-29T10:50:23Z","title":"Learning Time-Varying Multi-Region Communications via Scalable Markovian\n  Gaussian Processes","summary":"  Understanding and constructing brain communications that capture dynamic\ncommunications across multiple regions is fundamental to modern system\nneuroscience, yet current methods struggle to find time-varying region-level\ncommunications or scale to large neural datasets with long recording durations.\nWe present a novel framework using Markovian Gaussian Processes to learn brain\ncommunications with time-varying temporal delays from multi-region neural\nrecordings, named Adaptive Delay Model (ADM). Our method combines Gaussian\nProcesses with State Space Models and employs parallel scan inference\nalgorithms, enabling efficient scaling to large datasets while identifying\nconcurrent communication patterns that evolve over time. This time-varying\napproach captures how brain region interactions shift dynamically during\ncognitive processes. Validated on synthetic and multi-region neural recordings\ndatasets, our approach discovers both the directionality and temporal dynamics\nof neural communication. This work advances our understanding of distributed\nneural computation and provides a scalable tool for analyzing dynamic brain\nnetworks.\n","authors":["Weihan Li","Yule Wang","Chengrui Li","Anqi Wu"],"pdf_url":"https://arxiv.org/pdf/2407.00397v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06982v1","updated":"2025-02-10T19:20:02Z","published":"2025-02-10T19:20:02Z","title":"Machine Learning Fleet Efficiency: Analyzing and Optimizing Large-Scale\n  Google TPU Systems with ML Productivity Goodput","summary":"  Recent years have seen the emergence of machine learning (ML) workloads\ndeployed in warehouse-scale computing (WSC) settings, also known as ML fleets.\nAs the computational demands placed on ML fleets have increased due to the rise\nof large models and growing demand for ML applications, it has become\nincreasingly critical to measure and improve the efficiency of such systems.\nHowever, there is not yet an established methodology to characterize ML fleet\nperformance and identify potential performance optimizations accordingly. This\npaper presents a large-scale analysis of an ML fleet based on Google's TPUs,\nintroducing a framework to capture fleet-wide efficiency, systematically\nevaluate performance characteristics, and identify optimization strategies for\nthe fleet. We begin by defining an ML fleet, outlining its components, and\nanalyzing an example Google ML fleet in production comprising thousands of\naccelerators running diverse workloads. Our study reveals several critical\ninsights: first, ML fleets extend beyond the hardware layer, with model, data,\nframework, compiler, and scheduling layers significantly impacting performance;\nsecond, the heterogeneous nature of ML fleets poses challenges in\ncharacterizing individual workload performance; and third, traditional\nutilization-based metrics prove insufficient for ML fleet characterization. To\naddress these challenges, we present the \"ML Productivity Goodput\" (MPG) metric\nto measure ML fleet efficiency. We show how to leverage this metric to\ncharacterize the fleet across the ML system stack. We also present methods to\nidentify and optimize performance bottlenecks using MPG, providing strategies\nfor managing warehouse-scale ML systems in general. Lastly, we demonstrate\nquantitative evaluations from applying these methods to a real ML fleet for\ninternal-facing Google TPU workloads, where we observed tangible improvements.\n","authors":["Arissa Wongpanich","Tayo Oguntebi","Jose Baiocchi Paredes","Yu Emma Wang","Phitchaya Mangpo Phothilimthana","Ritwika Mitra","Zongwei Zhou","Naveen Kumar","Vijay Janapa Reddi"],"pdf_url":"https://arxiv.org/pdf/2502.06982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06978v1","updated":"2025-02-10T19:18:18Z","published":"2025-02-10T19:18:18Z","title":"Dual Conic Proxy for Semidefinite Relaxation of AC Optimal Power Flow","summary":"  The nonlinear, non-convex AC Optimal Power Flow (AC-OPF) problem is\nfundamental for power systems operations. The intrinsic complexity of AC-OPF\nhas fueled a growing interest in the development of optimization proxies for\nthe problem, i.e., machine learning models that predict high-quality,\nclose-to-optimal solutions. More recently, dual conic proxy architectures have\nbeen proposed, which combine machine learning and convex relaxations of AC-OPF,\nto provide valid certificates of optimality using learning-based methods.\nBuilding on this methodology, this paper proposes, for the first time, a dual\nconic proxy architecture for the semidefinite (SDP) relaxation of AC-OPF\nproblems. Although the SDP relaxation is stronger than the second-order cone\nrelaxation considered in previous work, its practical use has been hindered by\nits computational cost. The proposed method combines a neural network with a\ndifferentiable dual completion strategy that leverages the structure of the\ndual SDP problem. This approach guarantees dual feasibility, and therefore\nvalid dual bounds, while providing orders of magnitude of speedups compared to\ninterior-point algorithms. The paper also leverages self-supervised learning,\nwhich alleviates the need for time-consuming data generation and allows to\ntrain the proposed models efficiently. Numerical experiments are presented on\nseveral power grid benchmarks with up to 500 buses. The results demonstrate\nthat the proposed SDP-based proxies can outperform weaker conic relaxations,\nwhile providing several orders of magnitude speedups compared to a\nstate-of-the-art interior-point SDP solver.\n","authors":["Guancheng Qiu","Mathieu Tanneau","Pascal Van Hentenryck"],"pdf_url":"https://arxiv.org/pdf/2502.06978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06971v1","updated":"2025-02-10T19:12:12Z","published":"2025-02-10T19:12:12Z","title":"User-Preference Meets Pareto-Optimality: Multi-Objective Bayesian\n  Optimization with Local Gradient Search","summary":"  Incorporating user preferences into multi-objective Bayesian optimization\n(MOBO) allows for personalization of the optimization procedure. Preferences\nare often abstracted in the form of an unknown utility function, estimated\nthrough pairwise comparisons of potential outcomes. However, utility-driven\nMOBO methods can yield solutions that are dominated by nearby solutions, as\nnon-dominance is not enforced. Additionally, classical MOBO commonly relies on\nestimating the entire Pareto-front to identify the Pareto-optimal solutions,\nwhich can be expensive and ignore user preferences. Here, we present a new\nmethod, termed preference-utility-balanced MOBO (PUB-MOBO), that allows users\nto disambiguate between near-Pareto candidate solutions. PUB-MOBO combines\nutility-based MOBO with local multi-gradient descent to refine user-preferred\nsolutions to be near-Pareto-optimal. To this end, we propose a novel\npreference-dominated utility function that concurrently preserves\nuser-preferences and dominance amongst candidate solutions. A key advantage of\nPUB-MOBO is that the local search is restricted to a (small) region of the\nPareto-front directed by user preferences, alleviating the need to estimate the\nentire Pareto-front. PUB-MOBO is tested on three synthetic benchmark problems:\nDTLZ1, DTLZ2 and DH1, as well as on three real-world problems: Vehicle Safety,\nConceptual Marine Design, and Car Side Impact. PUB-MOBO consistently\noutperforms state-of-the-art competitors in terms of proximity to the\nPareto-front and utility regret across all the problems.\n","authors":["Joshua Hang Sai Ip","Ankush Chakrabarty","Ali Mesbah","Diego Romeres"],"pdf_url":"https://arxiv.org/pdf/2502.06971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06970v1","updated":"2025-02-10T19:11:48Z","published":"2025-02-10T19:11:48Z","title":"Model Diffusion for Certifiable Few-shot Transfer Learning","summary":"  In modern large-scale deep learning, a prevalent and effective workflow for\nsolving low-data problems is adapting powerful pre-trained foundation models\n(FMs) to new tasks via parameter-efficient fine-tuning (PEFT). However, while\nempirically effective, the resulting solutions lack generalisation guarantees\nto certify their accuracy - which may be required for ethical or legal reasons\nprior to deployment in high-importance applications. In this paper we develop a\nnovel transfer learning approach that is designed to facilitate non-vacuous\nlearning theoretic generalisation guarantees for downstream tasks, even in the\nlow-shot regime. Specifically, we first use upstream tasks to train a\ndistribution over PEFT parameters. We then learn the downstream task by a\nsample-and-evaluate procedure -- sampling plausible PEFTs from the trained\ndiffusion model and selecting the one with the highest likelihood on the\ndownstream data. Crucially, this confines our model hypothesis to a finite set\nof PEFT samples. In contrast to learning in the typical continuous hypothesis\nspaces of neural network weights, this facilitates tighter risk certificates.\nWe instantiate our bound and show non-trivial generalization guarantees\ncompared to existing learning approaches which lead to vacuous bounds in the\nlow-shot regime.\n","authors":["Fady Rezk","Royson Lee","Henry Gouk","Timothy Hospedales","Minyoung Kim"],"pdf_url":"https://arxiv.org/pdf/2502.06970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18001v3","updated":"2025-02-10T19:05:34Z","published":"2023-10-27T09:17:15Z","title":"DP-SGD with weight clipping","summary":"  Recently, due to the popularity of deep neural networks and other methods\nwhose training typically relies on the optimization of an objective function,\nand due to concerns for data privacy, there is a lot of interest in\ndifferentially private gradient descent methods. To achieve differential\nprivacy guarantees with a minimum amount of noise, it is important to be able\nto bound precisely the sensitivity of the information which the participants\nwill observe. In this study, we present a novel approach that mitigates the\nbias arising from traditional gradient clipping. By leveraging a public upper\nbound of the Lipschitz value of the current model and its current location\nwithin the search domain, we can achieve refined noise level adjustments. We\npresent a new algorithm with improved differential privacy guarantees and a\nsystematic empirical evaluation, showing that our new approach outperforms\nexisting approaches also in practice.\n","authors":["Antoine Barczewski","Jan Ramon"],"pdf_url":"https://arxiv.org/pdf/2310.18001v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03321v2","updated":"2025-02-10T19:05:03Z","published":"2024-07-03T17:59:53Z","title":"Planetarium: A Rigorous Benchmark for Translating Text to Structured\n  Planning Languages","summary":"  Recent works have explored using language models for planning problems. One\napproach examines translating natural language descriptions of planning tasks\ninto structured planning languages, such as the planning domain definition\nlanguage (PDDL). Existing evaluation methods struggle to ensure semantic\ncorrectness and rely on simple or unrealistic datasets. To bridge this gap, we\nintroduce \\textit{Planetarium}, a benchmark designed to evaluate language\nmodels' ability to generate PDDL code from natural language descriptions of\nplanning tasks. \\textit{Planetarium} features a novel PDDL equivalence\nalgorithm that flexibly evaluates the correctness of generated PDDL, along with\na dataset of 145,918 text-to-PDDL pairs across 73 unique state combinations\nwith varying levels of difficulty. Finally, we evaluate several API-access and\nopen-weight language models that reveal this task's complexity. For example,\n96.1\\% of the PDDL problem descriptions generated by GPT-4o are syntactically\nparseable, 94.4\\% are solvable, but only 24.8\\% are semantically correct,\nhighlighting the need for a more rigorous benchmark for this problem.\n","authors":["Max Zuo","Francisco Piedrahita Velez","Xiaochen Li","Michael L. Littman","Stephen H. Bach"],"pdf_url":"https://arxiv.org/pdf/2407.03321v2.pdf","comment":"NAACL Main Conference 2025"},{"id":"http://arxiv.org/abs/2502.06963v1","updated":"2025-02-10T19:02:20Z","published":"2025-02-10T19:02:20Z","title":"Task Offloading in Vehicular Edge Computing using Deep Reinforcement\n  Learning: A Survey","summary":"  The increasing demand for Intelligent Transportation Systems (ITS) has\nintroduced significant challenges in managing the complex,\ncomputation-intensive tasks generated by modern vehicles while offloading tasks\nto external computing infrastructures such as edge computing (EC), nearby\nvehicular , and UAVs has become influential solution to these challenges.\nHowever, traditional computational offloading strategies often struggle to\nadapt to the dynamic and heterogeneous nature of vehicular environments. In\nthis study, we explored the potential of Reinforcement Learning (RL) and Deep\nReinforcement Learning (DRL) frameworks to optimize computational offloading\nthrough adaptive, real-time decision-making, and we have thoroughly\ninvestigated the Markov Decision Process (MDP) approaches on the existing\nliterature. The paper focuses on key aspects such as standardized learning\nmodels, optimized reward structures, and collaborative multi-agent systems,\naiming to advance the understanding and application of DRL in vehicular\nnetworks. Our findings offer insights into enhancing the efficiency,\nscalability, and robustness of ITS, setting the stage for future innovations in\nthis rapidly evolving field.\n","authors":["Ashab Uddin","Ahmed Hamdi Sakr","Ning Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06963v1.pdf","comment":"27 Pages, 3 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2502.06939v1","updated":"2025-02-10T19:00:00Z","published":"2025-02-10T19:00:00Z","title":"Generalizable automated ischaemic stroke lesion segmentation with vision\n  transformers","summary":"  Ischaemic stroke, a leading cause of death and disability, critically relies\non neuroimaging for characterising the anatomical pattern of injury.\nDiffusion-weighted imaging (DWI) provides the highest expressivity in ischemic\nstroke but poses substantial challenges for automated lesion segmentation:\nsusceptibility artefacts, morphological heterogeneity, age-related\ncomorbidities, time-dependent signal dynamics, instrumental variability, and\nlimited labelled data. Current U-Net-based models therefore underperform, a\nproblem accentuated by inadequate evaluation metrics that focus on mean\nperformance, neglecting anatomical, subpopulation, and acquisition-dependent\nvariability. Here, we present a high-performance DWI lesion segmentation tool\naddressing these challenges through optimized vision transformer-based\narchitectures, integration of 3563 annotated lesions from multi-site data, and\nalgorithmic enhancements, achieving state-of-the-art results. We further\npropose a novel evaluative framework assessing model fidelity, equity (across\ndemographics and lesion subtypes), anatomical precision, and robustness to\ninstrumental variability, promoting clinical and research utility. This work\nadvances stroke imaging by reconciling model expressivity with domain-specific\nchallenges and redefining performance benchmarks to prioritize equity and\ngeneralizability, critical for personalized medicine and mechanistic research.\n","authors":["Chris Foulon","Robert Gray","James K. Ruffle","Jonathan Best","Tianbo Xu","Henry Watkins","Jane Rondina","Guilherme Pombo","Dominic Giles","Paul Wright","Marcela Ovando-Tellez","H. Rolf J√§ger","Jorge Cardoso","Sebastien Ourselin","Geraint Rees","Parashkev Nachev"],"pdf_url":"https://arxiv.org/pdf/2502.06939v1.pdf","comment":"29 pages, 7 figures, 2 tables, 1 supplementary table, 2 supplementary\n  figures"},{"id":"http://arxiv.org/abs/2502.06786v1","updated":"2025-02-10T18:59:10Z","published":"2025-02-10T18:59:10Z","title":"Matryoshka Quantization","summary":"  Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits. This\npaper proposes Matryoshka Quantization (MatQuant), a novel multi-scale\nquantization technique that addresses the challenge of needing multiple\nquantized models. It allows training and maintaining just one model, which can\nthen be served at different precision levels. Furthermore, due to the\nco-training and co-distillation regularization provided by MatQuant, the int2\nprecision models extracted by MatQuant can be up to $10\\%$ more accurate than\nstandard int2 quantization (using techniques like QAT or OmniQuant). This\nrepresents significant progress in model quantization, demonstrated by the fact\nthat, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more\naccurate than an int8 FFN-quantized Gemma-2 2B model.\n","authors":["Pranav Nair","Puranjay Datta","Jeff Dean","Prateek Jain","Aditya Kusupati"],"pdf_url":"https://arxiv.org/pdf/2502.06786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06785v1","updated":"2025-02-10T18:58:52Z","published":"2025-02-10T18:58:52Z","title":"DeepCrossAttention: Supercharging Transformer Residual Connections","summary":"  Transformer networks have achieved remarkable success across diverse domains,\nleveraging a variety of architectural innovations, including residual\nconnections. However, traditional residual connections, which simply sum the\noutputs of previous layers, can dilute crucial information. This work\nintroduces DeepCrossAttention (DCA), an approach that enhances residual\nlearning in transformers. DCA employs learnable, input-dependent weights to\ndynamically combine layer outputs, enabling the model to selectively focus on\nthe most relevant information in any of the previous layers. Furthermore, DCA\nincorporates depth-wise cross-attention, allowing for richer interactions\nbetween layers at different depths. Our language modeling experiments show that\nDCA achieves improved perplexity for a given training time. Moreover, DCA\nobtains the same model quality up to 3x faster while adding a negligible number\nof parameters. Theoretical analysis confirms that DCA provides an improved\ntrade-off between accuracy and model size when the ratio of collective layer\nranks to the ambient dimension falls below a critical threshold.\n","authors":["Mike Heddes","Adel Javanmard","Kyriakos Axiotis","Gang Fu","MohammadHossein Bateni","Vahab Mirrokni"],"pdf_url":"https://arxiv.org/pdf/2502.06785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06784v1","updated":"2025-02-10T18:58:40Z","published":"2025-02-10T18:58:40Z","title":"RelGNN: Composite Message Passing for Relational Deep Learning","summary":"  Predictive tasks on relational databases are critical in real-world\napplications spanning e-commerce, healthcare, and social media. To address\nthese tasks effectively, Relational Deep Learning (RDL) encodes relational data\nas graphs, enabling Graph Neural Networks (GNNs) to exploit relational\nstructures for improved predictions. However, existing heterogeneous GNNs often\noverlook the intrinsic structural properties of relational databases, leading\nto modeling inefficiencies. Here we introduce RelGNN, a novel GNN framework\nspecifically designed to capture the unique characteristics of relational\ndatabases. At the core of our approach is the introduction of atomic routes,\nwhich are sequences of nodes forming high-order tripartite structures. Building\nupon these atomic routes, RelGNN designs new composite message passing\nmechanisms between heterogeneous nodes, allowing direct single-hop interactions\nbetween them. This approach avoids redundant aggregations and mitigates\ninformation entanglement, ultimately leading to more efficient and accurate\npredictive modeling. RelGNN is evaluated on 30 diverse real-world tasks from\nRelBench (Fey et al., 2024), and consistently achieves state-of-the-art\naccuracy with up to 25% improvement.\n","authors":["Tianlang Chen","Charilaos Kanatsoulis","Jure Leskovec"],"pdf_url":"https://arxiv.org/pdf/2502.06784v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2502.06781v1","updated":"2025-02-10T18:57:29Z","published":"2025-02-10T18:57:29Z","title":"Exploring the Limit of Outcome Reward for Learning Mathematical\n  Reasoning","summary":"  Reasoning abilities, especially those for solving complex math problems, are\ncrucial components of general intelligence. Recent advances by proprietary\ncompanies, such as o-series models of OpenAI, have made remarkable progress on\nreasoning tasks. However, the complete technical details remain unrevealed, and\nthe techniques that are believed certainly to be adopted are only reinforcement\nlearning (RL) and the long chain of thoughts. This paper proposes a new RL\nframework, termed OREAL, to pursue the performance limit that can be achieved\nthrough \\textbf{O}utcome \\textbf{RE}w\\textbf{A}rd-based reinforcement\n\\textbf{L}earning for mathematical reasoning tasks, where only binary outcome\nrewards are easily accessible. We theoretically prove that behavior cloning on\npositive trajectories from best-of-N (BoN) sampling is sufficient to learn the\nKL-regularized optimal policy in binary feedback environments. This formulation\nfurther implies that the rewards of negative samples should be reshaped to\nensure the gradient consistency between positive and negative samples. To\nalleviate the long-existing difficulties brought by sparse rewards in RL, which\nare even exacerbated by the partial correctness of the long chain of thought\nfor reasoning tasks, we further apply a token-level reward model to sample\nimportant tokens in reasoning trajectories for learning. With OREAL, for the\nfirst time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL,\nbeing on par with 32B models. OREAL-32B also surpasses previous 32B models\ntrained by distillation with 95.0 pass@1 accuracy on MATH-500. Our\ninvestigation also indicates the importance of initial policy models and\ntraining queries for RL. Code, models, and data will be released to benefit\nfuture research\\footnote{https://github.com/InternLM/OREAL}.\n","authors":["Chengqi Lyu","Songyang Gao","Yuzhe Gu","Wenwei Zhang","Jianfei Gao","Kuikun Liu","Ziyi Wang","Shuaibin Li","Qian Zhao","Haian Huang","Weihan Cao","Jiangning Liu","Hongwei Liu","Junnan Liu","Songyang Zhang","Dahua Lin","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06781v1.pdf","comment":"We released our code, data, and model on\n  https://github.com/InternLM/OREAL"},{"id":"http://arxiv.org/abs/2502.06777v1","updated":"2025-02-10T18:54:41Z","published":"2025-02-10T18:54:41Z","title":"Learning an Optimal Assortment Policy under Observational Data","summary":"  We study the fundamental problem of offline assortment optimization under the\nMultinomial Logit (MNL) model, where sellers must determine the optimal subset\nof the products to offer based solely on historical customer choice data. While\nmost existing approaches to learning-based assortment optimization focus on the\nonline learning of the optimal assortment through repeated interactions with\ncustomers, such exploration can be costly or even impractical in many\nreal-world settings. In this paper, we consider the offline learning paradigm\nand investigate the minimal data requirements for efficient offline assortment\noptimization. To this end, we introduce Pessimistic Rank-Breaking (PRB), an\nalgorithm that combines rank-breaking with pessimistic estimation. We prove\nthat PRB is nearly minimax optimal by establishing the tight suboptimality\nupper bound and a nearly matching lower bound. This further shows that \"optimal\nitem coverage\" - where each item in the optimal assortment appears sufficiently\noften in the historical data - is both sufficient and necessary for efficient\noffline learning. This significantly relaxes the previous requirement of\nobserving the complete optimal assortment in the data. Our results provide\nfundamental insights into the data requirements for offline assortment\noptimization under the MNL model.\n","authors":["Yuxuan Han","Han Zhong","Miao Lu","Jose Blanchet","Zhengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.06777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06776v1","updated":"2025-02-10T18:54:05Z","published":"2025-02-10T18:54:05Z","title":"Towards Internet-Scale Training For Agents","summary":"  The predominant approach for training web navigation agents gathers human\ndemonstrations for a set of popular websites and hand-written tasks, but it is\nbecoming clear that human data are an inefficient resource. We develop a\npipeline to facilitate Internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM generates tasks for 150k diverse\nwebsites. In the next stage, LLM agents complete tasks and produce\ntrajectories. In the final stage, an LLM reviews the trajectories and judges\ntheir success. Language models are competitive with human annotators, detecting\nand filtering out harmful content with an accuracy of 97%, generating feasible\ntasks with an 89% rate, and judging successful trajectories with an 82.6%\naccuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of\ntasks for 150k sites. Training on the data generated by our pipeline is\ncompetitive with training on human demonstrations. In data-limited settings\nderived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and\n+122.1% respectively for agents trained on mixtures of data from our pipeline,\nand human data. When training agents with all available human data from these\nbenchmarks, agents fail to generalize to diverse real sites, and adding our\ndata improves their generalization by +149.0% for WebLINX and +156.3% for\nMind2Web. Code will be available at: data-for-agents.github.io.\n","authors":["Brandon Trabucco","Gunnar Sigurdsson","Robinson Piramuthu","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2502.06776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06775v1","updated":"2025-02-10T18:53:15Z","published":"2025-02-10T18:53:15Z","title":"Enhancing Performance of Explainable AI Models with Constrained Concept\n  Refinement","summary":"  The trade-off between accuracy and interpretability has long been a challenge\nin machine learning (ML). This tension is particularly significant for emerging\ninterpretable-by-design methods, which aim to redesign ML algorithms for\ntrustworthy interpretability but often sacrifice accuracy in the process. In\nthis paper, we address this gap by investigating the impact of deviations in\nconcept representations-an essential component of interpretable models-on\nprediction performance and propose a novel framework to mitigate these effects.\nThe framework builds on the principle of optimizing concept embeddings under\nconstraints that preserve interpretability. Using a generative model as a\ntest-bed, we rigorously prove that our algorithm achieves zero loss while\nprogressively enhancing the interpretability of the resulting model.\nAdditionally, we evaluate the practical performance of our proposed framework\nin generating explainable predictions for image classification tasks across\nvarious benchmarks. Compared to existing explainable methods, our approach not\nonly improves prediction accuracy while preserving model interpretability\nacross various large-scale benchmarks but also achieves this with significantly\nlower computational cost.\n","authors":["Geyu Liang","Senne Michielssen","Salar Fattahi"],"pdf_url":"https://arxiv.org/pdf/2502.06775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14161v3","updated":"2025-02-10T18:52:39Z","published":"2024-09-21T14:53:32Z","title":"When Witnesses Defend: A Witness Graph Topological Layer for Adversarial\n  Graph Learning","summary":"  Capitalizing on the intuitive premise that shape characteristics are more\nrobust to perturbations, we bridge adversarial graph learning with the emerging\ntools from computational topology, namely, persistent homology representations\nof graphs. We introduce the concept of witness complex to adversarial analysis\non graphs, which allows us to focus only on the salient shape characteristics\nof graphs, yielded by the subset of the most essential nodes (i.e., landmarks),\nwith minimal loss of topological information on the whole graph. The remaining\nnodes are then used as witnesses, governing which higher-order graph\nsubstructures are incorporated into the learning process. Armed with the\nwitness mechanism, we design Witness Graph Topological Layer (WGTL), which\nsystematically integrates both local and global topological graph feature\nrepresentations, the impact of which is, in turn, automatically controlled by\nthe robust regularized topological loss. Given the attacker's budget, we derive\nthe important stability guarantees of both local and global topology encodings\nand the associated robust topological loss. We illustrate the versatility and\nefficiency of WGTL by its integration with five GNNs and three existing\nnon-topological defense mechanisms. Our extensive experiments across six\ndatasets demonstrate that WGTL boosts the robustness of GNNs across a range of\nperturbations and against a range of adversarial attacks. Our datasets and\nsource codes are available at https://github.com/toggled/WGTL.\n","authors":["Naheed Anjum Arafat","Debabrota Basu","Yulia Gel","Yuzhou Chen"],"pdf_url":"https://arxiv.org/pdf/2409.14161v3.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2502.06773v1","updated":"2025-02-10T18:52:04Z","published":"2025-02-10T18:52:04Z","title":"On the Emergence of Thinking in LLMs I: Searching for the Right\n  Intuition","summary":"  Recent AI advancements, such as OpenAI's new models, are transforming LLMs\ninto LRMs (Large Reasoning Models) that perform reasoning during inference,\ntaking extra time and compute for higher-quality outputs. We aim to uncover the\nalgorithmic framework for training LRMs. Methods like self-consistency, PRM,\nand AlphaZero suggest reasoning as guided search. We ask: what is the simplest,\nmost scalable way to enable search in LLMs?\n  We propose a post-training framework called Reinforcement Learning via\nSelf-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with\nhuman or synthetic demonstrations of the reasoning process, (2) using an\nexploration reward signal to encourage diverse and efficient reasoning\nbehaviors, and (3) RL training with an outcome verifier to ensure correctness\nwhile preventing reward hacking. Our key innovation is to decouple exploration\nand correctness signals during PPO training, carefully balancing them to\nimprove performance and efficiency.\n  Empirical studies in the math domain show that RLSP improves reasoning. On\nthe Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500\ntest set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due\nto RLSP. However, a more important finding of this work is that the models\ntrained using RLSP, even with the simplest exploration reward that encourages\nthe model to take more intermediate steps, showed several emergent behaviors\nsuch as backtracking, exploration of ideas, and verification. These findings\ndemonstrate that RLSP framework might be enough to enable emergence of complex\nreasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why\nRLSP search strategy is more suitable for LLMs inspired by a remarkable result\nthat says CoT provably increases computational power of LLMs, which grows as\nthe number of steps in CoT \\cite{li2024chain,merrill2023expresssive}.\n","authors":["Guanghao Ye","Khiem Duc Pham","Xinzhi Zhang","Sivakanth Gopi","Baolin Peng","Beibin Li","Janardhan Kulkarni","Huseyin A. Inan"],"pdf_url":"https://arxiv.org/pdf/2502.06773v1.pdf","comment":"Abstract shortened for arXiv"},{"id":"http://arxiv.org/abs/2502.06927v1","updated":"2025-02-10T18:51:57Z","published":"2025-02-10T18:51:57Z","title":"Neighborhood-Order Learning Graph Attention Network for Fake News\n  Detection","summary":"  Fake news detection is a significant challenge in the digital age, which has\nbecome increasingly important with the proliferation of social media and online\ncommunication networks. Graph Neural Networks (GNN)-based methods have shown\nhigh potential in analyzing graph-structured data for this problem. However, a\nmajor limitation in conventional GNN architectures is their inability to\neffectively utilize information from neighbors beyond the network's layer\ndepth, which can reduce the model's accuracy and effectiveness. In this paper,\nwe propose a novel model called Neighborhood-Order Learning Graph Attention\nNetwork (NOL-GAT) for fake news detection. This model allows each node in each\nlayer to independently learn its optimal neighborhood order. By doing so, the\nmodel can purposefully and efficiently extract critical information from\ndistant neighbors. The NOL-GAT architecture consists of two main components: a\nHop Network that determines the optimal neighborhood order and an Embedding\nNetwork that updates node embeddings using these optimal neighborhoods. To\nevaluate the model's performance, experiments are conducted on various fake\nnews datasets. Results demonstrate that NOL-GAT significantly outperforms\nbaseline models in metrics such as accuracy and F1-score, particularly in\nscenarios with limited labeled data. Features such as mitigating the\nover-squashing problem, improving information flow, and reducing computational\ncomplexity further highlight the advantages of the proposed model.\n","authors":["Batool Lakzaei","Mostafa Haghir Chehreghani","Alireza Bagheri"],"pdf_url":"https://arxiv.org/pdf/2502.06927v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2502.06772v1","updated":"2025-02-10T18:51:47Z","published":"2025-02-10T18:51:47Z","title":"ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates","summary":"  We present that hierarchical LLM reasoning via scaling thought templates can\neffectively optimize the reasoning search space and outperform the mathematical\nreasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.\nWe train our ReasonFlux-32B model with only 8 GPUs and introduces three\ninnovations: (i) a structured and generic thought template library, containing\naround 500 high-level thought templates capable of generalizing to similar or\nrelevant reasoning problems; (ii) performing hierarchical reinforcement\nlearning on a sequence of thought templates instead of long CoTs, optimizing a\nbase LLM to plan out an optimal template trajectory for gradually handling\ncomplex problems; (iii) a brand new inference scaling system that enables\nhierarchical LLM reasoning by adaptively scaling thought templates at inference\ntime. With a template trajectory containing sequential thought templates, our\nReasonFlux-32B significantly advances math reasoning capabilities to\nstate-of-the-art levels. Notably, on the MATH benchmark, it achieves an\naccuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad\n(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,\nsurpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:\nhttps://github.com/Gen-Verse/ReasonFlux\n","authors":["Ling Yang","Zhaochen Yu","Bin Cui","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06772v1.pdf","comment":"Code: https://github.com/Gen-Verse/ReasonFlux"},{"id":"http://arxiv.org/abs/2502.06771v1","updated":"2025-02-10T18:50:50Z","published":"2025-02-10T18:50:50Z","title":"Unsupervised Particle Tracking with Neuromorphic Computing","summary":"  We study the application of a neural network architecture for identifying\ncharged particle trajectories via unsupervised learning of delays and synaptic\nweights using a spike-time-dependent plasticity rule. In the considered model,\nthe neurons receive time-encoded information on the position of particle hits\nin a tracking detector for a particle collider, modeled according to the\ngeometry of the Compact Muon Solenoid Phase II detector. We show how a spiking\nneural network is capable of successfully identifying in a completely\nunsupervised way the signal left by charged particles in the presence of\nconspicuous noise from accidental or combinatorial hits. These results open the\nway to applications of neuromorphic computing to particle tracking, motivating\nfurther studies into its potential for real-time, low-power particle tracking\nin future high-energy physics experiments.\n","authors":["Emanuele Coradin","Fabio Cufino","Muhammad Awais","Tommaso Dorigo","Enrico Lupi","Eleonora Porcu","Jinu Raj","Fredrik Sandin","Mia Tosi"],"pdf_url":"https://arxiv.org/pdf/2502.06771v1.pdf","comment":"24 pages, 21 figures, submitted to MDPI Particles"},{"id":"http://arxiv.org/abs/2502.06768v1","updated":"2025-02-10T18:47:21Z","published":"2025-02-10T18:47:21Z","title":"Train for the Worst, Plan for the Best: Understanding Token Ordering in\n  Masked Diffusions","summary":"  In recent years, masked diffusion models (MDMs) have emerged as a promising\nalternative approach for generative modeling over discrete domains. Compared to\nautoregressive models (ARMs), MDMs trade off complexity at training time with\nflexibility at inference time. At training time, they must learn to solve an\nexponentially large number of infilling problems, but at inference time, they\ncan decode tokens in essentially arbitrary order. In this work, we closely\nexamine these two competing effects. On the training front, we theoretically\nand empirically demonstrate that MDMs indeed train on computationally\nintractable subproblems compared to their autoregressive counterparts. On the\ninference front, we show that a suitable strategy for adaptively choosing the\ntoken decoding order significantly enhances the capabilities of MDMs, allowing\nthem to sidestep hard subproblems. On logic puzzles like Sudoku, we show that\nadaptive inference can boost solving accuracy in pretrained MDMs from $<7$% to\n$\\approx 90$%, even outperforming ARMs with $7\\times$ as many parameters and\nthat were explicitly trained via teacher forcing to learn the right order of\ndecoding.\n","authors":["Jaeyeon Kim","Kulin Shah","Vasilis Kontonis","Sham Kakade","Sitan Chen"],"pdf_url":"https://arxiv.org/pdf/2502.06768v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06765v1","updated":"2025-02-10T18:44:30Z","published":"2025-02-10T18:44:30Z","title":"Are all models wrong? Fundamental limits in distribution-free empirical\n  model falsification","summary":"  In statistics and machine learning, when we train a fitted model on available\ndata, we typically want to ensure that we are searching within a model class\nthat contains at least one accurate model -- that is, we would like to ensure\nan upper bound on the model class risk (the lowest possible risk that can be\nattained by any model in the class). However, it is also of interest to\nestablish lower bounds on the model class risk, for instance so that we can\ndetermine whether our fitted model is at least approximately optimal within the\nclass, or, so that we can decide whether the model class is unsuitable for the\nparticular task at hand. Particularly in the setting of interpolation learning\nwhere machine learning models are trained to reach zero error on the training\ndata, we might ask if, at the very least, a positive lower bound on the model\nclass risk is possible -- or are we unable to detect that \"all models are\nwrong\"? In this work, we answer these questions in a distribution-free setting\nby establishing a model-agnostic, fundamental hardness result for the problem\nof constructing a lower bound on the best test error achievable over a model\nclass, and examine its implications on specific model classes such as\ntree-based methods and linear regression.\n","authors":["Manuel M. M√ºller","Yuetian Luo","Rina Foygel Barber"],"pdf_url":"https://arxiv.org/pdf/2502.06765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06764v1","updated":"2025-02-10T18:44:25Z","published":"2025-02-10T18:44:25Z","title":"History-Guided Video Diffusion","summary":"  Classifier-free guidance (CFG) is a key technique for improving conditional\ngeneration in diffusion models, enabling more accurate control while enhancing\nsample quality. It is natural to extend this technique to video diffusion,\nwhich generates video conditioned on a variable number of context frames,\ncollectively referred to as history. However, we find two key challenges to\nguiding with variable-length history: architectures that only support\nfixed-size conditioning, and the empirical observation that CFG-style history\ndropout performs poorly. To address this, we propose the Diffusion Forcing\nTransformer (DFoT), a video diffusion architecture and theoretically grounded\ntraining objective that jointly enable conditioning on a flexible number of\nhistory frames. We then introduce History Guidance, a family of guidance\nmethods uniquely enabled by DFoT. We show that its simplest form, vanilla\nhistory guidance, already significantly improves video generation quality and\ntemporal consistency. A more advanced method, history guidance across time and\nfrequency further enhances motion dynamics, enables compositional\ngeneralization to out-of-distribution history, and can stably roll out\nextremely long videos. Website: https://boyuan.space/history-guidance\n","authors":["Kiwhan Song","Boyuan Chen","Max Simchowitz","Yilun Du","Russ Tedrake","Vincent Sitzmann"],"pdf_url":"https://arxiv.org/pdf/2502.06764v1.pdf","comment":"Project Website: https://boyuan.space/history-guidance"},{"id":"http://arxiv.org/abs/2502.06761v1","updated":"2025-02-10T18:40:48Z","published":"2025-02-10T18:40:48Z","title":"When, Where and Why to Average Weights?","summary":"  Averaging checkpoints along the training trajectory is a simple yet powerful\napproach to improve the generalization performance of Machine Learning models\nand reduce training time. Motivated by these potential gains, and in an effort\nto fairly and thoroughly benchmark this technique, we present an extensive\nevaluation of averaging techniques in modern Deep Learning, which we perform\nusing AlgoPerf \\citep{dahl_benchmarking_2023}, a large-scale benchmark for\noptimization algorithms. We investigate whether weight averaging can reduce\ntraining time, improve generalization, and replace learning rate decay, as\nsuggested by recent literature. Our evaluation across seven architectures and\ndatasets reveals that averaging significantly accelerates training and yields\nconsiderable efficiency gains, at the price of a minimal implementation and\nmemory cost, while mildly improving generalization across all considered\nworkloads. Finally, we explore the relationship between averaging and learning\nrate annealing and show how to optimally combine the two to achieve the best\nperformances.\n","authors":["Niccol√≤ Ajroldi","Antonio Orvieto","Jonas Geiping"],"pdf_url":"https://arxiv.org/pdf/2502.06761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13432v2","updated":"2025-02-10T18:37:12Z","published":"2025-01-23T07:35:47Z","title":"Emotion estimation from video footage with LSTM","summary":"  Emotion estimation in general is a field that has been studied for a long\ntime, and several approaches exist using machine learning. in this paper, we\npresent an LSTM model, that processes the blend-shapes produced by the library\nMediaPipe, for a face detected in a live stream of a camera, to estimate the\nmain emotion from the facial expressions, this model is trained on the FER2013\ndataset and delivers a result of 71% accuracy and 62% f1-score which meets the\naccuracy benchmark of the FER2013 dataset, with significantly reduced\ncomputation costs.\nhttps://github.com/Samir-atra/Emotion_estimation_from_video_footage_with_LSTM_ML_algorithm\n","authors":["Samer Attrah"],"pdf_url":"https://arxiv.org/pdf/2501.13432v2.pdf","comment":"12 pages, 5 figures, 34 references, 4 tables, 3 equations"},{"id":"http://arxiv.org/abs/2406.06621v2","updated":"2025-02-10T18:35:29Z","published":"2024-06-07T15:28:31Z","title":"LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph\n  Question-Answering","summary":"  We present LinkQ, a system that leverages a large language model (LLM) to\nfacilitate knowledge graph (KG) query construction through natural language\nquestion-answering. Traditional approaches often require detailed knowledge of\na graph querying language, limiting the ability for users -- even experts -- to\nacquire valuable insights from KGs. LinkQ simplifies this process by\nimplementing a multistep protocol in which the LLM interprets a user's\nquestion, then systematically converts it into a well-formed query. LinkQ helps\nusers iteratively refine any open-ended questions into precise ones, supporting\nboth targeted and exploratory analysis. Further, LinkQ guards against the LLM\nhallucinating outputs by ensuring users' questions are only ever answered from\nground truth KG data. We demonstrate the efficacy of LinkQ through a\nqualitative study with five KG practitioners. Our results indicate that\npractitioners find LinkQ effective for KG question-answering, and desire future\nLLM-assisted exploratory data analysis systems.\n","authors":["Harry Li","Gabriel Appleby","Ashley Suh"],"pdf_url":"https://arxiv.org/pdf/2406.06621v2.pdf","comment":"Open-source code: https://github.com/mit-ll/linkq"},{"id":"http://arxiv.org/abs/2210.04979v3","updated":"2025-02-10T18:34:10Z","published":"2022-10-10T19:27:37Z","title":"Label-free segmentation from cardiac ultrasound using self-supervised\n  learning","summary":"  Segmentation and measurement of cardiac chambers is critical in cardiac\nultrasound but is laborious and poorly reproducible. Neural networks can\nassist, but supervised approaches require the same laborious manual\nannotations. We built a pipeline for self-supervised (no manual labels)\nsegmentation combining computer vision, clinical domain knowledge, and deep\nlearning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393\nechocardiograms (4,476,266 images; mean 61 years, 51% female), using the\nresulting segmentations to calculate biometrics. We also tested against\nexternal images from an additional 10,030 patients with available manual\ntracings of the left ventricle. r2 between clinically measured and\npipeline-predicted measurements were similar to reported inter-clinician\nvariation and comparable to supervised learning across several different\nmeasurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber\nsize and function was 0.85 (range 0.71-0.97) compared to clinical measurements.\nA subset of test echocardiograms (n=553) had corresponding cardiac MRIs, where\nMRI is the gold standard. Correlation between pipeline and MRI measurements was\nsimilar to that between clinical echocardiogram and MRI. Finally, the pipeline\naccurately segments the left ventricle with an average Dice score of 0.89 (95%\nCI [0.89]) in the external, manually labeled dataset. Our results demonstrate a\nmanual-label free, clinically valid, and highly scalable method for\nsegmentation from ultrasound, a noisy but globally important imaging modality.\n","authors":["Danielle L. Ferreira","Zaynaf Salaymang","Rima Arnaout"],"pdf_url":"https://arxiv.org/pdf/2210.04979v3.pdf","comment":"37 pages, 3 Tables, 7 Figures"},{"id":"http://arxiv.org/abs/2502.06753v1","updated":"2025-02-10T18:31:15Z","published":"2025-02-10T18:31:15Z","title":"Case for a unified surrogate modelling framework in the age of AI","summary":"  Surrogate models are widely used in natural sciences, engineering, and\nmachine learning to approximate complex systems and reduce computational costs.\nHowever, the current landscape lacks standardisation across key stages of the\npipeline, including data collection, sampling design, model class selection,\nevaluation metrics, and downstream task performance analysis. This\nfragmentation limits reproducibility, reliability, and cross-domain\napplicability. The issue has only been exacerbated by the AI revolution and a\nnew suite of surrogate model classes that it offers. In this position paper, we\nargue for the urgent need for a unified framework to guide the development and\nevaluation of surrogate models. We outline essential steps for constructing a\ncomprehensive pipeline and discuss alternative perspectives, such as the\nbenefits of domain-specific frameworks. By advocating for a standardised\napproach, this paper seeks to improve the reliability of surrogate modelling,\nfoster cross-disciplinary knowledge transfer, and, as a result, accelerate\nscientific progress.\n","authors":["Elizaveta Semenova"],"pdf_url":"https://arxiv.org/pdf/2502.06753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04565v2","updated":"2025-02-10T18:28:24Z","published":"2025-02-06T23:38:50Z","title":"Private Federated Learning In Real World Application -- A Case Study","summary":"  This paper presents an implementation of machine learning model training\nusing private federated learning (PFL) on edge devices. We introduce a novel\nframework that uses PFL to address the challenge of training a model using\nusers' private data. The framework ensures that user data remain on individual\ndevices, with only essential model updates transmitted to a central server for\naggregation with privacy guarantees. We detail the architecture of our app\nselection model, which incorporates a neural network with attention mechanisms\nand ambiguity handling through uncertainty management. Experiments conducted\nthrough off-line simulations and on device training demonstrate the feasibility\nof our approach in real-world scenarios. Our results show the potential of PFL\nto improve the accuracy of an app selection model by adapting to changes in\nuser behavior over time, while adhering to privacy standards. The insights\ngained from this study are important for industries looking to implement PFL,\noffering a robust strategy for training a predictive model directly on edge\ndevices while ensuring user data privacy.\n","authors":["An Ji","Bortik Bandyopadhyay","Congzheng Song","Natarajan Krishnaswami","Prabal Vashisht","Rigel Smiroldo","Isabel Litton","Sayantan Mahinder","Mona Chitnis","Andrew W Hill"],"pdf_url":"https://arxiv.org/pdf/2502.04565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06751v1","updated":"2025-02-10T18:26:40Z","published":"2025-02-10T18:26:40Z","title":"What makes a good feedforward computational graph?","summary":"  As implied by the plethora of literature on graph rewiring, the choice of\ncomputational graph employed by a neural network can make a significant impact\non its downstream performance. Certain effects related to the computational\ngraph, such as under-reaching and over-squashing, may even render the model\nincapable of learning certain functions. Most of these effects have only been\nthoroughly studied in the domain of undirected graphs; however, recent years\nhave seen a significant rise in interest in feedforward computational graphs:\ndirected graphs without any back edges. In this paper, we study the desirable\nproperties of a feedforward computational graph, discovering two important\ncomplementary measures: fidelity and mixing time, and evaluating a few popular\nchoices of graphs through the lens of these measures. Our study is backed by\nboth theoretical analyses of the metrics' asymptotic behaviour for various\ngraphs, as well as correlating these metrics to the performance of trained\nneural network models using the corresponding graphs.\n","authors":["Alex Vitvitskyi","Jo√£o G. M. Ara√∫jo","Marc Lackenby","Petar Veliƒçkoviƒá"],"pdf_url":"https://arxiv.org/pdf/2502.06751v1.pdf","comment":"Work in progress -- comments welcome. 16 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00761v4","updated":"2025-02-10T18:26:14Z","published":"2024-08-01T17:59:12Z","title":"Tamper-Resistant Safeguards for Open-Weight LLMs","summary":"  Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after hundreds of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that progress on tamper-resistance is\npossible, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.\n","authors":["Rishub Tamirisa","Bhrugu Bharathi","Long Phan","Andy Zhou","Alice Gatti","Tarun Suresh","Maxwell Lin","Justin Wang","Rowan Wang","Ron Arel","Andy Zou","Dawn Song","Bo Li","Dan Hendrycks","Mantas Mazeika"],"pdf_url":"https://arxiv.org/pdf/2408.00761v4.pdf","comment":"Website: https://www.tamper-resistant-safeguards.com"},{"id":"http://arxiv.org/abs/2502.06925v1","updated":"2025-02-10T18:23:24Z","published":"2025-02-10T18:23:24Z","title":"Occam's model: Selecting simpler representations for better\n  transferability estimation","summary":"  Fine-tuning models that have been pre-trained on large datasets has become a\ncornerstone of modern machine learning workflows. With the widespread\navailability of online model repositories, such as Hugging Face, it is now\neasier than ever to fine-tune pre-trained models for specific tasks. This\nraises a critical question: which pre-trained model is most suitable for a\ngiven task? This problem is called transferability estimation. In this work, we\nintroduce two novel and effective metrics for estimating the transferability of\npre-trained models. Our approach is grounded in viewing transferability as a\nmeasure of how easily a pre-trained model's representations can be trained to\nseparate target classes, providing a unique perspective on transferability\nestimation. We rigorously evaluate the proposed metrics against\nstate-of-the-art alternatives across diverse problem settings, demonstrating\ntheir robustness and practical utility. Additionally, we present theoretical\ninsights that explain our metrics' efficacy and adaptability to various\nscenarios. We experimentally show that our metrics increase Kendall's Tau by up\nto 32% compared to the state-of-the-art baselines.\n","authors":["Prabhant Singh","Sibylle Hess","Joaquin Vanschoren"],"pdf_url":"https://arxiv.org/pdf/2502.06925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06749v1","updated":"2025-02-10T18:22:22Z","published":"2025-02-10T18:22:22Z","title":"Incentivizing Desirable Effort Profiles in Strategic Classification: The\n  Role of Causality and Uncertainty","summary":"  We study strategic classification in binary decision-making settings where\nagents can modify their features in order to improve their classification\noutcomes. Importantly, our work considers the causal structure across different\nfeatures, acknowledging that effort in a given feature may affect other\nfeatures. The main goal of our work is to understand \\emph{when and how much\nagent effort is invested towards desirable features}, and how this is\ninfluenced by the deployed classifier, the causal structure of the agent's\nfeatures, their ability to modify them, and the information available to the\nagent about the classifier and the feature causal graph.\n  In the complete information case, when agents know the classifier and the\ncausal structure of the problem, we derive conditions ensuring that rational\nagents focus on features favored by the principal. We show that designing\nclassifiers to induce desirable behavior is generally non-convex, though\ntractable in special cases. We also extend our analysis to settings where\nagents have incomplete information about the classifier or the causal graph.\nWhile optimal effort selection is again a non-convex problem under general\nuncertainty, we highlight special cases of partial uncertainty where this\nselection problem becomes tractable. Our results indicate that uncertainty\ndrives agents to favor features with higher expected importance and lower\nvariance, potentially misaligning with principal preferences. Finally,\nnumerical experiments based on a cardiovascular disease risk study illustrate\nhow to incentivize desirable modifications under uncertainty.\n","authors":["Valia Efthymiou","Chara Podimata","Diptangshu Sen","Juba Ziani"],"pdf_url":"https://arxiv.org/pdf/2502.06749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06742v1","updated":"2025-02-10T18:09:53Z","published":"2025-02-10T18:09:53Z","title":"Gradient Multi-Normalization for Stateless and Scalable LLM Training","summary":"  Training large language models (LLMs) typically relies on adaptive optimizers\nlike Adam (Kingma & Ba, 2015) which store additional state information to\naccelerate convergence but incur significant memory overhead. Recent efforts,\nsuch as SWAN (Ma et al., 2024) address this by eliminating the need for\noptimizer states while achieving performance comparable to Adam via a\nmulti-step preprocessing procedure applied to instantaneous gradients.\nMotivated by the success of SWAN, we introduce a novel framework for designing\nstateless optimizers that normalizes stochastic gradients according to multiple\nnorms. To achieve this, we propose a simple alternating scheme to enforce the\nnormalization of gradients w.r.t these norms. We show that our procedure can\nproduce, up to an arbitrary precision, a fixed-point of the problem, and that\nSWAN is a particular instance of our approach with carefully chosen norms,\nproviding a deeper understanding of its design. However, SWAN's computationally\nexpensive whitening/orthogonalization step limit its practicality for large\nLMs. Using our principled perspective, we develop of a more efficient,\nscalable, and practical stateless optimizer. Our algorithm relaxes the\nproperties of SWAN, significantly reducing its computational cost while\nretaining its memory efficiency, making it applicable to training large-scale\nmodels. Experiments on pre-training LLaMA models with up to 1 billion\nparameters demonstrate a 3X speedup over Adam with significantly reduced memory\nrequirements, outperforming other memory-efficient baselines.\n","authors":["Meyer Scetbon","Chao Ma","Wenbo Gong","Edward Meeds"],"pdf_url":"https://arxiv.org/pdf/2502.06742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06739v1","updated":"2025-02-10T18:07:51Z","published":"2025-02-10T18:07:51Z","title":"A note on the physical interpretation of neural PDE's","summary":"  We highlight a formal and substantial analogy between Machine Learning (ML)\nalgorithms and discrete dynamical systems (DDS) in relaxation form. The analogy\noffers a transparent interpretation of the weights in terms of physical\ninformation-propagation processes and identifies the model function of the\nforward ML step with the local attractor of the corresponding discrete\ndynamics. Besides improving the explainability of current ML applications, this\nanalogy may also facilitate the development of a new class ML algorithms with a\nreduced number of weights.\n","authors":["Sauro Succi"],"pdf_url":"https://arxiv.org/pdf/2502.06739v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2502.06738v1","updated":"2025-02-10T18:07:09Z","published":"2025-02-10T18:07:09Z","title":"Resurrecting saturated LLM benchmarks with adversarial encoding","summary":"  Recent work showed that small changes in benchmark questions can reduce LLMs'\nreasoning and recall. We explore two such changes: pairing questions and adding\nmore answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We\nfind that for more capable models, these predictably reduce performance,\nessentially heightening the performance ceiling of a benchmark and unsaturating\nit again. We suggest this approach can resurrect old benchmarks.\n","authors":["Igor Ivanov","Dmitrii Volkov"],"pdf_url":"https://arxiv.org/pdf/2502.06738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06737v1","updated":"2025-02-10T18:03:36Z","published":"2025-02-10T18:03:36Z","title":"VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data","summary":"  Process Reward Models (PRMs) have proven effective at enhancing mathematical\nreasoning for Large Language Models (LLMs) by leveraging increased\ninference-time computation. However, they are predominantly trained on\nmathematical data and their generalizability to non-mathematical domains has\nnot been rigorously studied. In response, this work first shows that current\nPRMs have poor performance in other domains. To address this limitation, we\nintroduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data\ngenerated using our novel data generation and annotation method. VersaPRM\nachieves consistent performance gains across diverse domains. For instance, in\nthe MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a\n7.9% performance gain over the majority voting baseline -- surpassing\nQwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by\nopen-sourcing all data, code and models for VersaPRM.\n","authors":["Thomas Zeng","Shuibai Zhang","Shutong Wu","Christian Classen","Daewon Chae","Ethan Ewer","Minjae Lee","Heeju Kim","Wonjun Kang","Jackson Kunde","Ying Fan","Jungtaek Kim","Hyung Il Koo","Kannan Ramchandran","Dimitris Papailiopoulos","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2502.06737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06733v1","updated":"2025-02-10T17:57:15Z","published":"2025-02-10T17:57:15Z","title":"Dynamic Loss-Based Sample Reweighting for Improved Large Language Model\n  Pretraining","summary":"  Pretraining large language models (LLMs) on vast and heterogeneous datasets\nis crucial for achieving state-of-the-art performance across diverse downstream\ntasks. However, current training paradigms treat all samples equally,\noverlooking the importance or relevance of individual samples throughout the\ntraining process. Existing reweighting strategies, which primarily focus on\ngroup-level data importance, fail to leverage fine-grained instance-level\ninformation and do not adapt dynamically to individual sample importance as\ntraining progresses. In this paper, we introduce novel algorithms for dynamic,\ninstance-level data reweighting aimed at improving both the efficiency and\neffectiveness of LLM pretraining. Our methods adjust the weight of each\ntraining sample based on its loss value in an online fashion, allowing the\nmodel to dynamically focus on more informative or important samples at the\ncurrent training stage. In particular, our framework allows us to\nsystematically devise reweighting strategies deprioritizing redundant or\nuninformative data, which we find tend to work best. Furthermore, we develop a\nnew theoretical framework for analyzing the impact of loss-based reweighting on\nthe convergence of gradient-based optimization, providing the first formal\ncharacterization of how these strategies affect convergence bounds. We\nempirically validate our approach across a spectrum of tasks, from pretraining\n7B and 1.4B parameter LLMs to smaller-scale language models and linear\nregression problems, demonstrating that our loss-based reweighting approach can\nlead to faster convergence and significantly improved performance.\n","authors":["Daouda Sow","Herbert Woisetschl√§ger","Saikiran Bulusu","Shiqiang Wang","Hans-Arno Jacobsen","Yingbin Liang"],"pdf_url":"https://arxiv.org/pdf/2502.06733v1.pdf","comment":"Accepted for publication at ICLR 2025. Code base available:\n  https://github.com/sowmaster/Sample-Level-Loss-Reweighting-ICLR-2025"},{"id":"http://arxiv.org/abs/2502.06728v1","updated":"2025-02-10T17:55:59Z","published":"2025-02-10T17:55:59Z","title":"FlexDeMo: Decoupled Momentum Optimization for Fully and Hybrid Sharded\n  Training","summary":"  Training large neural network models requires extensive computational\nresources, often distributed across several nodes and accelerators. Recent\nfindings suggest that it may be sufficient to only exchange the fast moving\ncomponents of the gradients, while accumulating momentum locally (Decoupled\nMomentum, or DeMo). However, when considering larger models that do not fit on\na single accelerate, the exchange of gradient information and the integration\nof DeMo needs to be reconsidered. Here, we propose employing a hybrid strategy,\nFlexDeMo, whereby nodes fully synchronize locally between different GPUs and\ninter-node communication is improved through only using the fast-moving\ncomponents. This effectively combines previous hybrid sharding strategies with\nthe advantages of decoupled momentum. Our experimental results show that\nFlexDeMo is on par with AdamW in terms of validation loss, demonstrating its\nviability.\n","authors":["Mogens Henrik From","Jacob Nielsen","Lukas Galke","Peter Schneider-Kamp"],"pdf_url":"https://arxiv.org/pdf/2502.06728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06719v1","updated":"2025-02-10T17:49:05Z","published":"2025-02-10T17:49:05Z","title":"Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient\n  Descent","summary":"  In this paper, we establish non-asymptotic convergence rates in the central\nlimit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient\ndescent (SGD). Our analysis builds on the result of the Gaussian approximation\nfor nonlinear statistics of independent random variables of Shao and Zhang\n(2022). Using this result, we prove the non-asymptotic validity of the\nmultiplier bootstrap for constructing the confidence sets for the optimal\nsolution of an optimization problem. In particular, our approach avoids the\nneed to approximate the limiting covariance of Polyak-Ruppert SGD iterates,\nwhich allows us to derive approximation rates in convex distance of order up to\n$1/\\sqrt{n}$.\n","authors":["Marina Sheshukova","Sergey Samsonov","Denis Belomestny","Eric Moulines","Qi-Man Shao","Zhuo-Song Zhang","Alexey Naumov"],"pdf_url":"https://arxiv.org/pdf/2502.06719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03359v2","updated":"2025-02-10T17:33:29Z","published":"2025-02-05T16:56:14Z","title":"GHOST: Gaussian Hypothesis Open-Set Technique","summary":"  Evaluations of large-scale recognition methods typically focus on overall\nperformance. While this approach is common, it often fails to provide insights\ninto performance across individual classes, which can lead to fairness issues\nand misrepresentation. Addressing these gaps is crucial for accurately\nassessing how well methods handle novel or unseen classes and ensuring a fair\nevaluation. To address fairness in Open-Set Recognition (OSR), we demonstrate\nthat per-class performance can vary dramatically. We introduce Gaussian\nHypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithm\nthat models deep features using class-wise multivariate Gaussian distributions\nwith diagonal covariance matrices. We apply Z-score normalization to logits to\nmitigate the impact of feature magnitudes that deviate from the model's\nexpectations, thereby reducing the likelihood of the network assigning a high\nscore to an unknown sample. We evaluate GHOST across multiple ImageNet-1K\npre-trained deep networks and test it with four different unknown datasets.\nUsing standard metrics such as AUOSCR, AUROC and FPR95, we achieve\nstatistically significant improvements, advancing the state-of-the-art in\nlarge-scale OSR. Source code is provided online.\n","authors":["Ryan Rabinowitz","Steve Cruz","Manuel G√ºnther","Terrance E. Boult"],"pdf_url":"https://arxiv.org/pdf/2502.03359v2.pdf","comment":"Accepted at AAAI Conference on Artificial Intelligence 2025"},{"id":"http://arxiv.org/abs/2502.06705v1","updated":"2025-02-10T17:33:22Z","published":"2025-02-10T17:33:22Z","title":"RSAttAE: An Information-Aware Attention-based Autoencoder Recommender\n  System","summary":"  Recommender systems play a crucial role in modern life, including information\nretrieval, the pharmaceutical industry, retail, and entertainment. The\nentertainment sector, in particular, attracts significant attention and\ngenerates substantial profits. This work proposes a new method for predicting\nunknown user-movie ratings to enhance customer satisfaction. To achieve this,\nwe utilize the MovieLens 100K dataset. Our approach introduces an\nattention-based autoencoder to create meaningful representations and the\nXGBoost method for rating predictions. The results demonstrate that our\nproposal outperforms most of the existing state-of-the-art methods.\nAvailability: github.com/ComputationIASBS/RecommSys\n","authors":["Amirhossein Dadashzadeh Taromi","Sina Heydari","Mohsen Hooshmand","Majid Ramezani"],"pdf_url":"https://arxiv.org/pdf/2502.06705v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2501.18727v2","updated":"2025-02-10T17:27:03Z","published":"2025-01-30T20:07:44Z","title":"Exploring Audio Editing Features as User-Centric Privacy Defenses\n  Against Large Language Model(LLM) Based Emotion Inference Attacks","summary":"  The rapid proliferation of speech-enabled technologies, including virtual\nassistants, video conferencing platforms, and wearable devices, has raised\nsignificant privacy concerns, particularly regarding the inference of sensitive\nemotional information from audio data. Existing privacy-preserving methods\noften compromise usability and security, limiting their adoption in practical\nscenarios. This paper introduces a novel, user-centric approach that leverages\nfamiliar audio editing techniques, specifically pitch and tempo manipulation,\nto protect emotional privacy without sacrificing usability. By analyzing\npopular audio editing applications on Android and iOS platforms, we identified\nthese features as both widely available and usable. We rigorously evaluated\ntheir effectiveness against a threat model, considering adversarial attacks\nfrom diverse sources, including Deep Neural Networks (DNNs), Large Language\nModels (LLMs), and and reversibility testing. Our experiments, conducted on\nthree distinct datasets, demonstrate that pitch and tempo manipulation\neffectively obfuscates emotional data. Additionally, we explore the design\nprinciples for lightweight, on-device implementation to ensure broad\napplicability across various devices and platforms.\n","authors":["Mohd. Farhan Israk Soumik","W. K. M. Mithsara","Abdur R. Shahid","Ahmed Imteaj"],"pdf_url":"https://arxiv.org/pdf/2501.18727v2.pdf","comment":"Accepted for presentation(Poster) at PPAI-25: The 6th AAAI Workshop\n  on Privacy-Preserving Artificial Intelligence"},{"id":"http://arxiv.org/abs/2502.06923v1","updated":"2025-02-10T17:21:39Z","published":"2025-02-10T17:21:39Z","title":"Do Attention Heads Compete or Cooperate during Counting?","summary":"  We present an in-depth mechanistic interpretability analysis of training\nsmall transformers on an elementary task, counting, which is a crucial\ndeductive step in many algorithms. In particular, we investigate the\ncollaboration/competition among the attention heads: we ask whether the\nattention heads behave as a pseudo-ensemble, all solving the same subtask, or\nthey perform different subtasks, meaning that they can only solve the original\ntask in conjunction. Our work presents evidence that on the semantics of the\ncounting task, attention heads behave as a pseudo-ensemble, but their outputs\nneed to be aggregated in a non-uniform manner in order to create an encoding\nthat conforms to the syntax. Our source code will be available upon\npublication.\n","authors":["P√°l Zs√°mboki","√Åd√°m Frakn√≥i","M√°t√© Gedeon","Andr√°s Kornai","Zsolt Zombori"],"pdf_url":"https://arxiv.org/pdf/2502.06923v1.pdf","comment":"14 pages, 15 figures"},{"id":"http://arxiv.org/abs/2502.06695v1","updated":"2025-02-10T17:18:54Z","published":"2025-02-10T17:18:54Z","title":"FairDropout: Using Example-Tied Dropout to Enhance Generalization of\n  Minority Groups","summary":"  Deep learning models frequently exploit spurious features in training data to\nachieve low training error, often resulting in poor generalization when faced\nwith shifted testing distributions. To address this issue, various methods from\nimbalanced learning, representation learning, and classifier recalibration have\nbeen proposed to enhance the robustness of deep neural networks against\nspurious correlations. In this paper, we observe that models trained with\nempirical risk minimization tend to generalize well for examples from the\nmajority groups while memorizing instances from minority groups. Building on\nrecent findings that show memorization can be localized to a limited number of\nneurons, we apply example-tied dropout as a method we term FairDropout, aimed\nat redirecting this memorization to specific neurons that we subsequently drop\nout during inference. We empirically evaluate FairDropout using the\nsubpopulation benchmark suite encompassing vision, language, and healthcare\ntasks, demonstrating that it significantly reduces reliance on spurious\ncorrelations, and outperforms state-of-the-art methods.\n","authors":["Geraldin Nanfack","Eugene Belilovsky"],"pdf_url":"https://arxiv.org/pdf/2502.06695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17281v2","updated":"2025-02-10T17:17:47Z","published":"2024-06-25T05:12:51Z","title":"Adaptive Reconstruction for Graph Neural Networks","summary":"  Graph Neural Networks (GNNs) have become fundamental in semi-supervised\nlearning for graph representation, leveraging their ability to capture complex\nnode relationships. A recent trend in GNN research focuses on \\textbf{adaptive\nk-hop structure learning}, moving beyond fixed-hop aggregation to more flexible\nand dynamic neighborhood selection. While GAMLP \\cite{Zhang_2022} employs\nseparate MLP layers for each k-hop domain and ImprovingTE\n\\cite{Yao2023ImprovingTE} enhances this by injecting contextualized\nsubstructure information, these methods still rely heavily on predefined\nsampling strategies, which may limit their ability to generalize and maintain\nstable accuracy. To address these limitations, we propose an \\textbf{adaptive\nreconstruction framework} that dynamically refines k-hop structure learning.\nInspired by \"coreset selection\" \\cite{guo2022deepcore}, our approach adaptively\n\\textbf{reconstructs} node neighborhoods to optimize message passing, ensuring\nmore \\textbf{effective and context-aware information flow} across the graph. To\nfurther enhance structural robustness, we introduce two key modules: the\n\\textbf{Distance Recomputator} and the \\textbf{Topology Reconstructor}\n(\\textcolor{blue}{DRTR}). The Distance Recomputator \\textbf{reassesses and\nrecalibrates} node distances based on adaptive graph properties, leading to\n\\textbf{improved node embeddings} that better reflect latent relationships.\nMeanwhile, the Topology Reconstructor \\textbf{dynamically refines local graph\nstructures}, enabling the model to \\textbf{adapt to evolving graph topologies}\nand mitigate the impact of noise and mislabeled data. Empirical evaluations\ndemonstrate that our \\textbf{adaptive reconstruction framework} achieves\n\\textbf{significant improvements} over existing k-hop-based models, providing\nmore \\textbf{stable and accurate} performance in various graph learning\nbenchmarks.\n","authors":["Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2406.17281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06693v1","updated":"2025-02-10T17:17:09Z","published":"2025-02-10T17:17:09Z","title":"Recent Advances, Applications and Open Challenges in Machine Learning\n  for Health: Reflections from Research Roundtables at ML4H 2024 Symposium","summary":"  The fourth Machine Learning for Health (ML4H) symposium was held in person on\nDecember 15th and 16th, 2024, in the traditional, ancestral, and unceded\nterritories of the Musqueam, Squamish, and Tsleil-Waututh Nations in Vancouver,\nBritish Columbia, Canada. The symposium included research roundtable sessions\nto foster discussions between participants and senior researchers on timely and\nrelevant topics for the ML4H community. The organization of the research\nroundtables at the conference involved 13 senior and 27 junior chairs across 13\ntables. Each roundtable session included an invited senior chair (with\nsubstantial experience in the field), junior chairs (responsible for\nfacilitating the discussion), and attendees from diverse backgrounds with an\ninterest in the session's topic.\n","authors":["Amin Adibi","Xu Cao","Zongliang Ji","Jivat Neet Kaur","Winston Chen","Elizabeth Healey","Brighton Nuwagira","Wenqian Ye","Geoffrey Woollard","Maxwell A Xu","Hejie Cui","Johnny Xi","Trenton Chang","Vasiliki Bikia","Nicole Zhang","Ayush Noori","Yuan Xia","Md. Belal Hossain","Hanna A. Frank","Alina Peluso","Yuan Pu","Shannon Zejiang Shen","John Wu","Adibvafa Fallahpour","Sazan Mahbub","Ross Duncan","Yuwei Zhang","Yurui Cao","Zuheng Xu","Michael Craig","Rahul G. Krishnan","Rahmatollah Beheshti","James M. Rehg","Mohammad Ehsanul Karim","Megan Coffee","Leo Anthony Celi","Jason Alan Fries","Mohsen Sadatsafavi","Dennis Shung","Shannon McWeeney","Jessica Dafflon","Sarah Jabbour"],"pdf_url":"https://arxiv.org/pdf/2502.06693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06922v1","updated":"2025-02-10T17:16:24Z","published":"2025-02-10T17:16:24Z","title":"Synthetic Audio Helps for Cognitive State Tasks","summary":"  The NLP community has broadly focused on text-only approaches of cognitive\nstate tasks, but audio can provide vital missing cues through prosody. We posit\nthat text-to-speech models learn to track aspects of cognitive state in order\nto produce naturalistic audio, and that the signal audio models implicitly\nidentify is orthogonal to the information that language models exploit. We\npresent Synthetic Audio Data fine-tuning (SAD), a framework where we show that\n7 tasks related to cognitive state modeling benefit from multimodal training on\nboth text and zero-shot synthetic audio data from an off-the-shelf TTS system.\nWe show an improvement over the text-only modality when adding synthetic audio\ndata to text-only corpora. Furthermore, on tasks and corpora that do contain\ngold audio, we show our SAD framework achieves competitive performance with\ntext and synthetic audio compared to text and gold audio.\n","authors":["Adil Soubki","John Murzaku","Peter Zeng","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2502.06922v1.pdf","comment":"John Murzaku and Adil Soubki contributed equally to this work"},{"id":"http://arxiv.org/abs/2403.16218v3","updated":"2025-02-10T17:15:52Z","published":"2024-03-24T16:18:27Z","title":"CoverUp: Coverage-Guided LLM-Based Test Generation","summary":"  Testing is an essential part of software development. Test generation tools\nattempt to automate the otherwise labor-intensive task of test creation, but\ngenerating high-coverage tests remains challenging. This paper proposes\nCoverUp, a novel approach to driving the generation of high-coverage Python\nregression tests. CoverUp combines coverage analysis, code context, and\nfeedback in prompts that iteratively guide the LLM to generate tests that\nimprove line and branch coverage. We evaluate our prototype CoverUp\nimplementation across a benchmark of challenging code derived from open-source\nPython projects and show that CoverUp substantially improves on the state of\nthe art. Compared to CodaMosa, a hybrid search/LLM-based test generator,\nCoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).\nCompared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves\nan overall line+branch coverage of 90% (vs. 77%). We also demonstrate that\nCoverUp's performance stems not only from the LLM used but from the combined\neffectiveness of its components.\n","authors":["Juan Altmayer Pizzorno","Emery D. Berger"],"pdf_url":"https://arxiv.org/pdf/2403.16218v3.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2502.06689v1","updated":"2025-02-10T17:15:11Z","published":"2025-02-10T17:15:11Z","title":"Neumann eigenmaps for landmark embedding","summary":"  We present Neumann eigenmaps (NeuMaps), a novel approach for enhancing the\nstandard diffusion map embedding using landmarks, i.e distinguished samples\nwithin the dataset. By interpreting these landmarks as a subgraph of the larger\ndata graph, NeuMaps are obtained via the eigendecomposition of a renormalized\nNeumann Laplacian. We show that NeuMaps offer two key advantages: (1) they\nprovide a computationally efficient embedding that accurately recovers the\ndiffusion distance associated with the reflecting random walk on the subgraph,\nand (2) they naturally incorporate the Nystr\\\"om extension within the diffusion\nmap framework through the discrete Neumann boundary condition. Through examples\nin digit classification and molecular dynamics, we demonstrate that NeuMaps not\nonly improve upon existing landmark-based embedding methods but also enhance\nthe stability of diffusion map embeddings to the removal of highly significant\npoints.\n","authors":["Shashank Sule","Wojciech Czaja"],"pdf_url":"https://arxiv.org/pdf/2502.06689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06685v1","updated":"2025-02-10T17:13:11Z","published":"2025-02-10T17:13:11Z","title":"No Trick, No Treat: Pursuits and Challenges Towards Simulation-free\n  Training of Neural Samplers","summary":"  We consider the sampling problem, where the aim is to draw samples from a\ndistribution whose density is known only up to a normalization constant. Recent\nbreakthroughs in generative modeling to approximate a high-dimensional data\ndistribution have sparked significant interest in developing neural\nnetwork-based methods for this challenging problem. However, neural samplers\ntypically incur heavy computational overhead due to simulating trajectories\nduring training. This motivates the pursuit of simulation-free training\nprocedures of neural samplers. In this work, we propose an elegant modification\nto previous methods, which allows simulation-free training with the help of a\ntime-dependent normalizing flow. However, it ultimately suffers from severe\nmode collapse. On closer inspection, we find that nearly all successful neural\nsamplers rely on Langevin preconditioning to avoid mode collapsing. We\nsystematically analyze several popular methods with various objective functions\nand demonstrate that, in the absence of Langevin preconditioning, most of them\nfail to adequately cover even a simple target. Finally, we draw attention to a\nstrong baseline by combining the state-of-the-art MCMC method, Parallel\nTempering (PT), with an additional generative model to shed light on future\nexplorations of neural samplers.\n","authors":["Jiajun He","Yuanqi Du","Francisco Vargas","Dinghuai Zhang","Shreyas Padhy","RuiKang OuYang","Carla Gomes","Jos√© Miguel Hern√°ndez-Lobato"],"pdf_url":"https://arxiv.org/pdf/2502.06685v1.pdf","comment":"21 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.06684v1","updated":"2025-02-10T17:11:20Z","published":"2025-02-10T17:11:20Z","title":"EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks","summary":"  Recent foundational models for tabular data, such as TabPFN, have\ndemonstrated remarkable effectiveness in adapting to new tasks through\nin-context learning. However, these models overlook a crucial equivariance\nproperty: the arbitrary ordering of target dimensions should not influence\nmodel predictions. In this study, we identify this oversight as a source of\nincompressible error, termed the equivariance gap, which introduces instability\nin predictions. To mitigate these issues, we propose a novel model designed to\npreserve equivariance across output dimensions. Our experimental results\nindicate that our proposed model not only addresses these pitfalls effectively\nbut also achieves competitive benchmark performance.\n","authors":["Michael Arbel","David Salinas","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2502.06684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01678v2","updated":"2025-02-10T17:11:15Z","published":"2025-02-02T04:19:35Z","title":"LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection","summary":"  Electroencephalogram (EEG) provides a non-invasive, highly accessible, and\ncost-effective solution for Alzheimer's Disease (AD) detection. However,\nexisting methods, whether based on manual feature extraction or deep learning,\nface two major challenges: the lack of large-scale datasets for robust feature\nlearning and evaluation, and poor detection performance due to inter-subject\nvariations. To address these challenges, we curate an EEG-AD corpus containing\n813 subjects, which forms the world's largest EEG-AD dataset to the best of our\nknowledge. Using this unique dataset, we propose LEAD, the first large\nfoundation model for EEG-based AD detection. Our method encompasses an entire\npipeline, from data selection and preprocessing to self-supervised contrastive\npretraining, fine-tuning, and key setups such as subject-independent evaluation\nand majority voting for subject-level detection. We pre-train the model on 11\nEEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervised\npre-training design includes sample-level and subject-level contrasting to\nextract useful general EEG features. Fine-tuning is performed on 5\nchannel-aligned datasets together. The backbone encoder incorporates temporal\nand channel embeddings to capture features across both temporal and spatial\ndimensions. Our method demonstrates outstanding AD detection performance,\nachieving up to a 9.86% increase in F1 score at the sample-level and up to a\n9.31% at the subject-level compared to state-of-the-art methods. The results of\nour model strongly confirm the effectiveness of contrastive pre-training and\nchannel-aligned unified fine-tuning for addressing inter-subject variation. The\nsource code is at https://github.com/DL4mHealth/LEAD.\n","authors":["Yihe Wang","Nan Huang","Nadia Mammone","Marco Cecchi","Xiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.01678v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06681v1","updated":"2025-02-10T17:07:43Z","published":"2025-02-10T17:07:43Z","title":"CHIRLA: Comprehensive High-resolution Identification and\n  Re-identification for Large-scale Analysis","summary":"  Person re-identification (Re-ID) is a key challenge in computer vision,\nrequiring the matching of individuals across different cameras, locations, and\ntime periods. While most research focuses on short-term scenarios with minimal\nappearance changes, real-world applications demand robust Re-ID systems capable\nof handling long-term scenarios, where persons' appearances can change\nsignificantly due to variations in clothing and physical characteristics. In\nthis paper, we present CHIRLA, Comprehensive High-resolution Identification and\nRe-identification for Large-scale Analysis, a novel dataset specifically\ndesigned for long-term person Re-ID. CHIRLA consists of recordings from\nstrategically placed cameras over a seven-month period, capturing significant\nvariations in both temporal and appearance attributes, including controlled\nchanges in participants' clothing and physical features. The dataset includes\n22 individuals, four connected indoor environments, and seven cameras. We\ncollected more than five hours of video that we semi-automatically labeled to\ngenerate around one million bounding boxes with identity annotations. By\nintroducing this comprehensive benchmark, we aim to facilitate the development\nand evaluation of Re-ID algorithms that can reliably perform in challenging,\nlong-term real-world scenarios.\n","authors":["Bessie Dominguez-Dager","Felix Escalona","Francisco Gomez-Donoso","Miguel Cazorla"],"pdf_url":"https://arxiv.org/pdf/2502.06681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12016v4","updated":"2025-02-10T17:06:04Z","published":"2024-05-20T13:39:58Z","title":"Conformalized Strategy-Proof Auctions","summary":"  Auctions are key for maximizing sellers' revenue and ensuring truthful\nbidding among buyers. Recently, an approach known as differentiable economics\nbased on machine learning (ML) has shown promise in learning powerful auction\nmechanisms for multiple items and participants. However, this approach has no\nguarantee of strategy-proofness at test time. Strategy-proofness is crucial as\nit ensures that buyers are incentivized to bid their true valuations, leading\nto optimal and fair auction outcomes without the risk of manipulation. In this\nwork, we propose a formulation of statistical strategy-proofness auction\nmechanism, ensuring that the probability of regret exceeding a predefined\nthreshold is strictly controlled. Building upon conformal prediction\ntechniques, we develop an auction acceptance rule that leverages regret\npredictions to guarantee that the data-driven auction mechanism meets the\nstatistical strategy-proofness requirement with high probability. Our approach\nrepresents a practical middle-ground between two extremes: forcing zero-regret\nat the cost of significant revenue loss, and naively using ML to construct\nauctions with the hope of attaining low regret at test time. Numerical\nexperiments demonstrate the necessity of the proposed method, the validity of\nour theoretical result, and its applicability.\n","authors":["Roy Maor Lotan","Inbal Talgam-Cohen","Yaniv Romano"],"pdf_url":"https://arxiv.org/pdf/2405.12016v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06678v1","updated":"2025-02-10T17:03:33Z","published":"2025-02-10T17:03:33Z","title":"Quantile Multi-Armed Bandits with 1-bit Feedback","summary":"  In this paper, we study a variant of best-arm identification involving\nelements of risk sensitivity and communication constraints. Specifically, the\ngoal of the learner is to identify the arm with the highest quantile reward,\nwhile the communication from an agent (who observes rewards) and the learner\n(who chooses actions) is restricted to only one bit of feedback per arm pull.\nWe propose an algorithm that utilizes noisy binary search as a subroutine,\nallowing the learner to estimate quantile rewards through 1-bit feedback. We\nderive an instance-dependent upper bound on the sample complexity of our\nalgorithm and provide an algorithm-independent lower bound for specific\ninstances, with the two matching to within logarithmic factors under mild\nconditions, or even to within constant factors in certain low error probability\nscaling regimes. The lower bound is applicable even in the absence of\ncommunication constraints, and thus we conclude that restricting to 1-bit\nfeedback has a minimal impact on the scaling of the sample complexity.\n","authors":["Ivan Lau","Jonathan Scarlett"],"pdf_url":"https://arxiv.org/pdf/2502.06678v1.pdf","comment":"ALT 2025"},{"id":"http://arxiv.org/abs/2502.06674v1","updated":"2025-02-10T17:00:32Z","published":"2025-02-10T17:00:32Z","title":"RAILS: Risk-Aware Iterated Local Search for Joint SLA Decomposition and\n  Service Provider Management in Multi-Domain Networks","summary":"  The emergence of the fifth generation (5G) technology has transformed mobile\nnetworks into multi-service environments, necessitating efficient network\nslicing to meet diverse Service Level Agreements (SLAs). SLA decomposition\nacross multiple network domains, each potentially managed by different service\nproviders, poses a significant challenge due to limited visibility into\nreal-time underlying domain conditions. This paper introduces Risk-Aware\nIterated Local Search (RAILS), a novel risk model-driven meta-heuristic\nframework designed to jointly address SLA decomposition and service provider\nselection in multi-domain networks. By integrating online risk modeling with\niterated local search principles, RAILS effectively navigates the complex\noptimization landscape, utilizing historical feedback from domain controllers.\nWe formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP)\nproblem and prove its NP-hardness. Extensive simulations demonstrate that RAILS\nachieves near-optimal performance, offering an efficient, real-time solution\nfor adaptive SLA management in modern multi-domain networks.\n","authors":["Cyril Shih-Huan Hsu","Chrysa Papagianni","Paola Grosso"],"pdf_url":"https://arxiv.org/pdf/2502.06674v1.pdf","comment":"The paper has been submitted to IEEE HPSR 2025"},{"id":"http://arxiv.org/abs/2408.01022v2","updated":"2025-02-10T16:52:07Z","published":"2024-08-02T05:46:17Z","title":"A Family of Distributions of Random Subsets for Controlling Positive and\n  Negative Dependence","summary":"  Positive and negative dependence are fundamental concepts that characterize\nthe attractive and repulsive behavior of random subsets. Although some\nprobabilistic models are known to exhibit positive or negative dependence, it\nis challenging to seamlessly bridge them with a practicable probabilistic\nmodel. In this study, we introduce a new family of distributions, named the\ndiscrete kernel point process (DKPP), which includes determinantal point\nprocesses and parts of Boltzmann machines. We also develop some computational\nmethods for probabilistic operations and inference with DKPPs, such as\ncalculating marginal and conditional probabilities and learning the parameters.\nOur numerical experiments demonstrate the controllability of positive and\nnegative dependence and the effectiveness of the computational methods for\nDKPPs.\n","authors":["Takahiro Kawashima","Hideitsu Hino"],"pdf_url":"https://arxiv.org/pdf/2408.01022v2.pdf","comment":"Accepted by AISTATS2025"},{"id":"http://arxiv.org/abs/2502.06664v1","updated":"2025-02-10T16:51:11Z","published":"2025-02-10T16:51:11Z","title":"Evaluation of Deep Audio Representations for Hearables","summary":"  Effectively steering hearable devices requires understanding the acoustic\nenvironment around the user. In the computational analysis of sound scenes,\nfoundation models have emerged as the state of the art to produce\nhigh-performance, robust, multi-purpose audio representations. We introduce and\nrelease Deep Evaluation of Audio Representations (DEAR), the first dataset and\nbenchmark to evaluate the efficacy of foundation models in capturing essential\nacoustic properties for hearables. The dataset includes 1,158 audio tracks,\neach 30 seconds long, created by spatially mixing proprietary monologues with\ncommercial, high-quality recordings of everyday acoustic scenes. Our benchmark\nencompasses eight tasks that assess the general context, speech sources, and\ntechnical acoustic properties of the audio scenes. Through our evaluation of\nfour general-purpose audio representation models, we demonstrate that the BEATs\nmodel significantly surpasses its counterparts. This superiority underscores\nthe advantage of models trained on diverse audio collections, confirming their\napplicability to a wide array of auditory tasks, including encoding the\nenvironment properties necessary for hearable steering. The DEAR dataset and\nassociated code are available at https://dear-dataset.github.io.\n","authors":["Fabian Gr√∂ger","Pascal Baumann","Ludovic Amruthalingam","Laurent Simon","Ruksana Giurda","Simone Lionetti"],"pdf_url":"https://arxiv.org/pdf/2502.06664v1.pdf","comment":"Accepted at International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2502.06661v1","updated":"2025-02-10T16:49:46Z","published":"2025-02-10T16:49:46Z","title":"iLOCO: Distribution-Free Inference for Feature Interactions","summary":"  Feature importance measures are widely studied and are essential for\nunderstanding model behavior, guiding feature selection, and enhancing\ninterpretability. However, many machine learning fitted models involve complex,\nhigher-order interactions between features. Existing feature importance metrics\nfail to capture these higher-order effects while existing interaction metrics\noften suffer from limited applicability or excessive computation; no methods\nexist to conduct statistical inference for feature interactions. To bridge this\ngap, we first propose a new model-agnostic metric, interaction\nLeave-One-Covariate-Out iLOCO, for measuring the importance of higher-order\nfeature interactions. Next, we leverage recent advances in LOCO inference to\ndevelop distribution-free and assumption-light confidence intervals for our\niLOCO metric. To address computational challenges, we also introduce an\nensemble learning method for calculating the iLOCO metric and confidence\nintervals that we show is both computationally and statistically efficient. We\nvalidate our iLOCO metric and our confidence intervals on both synthetic and\nreal data sets, showing that our approach outperforms existing methods and\nprovides the first inferential approach to detecting feature interactions.\n","authors":["Camille Little","Lili Zheng","Genevera Allen"],"pdf_url":"https://arxiv.org/pdf/2502.06661v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06658v1","updated":"2025-02-10T16:48:48Z","published":"2025-02-10T16:48:48Z","title":"Generating Samples to Question Trained Models","summary":"  There is a growing need for investigating how machine learning models\noperate. With this work, we aim to understand trained machine learning models\nby questioning their data preferences. We propose a mathematical framework that\nallows us to probe trained models and identify their preferred samples in\nvarious scenarios including prediction-risky, parameter-sensitive, or\nmodel-contrastive samples. To showcase our framework, we pose these queries to\na range of models trained on a range of classification and regression tasks,\nand receive answers in the form of generated data.\n","authors":["E. Mehmet Kƒ±ral","Nur≈üen Aydƒ±n","≈û. ƒ∞lker Birbil"],"pdf_url":"https://arxiv.org/pdf/2502.06658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19020v3","updated":"2025-02-10T16:42:23Z","published":"2024-09-25T07:03:31Z","title":"DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications","summary":"  The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research.\n","authors":["Sathya Krishnan Suresh","Wu Mengjun","Tushar Pranav","Eng Siong Chng"],"pdf_url":"https://arxiv.org/pdf/2409.19020v3.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.06649v1","updated":"2025-02-10T16:38:13Z","published":"2025-02-10T16:38:13Z","title":"Estimation of Food Intake Quantity Using Inertial Signals from\n  Smartwatches","summary":"  Accurate monitoring of eating behavior is crucial for managing obesity and\neating disorders such as bulimia nervosa. At the same time, existing methods\nrely on multiple and/or specialized sensors, greatly harming adherence and\nultimately, the quality and continuity of data. This paper introduces a novel\napproach for estimating the weight of a bite, from a commercial smartwatch. Our\npublicly-available dataset contains smartwatch inertial data from ten\nparticipants, with manually annotated start and end times of each bite along\nwith their corresponding weights from a smart scale, under semi-controlled\nconditions. The proposed method combines extracted behavioral features such as\nthe time required to load the utensil with food, with statistical features of\ninertial signals, that serve as input to a Support Vector Regression model to\nestimate bite weights. Under a leave-one-subject-out cross-validation scheme,\nour approach achieves a mean absolute error (MAE) of 3.99 grams per bite. To\ncontextualize this performance, we introduce the improvement metric, that\nmeasures the relative MAE difference compared to a baseline model. Our method\ndemonstrates a 17.41% improvement, while the adapted state-of-the art method\nshows a -28.89% performance against that same baseline. The results presented\nin this work establish the feasibility of extracting meaningful bite weight\nestimates from commercial smartwatch inertial sensors alone, laying the\ngroundwork for future accessible, non-invasive dietary monitoring systems.\n","authors":["Ioannis Levi","Konstantinos Kyritsis","Vasileios Papapanagiotou","Georgios Tsakiridis","Anastasios Delopoulos"],"pdf_url":"https://arxiv.org/pdf/2502.06649v1.pdf","comment":"Manuscript submitted for review to 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)\n  2025"},{"id":"http://arxiv.org/abs/2502.06645v1","updated":"2025-02-10T16:35:08Z","published":"2025-02-10T16:35:08Z","title":"Koopman-Equivariant Gaussian Processes","summary":"  Credible forecasting and representation learning of dynamical systems are of\never-increasing importance for reliable decision-making. To that end, we\npropose a family of Gaussian processes (GP) for dynamical systems with linear\ntime-invariant responses, which are nonlinear only in initial conditions. This\nlinearity allows us to tractably quantify forecasting and representational\nuncertainty, simultaneously alleviating the challenge of computing the\ndistribution of trajectories from a GP-based dynamical system and enabling a\nnew probabilistic treatment of learning Koopman operator representations. Using\na trajectory-based equivariance -- which we refer to as \\textit{Koopman\nequivariance} -- we obtain a GP model with enhanced generalization\ncapabilities. To allow for large-scale regression, we equip our framework with\nvariational inference based on suitable inducing points. Experiments\ndemonstrate on-par and often better forecasting performance compared to\nkernel-based methods for learning dynamical systems.\n","authors":["Petar Bevanda","Max Beier","Armin Lederer","Alexandre Capone","Stefan Sosnowski","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2502.06645v1.pdf","comment":"Accepted to the 28th International Conference on Artificial\n  Intelligence and Statistics (AISTATS)"},{"id":"http://arxiv.org/abs/2502.06643v1","updated":"2025-02-10T16:34:36Z","published":"2025-02-10T16:34:36Z","title":"MoETuner: Optimized Mixture of Expert Serving with Balanced Expert\n  Placement and Token Routing","summary":"  Mixture-of-Experts (MoE) model architecture has emerged as a promising\nsolution for scaling transformer models efficiently, offering sparse activation\nthat reduces computational costs while increasing model capacity. However, as\nMoE models scale, they need to be distributed across GPU devices, thus face\ncritical performance bottlenecks due to their large memory footprint. Expert\nparallelism distributes experts across GPUs, however, faces key challenges\nincluding an unbalanced token routing and expert activation, resulting in\ncommunication tail latency and processing inefficiencies. While existing\nsolutions address some of these issues, they fail to resolve the dual\nchallenges of load imbalance and communication skew. The imbalance in token\nprocessing load across experts causes uneven processing times on different\nGPUs, while communication skew between GPUs leads to unbalanced inter-GPU data\ntransfers. These factors degrade the performance of MoE models by increasing\ntail latency and reducing overall throughput. To address these limitations, we\npropose an Integer Linear Programming (ILP) formulation to optimize expert\nplacement by jointly considering token load, communication, and computation\ncosts. We exploit the property that there is a token routing dependency across\nlayers, where tokens routed to a specific expert in one layer are likely to be\nrouted to a limited set of experts in the subsequent layer. Our solution,\nMoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU\ntoken routing costs and balances token processing across devices, thereby\nreducing tail latency and end-to-end execution time. Experimental results\ndemonstrate 9.3% and 17.5% of end-to-end speedups for single-node and\nmulti-node inference respectively, showcasing the potential of our ILP-based\noptimization for offering expert parallel solutions for next-generation MoEs.\n","authors":["Seokjin Go","Divya Mahajan"],"pdf_url":"https://arxiv.org/pdf/2502.06643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04419v2","updated":"2025-02-10T16:34:03Z","published":"2025-02-06T15:20:58Z","title":"Understanding and Mitigating the Bias Inheritance in LLM-based Data\n  Augmentation on Downstream Tasks","summary":"  Generating synthetic datasets via large language models (LLMs) themselves has\nemerged as a promising approach to improve LLM performance. However, LLMs\ninherently reflect biases present in their training data, leading to a critical\nchallenge: when these models generate synthetic data for training, they may\npropagate and amplify their inherent biases that can significantly impact model\nfairness and robustness on downstream tasks--a phenomenon we term bias\ninheritance. This work presents the first systematic investigation in\nunderstanding, analyzing, and mitigating bias inheritance. We study this\nproblem by fine-tuning LLMs with a combined dataset consisting of original and\nLLM-augmented data, where bias ratio represents the proportion of augmented\ndata. Through systematic experiments across 10 classification and generation\ntasks, we analyze how 6 different types of biases manifest at varying bias\nratios. Our results reveal that bias inheritance has nuanced effects on\ndownstream tasks, influencing both classification tasks and generation tasks\ndifferently. Then, our analysis identifies three key misalignment factors:\nmisalignment of values, group data, and data distributions. Based on these\ninsights, we propose three mitigation strategies: token-based, mask-based, and\nloss-based approaches. Experiments demonstrate that these strategies also work\ndifferently on various tasks and bias, indicating the substantial challenges to\nfully mitigate bias inheritance. We hope this work can provide valuable\ninsights to the research of LLM data augmentation.\n","authors":["Miaomiao Li","Hao Chen","Yang Wang","Tingyuan Zhu","Weijia Zhang","Kaijie Zhu","Kam-Fai Wong","Jindong Wang"],"pdf_url":"https://arxiv.org/pdf/2502.04419v2.pdf","comment":"Technical report; 31 pages"},{"id":"http://arxiv.org/abs/2411.18676v2","updated":"2025-02-10T16:32:27Z","published":"2024-11-27T18:57:26Z","title":"Embodied Red Teaming for Auditing Robotic Foundation Models","summary":"  Language-conditioned robot models have the potential to enable robots to\nperform a wide range of tasks based on natural language instructions. However,\nassessing their safety and effectiveness remains challenging because it is\ndifficult to test all the different ways a single task can be phrased. Current\nbenchmarks have two key limitations: they rely on a limited set of\nhuman-generated instructions, missing many challenging cases, and focus only on\ntask performance without assessing safety, such as avoiding damage. To address\nthese gaps, we introduce Embodied Red Teaming (ERT), a new evaluation method\nthat generates diverse and challenging instructions to test these models. ERT\nuses automated red teaming techniques with Vision Language Models (VLMs) to\ncreate contextually grounded, difficult instructions. Experimental results show\nthat state-of-the-art language-conditioned robot models fail or behave unsafely\non ERT-generated instructions, underscoring the shortcomings of current\nbenchmarks in evaluating real-world performance and safety. Code and videos are\navailable at: https://s-karnik.github.io/embodied-red-team-project-page.\n","authors":["Sathwik Karnik","Zhang-Wei Hong","Nishant Abhangi","Yen-Chen Lin","Tsun-Hsuan Wang","Christophe Dupuy","Rahul Gupta","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2411.18676v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01591v3","updated":"2025-02-10T16:32:04Z","published":"2023-11-02T20:57:44Z","title":"Better Fair than Sorry: Adversarial Missing Data Imputation for Fair\n  GNNs","summary":"  Graph Neural Networks (GNNs) have achieved state-of-the-art results in many\nrelevant tasks where decisions might disproportionately impact specific\ncommunities. However, existing work on fair GNNs often assumes that either\nprotected attributes are fully observed or that the missing protected attribute\nimputation is fair. In practice, biases in the imputation will propagate to the\nmodel outcomes, leading them to overestimate the fairness of their predictions.\nWe address this challenge by proposing Better Fair than Sorry (BFtS), a fair\nmissing data imputation model for protected attributes. The key design\nprinciple behind BFtS is that imputations should approximate the worst-case\nscenario for fairness -- i.e. when optimizing fairness is the hardest. We\nimplement this idea using a 3-player adversarial scheme where two adversaries\ncollaborate against a GNN-based classifier, and the classifier minimizes the\nmaximum bias. Experiments using synthetic and real datasets show that BFtS\noften achieves a better fairness x accuracy trade-off than existing\nalternatives.\n","authors":["Debolina Halder Lina","Arlei Silva"],"pdf_url":"https://arxiv.org/pdf/2311.01591v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09038v2","updated":"2025-02-10T16:31:57Z","published":"2025-01-14T20:59:37Z","title":"Do generative video models learn physical principles from watching\n  videos?","summary":"  AI video generation is undergoing a revolution, with quality and realism\nadvancing rapidly. These advances have led to a passionate scientific debate:\nDo video models learn \"world models\" that discover laws of physics -- or,\nalternatively, are they merely sophisticated pixel predictors that achieve\nvisual realism without understanding the physical principles of reality? We\naddress this question by developing Physics-IQ, a comprehensive benchmark\ndataset that can only be solved by acquiring a deep understanding of various\nphysical principles, like fluid dynamics, optics, solid mechanics, magnetism\nand thermodynamics. We find that across a range of current models (Sora,\nRunway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical\nunderstanding is severely limited, and unrelated to visual realism. At the same\ntime, some test cases can already be successfully solved. This indicates that\nacquiring certain physical principles from observation alone may be possible,\nbut significant challenges remain. While we expect rapid advances ahead, our\nwork demonstrates that visual realism does not imply physical understanding.\nOur project page is at https://physics-iq.github.io; code at\nhttps://github.com/google-deepmind/physics-IQ-benchmark.\n","authors":["Saman Motamed","Laura Culp","Kevin Swersky","Priyank Jaini","Robert Geirhos"],"pdf_url":"https://arxiv.org/pdf/2501.09038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06634v1","updated":"2025-02-10T16:29:21Z","published":"2025-02-10T16:29:21Z","title":"Automatic Annotation Augmentation Boosts Translation between Molecules\n  and Natural Language","summary":"  Recent advancements in AI for biological research focus on integrating\nmolecular data with natural language to accelerate drug discovery. However, the\nscarcity of high-quality annotations limits progress in this area. This paper\nintroduces LA$^3$, a Language-based Automatic Annotation Augmentation framework\nthat leverages large language models to augment existing datasets, thereby\nimproving AI training. We demonstrate the effectiveness of LA$^3$ by creating\nan enhanced dataset, LaChEBI-20, where we systematically rewrite the\nannotations of molecules from an established dataset. These rewritten\nannotations preserve essential molecular information while providing more\nvaried sentence structures and vocabulary. Using LaChEBI-20, we train LaMolT5\nbased on a benchmark architecture to learn the mapping between molecular\nrepresentations and augmented annotations.\n  Experimental results on text-based *de novo* molecule generation and molecule\ncaptioning demonstrate that LaMolT5 outperforms state-of-the-art models.\nNotably, incorporating LA$^3$ leads to improvements of up to 301% over the\nbenchmark architecture. Furthermore, we validate the effectiveness of LA$^3$\nnotable applications in *image*, *text* and *graph* tasks, affirming its\nversatility and utility.\n","authors":["Zhiqiang Zhong","Simon Sataa-Yu Larsen","Haoyu Guo","Tao Tang","Kuangyu Zhou","Davide Mottin"],"pdf_url":"https://arxiv.org/pdf/2502.06634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06632v1","updated":"2025-02-10T16:28:35Z","published":"2025-02-10T16:28:35Z","title":"Few-Shot Classification and Anatomical Localization of Tissues in SPECT\n  Imaging","summary":"  Accurate classification and anatomical localization are essential for\neffective medical diagnostics and research, which may be efficiently performed\nusing deep learning techniques. However, availability of limited labeled data\nposes a significant challenge. To address this, we adapted Prototypical\nNetworks and the Propagation-Reconstruction Network (PRNet) for few-shot\nclassification and localization, respectively, in Single Photon Emission\nComputed Tomography (SPECT) images. For the proof of concept we used a\n2D-sliced image cropped around heart. The Prototypical Network, with a\npre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver\ntissues with 96.67% training and 93.33% validation accuracy. PRNet, adapted for\n2D imaging with an encoder-decoder architecture and skip connections, achieved\na training loss of 1.395, accurately reconstructing patches and capturing\nspatial relationships. These results highlight the potential of Prototypical\nNetworks for tissue classification with limited labeled data and PRNet for\nanatomical landmark localization, paving the way for improved performance in\ndeep learning frameworks.\n","authors":["Mohammed Abdul Hafeez Khan","Samuel Morries Boddepalli","Siddhartha Bhattacharyya","Debasis Mitra"],"pdf_url":"https://arxiv.org/pdf/2502.06632v1.pdf","comment":"2 pages, 2 figures"},{"id":"http://arxiv.org/abs/2502.06920v1","updated":"2025-02-10T16:24:18Z","published":"2025-02-10T16:24:18Z","title":"Direct Estimation of Pediatric Heart Rate Variability from BOLD-fMRI: A\n  Machine Learning Approach Using Dynamic Connectivity","summary":"  In many pediatric fMRI studies, cardiac signals are often missing or of poor\nquality. A tool to extract Heart Rate Variation (HRV) waveforms directly from\nfMRI data, without the need for peripheral recording devices, would be highly\nbeneficial. We developed a machine learning framework to accurately reconstruct\nHRV for pediatric applications. A hybrid model combining one-dimensional\nConvolutional Neural Networks (1D-CNN) and Gated Recurrent Units (GRU) analyzed\nBOLD signals from 628 ROIs, integrating past and future data. The model\nachieved an 8% improvement in HRV accuracy, as evidenced by enhanced\nperformance metrics. This approach eliminates the need for peripheral\nphotoplethysmography devices, reduces costs, and simplifies procedures in\npediatric fMRI. Additionally, it improves the robustness of pediatric fMRI\nstudies, which are more sensitive to physiological and developmental variations\nthan those in adults.\n","authors":["Abdoljalil Addeh","Karen Ardila","Rebecca J Williams","G. Bruce Pike","M. Ethan MacDonald"],"pdf_url":"https://arxiv.org/pdf/2502.06920v1.pdf","comment":"5 pages, 5 figures, ISMSMR 2025"},{"id":"http://arxiv.org/abs/2410.03380v2","updated":"2025-02-10T16:21:03Z","published":"2024-10-04T12:48:21Z","title":"Identifying perturbation targets through causal differential networks","summary":"  Identifying variables responsible for changes to a biological system enables\napplications in drug target discovery and cell engineering. Given a pair of\nobservational and interventional datasets, the goal is to isolate the subset of\nobserved variables that were the targets of the intervention. Directly applying\ncausal discovery algorithms is challenging: the data may contain thousands of\nvariables with as few as tens of samples per intervention, and biological\nsystems do not adhere to classical causality assumptions. We propose a\ncausality-inspired approach to address this practical setting. First, we infer\nnoisy causal graphs from the observational and interventional data. Then, we\nlearn to map the differences between these graphs, along with additional\nstatistical features, to sets of variables that were intervened upon. Both\nmodules are jointly trained in a supervised framework, on simulated and real\ndata that reflect the nature of biological interventions. This approach\nconsistently outperforms baselines for perturbation modeling on seven\nsingle-cell transcriptomics datasets. We also demonstrate significant\nimprovements over current causal discovery methods for predicting soft and hard\nintervention targets across a variety of synthetic data.\n","authors":["Menghua Wu","Umesh Padia","Sean H. Murphy","Regina Barzilay","Tommi Jaakkola"],"pdf_url":"https://arxiv.org/pdf/2410.03380v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01114v2","updated":"2025-02-10T16:13:51Z","published":"2024-05-02T09:22:54Z","title":"Continual Learning from Simulated Interactions via Multitask Prospective\n  Rehearsal for Bionic Limb Behavior Modeling","summary":"  Lower limb amputations and neuromuscular impairments severely restrict\nmobility, necessitating advancements beyond conventional prosthetics. While\nmotorized bionic limbs show promise, their effectiveness depends on replicating\nthe dynamic coordination of human movement across diverse environments. In this\npaper, we introduce a model for human behavior in the context of bionic\nprosthesis control. Our approach leverages human locomotion demonstrations to\nlearn the synergistic coupling of the lower limbs, enabling the prediction of\nthe kinematic behavior of a missing limb during tasks such as walking, climbing\ninclines, and stairs. We propose a multitasking, continually adaptive model\nthat anticipates and refines movements over time. At the core of our method is\na technique called multitask prospective rehearsal, that anticipates and\nsynthesizes future movements based on the previous prediction and employs a\ncorrective mechanism for subsequent predictions. Our evolving architecture\nmerges lightweight, task-specific modules on a shared backbone, ensuring both\nspecificity and scalability. We validate our model through experiments on\nreal-world human gait datasets, including transtibial amputees, across a wide\nrange of locomotion tasks. Results demonstrate that our approach consistently\noutperforms baseline models, particularly in scenarios with distributional\nshifts, adversarial perturbations, and noise.\n","authors":["Sharmita Dey","Benjamin Paassen","Sarath Ravindran Nair","Sabri Boughorbel","Arndt F. Schilling"],"pdf_url":"https://arxiv.org/pdf/2405.01114v2.pdf","comment":"Accepted at Transactions on Machine Learning Research (TMLR) 2025"},{"id":"http://arxiv.org/abs/2502.06919v1","updated":"2025-02-10T16:07:28Z","published":"2025-02-10T16:07:28Z","title":"Select before Act: Spatially Decoupled Action Repetition for Continuous\n  Control","summary":"  Reinforcement Learning (RL) has achieved remarkable success in various\ncontinuous control tasks, such as robot manipulation and locomotion. Different\nto mainstream RL which makes decisions at individual steps, recent studies have\nincorporated action repetition into RL, achieving enhanced action persistence\nwith improved sample efficiency and superior performance. However, existing\nmethods treat all action dimensions as a whole during repetition, ignoring\nvariations among them. This constraint leads to inflexibility in decisions,\nwhich reduces policy agility with inferior effectiveness. In this work, we\npropose a novel repetition framework called SDAR, which implements Spatially\nDecoupled Action Repetition through performing closed-loop act-or-repeat\nselection for each action dimension individually. SDAR achieves more flexible\nrepetition strategies, leading to an improved balance between action\npersistence and diversity. Compared to existing repetition frameworks, SDAR is\nmore sample efficient with higher policy performance and reduced action\nfluctuation. Experiments are conducted on various continuous control scenarios,\ndemonstrating the effectiveness of spatially decoupled repetition design\nproposed in this work.\n","authors":["Buqing Nie","Yangqing Fu","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2502.06919v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2407.05237v3","updated":"2025-02-10T16:01:43Z","published":"2024-07-07T02:35:55Z","title":"Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex\n  composite losses","summary":"  Differentially-private stochastic gradient descent (DP-SGD) is a family of\niterative machine learning training algorithms that privatize gradients to\ngenerate a sequence of differentially-private (DP) model parameters. It is also\nthe standard tool used to train DP models in practice, even though most users\nare only interested in protecting the privacy of the final model. Tight DP\naccounting for the last iterate would minimize the amount of noise required\nwhile maintaining the same privacy guarantee and potentially increasing model\nutility. However, last-iterate accounting is challenging, and existing works\nrequire strong assumptions not satisfied by most implementations. These include\nassuming (i) the global sensitivity constant is known - to avoid gradient\nclipping; (ii) the loss function is Lipschitz or convex; and (iii) input\nbatches are sampled randomly.\n  In this work, we forego any unrealistic assumptions and provide privacy\nbounds for the most commonly used variant of DP-SGD, in which data is traversed\ncyclically, gradients are clipped, and only the last model is released. More\nspecifically, we establish new Renyi differential privacy (RDP) upper bounds\nfor the last iterate under realistic assumptions of small stepsize and\nLipschitz smoothness of the loss function. Our general bounds also recover the\nspecial-case convex bounds when the weak-convexity parameter of the objective\nfunction approaches zero and no clipping is performed. The approach itself\nleverages optimal transport techniques for last iterate bounds, which is a\nnontrivial task when the data is traversed cyclically and the loss function is\nnonconvex.\n","authors":["Weiwei Kong","M√≥nica Ribero"],"pdf_url":"https://arxiv.org/pdf/2407.05237v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06717v3","updated":"2025-02-10T16:01:04Z","published":"2024-10-09T09:41:28Z","title":"Exact full-RSB SAT/UNSAT transition in infinitely wide two-layer neural\n  networks","summary":"  We analyze the problem of storing random pattern-label associations using two\nclasses of continuous non-convex weights models, namely the perceptron with\nnegative margin and an infinite-width two-layer neural network with\nnon-overlapping receptive fields and generic activation function. Using a\nfull-RSB ansatz we compute the exact value of the SAT/UNSAT transition.\nFurthermore, in the case of the negative perceptron we show that the overlap\ndistribution of typical states displays an overlap gap (a disconnected support)\nin certain regions of the phase diagram defined by the value of the margin and\nthe density of patterns to be stored. This implies that some recent theorems\nthat ensure convergence of Approximate Message Passing (AMP) based algorithms\nto capacity are not applicable. Finally, we show that Gradient Descent is not\nable to reach the maximal capacity, irrespectively of the presence of an\noverlap gap for typical states. This finding, similarly to what occurs in\nbinary weight models, suggests that gradient-based algorithms are biased\ntowards highly atypical states, whose inaccessibility determines the\nalgorithmic threshold.\n","authors":["Brandon L. Annesi","Enrico M. Malatesta","Francesco Zamponi"],"pdf_url":"https://arxiv.org/pdf/2410.06717v3.pdf","comment":"39 pages, 12 figures"},{"id":"http://arxiv.org/abs/2502.06601v1","updated":"2025-02-10T16:00:48Z","published":"2025-02-10T16:00:48Z","title":"Amortized In-Context Bayesian Posterior Estimation","summary":"  Bayesian inference provides a natural way of incorporating prior beliefs and\nassigning a probability measure to the space of hypotheses. Current solutions\nrely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and\nVariational Inference (VI), which need to be re-run whenever new observations\nare available. Amortization, through conditional estimation, is a viable\nstrategy to alleviate such difficulties and has been the guiding principle\nbehind simulation-based inference, neural processes and in-context methods\nusing pre-trained models. In this work, we conduct a thorough comparative\nanalysis of amortized in-context Bayesian posterior estimation methods from the\nlens of different optimization objectives and architectural choices. Such\nmethods train an amortized estimator to perform posterior parameter inference\nby conditioning on a set of data examples passed as context to a sequence model\nsuch as a transformer. In contrast to language models, we leverage permutation\ninvariant architectures as the true posterior is invariant to the ordering of\ncontext examples. Our empirical study includes generalization to\nout-of-distribution tasks, cases where the assumed underlying model is\nmisspecified, and transfer from simulated to real problems. Subsequently, it\nhighlights the superiority of the reverse KL estimator for predictive problems,\nespecially when combined with the transformer architecture and normalizing\nflows.\n","authors":["Sarthak Mittal","Niels Leif Bracher","Guillaume Lajoie","Priyank Jaini","Marcus Brubaker"],"pdf_url":"https://arxiv.org/pdf/2502.06601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06597v1","updated":"2025-02-10T15:58:26Z","published":"2025-02-10T15:58:26Z","title":"Continual Release Moment Estimation with Differential Privacy","summary":"  We propose Joint Moment Estimation (JME), a method for continually and\nprivately estimating both the first and second moments of data with reduced\nnoise compared to naive approaches. JME uses the matrix mechanism and a joint\nsensitivity analysis to allow the second moment estimation with no additional\nprivacy cost, thereby improving accuracy while maintaining privacy. We\ndemonstrate JME's effectiveness in two applications: estimating the running\nmean and covariance matrix for Gaussian density estimation, and model training\nwith DP-Adam on CIFAR-10.\n","authors":["Nikita P. Kalinin","Jalaj Upadhyay","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2502.06597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00075v2","updated":"2025-02-10T15:57:43Z","published":"2024-10-31T16:32:04Z","title":"ŒºP$^2$: Effective Sharpness Aware Minimization Requires Layerwise\n  Perturbation Scaling","summary":"  Sharpness Aware Minimization (SAM) enhances performance across various neural\narchitectures and datasets. As models are continually scaled up to improve\nperformance, a rigorous understanding of SAM's scaling behaviour is paramount.\nTo this end, we study the infinite-width limit of neural networks trained with\nSAM, using the Tensor Programs framework. Our findings reveal that the dynamics\nof standard SAM effectively reduce to applying SAM solely in the last layer in\nwide neural networks, even with optimal hyperparameters. In contrast, we\nidentify a stable parameterization with layerwise perturbation scaling, which\nwe call $\\textit{Maximal Update and Perturbation Parameterization}$\n($\\mu$P$^2$), that ensures all layers are both feature learning and effectively\nperturbed in the limit. Through experiments with MLPs, ResNets and Vision\nTransformers, we empirically demonstrate that $\\mu$P$^2$ achieves\nhyperparameter transfer of the joint optimum of learning rate and perturbation\nradius across model scales. Moreover, we provide an intuitive condition to\nderive $\\mu$P$^2$ for other perturbation rules like Adaptive SAM and SAM-ON,\nalso ensuring balanced perturbation effects across all layers.\n","authors":["Moritz Haas","Jin Xu","Volkan Cevher","Leena Chennuru Vankadara"],"pdf_url":"https://arxiv.org/pdf/2411.00075v2.pdf","comment":"Final NeurIPS 2024 camera-ready version. Differences to v1: Cleaner\n  Figure 1, added Appendix H.3.2 showing that even MLPs can transfer optimal\n  HPs in some versions of SP on CIFAR-10, small improvements in writing"},{"id":"http://arxiv.org/abs/2502.06591v1","updated":"2025-02-10T15:55:08Z","published":"2025-02-10T15:55:08Z","title":"Diffeomorphic Temporal Alignment Nets for Time-series Joint Alignment\n  and Averaging","summary":"  In time-series analysis, nonlinear temporal misalignment remains a pivotal\nchallenge that forestalls even simple averaging. Since its introduction, the\nDiffeomorphic Temporal Alignment Net (DTAN), which we first introduced (Weber\net al., 2019) and further developed in (Weber & Freifeld, 2023), has proven\nitself as an effective solution for this problem (these conference papers are\nearlier partial versions of the current manuscript). DTAN predicts and applies\ndiffeomorphic transformations in an input-dependent manner, thus facilitating\nthe joint alignment (JA) and averaging of time-series ensembles in an\nunsupervised or a weakly-supervised manner. The inherent challenges of the\nweakly/unsupervised setting, particularly the risk of trivial solutions through\nexcessive signal distortion, are mitigated using either one of two distinct\nstrategies: 1) a regularization term for warps; 2) using the Inverse\nConsistency Averaging Error (ICAE). The latter is a novel, regularization-free\napproach which also facilitates the JA of variable-length signals. We also\nfurther extend our framework to incorporate multi-task learning (MT-DTAN),\nenabling simultaneous time-series alignment and classification. Additionally,\nwe conduct a comprehensive evaluation of different backbone architectures,\ndemonstrating their efficacy in time-series alignment tasks. Finally, we\nshowcase the utility of our approach in enabling Principal Component Analysis\n(PCA) for misaligned time-series data. Extensive experiments across 128 UCR\ndatasets validate the superiority of our approach over contemporary averaging\nmethods, including both traditional and learning-based approaches, marking a\nsignificant advancement in the field of time-series analysis.\n","authors":["Ron Shapira Weber","Oren Freifeld"],"pdf_url":"https://arxiv.org/pdf/2502.06591v1.pdf","comment":"This manuscript covers and extends the papers: Diffeomorphic Temporal\n  Alignment Nets (DTAN; NeruIPS 2019) and Regularization-free Diffeomorphic\n  Temporal Alignment Nets (ICML 2023). Additional contributions: Multi-tasking\n  DTAN, PCA-DTAN and more"},{"id":"http://arxiv.org/abs/2502.06589v1","updated":"2025-02-10T15:54:34Z","published":"2025-02-10T15:54:34Z","title":"Hephaestus: Improving Fundamental Agent Capabilities of Large Language\n  Models through Continual Pre-Training","summary":"  Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous\nagents typically rely on complex prompting or extensive fine-tuning, which\noften fails to introduce new capabilities while preserving strong\ngeneralizability. We introduce Hephaestus-Forge, the first large-scale\npre-training corpus designed to enhance the fundamental capabilities of LLM\nagents in API function calling, intrinsic reasoning and planning, and adapting\nto environmental feedback. Hephaestus-Forge comprises 103B agent-specific data\nencompassing 76,537 APIs, including both tool documentation to introduce\nknowledge of API functions and function calling trajectories to strengthen\nintrinsic reasoning. To explore effective training protocols, we investigate\nscaling laws to identify the optimal recipe in data mixing ratios. By continual\npre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale\nopen-source LLMs and rivals commercial LLMs on three agent benchmarks,\ndemonstrating the effectiveness of our pre-training corpus in enhancing\nfundamental agentic capabilities and generalization of LLMs to new tasks or\nenvironments.\n","authors":["Yuchen Zhuang","Jingfeng Yang","Haoming Jiang","Xin Liu","Kewei Cheng","Sanket Lokegaonkar","Yifan Gao","Qing Ping","Tianyi Liu","Binxuan Huang","Zheng Li","Zhengyang Wang","Pei Chen","Ruijie Wang","Rongzhi Zhang","Nasser Zalmout","Priyanka Nigam","Bing Yin","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.06589v1.pdf","comment":"Accepted to NAACL 2025 main conference"},{"id":"http://arxiv.org/abs/2502.06587v1","updated":"2025-02-10T15:53:26Z","published":"2025-02-10T15:53:26Z","title":"evclust: Python library for evidential clustering","summary":"  A recent developing trend in clustering is the advancement of algorithms that\nnot only identify clusters within data, but also express and capture the\nuncertainty of cluster membership. Evidential clustering addresses this by\nusing the Dempster-Shafer theory of belief functions, a framework designed to\nmanage and represent uncertainty. This approach results in a credal partition,\na structured set of mass functions that quantify the uncertain assignment of\neach object to potential groups. The Python framework evclust, presented in\nthis paper, offers a suite of efficient evidence clustering algorithms as well\nas tools for visualizing, evaluating and analyzing credal partitions.\n","authors":["Armel Soubeiga","Violaine Antoine"],"pdf_url":"https://arxiv.org/pdf/2502.06587v1.pdf","comment":"13 pages, 2 figures, Preprint"},{"id":"http://arxiv.org/abs/2502.06584v1","updated":"2025-02-10T15:52:55Z","published":"2025-02-10T15:52:55Z","title":"Deep Reinforcement Learning based Triggering Function for Early\n  Classifiers of Time Series","summary":"  Early Classification of Time Series (ECTS) has been recognized as an\nimportant problem in many areas where decisions have to be taken as soon as\npossible, before the full data availability, while time pressure increases.\nNumerous ECTS approaches have been proposed, based on different triggering\nfunctions, each taking into account various pieces of information related to\nthe incoming time series and/or the output of a classifier. Although their\nperformances have been empirically compared in the literature, no studies have\nbeen carried out on the optimality of these triggering functions that involve\n``man-tailored'' decision rules. Based on the same information, could there be\nbetter triggering functions? This paper presents one way to investigate this\nquestion by showing first how to translate ECTS problems into Reinforcement\nLearning (RL) ones, where the very same information is used in the state space.\nA thorough comparison of the performance obtained by ``handmade'' approaches\nand their ``RL-based'' counterparts has been carried out. A second question\ninvestigated in this paper is whether a different combination of information,\ndefining the state space in RL systems, can achieve even better performance.\nExperiments show that the system we describe, called \\textsc{Alert},\nsignificantly outperforms its state-of-the-art competitors on a large number of\ndatasets.\n","authors":["Aur√©lien Renault","Alexis Bondu","Antoine Cornu√©jols","Vincent Lemaire"],"pdf_url":"https://arxiv.org/pdf/2502.06584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06577v1","updated":"2025-02-10T15:45:18Z","published":"2025-02-10T15:45:18Z","title":"The Minimal Search Space for Conditional Causal Bandits","summary":"  Causal knowledge can be used to support decision-making problems. This has\nbeen recognized in the causal bandits literature, where a causal (multi-armed)\nbandit is characterized by a causal graphical model and a target variable. The\narms are then interventions on the causal model, and rewards are samples of the\ntarget variable. Causal bandits were originally studied with a focus on hard\ninterventions. We focus instead on cases where the arms are conditional\ninterventions, which more accurately model many real-world decision-making\nproblems by allowing the value of the intervened variable to be chosen based on\nthe observed values of other variables. This paper presents a graphical\ncharacterization of the minimal set of nodes guaranteed to contain the optimal\nconditional intervention, which maximizes the expected reward. We then propose\nan efficient algorithm with a time complexity of $O(|V| + |E|)$ to identify\nthis minimal set of nodes. We prove that the graphical characterization and the\nproposed algorithm are correct. Finally, we empirically demonstrate that our\nalgorithm significantly prunes the search space and substantially accelerates\nconvergence rates when integrated into standard multi-armed bandit algorithms.\n","authors":["Francisco N. F. Q. Simoes","Itai Feigenbaum","Mehdi Dastani","Thijs van Ommen"],"pdf_url":"https://arxiv.org/pdf/2502.06577v1.pdf","comment":"Submitted to ICML2025"},{"id":"http://arxiv.org/abs/2502.06575v1","updated":"2025-02-10T15:44:34Z","published":"2025-02-10T15:44:34Z","title":"Predictive Red Teaming: Breaking Policies Without Breaking Robots","summary":"  Visuomotor policies trained via imitation learning are capable of performing\nchallenging manipulation tasks, but are often extremely brittle to lighting,\nvisual distractors, and object locations. These vulnerabilities can depend\nunpredictably on the specifics of training, and are challenging to expose\nwithout time-consuming and expensive hardware evaluations. We propose the\nproblem of predictive red teaming: discovering vulnerabilities of a policy with\nrespect to environmental factors, and predicting the corresponding performance\ndegradation without hardware evaluations in off-nominal scenarios. In order to\nachieve this, we develop RoboART: an automated red teaming (ART) pipeline that\n(1) modifies nominal observations using generative image editing to vary\ndifferent environmental factors, and (2) predicts performance under each\nvariation using a policy-specific anomaly detector executed on edited\nobservations. Experiments across 500+ hardware trials in twelve off-nominal\nconditions for visuomotor diffusion policies demonstrate that RoboART predicts\nperformance degradation with high accuracy (less than 0.19 average difference\nbetween predicted and real success rates). We also demonstrate how predictive\nred teaming enables targeted data collection: fine-tuning with data collected\nunder conditions predicted to be adverse boosts baseline performance by 2-7x.\n","authors":["Anirudha Majumdar","Mohit Sharma","Dmitry Kalashnikov","Sumeet Singh","Pierre Sermanet","Vikas Sindhwani"],"pdf_url":"https://arxiv.org/pdf/2502.06575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00615v2","updated":"2025-02-10T15:43:01Z","published":"2024-11-01T14:23:48Z","title":"Apriori_Goal algorithm for constructing association rules for a database\n  with a given classification","summary":"  An efficient Apriori_Goal algorithm is proposed for constructing association\nrules in a relational database with predefined classification. The target\nparameter of the database specifies a finite number of goals $Goal_k$, for each\nof which the algorithm constructs association rules of the form $X \\Rightarrow\nGoal_k$. The quality of the generated rules is characterized by five criteria:\ntwo represent rule frequency, two reflect rule reliability, and the fifth is a\nweighted sum of these four criteria.\n  The algorithm initially generates rules with single premises, where the\ncorrelation criterion between the premise $X$ and the conclusion $Goal_k$\nexceeds a specified threshold. Then, rules with extended premises are built\nbased on the anti-monotonicity of rule frequency criteria and the monotonicity\nof rule reliability criteria. Newly constructed rules tend to decrease in\nfrequency while increasing in reliability. The article proves several\nstatements that justify the rule construction process.\n  The algorithm enables the construction of both high-frequency and rare rules\nwith low occurrence frequency but high reliability. It also allows for the\ngeneration of negative rules with negative correlation between the premise and\nconclusion, which can be valuable in practical applications for filtering out\nundesired goals.\n  The efficiency of the algorithm is based on two factors: the method of\nencoding the database and its partitioning into subsets linked to the target\nparameter. Time complexity estimates for rule construction are provided using a\nmedical database as an example.\n","authors":["Vladimir Billig"],"pdf_url":"https://arxiv.org/pdf/2411.00615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06574v1","updated":"2025-02-10T15:42:38Z","published":"2025-02-10T15:42:38Z","title":"On the Impact of the Utility in Semivalue-based Data Valuation","summary":"  Semivalue-based data valuation in machine learning (ML) quantifies the\ncontribution of individual data points to a downstream ML task by leveraging\nprinciples from cooperative game theory and the notion of utility. While this\nframework has been used in practice for assessing data quality, our experiments\nreveal inconsistent valuation outcomes across different utilities, albeit all\nrelated to ML performance. Beyond raising concerns about the reliability of\ndata valuation, this inconsistency is challenging to interpret, as it stems\nfrom the complex interaction of the utility with data points and semivalue\nweights, which has barely been studied in prior work. In this paper, we take a\nfirst step toward clarifying the utility impact on semivalue-based data\nvaluation. Specifically, we provide geometric interpretations of this impact\nfor a broad family of classification utilities, which includes the accuracy and\nthe arithmetic mean. We introduce the notion of spatial signatures: given a\nsemivalue, data points can be embedded into a two-dimensional space, and\nutility functions map to the dual of this space. This geometric perspective\nseparates the influence of the dataset and semivalue from that of the utility,\nproviding a theoretical explanation for the experimentally observed sensitivity\nof valuation outcomes to the utility choice.\n","authors":["M√©lissa Tamine","Benjamin Heymann","Patrick Loiseau","Maxime Vono"],"pdf_url":"https://arxiv.org/pdf/2502.06574v1.pdf","comment":"34 pages, 21 figures"},{"id":"http://arxiv.org/abs/2408.05212v2","updated":"2025-02-10T15:42:08Z","published":"2024-08-10T05:41:19Z","title":"Preserving Privacy in Large Language Models: A Survey on Current Threats\n  and Solutions","summary":"  Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.\n","authors":["Michele Miranda","Elena Sofia Ruzzetti","Andrea Santilli","Fabio Massimo Zanzotto","S√©bastien Brati√®res","Emanuele Rodol√†"],"pdf_url":"https://arxiv.org/pdf/2408.05212v2.pdf","comment":"Published in Transactions on Machine Learning Research (TMLR)\n  https://openreview.net/forum?id=Ss9MTTN7OL"},{"id":"http://arxiv.org/abs/2502.04809v2","updated":"2025-02-10T15:38:21Z","published":"2025-02-07T10:28:39Z","title":"Humans Co-exist, So Must Embodied Artificial Agents","summary":"  Modern embodied artificial agents excel in static, predefined tasks but fall\nshort in dynamic and long-term interactions with humans. On the other hand,\nhumans can adapt and evolve continuously, exploiting the situated knowledge\nembedded in their environment and other agents, thus contributing to meaningful\ninteractions. We introduce the concept of co-existence for embodied artificial\nagents and argues that it is a prerequisite for meaningful, long-term\ninteraction with humans. We take inspiration from biology and design theory to\nunderstand how human and non-human organisms foster entities that co-exist\nwithin their specific niches. Finally, we propose key research directions for\nthe machine learning community to foster co-existing embodied agents, focusing\non the principles, hardware and learning methods responsible for shaping them.\n","authors":["Hannah Kuehn","Joseph La Delfa","Miguel Vasco","Danica Kragic","Iolanda Leite"],"pdf_url":"https://arxiv.org/pdf/2502.04809v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06567v1","updated":"2025-02-10T15:34:42Z","published":"2025-02-10T15:34:42Z","title":"Membership Inference Risks in Quantized Models: A Theoretical and\n  Empirical Study","summary":"  Quantizing machine learning models has demonstrated its effectiveness in\nlowering memory and inference costs while maintaining performance levels\ncomparable to the original models. In this work, we investigate the impact of\nquantization procedures on the privacy of data-driven models, specifically\nfocusing on their vulnerability to membership inference attacks. We derive an\nasymptotic theoretical analysis of Membership Inference Security (MIS),\ncharacterizing the privacy implications of quantized algorithm weights against\nthe most powerful (and possibly unknown) attacks. Building on these theoretical\ninsights, we propose a novel methodology to empirically assess and rank the\nprivacy levels of various quantization procedures. Using synthetic datasets, we\ndemonstrate the effectiveness of our approach in assessing the MIS of different\nquantizers. Furthermore, we explore the trade-off between privacy and\nperformance using real-world data and models in the context of molecular\nmodeling.\n","authors":["Eric Aubinais","Philippe Formont","Pablo Piantanida","Elisabeth Gassiat"],"pdf_url":"https://arxiv.org/pdf/2502.06567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06918v1","updated":"2025-02-10T15:34:37Z","published":"2025-02-10T15:34:37Z","title":"Leveraging GPT-4o Efficiency for Detecting Rework Anomaly in Business\n  Processes","summary":"  This paper investigates the effectiveness of GPT-4o-2024-08-06, one of the\nLarge Language Models (LLM) from OpenAI, in detecting business process\nanomalies, with a focus on rework anomalies. In our study, we developed a\nGPT-4o-based tool capable of transforming event logs into a structured format\nand identifying reworked activities within business event logs. The analysis\nwas performed on a synthetic dataset designed to contain rework anomalies but\nfree of loops. To evaluate the anomaly detection capabilities of GPT\n4o-2024-08-06, we used three prompting techniques: zero-shot, one-shot, and\nfew-shot. These techniques were tested on different anomaly distributions,\nnamely normal, uniform, and exponential, to identify the most effective\napproach for each case. The results demonstrate the strong performance of\nGPT-4o-2024-08-06. On our dataset, the model achieved 96.14% accuracy with\none-shot prompting for the normal distribution, 97.94% accuracy with few-shot\nprompting for the uniform distribution, and 74.21% accuracy with few-shot\nprompting for the exponential distribution. These results highlight the model's\npotential as a reliable tool for detecting rework anomalies in event logs and\nhow anomaly distribution and prompting strategy influence the model's\nperformance.\n","authors":["Mohammad Derakhshan","Paolo Ceravolo","Fatemeh Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2502.06918v1.pdf","comment":"14 pages, 5 images, 4 tables"},{"id":"http://arxiv.org/abs/2502.06564v1","updated":"2025-02-10T15:31:57Z","published":"2025-02-10T15:31:57Z","title":"Robust Scatter Matrix Estimation for Elliptical Distributions in\n  Polynomial Time","summary":"  We study the problem of computationally efficient robust estimation of\nscatter matrices of elliptical distributions under the strong contamination\nmodel. We design polynomial time algorithms that achieve dimension-independent\nerror in Frobenius norm.\n  Our first result is a sequence of efficient algorithms that approaches nearly\noptimal error. Specifically, under a mild assumption on the eigenvalues of the\nscatter matrix $\\Sigma$, for every $t \\in \\mathbb{N}$, we design an estimator\nthat, given $n = d^{O(t)}$ samples, in time $n^{O(t)}$ finds $\\hat{\\Sigma}$\nsuch that $ \\Vert{\\Sigma^{-1/2}\\, ({\\hat{\\Sigma} - \\Sigma})\\,\n\\Sigma^{-1/2}}\\Vert_{\\text{F}} \\le O(t \\cdot \\varepsilon^{1-\\frac{1}{t}})$,\nwhere $\\varepsilon$ is the fraction of corruption. We do not require any\nassumptions on the moments of the distribution, while all previously known\ncomputationally efficient algorithms for robust covariance/scatter estimation\nwith dimension-independent error rely on strong assumptions on the moments,\nsuch as sub-Gaussianity or (certifiable) hypercontractivity.\n  Furthermore, under a stronger assumption on the eigenvalues of $\\Sigma$\n(that, in particular, is satisfied by all matrices with constant condition\nnumber),\n  we provide a fast (sub-quadratic in the input size) algorithm that, given\nnearly optimal number of samples $n = \\tilde{O}(d^2/\\varepsilon)$, in time\n$\\tilde{O}({nd^2 poly(1/\\varepsilon)})$ finds $\\hat{\\Sigma}$ such that\n$\\Vert\\hat{\\Sigma} - \\Sigma\\Vert_{\\text{F}} \\le O(\\Vert{\\Sigma}\\Vert \\cdot\n\\sqrt{\\varepsilon})$.\n  Our approach is based on robust covariance estimation of the spatial sign\n(the projection onto the sphere of radius $\\sqrt{d}$) of elliptical\ndistributions.\n","authors":["Gleb Novikov"],"pdf_url":"https://arxiv.org/pdf/2502.06564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06108v2","updated":"2025-02-10T15:28:53Z","published":"2025-01-10T17:01:09Z","title":"Inferring High-Order Couplings with Neural Networks","summary":"  Maximum entropy methods, based on the inverse Ising/Potts problem from\nstatistical mechanics, are essential for modeling interactions between pairs of\nvariables in data-driven problems across disciplines such as bioinformatics,\necology, and neuroscience. Despite their considerable success, these methods\ntypically fail to capture higher-order interactions that are often essential\nfor understanding complex systems. Conversely, modern machine learning methods\ncapture these complex interactions, but the computational cost of interpretable\nframeworks makes them impractical for real-world applications. Restricted\nBoltzmann Machines (RBMs) provide a computationally efficient way to capture\nstatistical correlations using hidden nodes in a bipartite neural network. In\nthis study, we introduce a new method that maps RBMs to generalized Potts\nmodels, allowing for the extraction of interactions up to any specified order.\nThis method utilizes large-$N$ approximations, enabled by the RBM's simple\nstructure, to extract effective many-body couplings with minimal computational\neffort. Furthermore, we propose a robust framework for extracting higher-order\ninteractions in more complex probabilistic models and a simple gauge-fixing\nmethod within the effective many-body Potts model. Our validation on synthetic\ndatasets confirms the method's ability to recover two- and three-body\ninteractions accurately. When applied to protein sequence data, the framework\ncompetently reconstructs protein contact maps and provides performance\ncomparable to the best inverse Potts models. These findings confirm that RBMs\nare an effective and streamlined tool for exploring higher-order interactions\nwithin complex systems.\n","authors":["Aur√©lien Decelle","Alfonso de Jes√∫s Navas G√≥mez","Beatriz Seoane"],"pdf_url":"https://arxiv.org/pdf/2501.06108v2.pdf","comment":"16 Pages and 5 Figures"},{"id":"http://arxiv.org/abs/2501.18280v2","updated":"2025-02-10T15:27:04Z","published":"2025-01-30T11:37:40Z","title":"Jailbreaking LLMs' Safeguard with Universal Magic Words for Text\n  Embedding Models","summary":"  The security issue of large language models (LLMs) has gained significant\nattention recently, with various defense mechanisms developed to prevent\nharmful outputs, among which safeguards based on text embedding models serve as\na fundamental defense. Through testing, we discover that the distribution of\ntext embedding model outputs is significantly biased with a large mean.\nInspired by this observation, we propose novel efficient methods to search for\nuniversal magic words that can attack text embedding models. The universal\nmagic words as suffixes can move the embedding of any text towards the bias\ndirection, therefore manipulate the similarity of any text pair and mislead\nsafeguards. By appending magic words to user prompts and requiring LLMs to end\nanswers with magic words, attackers can jailbreak the safeguard. To eradicate\nthis security risk, we also propose defense mechanisms against such attacks,\nwhich can correct the biased distribution of text embeddings in a train-free\nmanner.\n","authors":["Haoyu Liang","Youran Sun","Yunfeng Cai","Jun Zhu","Bo Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.18280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05506v2","updated":"2025-02-10T15:26:28Z","published":"2024-12-07T02:34:30Z","title":"Confidence Diagram of Nonparametric Ranking for Uncertainty Assessment\n  in Large Language Models Evaluation","summary":"  We consider the inference for the ranking of large language models (LLMs).\nAlignment arises as a significant challenge to mitigate hallucinations in the\nuse of LLMs. Ranking LLMs has proven to be an effective tool to improve\nalignment based on the best-of-$N$ policy. In this paper, we propose a new\ninferential framework for hypothesis testing among the ranking for language\nmodels. Our framework is based on a nonparametric contextual ranking framework\ndesigned to assess large language models' domain-specific expertise, leveraging\nnonparametric scoring methods to account for their sensitivity to the prompts.\nTo characterize the combinatorial complexity of the ranking, we introduce a\nnovel concept of confidence diagram, which leverages a Hasse diagram to\nrepresent the entire confidence set of rankings by a single directed graph. We\nshow the validity of the proposed confidence diagram by advancing the Gaussian\nmultiplier bootstrap theory to accommodate the supremum of independent\nempirical processes that are not necessarily identically distributed. Extensive\nnumerical experiments conducted on both synthetic and real data demonstrate\nthat our approach offers valuable insight into the evaluation for the\nperformance of different LLMs across various medical domains.\n","authors":["Zebin Wang","Yi Han","Ethan X. Fang","Lan Wang","Junwei Lu"],"pdf_url":"https://arxiv.org/pdf/2412.05506v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06555v1","updated":"2025-02-10T15:23:52Z","published":"2025-02-10T15:23:52Z","title":"Is API Access to LLMs Useful for Generating Private Synthetic Tabular\n  Data?","summary":"  Differentially private (DP) synthetic data is a versatile tool for enabling\nthe analysis of private data. Recent advancements in large language models\n(LLMs) have inspired a number of algorithm techniques for improving DP\nsynthetic data generation. One family of approaches uses DP finetuning on the\nfoundation model weights; however, the model weights for state-of-the-art\nmodels may not be public. In this work we propose two DP synthetic tabular data\nalgorithms that only require API access to the foundation model. We adapt the\nPrivate Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was\ndesigned for image and text data -- to the tabular data domain. In our\nextension of Private Evolution, we define a query workload-based distance\nmeasure, which may be of independent interest. We propose a family of\nalgorithms that use one-shot API access to LLMs, rather than adaptive queries\nto the LLM. Our findings reveal that API-access to powerful LLMs does not\nalways improve the quality of DP synthetic data compared to established\nbaselines that operate without such access. We provide insights into the\nunderlying reasons and propose improvements to LLMs that could make them more\neffective for this application.\n","authors":["Marika Swanberg","Ryan McKenna","Edo Roth","Albert Cheu","Peter Kairouz"],"pdf_url":"https://arxiv.org/pdf/2502.06555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06547v1","updated":"2025-02-10T15:16:25Z","published":"2025-02-10T15:16:25Z","title":"Data Augmentation and Regularization for Learning Group Equivariance","summary":"  In many machine learning tasks, known symmetries can be used as an inductive\nbias to improve model performance. In this paper, we consider learning group\nequivariance through training with data augmentation. We summarize results from\na previous paper of our own, and extend the results to show that equivariance\nof the trained model can be achieved through training on augmented data in\ntandem with regularization.\n","authors":["Oskar Nordenfors","Axel Flinth"],"pdf_url":"https://arxiv.org/pdf/2502.06547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06917v1","updated":"2025-02-10T15:15:50Z","published":"2025-02-10T15:15:50Z","title":"Krum Federated Chain (KFC): Using blockchain to defend against\n  adversarial attacks in Federated Learning","summary":"  Federated Learning presents a nascent approach to machine learning, enabling\ncollaborative model training across decentralized devices while safeguarding\ndata privacy. However, its distributed nature renders it susceptible to\nadversarial attacks. Integrating blockchain technology with Federated Learning\noffers a promising avenue to enhance security and integrity. In this paper, we\ntackle the potential of blockchain in defending Federated Learning against\nadversarial attacks. First, we test Proof of Federated Learning, a well known\nconsensus mechanism designed ad-hoc to federated contexts, as a defense\nmechanism demonstrating its efficacy against Byzantine and backdoor attacks\nwhen at least one miner remains uncompromised. Second, we propose Krum\nFederated Chain, a novel defense strategy combining Krum and Proof of Federated\nLearning, valid to defend against any configuration of Byzantine or backdoor\nattacks, even when all miners are compromised. Our experiments conducted on\nimage classification datasets validate the effectiveness of our proposed\napproaches.\n","authors":["Mario Garc√≠a-M√°rquez","Nuria Rodr√≠guez-Barroso","M. Victoria Luz√≥n","Francisco Herrera"],"pdf_url":"https://arxiv.org/pdf/2502.06917v1.pdf","comment":"Submitted to Neural Networks"},{"id":"http://arxiv.org/abs/2406.18038v4","updated":"2025-02-10T15:13:16Z","published":"2024-06-26T03:12:07Z","title":"MT2ST: Adaptive Multi-Task to Single-Task Learning","summary":"  Efficient machine learning (ML) has become increasingly important as models\ngrow larger and data volumes expand. In this work, we address the trade-off\nbetween generalization in multi-task learning (MTL) and precision in\nsingle-task learning (STL) by introducing the Multi-Task to Single-Task (MT2ST)\nframework. MT2ST is designed to enhance training efficiency and accuracy in\nword embedding tasks, showcasing its value as a practical application of\nefficient ML.\n  Our framework employs two strategies: *Diminish*, which gradually reduces the\ninfluence of auxiliary tasks, and *Switch*, which transitions training from MTL\nto STL at a specific point. Empirical results show that MT2ST reduces training\ntime by 67\\% compared to STL and by 13\\% compared to traditional MTL, while\nmaintaining high accuracy. These findings highlight MT2ST as an efficient ML\nsolution tailored for optimizing word embedding training. Code is available at\nhttps://github.com/NoakLiu/MT2ST.\n","authors":["Dong Liu","Yanxuan Yu"],"pdf_url":"https://arxiv.org/pdf/2406.18038v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06545v1","updated":"2025-02-10T15:10:06Z","published":"2025-02-10T15:10:06Z","title":"Dimension-free Regret for Learning Asymmetric Linear Dynamical Systems","summary":"  Previously, methods for learning marginally stable linear dynamical systems\neither required the transition matrix to be symmetric or incurred regret bounds\nthat scale polynomially with the system's hidden dimension. In this work, we\nintroduce a novel method that overcomes this trade-off, achieving\ndimension-free regret despite the presence of asymmetric matrices and marginal\nstability. Our method combines spectral filtering with linear predictors and\nemploys Chebyshev polynomials in the complex plane to construct a novel\nspectral filtering basis. This construction guarantees sublinear regret in an\nonline learning framework, without relying on any statistical or generative\nassumptions. Specifically, we prove that as long as the transition matrix has\neigenvalues with complex component bounded by $1/\\mathrm{poly} \\log T$, then\nour method achieves regret $\\tilde{O}(T^{9/10})$ when compared to the best\nlinear dynamical predictor in hindsight.\n","authors":["Annie Marsden","Elad Hazan"],"pdf_url":"https://arxiv.org/pdf/2502.06545v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2502.06544v1","updated":"2025-02-10T15:09:56Z","published":"2025-02-10T15:09:56Z","title":"Sequence Transferability and Task Order Selection in Continual Learning","summary":"  In continual learning, understanding the properties of task sequences and\ntheir relationships to model performance is important for developing advanced\nalgorithms with better accuracy. However, efforts in this direction remain\nunderdeveloped despite encouraging progress in methodology development. In this\nwork, we investigate the impacts of sequence transferability on continual\nlearning and propose two novel measures that capture the total transferability\nof a task sequence, either in the forward or backward direction. Based on the\nempirical properties of these measures, we then develop a new method for the\ntask order selection problem in continual learning. Our method can be shown to\noffer a better performance than the conventional strategy of random task\nselection.\n","authors":["Thinh Nguyen","Cuong N. Nguyen","Quang Pham","Binh T. Nguyen","Savitha Ramasamy","Xiaoli Li","Cuong V. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.06544v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.03245v2","updated":"2025-02-10T15:09:52Z","published":"2025-02-05T15:02:40Z","title":"Calibrated Unsupervised Anomaly Detection in Multivariate Time-series\n  using Reinforcement Learning","summary":"  This paper investigates unsupervised anomaly detection in multivariate\ntime-series data using reinforcement learning (RL) in the latent space of an\nautoencoder. A significant challenge is the limited availability of anomalous\ndata, often leading to misclassifying anomalies as normal events, thus raising\nfalse negatives. RL can help overcome this limitation by promoting exploration\nand balancing exploitation during training, effectively preventing overfitting.\nWavelet analysis is also utilized to enhance anomaly detection, enabling\ntime-series data decomposition into both time and frequency domains. This\napproach captures anomalies at multiple resolutions, with wavelet coefficients\nextracted to detect both sudden and subtle shifts in the data, thereby refining\nthe anomaly detection process. We calibrate the decision boundary by generating\nsynthetic anomalies and embedding a supervised framework within the model. This\nsupervised element aids the unsupervised learning process by fine-tuning the\ndecision boundary and increasing the model's capacity to distinguish between\nnormal and anomalous patterns effectively.\n","authors":["Saba Sanami","Amir G. Aghdam"],"pdf_url":"https://arxiv.org/pdf/2502.03245v2.pdf","comment":"This paper has been accepted for publication and presentation at the\n  2025 IEEE International systems Conference (SysCon)"},{"id":"http://arxiv.org/abs/2407.10994v4","updated":"2025-02-10T15:08:07Z","published":"2024-06-24T12:09:34Z","title":"Panza: Design and Analysis of a Fully-Local Personalized Text Writing\n  Assistant","summary":"  The availability of powerful open-source large language models (LLMs) opens\nexciting use-cases, such as using personal data to fine-tune these models to\nimitate a user's unique writing style. Two key requirements for such assistants\nare personalization - in the sense that the assistant should recognizably\nreflect the user's own writing style - and privacy - users may justifiably be\nwary of uploading extremely personal data, such as their email archive, to a\nthird-party service. In this paper, we present a new design and evaluation for\nsuch an automated assistant, for the specific use case of email generation,\nwhich we call Panza. Panza's personalization features are based on a\ncombination of fine-tuning using a variant of the Reverse Instructions\ntechnique together with Retrieval-Augmented Generation (RAG). We demonstrate\nthat this combination allows us to fine-tune an LLM to reflect a user's writing\nstyle using limited data, while executing on extremely limited resources, e.g.\non a free Google Colab instance. Our key methodological contribution is the\nfirst detailed study of evaluation metrics for this personalized writing task,\nand of how different choices of system components--the use of RAG and of\ndifferent fine-tuning approaches-impact the system's performance. Additionally,\nwe demonstrate that very little data - under 100 email samples - are sufficient\nto create models that convincingly imitate humans. This finding showcases a\npreviously-unknown attack vector in language models - that access to a small\nnumber of writing samples can allow a bad actor to cheaply create generative\nmodels that imitate a target's writing style. We are releasing the full Panza\ncode as well as three new email datasets licensed for research use at\nhttps://github.com/IST-DASLab/PanzaMail.\n","authors":["Armand Nicolicioiu","Eugenia Iofinova","Andrej Jovanovic","Eldar Kurtic","Mahdi Nikdan","Andrei Panferov","Ilia Markov","Nir Shavit","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2407.10994v4.pdf","comment":"Panza is available at https://github.com/IST-DASLab/PanzaMail"},{"id":"http://arxiv.org/abs/2405.20973v2","updated":"2025-02-10T15:04:53Z","published":"2024-05-31T16:21:05Z","title":"LCQ: Low-Rank Codebook based Quantization for Large Language Models","summary":"  Large language models~(LLMs) have recently demonstrated promising performance\nin many tasks. However, the high storage and computational cost of LLMs has\nbecome a challenge for deploying LLMs. Weight quantization has been widely used\nfor model compression, which can reduce both storage and computational cost.\nMost existing weight quantization methods for LLMs use a rank-one codebook for\nquantization, which results in substantial accuracy loss when the compression\nratio is high. In this paper, we propose a novel weight quantization method,\ncalled low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a\nlow-rank codebook, the rank of which can be larger than one, for quantization.\nExperiments show that LCQ can achieve better accuracy than existing methods\nwith a negligibly extra storage cost.\n","authors":["Wen-Pu Cai","Ming-Yang Li","Wu-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2405.20973v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.06536v1","updated":"2025-02-10T15:01:56Z","published":"2025-02-10T15:01:56Z","title":"Sample-efficient Learning of Concepts with Theoretical Guarantees: from\n  Data to Concepts without Interventions","summary":"  Machine learning is a vital part of many real-world systems, but several\nconcerns remain about the lack of interpretability, explainability and\nrobustness of black-box AI systems. Concept-based models (CBM) address some of\nthese challenges by learning interpretable concepts from high-dimensional data,\ne.g. images, which are used to predict labels. An important issue in CBMs is\nconcept leakage, i.e., spurious information in the learned concepts, which\neffectively leads to learning \"wrong\" concepts. Current mitigating strategies\nare heuristic, have strong assumptions, e.g., they assume that the concepts are\nstatistically independent of each other, or require substantial human\ninteraction in terms of both interventions and labels provided by annotators.\nIn this paper, we describe a framework that provides theoretical guarantees on\nthe correctness of the learned concepts and on the number of required labels,\nwithout requiring any interventions. Our framework leverages causal\nrepresentation learning (CRL) to learn high-level causal variables from\nlow-level data, and learns to align these variables with interpretable\nconcepts. We propose a linear and a non-parametric estimator for this mapping,\nproviding a finite-sample high probability result in the linear case and an\nasymptotic consistency result for the non-parametric estimator. We implement\nour framework with state-of-the-art CRL methods, and show its efficacy in\nlearning the correct concepts in synthetic and image benchmarks.\n","authors":["Hidde Fokkema","Tim van Erven","Sara Magliacane"],"pdf_url":"https://arxiv.org/pdf/2502.06536v1.pdf","comment":"47 pages, 16 figures, 9 Tables, Preprint"},{"id":"http://arxiv.org/abs/2405.14620v2","updated":"2025-02-10T15:01:34Z","published":"2024-05-23T14:29:15Z","title":"Closed-form Solutions: A New Perspective on Solving Differential\n  Equations","summary":"  The pursuit of analytical solutions for differential equations has\nhistorically been limited by the need for extensive prior knowledge and\nmathematical prowess; however, machine learning methods like genetic algorithms\nhave recently been applied to this end, albeit with issues of significant time\nconsumption and complexity. This paper presents a novel machine learning-based\nsolver, SSDE (Symbolic Solver for Differential Equations), which employs\nreinforcement learning to derive symbolic closed-form solutions for various\ndifferential equations. Our evaluations on a range of ordinary and partial\ndifferential equations demonstrate that SSDE provides superior performance in\nachieving analytical solutions compared to other machine learning approaches.\n","authors":["Shu Wei","Yanjie Li","Lina Yu","Weijun Li","Min Wu","Linjun Sun","Jufeng Han","Yan Pang"],"pdf_url":"https://arxiv.org/pdf/2405.14620v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18609v3","updated":"2025-02-10T15:01:09Z","published":"2024-07-26T09:00:18Z","title":"Denoising L√©vy Probabilistic Models","summary":"  Exploring noise distributions beyond Gaussian in diffusion models remains an\nopen challenge. While Gaussian-based models succeed within a unified SDE\nframework, recent studies suggest that heavy-tailed noise distributions, like\n$\\alpha$-stable distributions, may better handle mode collapse and effectively\nmanage datasets exhibiting class imbalance, heavy tails, or prominent outliers.\nRecently, Yoon et al.\\ (NeurIPS 2023), presented the L\\'evy-It\\^o model (LIM),\ndirectly extending the SDE-based framework to a class of heavy-tailed SDEs,\nwhere the injected noise followed an $\\alpha$-stable distribution, a rich class\nof heavy-tailed distributions. However, the LIM framework relies on highly\ninvolved mathematical techniques with limited flexibility, potentially\nhindering broader adoption and further development. In this study, instead of\nstarting from the SDE formulation, we extend the denoising diffusion\nprobabilistic model (DDPM) by replacing the Gaussian noise with $\\alpha$-stable\nnoise. By using only elementary proof techniques, the proposed approach,\nDenoising L\\'evy Probabilistic Models (DLPM), boils down to vanilla DDPM with\nminor modifications. As opposed to the Gaussian case, DLPM and LIM yield\ndifferent training algorithms and different backward processes, leading to\ndistinct sampling algorithms. These fundamental differences translate favorably\nfor DLPM as compared to LIM: our experiments show improvements in coverage of\ndata distribution tails, better robustness to unbalanced datasets, and improved\ncomputation times requiring smaller number of backward steps.\n","authors":["Dario Shariatian","Umut Simsekli","Alain Durmus"],"pdf_url":"https://arxiv.org/pdf/2407.18609v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06533v1","updated":"2025-02-10T14:56:25Z","published":"2025-02-10T14:56:25Z","title":"Ignore the KL Penalty! Boosting Exploration on Critical Tokens to\n  Enhance RL Fine-Tuning","summary":"  The ability to achieve long-term goals is a key challenge in the current\ndevelopment of large language models (LLMs). To address this, pre-trained LLMs\ncan be fine-tuned with reinforcement learning (RL) to explore solutions that\noptimize a given goal. However, exploration with LLMs is difficult, as a\nbalance has to be struck between discovering new solutions and staying close\nenough to the pre-trained model, so as not to degrade basic capabilities. This\nis typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we\ninvestigate the exploration dynamics of a small language model on a simple\narithmetic task. We show how varying degrees of pre-training influence\nexploration and demonstrate the importance of \"critical tokens\" which have a\ndramatic impact on the final outcome. Consequently, we introduce a simple\nmodification to the KL penalty that favors exploration on critical tokens,\nincreasing the efficiency of the RL fine-tuning stage.\n","authors":["Jean Vassoyan","Nathana√´l Beau","Roman Plaud"],"pdf_url":"https://arxiv.org/pdf/2502.06533v1.pdf","comment":"11 pages, 6 figures, 5 tables. Accepted for publication in the\n  Findings of the North American Chapter of the Association for Computational\n  Linguistics (NAACL) 2025"},{"id":"http://arxiv.org/abs/2502.06525v1","updated":"2025-02-10T14:49:22Z","published":"2025-02-10T14:49:22Z","title":"Properties of Wasserstein Gradient Flows for the Sliced-Wasserstein\n  Distance","summary":"  In this paper, we investigate the properties of the Sliced Wasserstein\nDistance (SW) when employed as an objective functional. The SW metric has\ngained significant interest in the optimal transport and machine learning\nliterature, due to its ability to capture intricate geometric properties of\nprobability distributions while remaining computationally tractable, making it\na valuable tool for various applications, including generative modeling and\ndomain adaptation. Our study aims to provide a rigorous analysis of the\ncritical points arising from the optimization of the SW objective. By computing\nexplicit perturbations, we establish that stable critical points of SW cannot\nconcentrate on segments. This stability analysis is crucial for understanding\nthe behaviour of optimization algorithms for models trained using the SW\nobjective. Furthermore, we investigate the properties of the SW objective,\nshedding light on the existence and convergence behavior of critical points. We\nillustrate our theoretical results through numerical experiments.\n","authors":["Christophe Vauthier","Quentin M√©rigot","Anna Korba"],"pdf_url":"https://arxiv.org/pdf/2502.06525v1.pdf","comment":"32p"},{"id":"http://arxiv.org/abs/2502.06516v1","updated":"2025-02-10T14:37:26Z","published":"2025-02-10T14:37:26Z","title":"Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation","summary":"  Minority samples are underrepresented instances located in low-density\nregions of a data manifold, and are valuable in many generative AI\napplications, such as data augmentation, creative content generation, etc.\nUnfortunately, existing diffusion-based minority generators often rely on\ncomputationally expensive guidance dedicated for minority generation. To\naddress this, here we present a simple yet powerful guidance-free approach\ncalled Boost-and-Skip for generating minority samples using diffusion models.\nThe key advantage of our framework requires only two minimal changes to\nstandard generative processes: (i) variance-boosted initialization and (ii)\ntimestep skipping. We highlight that these seemingly-trivial modifications are\nsupported by solid theoretical and empirical evidence, thereby effectively\npromoting emergence of underrepresented minority features. Our comprehensive\nexperiments demonstrate that Boost-and-Skip greatly enhances the capability of\ngenerating minority samples, even rivaling guidance-based state-of-the-art\napproaches while requiring significantly fewer computations.\n","authors":["Soobin Um","Beomsu Kim","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2502.06516v1.pdf","comment":"29 pages, 11 figures"},{"id":"http://arxiv.org/abs/2502.06491v1","updated":"2025-02-10T14:08:55Z","published":"2025-02-10T14:08:55Z","title":"Model-Based Offline Reinforcement Learning with Reliability-Guaranteed\n  Sequence Modeling","summary":"  Model-based offline reinforcement learning (MORL) aims to learn a policy by\nexploiting a dynamics model derived from an existing dataset. Applying\nconservative quantification to the dynamics model, most existing works on MORL\ngenerate trajectories that approximate the real data distribution to facilitate\npolicy learning by using current information (e.g., the state and action at\ntime step $t$). However, these works neglect the impact of historical\ninformation on environmental dynamics, leading to the generation of unreliable\ntrajectories that may not align with the real data distribution. In this paper,\nwe propose a new MORL algorithm \\textbf{R}eliability-guaranteed\n\\textbf{T}ransformer (RT), which can eliminate unreliable trajectories by\ncalculating the cumulative reliability of the generated trajectory (i.e., using\na weighted variational distance away from the real data). Moreover, by sampling\ncandidate actions with high rewards, RT can efficiently generate high-return\ntrajectories from the existing offline data. We theoretically prove the\nperformance guarantees of RT in policy learning, and empirically demonstrate\nits effectiveness against state-of-the-art model-based methods on several\nbenchmark tasks.\n","authors":["Shenghong He"],"pdf_url":"https://arxiv.org/pdf/2502.06491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06485v1","updated":"2025-02-10T14:04:23Z","published":"2025-02-10T14:04:23Z","title":"WyckoffDiff - A Generative Diffusion Model for Crystal Symmetry","summary":"  Crystalline materials often exhibit a high level of symmetry. However, most\ngenerative models do not account for symmetry, but rather model each atom\nwithout any constraints on its position or element. We propose a generative\nmodel, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based\ndescriptions of crystals. This is enabled by considering a crystal structure\nrepresentation that encodes all symmetry, and we design a novel neural network\narchitecture which enables using this representation inside a discrete\ngenerative model framework. In addition to respecting symmetry by construction,\nthe discrete nature of our model enables fast generation. We additionally\npresent a new metric, Fr\\'echet Wrenformer Distance, which captures the\nsymmetry aspects of the materials generated, and we benchmark WyckoffDiff\nagainst recently proposed generative models for crystal generation.\n","authors":["Filip Ekstr√∂m Kelvinius","Oskar B. Andersson","Abhijith S. Parackal","Dong Qian","Rickard Armiento","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2502.06485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10958v4","updated":"2025-02-10T14:02:49Z","published":"2024-11-17T04:35:49Z","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization","summary":"  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrices $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK^\\top$. Third, we propose a two-level accumulation strategy\nfor $\\widetilde PV$ to enhance the accuracy of FP8 $\\widetilde PV$. The\noperations per second (OPS) of SageAttention2 surpass FlashAttention2 and\nxformers by about 3x and 4.5x on RTX4090, respectively. Moreover,\nSageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs,\nwhile delivering much higher accuracy. Comprehensive experiments confirm that\nour approach incurs negligible end-to-end metrics loss across diverse models,\nincluding those for language, image, and video generation. The code is\navailable at https://github.com/thu-ml/SageAttention.\n","authors":["Jintao Zhang","Haofeng Huang","Pengle Zhang","Jia Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2411.10958v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06480v1","updated":"2025-02-10T13:59:00Z","published":"2025-02-10T13:59:00Z","title":"Logarithmic Regret of Exploration in Average Reward Markov Decision\n  Processes","summary":"  In average reward Markov decision processes, state-of-the-art algorithms for\nregret minimization follow a well-established framework: They are model-based,\noptimistic and episodic. First, they maintain a confidence region from which\noptimistic policies are computed using a well-known subroutine called Extended\nValue Iteration (EVI). Second, these policies are used over time windows called\nepisodes, each ended by the Doubling Trick (DT) rule or a variant thereof. In\nthis work, without modifying EVI, we show that there is a significant advantage\nin replacing (DT) by another simple rule, that we call the Vanishing\nMultiplicative (VM) rule. When managing episodes with (VM), the algorithm's\nregret is, both in theory and in practice, as good if not better than with\n(DT), while the one-shot behavior is greatly improved. More specifically, the\nmanagement of bad episodes (when sub-optimal policies are being used) is much\nbetter under (VM) than (DT) by making the regret of exploration logarithmic\nrather than linear. These results are made possible by a new in-depth\nunderstanding of the contrasting behaviors of confidence regions during good\nand bad episodes.\n","authors":["Victor Boone","Bruno Gaujal"],"pdf_url":"https://arxiv.org/pdf/2502.06480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08170v2","updated":"2025-02-10T13:57:58Z","published":"2023-11-14T13:54:35Z","title":"Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning\n  Approach","summary":"  Lattice reduction is a combinatorial optimization problem aimed at finding\nthe most orthogonal basis in a given lattice. The Lenstra-Lenstra-Lov\\'asz\n(LLL) algorithm is the best algorithm in the literature for solving this\nproblem. In light of recent research on algorithm discovery, in this work, we\nwould like to answer this question: is it possible to parametrize the algorithm\nspace for lattice reduction problem with neural networks and find an algorithm\nwithout supervised data? Our strategy is to use equivariant and invariant\nparametrizations and train in a self-supervised way. We design a deep neural\nmodel outputting factorized unimodular matrices and train it in a\nself-supervised manner by penalizing non-orthogonal lattice bases. We\nincorporate the symmetries of lattice reduction into the model by making it\ninvariant to isometries and scaling of the ambient space and equivariant with\nrespect to the hyperocrahedral group permuting and flipping the lattice basis\nelements. We show that this approach yields an algorithm with comparable\ncomplexity and performance to the LLL algorithm on a set of benchmarks.\nAdditionally, motivated by certain applications for wireless communication, we\nextend our method to a convolutional architecture which performs joint\nreduction of spatially-correlated lattices arranged in a grid, thereby\namortizing its cost over multiple lattices.\n","authors":["Giovanni Luca Marchetti","Gabriele Cesa","Pratik Kumar","Arash Behboodi"],"pdf_url":"https://arxiv.org/pdf/2311.08170v2.pdf","comment":"Accepted at TMLR"},{"id":"http://arxiv.org/abs/2409.06282v2","updated":"2025-02-10T13:54:27Z","published":"2024-09-10T07:34:19Z","title":"Automated Data Augmentation for Few-Shot Time Series Forecasting: A\n  Reinforcement Learning Approach Guided by a Model Zoo","summary":"  Time series forecasting, particularly in few-shot learning scenarios, is\nchallenging due to the limited availability of high-quality training data. To\naddress this, we present a pilot study on using reinforcement learning (RL) for\ntime series data augmentation. Our method, ReAugment, tackles three critical\nquestions: which parts of the training set should be augmented, how the\naugmentation should be performed, and what advantages RL brings to the process.\nSpecifically, our approach maintains a forecasting model zoo, and by measuring\nprediction diversity across the models, we identify samples with higher\nprobabilities for overfitting and use them as the anchor points for\naugmentation. Leveraging RL, our method adaptively transforms the overfit-prone\nsamples into new data that not only enhances training set diversity but also\ndirects the augmented data to target regions where the forecasting models are\nprone to overfitting. We validate the effectiveness of ReAugment across a wide\nrange of base models, showing its advantages in both standard time series\nforecasting and few-shot learning tasks.\n","authors":["Haochen Yuan","Yutong Wang","Yihong Chen","Yunbo Wang"],"pdf_url":"https://arxiv.org/pdf/2409.06282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07632v2","updated":"2025-02-10T13:48:49Z","published":"2024-10-10T05:54:01Z","title":"Provable Privacy Attacks on Trained Shallow Neural Networks","summary":"  We study what provable privacy attacks can be shown on trained, 2-layer ReLU\nneural networks. We explore two types of attacks; data reconstruction attacks,\nand membership inference attacks. We prove that theoretical results on the\nimplicit bias of 2-layer neural networks can be used to provably reconstruct a\nset of which at least a constant fraction are training points in a univariate\nsetting, and can also be used to identify with high probability whether a given\npoint was used in the training set in a high dimensional setting. To the best\nof our knowledge, our work is the first to show provable vulnerabilities in\nthis implicit-bias-driven setting.\n","authors":["Guy Smorodinsky","Gal Vardi","Itay Safran"],"pdf_url":"https://arxiv.org/pdf/2410.07632v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10947v4","updated":"2025-02-10T13:40:27Z","published":"2023-06-19T14:07:10Z","title":"PAC-Chernoff Bounds: Understanding Generalization in the Interpolation\n  Regime","summary":"  This paper introduces a distribution-dependent PAC-Chernoff bound that\nexhibits perfect tightness for interpolators, even within over-parameterized\nmodel classes. This bound, which relies on basic principles of Large Deviation\nTheory, defines a natural measure of the smoothness of a model, characterized\nby simple real-valued functions. Building upon this bound and the new concept\nof smoothness, we present an unified theoretical framework revealing why\ncertain interpolators show an exceptional generalization, while others falter.\nWe theoretically show how a wide spectrum of modern learning methodologies,\nencompassing techniques such as $\\ell_2$-norm, distance-from-initialization and\ninput-gradient regularization, in combination with data augmentation, invariant\narchitectures, and over-parameterization, collectively guide the optimizer\ntoward smoother interpolators, which, according to our theoretical framework,\nare the ones exhibiting superior generalization performance. This study shows\nthat distribution-dependent bounds serve as a powerful tool to understand the\ncomplex dynamics behind the generalization capabilities of over-parameterized\ninterpolators.\n","authors":["Andr√©s R. Masegosa","Luis A. Ortega"],"pdf_url":"https://arxiv.org/pdf/2306.10947v4.pdf","comment":"60 pages, 12 figures, published at JAIR 2025"},{"id":"http://arxiv.org/abs/2501.01895v2","updated":"2025-02-10T13:36:02Z","published":"2025-01-03T17:00:33Z","title":"EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation","summary":"  We introduce EnerVerse, a generative robotics foundation model that\nconstructs and interprets embodied spaces. EnerVerse employs an autoregressive\nvideo diffusion framework to predict future embodied spaces from instructions,\nenhanced by a sparse context memory for long-term reasoning. To model the 3D\nrobotics world, we propose Free Anchor Views (FAVs), a multi-view video\nrepresentation offering flexible, task-adaptive perspectives to address\nchallenges like motion ambiguity and environmental constraints. Additionally,\nwe present EnerVerse-D, a data engine pipeline combining the generative model\nwith 4D Gaussian Splatting, forming a self-reinforcing data loop to reduce the\nsim-to-real gap. Leveraging these innovations, EnerVerse translates 4D world\nrepresentations into physical actions via a policy head (EnerVerse-A), enabling\nrobots to execute task instructions. EnerVerse-A achieves state-of-the-art\nperformance in both simulation and real-world settings.\n","authors":["Siyuan Huang","Liliang Chen","Pengfei Zhou","Shengcong Chen","Zhengkai Jiang","Yue Hu","Yue Liao","Peng Gao","Hongsheng Li","Maoqing Yao","Guanghui Ren"],"pdf_url":"https://arxiv.org/pdf/2501.01895v2.pdf","comment":"Website: https://sites.google.com/view/enerverse"},{"id":"http://arxiv.org/abs/2502.06443v1","updated":"2025-02-10T13:19:30Z","published":"2025-02-10T13:19:30Z","title":"Low-dimensional Functions are Efficiently Learnable under Randomly\n  Biased Distributions","summary":"  The problem of learning single index and multi index models has gained\nsignificant interest as a fundamental task in high-dimensional statistics. Many\nrecent works have analysed gradient-based methods, particularly in the setting\nof isotropic data distributions, often in the context of neural network\ntraining. Such studies have uncovered precise characterisations of algorithmic\nsample complexity in terms of certain analytic properties of the target\nfunction, such as the leap, information, and generative exponents. These\nproperties establish a quantitative separation between low and high complexity\nlearning tasks. In this work, we show that high complexity cases are rare.\nSpecifically, we prove that introducing a small random perturbation to the data\ndistribution--via a random shift in the first moment--renders any Gaussian\nsingle index model as easy to learn as a linear function. We further extend\nthis result to a class of multi index models, namely sparse Boolean functions,\nalso known as Juntas.\n","authors":["Elisabetta Cornacchia","Dan Mikulincer","Elchanan Mossel"],"pdf_url":"https://arxiv.org/pdf/2502.06443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06439v1","updated":"2025-02-10T13:16:01Z","published":"2025-02-10T13:16:01Z","title":"Testing software for non-discrimination: an updated and extended audit\n  in the Italian car insurance domain","summary":"  Context. As software systems become more integrated into society's\ninfrastructure, the responsibility of software professionals to ensure\ncompliance with various non-functional requirements increases. These\nrequirements include security, safety, privacy, and, increasingly,\nnon-discrimination.\n  Motivation. Fairness in pricing algorithms grants equitable access to basic\nservices without discriminating on the basis of protected attributes.\n  Method. We replicate a previous empirical study that used black box testing\nto audit pricing algorithms used by Italian car insurance companies, accessible\nthrough a popular online system. With respect to the previous study, we\nenlarged the number of tests and the number of demographic variables under\nanalysis.\n  Results. Our work confirms and extends previous findings, highlighting the\nproblematic permanence of discrimination across time: demographic variables\nsignificantly impact pricing to this day, with birthplace remaining the main\ndiscriminatory factor against individuals not born in Italian cities. We also\nfound that driver profiles can determine the number of quotes available to the\nuser, denying equal opportunities to all.\n  Conclusion. The study underscores the importance of testing for\nnon-discrimination in software systems that affect people's everyday lives.\nPerforming algorithmic audits over time makes it possible to evaluate the\nevolution of such algorithms. It also demonstrates the role that empirical\nsoftware engineering can play in making software systems more accountable.\n","authors":["Marco Rondina","Antonio Vetr√≤","Riccardo Coppola","Oumaima Regragrui","Alessandro Fabris","Gianmaria Silvello","Gian Antonio Susto","Juan Carlos De Martin"],"pdf_url":"https://arxiv.org/pdf/2502.06439v1.pdf","comment":"14 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.06438v1","updated":"2025-02-10T13:15:52Z","published":"2025-02-10T13:15:52Z","title":"FEMBA: Efficient and Scalable EEG Analysis with a Bidirectional Mamba\n  Foundation Model","summary":"  Accurate and efficient electroencephalography (EEG) analysis is essential for\ndetecting seizures and artifacts in long-term monitoring, with applications\nspanning hospital diagnostics to wearable health devices. Robust EEG analytics\nhave the potential to greatly improve patient care. However, traditional deep\nlearning models, especially Transformer-based architectures, are hindered by\ntheir quadratic time and memory complexity, making them less suitable for\nresource-constrained environments. To address these challenges, we present\nFEMBA (Foundational EEG Mamba + Bidirectional Architecture), a novel\nself-supervised framework that establishes new efficiency benchmarks for EEG\nanalysis through bidirectional state-space modeling. Unlike Transformer-based\nmodels, which incur quadratic time and memory complexity, FEMBA scales linearly\nwith sequence length, enabling more scalable and efficient processing of\nextended EEG recordings. Trained on over 21,000 hours of unlabeled EEG and\nfine-tuned on three downstream tasks, FEMBA achieves competitive performance in\ncomparison with transformer models, with significantly lower computational\ncost. Specifically, it reaches 81.82% balanced accuracy (0.8921 AUROC) on TUAB\nand 0.949 AUROC on TUAR, while a tiny 7.8M-parameter variant demonstrates\nviability for resource-constrained devices. These results pave the way for\nscalable, general-purpose EEG analytics in both clinical and highlight FEMBA as\na promising candidate for wearable applications.\n","authors":["Anna Tegon","Thorir Mar Ingolfsson","Xiaying Wang","Luca Benini","Yawei Li"],"pdf_url":"https://arxiv.org/pdf/2502.06438v1.pdf","comment":"7 pages, 3 figures, 5 tables, pre-print"},{"id":"http://arxiv.org/abs/2305.16272v4","updated":"2025-02-10T13:12:19Z","published":"2023-05-25T17:28:41Z","title":"Incentivizing Honesty among Competitors in Collaborative Learning and\n  Optimization","summary":"  Collaborative learning techniques have the potential to enable training\nmachine learning models that are superior to models trained on a single\nentity's data. However, in many cases, potential participants in such\ncollaborative schemes are competitors on a downstream task, such as firms that\neach aim to attract customers by providing the best recommendations. This can\nincentivize dishonest updates that damage other participants' models,\npotentially undermining the benefits of collaboration. In this work, we\nformulate a game that models such interactions and study two learning tasks\nwithin this framework: single-round mean estimation and multi-round SGD on\nstrongly-convex objectives. For a natural class of player actions, we show that\nrational clients are incentivized to strongly manipulate their updates,\npreventing learning. We then propose mechanisms that incentivize honest\ncommunication and ensure learning quality comparable to full cooperation.\nLastly, we empirically demonstrate the effectiveness of our incentive scheme on\na standard non-convex federated learning benchmark. Our work shows that\nexplicitly modeling the incentives and actions of dishonest clients, rather\nthan assuming them malicious, can enable strong robustness guarantees for\ncollaborative learning.\n","authors":["Florian E. Dorner","Nikola Konstantinov","Georgi Pashaliev","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2305.16272v4.pdf","comment":"Updated experimental results after fixing a mistake in the code.\n  Previous version published in NeurIPS 2023; 37 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.06434v1","updated":"2025-02-10T13:11:40Z","published":"2025-02-10T13:11:40Z","title":"Rethinking Large-scale Dataset Compression: Shifting Focus From Labels\n  to Images","summary":"  Dataset distillation and dataset pruning are two prominent techniques for\ncompressing datasets to improve computational and storage efficiency. Despite\ntheir overlapping objectives, these approaches are rarely compared directly.\nEven within each field, the evaluation protocols are inconsistent across\nvarious methods, which complicates fair comparisons and hinders\nreproducibility. Considering these limitations, we introduce in this paper a\nbenchmark that equitably evaluates methodologies across both distillation and\npruning literatures. Notably, our benchmark reveals that in the mainstream\ndataset distillation setting for large-scale datasets, which heavily rely on\nsoft labels from pre-trained models, even randomly selected subsets can achieve\nsurprisingly competitive performance. This finding suggests that an\noveremphasis on soft labels may be diverting attention from the intrinsic value\nof the image data, while also imposing additional burdens in terms of\ngeneration, storage, and application. To address these issues, we propose a new\nframework for dataset compression, termed Prune, Combine, and Augment (PCA),\nwhich focuses on leveraging image data exclusively, relies solely on hard\nlabels for evaluation, and achieves state-of-the-art performance in this setup.\nBy shifting the emphasis back to the images, our benchmark and PCA framework\npave the way for more balanced and accessible techniques in dataset compression\nresearch. Our code is available at:\nhttps://github.com/ArmandXiao/Rethinking-Dataset-Compression\n","authors":["Lingao Xiao","Songhua Liu","Yang He","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.06434v1.pdf","comment":"Work In Progress"},{"id":"http://arxiv.org/abs/2502.06916v1","updated":"2025-02-10T13:06:56Z","published":"2025-02-10T13:06:56Z","title":"Hyper Compressed Fine-Tuning of Large Foundation Models with Quantum\n  Inspired Adapters","summary":"  Fine-tuning pre-trained large foundation models for specific tasks has become\nincreasingly challenging due to the computational and storage demands\nassociated with full parameter updates. Parameter-Efficient Fine-Tuning (PEFT)\nmethods address this issue by updating only a small subset of model parameters\nusing adapter modules. In this work, we propose \\emph{Quantum-Inspired\nAdapters}, a PEFT approach inspired by Hamming-weight preserving quantum\ncircuits from quantum machine learning literature. These models can be both\nexpressive and parameter-efficient by operating in a combinatorially large\nspace while simultaneously preserving orthogonality in weight parameters. We\ntest our proposed adapters by adapting large language models and large vision\ntransformers on benchmark datasets. Our method can achieve 99.2\\% of the\nperformance of existing fine-tuning methods such LoRA with a 44x parameter\ncompression on language understanding datasets like GLUE and VTAB. Compared to\nexisting orthogonal fine-tuning methods such as OFT or BOFT, we achieve 98\\%\nrelative performance with 25x fewer parameters. This demonstrates competitive\nperformance paired with a significant reduction in trainable parameters.\nThrough ablation studies, we determine that combining multiple Hamming-weight\norders with orthogonality and matrix compounding are essential for performant\nfine-tuning. Our findings suggest that Quantum-Inspired Adapters offer a\npromising direction for efficient adaptation of language and vision models in\nresource-constrained environments.\n","authors":["Snehal Raj","Brian Coyle"],"pdf_url":"https://arxiv.org/pdf/2502.06916v1.pdf","comment":"16 pages, 9 figures, 6 tables"},{"id":"http://arxiv.org/abs/2502.06424v1","updated":"2025-02-10T13:00:49Z","published":"2025-02-10T13:00:49Z","title":"CS-SHAP: Extending SHAP to Cyclic-Spectral Domain for Better\n  Interpretability of Intelligent Fault Diagnosis","summary":"  Neural networks (NNs), with their powerful nonlinear mapping and end-to-end\ncapabilities, are widely applied in mechanical intelligent fault diagnosis\n(IFD). However, as typical black-box models, they pose challenges in\nunderstanding their decision basis and logic, limiting their deployment in\nhigh-reliability scenarios. Hence, various methods have been proposed to\nenhance the interpretability of IFD. Among these, post-hoc approaches can\nprovide explanations without changing model architecture, preserving its\nflexibility and scalability. However, existing post-hoc methods often suffer\nfrom limitations in explanation forms. They either require preprocessing that\ndisrupts the end-to-end nature or overlook fault mechanisms, leading to\nsuboptimal explanations. To address these issues, we derived the\ncyclic-spectral (CS) transform and proposed the CS-SHAP by extending Shapley\nadditive explanations (SHAP) to the CS domain. CS-SHAP can evaluate\ncontributions from both carrier and modulation frequencies, aligning more\nclosely with fault mechanisms and delivering clearer and more accurate\nexplanations. Three datasets are utilized to validate the superior\ninterpretability of CS-SHAP, ensuring its correctness, reproducibility, and\npractical performance. With open-source code and outstanding interpretability,\nCS-SHAP has the potential to be widely adopted and become the post-hoc\ninterpretability benchmark in IFD, even in other classification tasks. The code\nis available on https://github.com/ChenQian0618/CS-SHAP.\n","authors":["Qian Chen","Xingjian Dong","Kui Hu","Kangkang Chen","Zhike Peng","Guang Meng"],"pdf_url":"https://arxiv.org/pdf/2502.06424v1.pdf","comment":"21 pages, 21 figures"},{"id":"http://arxiv.org/abs/2312.08008v3","updated":"2025-02-10T12:55:57Z","published":"2023-12-13T09:31:30Z","title":"Learning in Zero-Sum Markov Games: Relaxing Strong Reachability and\n  Mixing Time Assumptions","summary":"  We address payoff-based decentralized learning in infinite-horizon zero-sum\nMarkov games. In this setting, each player makes decisions based solely on\nreceived rewards, without observing the opponent's strategy or actions nor\nsharing information. Prior works established finite-time convergence to an\napproximate Nash equilibrium under strong reachability and mixing time\nassumptions. We propose a convergent algorithm that significantly relaxes these\nassumptions, requiring only the existence of a single policy (not necessarily\nknown) with bounded reachability and mixing time. Our key technical novelty is\nintroducing Tsallis entropy regularization to smooth the best-response policy\nupdates. By suitably tuning this regularization, we ensure sufficient\nexploration, thus bypassing previous stringent assumptions on the MDP. By\nestablishing novel properties of the value and policy updates induced by the\nTsallis entropy regularizer, we prove finite-time convergence to an approximate\nNash equilibrium.\n","authors":["Reda Ouhamma","Maryam Kamgarpour"],"pdf_url":"https://arxiv.org/pdf/2312.08008v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.07128v1","updated":"2025-02-10T23:47:35Z","published":"2025-02-10T23:47:35Z","title":"Cardiverse: Harnessing LLMs for Novel Card Game Prototyping","summary":"  The prototyping of computer games, particularly card games, requires\nextensive human effort in creative ideation and gameplay evaluation. Recent\nadvances in Large Language Models (LLMs) offer opportunities to automate and\nstreamline these processes. However, it remains challenging for LLMs to design\nnovel game mechanics beyond existing databases, generate consistent gameplay\nenvironments, and develop scalable gameplay AI for large-scale evaluations.\nThis paper addresses these challenges by introducing a comprehensive automated\ncard game prototyping framework. The approach highlights a graph-based indexing\nmethod for generating novel game designs, an LLM-driven system for consistent\ngame code generation validated by gameplay records, and a gameplay AI\nconstructing method that uses an ensemble of LLM-generated action-value\nfunctions optimized through self-play. These contributions aim to accelerate\ncard game prototyping, reduce human labor, and lower barriers to entry for game\ndevelopers.\n","authors":["Danrui Li","Sen Zhang","Sam S. Sohn","Kaidong Hu","Muhammad Usman","Mubbasir Kapadia"],"pdf_url":"https://arxiv.org/pdf/2502.07128v1.pdf","comment":"13 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2502.06710v1","updated":"2025-02-10T17:41:57Z","published":"2025-02-10T17:41:57Z","title":"Learning Musical Representations for Music Performance Question\n  Answering","summary":"  Music performances are representative scenarios for audio-visual modeling.\nUnlike common scenarios with sparse audio, music performances continuously\ninvolve dense audio signals throughout. While existing multimodal learning\nmethods on the audio-video QA demonstrate impressive capabilities in general\nscenarios, they are incapable of dealing with fundamental problems within the\nmusic performances: they underexplore the interaction between the multimodal\nsignals in performance and fail to consider the distinctive characteristics of\ninstruments and music. Therefore, existing methods tend to answer questions\nregarding musical performances inaccurately. To bridge the above research gaps,\n(i) given the intricate multimodal interconnectivity inherent to music data,\nour primary backbone is designed to incorporate multimodal interactions within\nthe context of music; (ii) to enable the model to learn music characteristics,\nwe annotate and release rhythmic and music sources in the current music\ndatasets; (iii) for time-aware audio-visual modeling, we align the model's\nmusic predictions with the temporal dimension. Our experiments show\nstate-of-the-art effects on the Music AVQA datasets. Our code is available at\nhttps://github.com/xid32/Amuse.\n","authors":["Xingjian Diao","Chunhui Zhang","Tingxuan Wu","Ming Cheng","Zhongyu Ouyang","Weiyi Wu","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2502.06710v1.pdf","comment":"Accepted at EMNLP 2024"},{"id":"http://arxiv.org/abs/2502.06616v1","updated":"2025-02-10T16:12:47Z","published":"2025-02-10T16:12:47Z","title":"From Code to Canvas","summary":"  The web-based dynamic geometry software CindyJS is a versatile tool to create\ninteractive applications for mathematics and other topics. In this workshop, we\nwill look at a code package that makes the creation of animations in CindyJS\neasier and more streamlined. Animations, which can then be embedded into\npresentations or be used in (lecture) videos. The focus lies on the creation of\nthe animations themselves and some of the technical and artistic fundamentals\nto do so.\n","authors":["Bernhard O. Werner"],"pdf_url":"https://arxiv.org/pdf/2502.06616v1.pdf","comment":"A workshop paper for the Bridges 2025 conference"},{"id":"http://arxiv.org/abs/2502.06490v1","updated":"2025-02-10T14:08:25Z","published":"2025-02-10T14:08:25Z","title":"Recent Advances in Discrete Speech Tokens: A Review","summary":"  The rapid advancement of speech generation technologies in the era of large\nlanguage models (LLMs) has established discrete speech tokens as a foundational\nparadigm for speech representation. These tokens, characterized by their\ndiscrete, compact, and concise nature, are not only advantageous for efficient\ntransmission and storage, but also inherently compatible with the language\nmodeling framework, enabling seamless integration of speech into text-dominated\nLLM architectures. Current research categorizes discrete speech tokens into two\nprincipal classes: acoustic tokens and semantic tokens, each of which has\nevolved into a rich research domain characterized by unique design philosophies\nand methodological approaches. This survey systematically synthesizes the\nexisting taxonomy and recent innovations in discrete speech tokenization,\nconducts a critical examination of the strengths and limitations of each\nparadigm, and presents systematic experimental comparisons across token types.\nFurthermore, we identify persistent challenges in the field and propose\npotential research directions, aiming to offer actionable insights to inspire\nfuture advancements in the development and application of discrete speech\ntokens.\n","authors":["Yiwei Guo","Zhihan Li","Hankun Wang","Bohan Li","Chongtian Shao","Hanglei Zhang","Chenpeng Du","Xie Chen","Shujie Liu","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2502.06490v1.pdf","comment":"26 pages, 8 figures, 3 tables. Work in progress"},{"id":"http://arxiv.org/abs/2408.14823v2","updated":"2025-02-10T11:59:52Z","published":"2024-08-27T07:06:49Z","title":"LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive\n  Streaming","summary":"  The rise of Extended Reality (XR) requires efficient streaming of 3D online\nworlds, challenging current 3DGS representations to adapt to\nbandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS\nthat supports adaptive streaming and progressive rendering. Our method\nconstructs a layered structure for cumulative representation, incorporates\ndynamic opacity optimization to maintain visual fidelity, and utilizes\noccupancy maps to efficiently manage Gaussian splats. This proposed model\noffers a progressive representation supporting a continuous rendering quality\nadapted for bandwidth-aware streaming. Extensive experiments validate the\neffectiveness of our approach in balancing visual fidelity with the compactness\nof the model, with up to 50.71% improvement in SSIM, 286.53% improvement in\nLPIPS with 23% of the original model size, and shows its potential for\nbandwidth-adapted 3D streaming and rendering applications.\n","authors":["Yuang Shi","G√©raldine Morin","Simone Gasparini","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2408.14823v2.pdf","comment":"3DV 2025; Project Page: https://yuang-ian.github.io/lapisgs/ ; Code:\n  https://github.com/nus-vv-streams/lapis-gs"},{"id":"http://arxiv.org/abs/2412.21009v2","updated":"2025-02-10T08:55:18Z","published":"2024-12-30T15:21:36Z","title":"Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline","summary":"  Recent advancements in deep learning have significantly enhanced\ncontent-based retrieval methods, notably through models like CLIP that map\nimages and texts into a shared embedding space. However, these methods often\nstruggle with domain-specific entities and long-tail concepts absent from their\ntraining data, particularly in identifying specific individuals. In this paper,\nwe explore the task of identity-aware cross-modal retrieval, which aims to\nretrieve images of persons in specific contexts based on natural language\nqueries. This task is critical in various scenarios, such as for searching and\nbrowsing personalized video collections or large audio-visual archives\nmaintained by national broadcasters. We introduce a novel dataset, COCO Person\nFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched\nwith deepfake-generated faces from VGGFace2. This dataset addresses the lack of\nlarge-scale datasets needed for training and evaluating models for this task.\nOur experiments assess the performance of different CLIP variations repurposed\nfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), which\nachieves competitive retrieval performance through targeted fine-tuning. Our\ncontributions lay the groundwork for more robust cross-modal retrieval systems\ncapable of recognizing long-tail identities and contextual nuances. Data and\ncode are available at https://github.com/mesnico/IdCLIP.\n","authors":["Nicola Messina","Lucia Vadicamo","Leo Maltese","Claudio Gennaro"],"pdf_url":"https://arxiv.org/pdf/2412.21009v2.pdf","comment":"Accepted as full paper at ECIR 2025"},{"id":"http://arxiv.org/abs/2406.02345v2","updated":"2025-02-10T06:05:46Z","published":"2024-06-04T14:21:41Z","title":"Progressive Confident Masking Attention Network for Audio-Visual\n  Segmentation","summary":"  Audio and visual signals typically occur simultaneously, and humans possess\nan innate ability to correlate and synchronize information from these two\nmodalities. Recently, a challenging problem known as Audio-Visual Segmentation\n(AVS) has emerged, intending to produce segmentation maps for sounding objects\nwithin a scene. However, the methods proposed so far have not sufficiently\nintegrated audio and visual information, and the computational costs have been\nextremely high. Additionally, the outputs of different stages have not been\nfully utilized. To facilitate this research, we introduce a novel Progressive\nConfident Masking Attention Network (PMCANet). It leverages attention\nmechanisms to uncover the intrinsic correlations between audio signals and\nvisual frames. Furthermore, we design an efficient and effective\ncross-attention module to enhance semantic perception by selecting query\ntokens. This selection is determined through confidence-driven units based on\nthe network's multi-stage predictive outputs. Experiments demonstrate that our\nnetwork outperforms other AVS methods while requiring less computational\nresources. The code is available at: https://github.com/PrettyPlate/PCMANet.\n","authors":["Yuxuan Wang","Jinchao Zhu","Feng Dong","Shuyue Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.02345v2.pdf","comment":"23 pages, 11 figures, submitted to Elsevier Knowledge-Based System"}]},"2025-02-09T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2502.06020v1","updated":"2025-02-09T20:26:30Z","published":"2025-02-09T20:26:30Z","title":"Temporal Working Memory: Query-Guided Segment Refinement for Enhanced\n  Multimodal Understanding","summary":"  Multimodal foundation models (MFMs) have demonstrated significant success in\ntasks such as visual captioning, question answering, and image-text retrieval.\nHowever, these models face inherent limitations due to their finite internal\ncapacity, which restricts their ability to process extended temporal sequences,\na crucial requirement for comprehensive video and audio analysis. To overcome\nthese challenges, we introduce a specialized cognitive module, temporal working\nmemory (TWM), which aims to enhance the temporal modeling capabilities of MFMs.\nIt selectively retains task-relevant information across temporal dimensions,\nensuring that critical details are preserved throughout the processing of video\nand audio content. The TWM uses a query-guided attention approach to focus on\nthe most informative multimodal segments within temporal sequences. By\nretaining only the most relevant content, TWM optimizes the use of the model's\nlimited capacity, enhancing its temporal modeling ability. This plug-and-play\nmodule can be easily integrated into existing MFMs. With our TWM, nine\nstate-of-the-art models exhibit significant performance improvements across\ntasks such as video captioning, question answering, and video-text retrieval.\nBy enhancing temporal modeling, TWM extends the capability of MFMs to handle\ncomplex, time-sensitive data effectively. Our code is available at\nhttps://github.com/xid32/NAACL_2025_TWM.\n","authors":["Xingjian Diao","Chunhui Zhang","Weiyi Wu","Zhongyu Ouyang","Peijun Qing","Ming Cheng","Soroush Vosoughi","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2502.06020v1.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2502.06012v1","updated":"2025-02-09T20:06:42Z","published":"2025-02-09T20:06:42Z","title":"Speaker Embedding Informed Audiovisual Active Speaker Detection for\n  Egocentric Recordings","summary":"  Audiovisual active speaker detection (ASD) addresses the task of determining\nthe speech activity of a candidate speaker given acoustic and visual data.\nTypically, systems model the temporal correspondence of audiovisual cues, such\nas the synchronisation between speech and lip movement. Recent work has\nexplored extending this paradigm by additionally leveraging speaker embeddings\nextracted from candidate speaker reference speech. This paper proposes the\nspeaker comparison auxiliary network (SCAN) which uses speaker-specific\ninformation from both reference speech and the candidate audio signal to\ndisambiguate challenging scenes when the visual signal is unresolvable.\nFurthermore, an improved method for enrolling face-speaker libraries is\ndeveloped, which implements a self-supervised approach to video-based face\nrecognition. Fitting with the recent proliferation of wearable devices, this\nwork focuses on improving speaker-embedding-informed ASD in the context of\negocentric recordings, which can be characterised by acoustic noise and highly\ndynamic scenes. SCAN is implemented with two well-established baselines, namely\nTalkNet and Light-ASD; yielding a relative improvement in mAP of 14.5% and\n10.3% on the Ego4D benchmark, respectively.\n","authors":["Jason Clarke","Yoshihiko Gotoh","Stefan Goetze"],"pdf_url":"https://arxiv.org/pdf/2502.06012v1.pdf","comment":"Accepted to ICASSP 2025. 5 pages, 4 figures. To appear in Proceedings\n  of IEEE International Conference on Acoustics, Speech and Signal Processing\n  (ICASSP), April 6-11, 2025, Hyderabad, India"},{"id":"http://arxiv.org/abs/2502.05922v1","updated":"2025-02-09T14:32:40Z","published":"2025-02-09T14:32:40Z","title":"A Large-scale Dataset with Behavior, Attributes, and Content of Mobile\n  Short-video Platform","summary":"  Short-video platforms show an increasing impact on people's daily lives\nnowadays, with billions of active users spending plenty of time each day. The\ninteractions between users and online platforms give rise to many scientific\nproblems across computational social science and artificial intelligence.\nHowever, despite the rapid development of short-video platforms, currently\nthere are serious shortcomings in existing relevant datasets on three aspects:\ninadequate user-video feedback, limited user attributes and lack of video\ncontent. To address these problems, we provide a large-scale dataset with rich\nuser behavior, attributes and video content from a real mobile short-video\nplatform. This dataset covers 10,000 voluntary users and 153,561 videos, and we\nconduct four-fold technical validations of the dataset. First, we verify the\nrichness of the behavior and attribute data. Second, we confirm the\nrepresenting ability of the content features. Third, we provide benchmarking\nresults on recommendation algorithms with our dataset. Finally, we explore the\nfilter bubble phenomenon on the platform using the dataset. We believe the\ndataset could support the broad research community, including but not limited\nto user modeling, social science, human behavior understanding, etc. The\ndataset and code is available at\nhttps://github.com/tsinghua-fib-lab/ShortVideo_dataset.\n","authors":["Yu Shang","Chen Gao","Nian Li","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2502.05922v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2502.06893v1","updated":"2025-02-09T12:37:48Z","published":"2025-02-09T12:37:48Z","title":"A New Hybrid Intelligent Approach for Multimodal Detection of Suspected\n  Disinformation on TikTok","summary":"  In the context of the rapid dissemination of multimedia content, identifying\ndisinformation on social media platforms such as TikTok represents a\nsignificant challenge. This study introduces a hybrid framework that combines\nthe computational power of deep learning with the interpretability of fuzzy\nlogic to detect suspected disinformation in TikTok videos. The methodology is\ncomprised of two core components: a multimodal feature analyser that extracts\nand evaluates data from text, audio, and video; and a multimodal disinformation\ndetector based on fuzzy logic. These systems operate in conjunction to evaluate\nthe suspicion of spreading disinformation, drawing on human behavioural cues\nsuch as body language, speech patterns, and text coherence. Two experiments\nwere conducted: one focusing on context-specific disinformation and the other\non the scalability of the model across broader topics. For each video\nevaluated, high-quality, comprehensive, well-structured reports are generated,\nproviding a detailed view of the disinformation behaviours.\n","authors":["Jared D. T. Guerrero-Sosa","Andres Montoro-Montarroso","Francisco P. Romero","Jesus Serrano-Guerrero","Jose A. Olivas"],"pdf_url":"https://arxiv.org/pdf/2502.06893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05863v1","updated":"2025-02-09T11:46:05Z","published":"2025-02-09T11:46:05Z","title":"Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education","summary":"  In AI-facilitated teaching, leveraging various query styles to interpret\nabstract text descriptions is crucial for ensuring high-quality teaching.\nHowever, current retrieval models primarily focus on natural text-image\nretrieval, making them insufficiently tailored to educational scenarios due to\nthe ambiguities in the retrieval process. In this paper, we propose a diverse\nexpression retrieval task tailored to educational scenarios, supporting\nretrieval based on multiple query styles and expressions. We introduce the STEM\nEducation Retrieval Dataset (SER), which contains over 24,000 query pairs of\ndifferent styles, and the Uni-Retrieval, an efficient and style-diversified\nretrieval vision-language model based on prompt tuning. Uni-Retrieval extracts\nquery style features as prototypes and builds a continuously updated Prompt\nBank containing prompt tokens for diverse queries. This bank can updated during\ntest time to represent domain-specific knowledge for different subject\nretrieval scenarios. Our framework demonstrates scalability and robustness by\ndynamically retrieving prompt tokens based on prototype similarity, effectively\nfacilitating learning for unknown queries. Experimental results indicate that\nUni-Retrieval outperforms existing retrieval models in most retrieval tasks.\nThis advancement provides a scalable and precise solution for diverse\neducational needs.\n","authors":["Yanhao Jia","Xinyi Wu","Hao Li","Qinglin Zhang","Yuxiao Hu","Shuai Zhao","Wenqi Fan"],"pdf_url":"https://arxiv.org/pdf/2502.05863v1.pdf","comment":null}]},"2025-02-08T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2502.05695v1","updated":"2025-02-08T21:14:28Z","published":"2025-02-08T21:14:28Z","title":"Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models\n  for Wireless Networks","summary":"  This paper proposes a novel framework for real-time adaptive-bitrate video\nstreaming by integrating latent diffusion models (LDMs) within the FFmpeg\ntechniques. This solution addresses the challenges of high bandwidth usage,\nstorage inefficiencies, and quality of experience (QoE) degradation associated\nwith traditional constant bitrate streaming (CBS) and adaptive bitrate\nstreaming (ABS). The proposed approach leverages LDMs to compress I-frames into\na latent space, offering significant storage and semantic transmission savings\nwithout sacrificing high visual quality. While it keeps B-frames and P-frames\nas adjustment metadata to ensure efficient video reconstruction at the user\nside, the proposed framework is complemented with the most state-of-the-art\ndenoising and video frame interpolation (VFI) techniques. These techniques\nmitigate semantic ambiguity and restore temporal coherence between frames, even\nin noisy wireless communication environments. Experimental results demonstrate\nthe proposed method achieves high-quality video streaming with optimized\nbandwidth usage, outperforming state-of-the-art solutions in terms of QoE and\nresource efficiency. This work opens new possibilities for scalable real-time\nvideo streaming in 5G and future post-5G networks.\n","authors":["Zijiang Yan","Jianhua Pei","Hongda Wu","Hina Tabassum","Ping Wang"],"pdf_url":"https://arxiv.org/pdf/2502.05695v1.pdf","comment":"Submission for possible publication"},{"id":"http://arxiv.org/abs/2502.03897v2","updated":"2025-02-08T09:37:13Z","published":"2025-02-06T09:18:30Z","title":"UniForm: A Unified Diffusion Transformer for Audio-Video Generation","summary":"  As a natural multimodal content, audible video delivers an immersive sensory\nexperience. Consequently, audio-video generation systems have substantial\npotential. However, existing diffusion-based studies mainly employ relatively\nindependent modules for generating each modality, which lack exploration of\nshared-weight generative modules. This approach may under-use the intrinsic\ncorrelations between audio and visual modalities, potentially resulting in\nsub-optimal generation quality. To address this, we propose UniForm, a unified\ndiffusion transformer designed to enhance cross-modal consistency. By\nconcatenating auditory and visual information, UniForm learns to generate audio\nand video simultaneously within a unified latent space, facilitating the\ncreation of high-quality and well-aligned audio-visual pairs. Extensive\nexperiments demonstrate the superior performance of our method in joint\naudio-video generation, audio-guided video generation, and video-guided audio\ngeneration tasks. Our demos are available at https://uniform-t2av.github.io/.\n","authors":["Lei Zhao","Linfeng Feng","Dongxu Ge","Fangqiu Yi","Chi Zhang","Xiao-Lei Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2502.03897v2.pdf","comment":"Our demos are available at https://uniform-t2av.github.io/"},{"id":"http://arxiv.org/abs/2403.08505v5","updated":"2025-02-08T09:33:17Z","published":"2024-03-13T13:12:57Z","title":"CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression","summary":"  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed. Code\nis available at https://github.com/Xinjie-Q/CAMSIC.\n","authors":["Xinjie Zhang","Shenyuan Gao","Zhening Liu","Jiawei Shao","Xingtong Ge","Dailan He","Tongda Xu","Yan Wang","Jun Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08505v5.pdf","comment":"Accepted by AAAI 2025"}]},"2025-02-07T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2502.03238v2","updated":"2025-02-07T18:37:47Z","published":"2025-02-05T14:57:23Z","title":"Long-tailed Medical Diagnosis with Relation-aware Representation\n  Learning and Iterative Classifier Calibration","summary":"  Recently computer-aided diagnosis has demonstrated promising performance,\neffectively alleviating the workload of clinicians. However, the inherent\nsample imbalance among different diseases leads algorithms biased to the\nmajority categories, leading to poor performance for rare categories. Existing\nworks formulated this challenge as a long-tailed problem and attempted to\ntackle it by decoupling the feature representation and classification. Yet, due\nto the imbalanced distribution and limited samples from tail classes, these\nworks are prone to biased representation learning and insufficient classifier\ncalibration. To tackle these problems, we propose a new Long-tailed Medical\nDiagnosis (LMD) framework for balanced medical image classification on\nlong-tailed datasets. In the initial stage, we develop a Relation-aware\nRepresentation Learning (RRL) scheme to boost the representation ability by\nencouraging the encoder to capture intrinsic semantic features through\ndifferent data augmentations. In the subsequent stage, we propose an Iterative\nClassifier Calibration (ICC) scheme to calibrate the classifier iteratively.\nThis is achieved by generating a large number of balanced virtual features and\nfine-tuning the encoder using an Expectation-Maximization manner. The proposed\nICC compensates for minority categories to facilitate unbiased classifier\noptimization while maintaining the diagnostic knowledge in majority classes.\nComprehensive experiments on three public long-tailed medical datasets\ndemonstrate that our LMD framework significantly surpasses state-of-the-art\napproaches. The source code can be accessed at\nhttps://github.com/peterlipan/LMD.\n","authors":["Li Pan","Yupei Zhang","Qiushi Yang","Tan Li","Zhen Chen"],"pdf_url":"https://arxiv.org/pdf/2502.03238v2.pdf","comment":"This work has been accepted in Computers in Biology and Medicine"},{"id":"http://arxiv.org/abs/2502.05130v1","updated":"2025-02-07T18:02:47Z","published":"2025-02-07T18:02:47Z","title":"Latent Swap Joint Diffusion for Long-Form Audio Generation","summary":"  Previous work on long-form audio generation using global-view diffusion or\niterative generation demands significant training or inference costs. While\nrecent advancements in multi-view joint diffusion for panoramic generation\nprovide an efficient option, they struggle with spectrum generation with severe\noverlap distortions and high cross-view consistency costs. We initially explore\nthis phenomenon through the connectivity inheritance of latent maps and uncover\nthat averaging operations excessively smooth the high-frequency components of\nthe latent map. To address these issues, we propose Swap Forward (SaFa), a\nframe-level latent swap framework that synchronizes multiple diffusions to\nproduce a globally coherent long audio with more spectrum details in a\nforward-only manner. At its core, the bidirectional Self-Loop Latent Swap is\napplied between adjacent views, leveraging stepwise diffusion trajectory to\nadaptively enhance high-frequency components without disrupting low-frequency\ncomponents. Furthermore, to ensure cross-view consistency, the unidirectional\nReference-Guided Latent Swap is applied between the reference and the\nnon-overlap regions of each subview during the early stages, providing\ncentralized trajectory guidance. Quantitative and qualitative experiments\ndemonstrate that SaFa significantly outperforms existing joint diffusion\nmethods and even training-based long audio generation models. Moreover, we find\nthat it also adapts well to panoramic generation, achieving comparable\nstate-of-the-art performance with greater efficiency and model\ngeneralizability. Project page is available at https://swapforward.github.io/.\n","authors":["Yusheng Dai","Chenxi Wang","Chang Li","Chen Wang","Jun Du","Kewei Li","Ruoyu Wang","Jiefeng Ma","Lei Sun","Jianqing Gao"],"pdf_url":"https://arxiv.org/pdf/2502.05130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04976v1","updated":"2025-02-07T14:50:10Z","published":"2025-02-07T14:50:10Z","title":"Towards Multimodal Empathetic Response Generation: A Rich\n  Text-Speech-Vision Avatar-based Benchmark","summary":"  Empathetic Response Generation (ERG) is one of the key tasks of the affective\ncomputing area, which aims to produce emotionally nuanced and compassionate\nresponses to user's queries. However, existing ERG research is predominantly\nconfined to the singleton text modality, limiting its effectiveness since human\nemotions are inherently conveyed through multiple modalities. To combat this,\nwe introduce an avatar-based Multimodal ERG (MERG) task, entailing rich text,\nspeech, and facial vision information. We first present a large-scale\nhigh-quality benchmark dataset, \\textbf{AvaMERG}, which extends traditional\ntext ERG by incorporating authentic human speech audio and dynamic talking-face\navatar videos, encompassing a diverse range of avatar profiles and broadly\ncovering various topics of real-world scenarios. Further, we deliberately\ntailor a system, named \\textbf{Empatheia}, for MERG. Built upon a Multimodal\nLarge Language Model (MLLM) with multimodal encoder, speech and avatar\ngenerators, Empatheia performs end-to-end MERG, with Chain-of-Empathetic\nreasoning mechanism integrated for enhanced empathy understanding and\nreasoning. Finally, we devise a list of empathetic-enhanced tuning strategies,\nstrengthening the capabilities of emotional accuracy and content,\navatar-profile consistency across modalities. Experimental results on AvaMERG\ndata demonstrate that Empatheia consistently shows superior performance than\nbaseline methods on both textual ERG and MERG. Overall, this work is expected\nto pioneer the MERG research by introducing a novel benchmark and an end-to-end\nmodel, laying a solid foundation for future advancements in multimodal\nempathetic response generation.\n","authors":["Han Zhang","Zixiang Meng","Meng Luo","Hong Han","Lizi Liao","Erik Cambria","Hao Fei"],"pdf_url":"https://arxiv.org/pdf/2502.04976v1.pdf","comment":"Accepted by TheWebConf (WWW) 2025"},{"id":"http://arxiv.org/abs/2403.05192v3","updated":"2025-02-07T14:34:28Z","published":"2024-03-08T10:14:32Z","title":"An End-to-End Pipeline Perspective on Video Streaming in Best-Effort\n  Networks: A Survey and Tutorial","summary":"  Remaining a dominant force in Internet traffic, video streaming captivates\nend users, service providers, and researchers. This paper takes a pragmatic\napproach to reviewing recent advances in the field by focusing on the prevalent\nstreaming paradigm that involves delivering long-form two-dimensional videos\nover the best-effort Internet with client-side adaptive bitrate (ABR)\nalgorithms and assistance from content delivery networks (CDNs). To enhance\naccessibility, we supplement the survey with tutorial material. Unlike existing\nsurveys that offer fragmented views, our work provides a holistic perspective\non the entire end-to-end streaming pipeline, from video capture by a\ncamera-equipped device to playback by the end user. Our novel perspective\ncovers the ingestion, processing, and distribution stages of the pipeline and\naddresses key challenges such as video compression, upload, transcoding, ABR\nalgorithms, CDN support, and quality of experience. We review over 200 papers\nand classify streaming designs by their problem-solving methodology, whether\nbased on intuition (simple heuristics), theory (formal optimization), or\nmachine learning (generalizable data patterns). The survey further refines\nthese methodology-based categories and characterizes each design by additional\ntraits such as compatible codecs and use of super resolution. We connect the\nreviewed research to real-world applications by discussing the practices of\ncommercial streaming platforms. Finally, the survey highlights prominent\ncurrent trends and outlines future directions in video streaming.\n","authors":["Leonardo Peroni","Sergey Gorinsky"],"pdf_url":"https://arxiv.org/pdf/2403.05192v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04691v1","updated":"2025-02-07T06:35:50Z","published":"2025-02-07T06:35:50Z","title":"PDStream: Slashing Long-Tail Delay in Interactive Video Streaming via\n  Pseudo-Dual Streaming","summary":"  End-to-end (E2E) delay is critical for interactive video streaming (IVS)\nexperiences, but remains unsatisfactory for its long-tail distribution caused\nby periodic large keyframes. Conventional optimization strategies, such as\njitter buffer, bitrate adaptation, and customized encoding, either sacrifice\nclarity, average delay, or compatibility. To address this issue, we propose\nPDStream, a novel pseudo-dual streaming algorithm, aimed at minimizing E2E\ndelay while maintaining video clarity. The core idea is to split the two\nfunctions, delay-sensitive playback and delay-tolerant reference, on keyframes\nthrough dual streaming. Specifically, the playback function is held by a second\nparallel stream, which comprises much smaller non-keyframes and is allocated\nmore immediate bandwidth for real-time performance. The reference function is\nensured by the first stream with keyframe preservation, allocated more\nsubsequent bandwidth to smooth out bursty traffic. Additionally, ``pseudo''\nminimizes computational and transmission overheads by restricting dual streams\nto brief activation only when keyframes appear, supported by corresponding\ndual-stream bitrate allocation and adaptation to ensure delay and clarity. We\nimplement PDStream on a WebRTC-based IVS testbed with real-world network\ntraces. Results show that PDStream significantly outperforms prior algorithms,\nreducing average E2E delay by 17.5\\% and slashing its 97th percentile by\n33.3\\%, while keeping clarity under varying bandwidth.\n","authors":["Xuedou Xiao","Yingying Zuo","Mingxuan Yan","Kezhong Liu","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.04691v1.pdf","comment":"IEEE INFOCOM 2025"}]},"2025-02-06T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2411.18855v2","updated":"2025-02-06T22:25:39Z","published":"2024-11-28T01:51:46Z","title":"Improving Accuracy and Generalization for Efficient Visual Tracking","summary":"  Efficient visual trackers overfit to their training distributions and lack\ngeneralization abilities, resulting in them performing well on their respective\nin-distribution (ID) test sets and not as well on out-of-distribution (OOD)\nsequences, imposing limitations to their deployment in-the-wild under\nconstrained resources. We introduce SiamABC, a highly efficient Siamese tracker\nthat significantly improves tracking performance, even on OOD sequences.\nSiamABC takes advantage of new architectural designs in the way it bridges the\ndynamic variability of the target, and of new losses for training. Also, it\ndirectly addresses OOD tracking generalization by including a fast\nbackward-free dynamic test-time adaptation method that continuously adapts the\nmodel according to the dynamic visual changes of the target. Our extensive\nexperiments suggest that SiamABC shows remarkable performance gains in OOD sets\nwhile maintaining accurate performance on the ID benchmarks. SiamABC\noutperforms MixFormerV2-S by 7.6\\% on the OOD AVisT benchmark while being 3x\nfaster (100 FPS) on a CPU. Our code and models are available at\nhttps://wvuvl.github.io/SiamABC/.\n","authors":["Ram Zaveri","Shivang Patel","Yu Gu","Gianfranco Doretto"],"pdf_url":"https://arxiv.org/pdf/2411.18855v2.pdf","comment":"WACV 2025"},{"id":"http://arxiv.org/abs/2502.04128v1","updated":"2025-02-06T15:04:00Z","published":"2025-02-06T15:04:00Z","title":"Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based\n  Speech Synthesis","summary":"  Recent advances in text-based large language models (LLMs), particularly in\nthe GPT series and the o1 model, have demonstrated the effectiveness of scaling\nboth training-time and inference-time compute. However, current\nstate-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring\nseparate models (e.g., diffusion models after LLM), complicating the decision\nof whether to scale a particular model during training or testing. This work\nmakes the following contributions: First, we explore the scaling of train-time\nand inference-time compute for speech synthesis. Second, we propose a simple\nframework Llasa for speech synthesis that employs a single-layer vector\nquantizer (VQ) codec and a single Transformer architecture to fully align with\nstandard LLMs such as Llama. Our experiments reveal that scaling train-time\ncompute for Llasa consistently improves the naturalness of synthesized speech\nand enables the generation of more complex and accurate prosody patterns.\nFurthermore, from the perspective of scaling inference-time compute, we employ\nspeech understanding models as verifiers during the search, finding that\nscaling inference-time compute shifts the sampling modes toward the preferences\nof specific verifiers, thereby improving emotional expressiveness, timbre\nconsistency, and content accuracy. In addition, we released the checkpoint and\ntraining code for our TTS model (1B, 3B, 8B) and codec model publicly\navailable.\n","authors":["Zhen Ye","Xinfa Zhu","Chi-Min Chan","Xinsheng Wang","Xu Tan","Jiahe Lei","Yi Peng","Haohe Liu","Yizhu Jin","Zheqi DAI","Hongzhan Lin","Jianyi Chen","Xingjian Du","Liumeng Xue","Yunlin Chen","Zhifei Li","Lei Xie","Qiuqiang Kong","Yike Guo","Wei Xue"],"pdf_url":"https://arxiv.org/pdf/2502.04128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04078v1","updated":"2025-02-06T13:42:07Z","published":"2025-02-06T13:42:07Z","title":"CDIO: Cross-Domain Inference Optimization with Resource Preference\n  Prediction for Edge-Cloud Collaboration","summary":"  Currently, massive video tasks are processed by edge-cloud collaboration.\nHowever, the diversity of task requirements and the dynamics of resources pose\ngreat challenges to efficient inference, resulting in many wasted resources. In\nthis paper, we present CDIO, a cross-domain inference optimization framework\ndesigned for edge-cloud collaboration. For diverse input tasks, CDIO can\npredict resource preference types by analyzing spatial complexity and\nprocessing requirements of the task. Subsequently, a cross-domain collaborative\noptimization algorithm is employed to guide resource allocation in the\nedge-cloud system. By ensuring that each task is matched with the ideal\nservers, the edge-cloud system can achieve higher efficiency inference. The\nevaluation results on public datasets demonstrate that CDIO can effectively\nmeet the accuracy and delay requirements for task processing. Compared to\nstate-of-the-art edge-cloud solutions, CDIO achieves a computing and bandwidth\nconsumption reduction of 20%-40%. And it can reduce energy consumption by more\nthan 40%.\n","authors":["Zheming Yang","Wen Ji","Qi Guo","Dieli Hu","Chang Zhao","Xiaowei Li","Xuanlei Zhao","Yi Zhao","Chaoyu Gong","Yang You"],"pdf_url":"https://arxiv.org/pdf/2502.04078v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2502.04400v1","updated":"2025-02-06T07:28:05Z","published":"2025-02-06T07:28:05Z","title":"Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed\n  Modalities and Heterogeneous Tasks","summary":"  Multimodal Federated Learning (MFL) enables multiple clients to\ncollaboratively train models on multimodal data while ensuring clients'\nprivacy. However, modality and task heterogeneity hinder clients from learning\na unified representation, weakening local model generalization, especially in\nMFL with mixed modalities where only some clients have multimodal data. In this\nwork, we propose an Adaptive prototype-based Multimodal Federated Learning\n(AproMFL) framework for mixed modalities and heterogeneous tasks to address the\naforementioned issues. Our AproMFL transfers knowledge through\nadaptively-constructed prototypes without a prior public dataset. Clients\nadaptively select prototype construction methods in line with tasks; server\nconverts client prototypes into unified multimodal prototypes and aggregates\nthem to form global prototypes, avoid clients keeping unified labels. We divide\nthe model into various modules and only aggregate mapping modules to reduce\ncommunication and computation overhead. To address aggregation issues in\nheterogeneity, we develop a client relationship graph-based scheme to\ndynamically adjust aggregation weights. Extensive experiments on representative\ndatasets evidence effectiveness of AproMFL.\n","authors":["Keke Gai","Mohan Wang","Jing Yu","Dongjue Wang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2502.04400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03724v1","updated":"2025-02-06T02:26:47Z","published":"2025-02-06T02:26:47Z","title":"MD-BERT: Action Recognition in Dark Videos via Dynamic Multi-Stream\n  Fusion and Temporal Modeling","summary":"  Action recognition in dark, low-light (under-exposed) or noisy videos is a\nchallenging task due to visibility degradation, which can hinder critical\nspatiotemporal details. This paper proposes MD-BERT, a novel multi-stream\napproach that integrates complementary pre-processing techniques such as gamma\ncorrection and histogram equalization alongside raw dark frames to address\nthese challenges. We introduce the Dynamic Feature Fusion (DFF) module,\nextending existing attentional fusion methods to a three-stream setting,\nthereby capturing fine-grained and global contextual information across\ndifferent brightness and contrast enhancements. The fused spatiotemporal\nfeatures are then processed by a BERT-based temporal model, which leverages its\nbidirectional self-attention to effectively capture long-range dependencies and\ncontextual relationships across frames. Extensive experiments on the ARID V1.0\nand ARID V1.5 dark video datasets show that MD-BERT outperforms existing\nmethods, establishing a new state-of-the-art performance. Ablation studies\nfurther highlight the individual contributions of each input stream and the\neffectiveness of the proposed DFF and BERT modules. The official website of\nthis work is available at: https://github.com/HrishavBakulBarua/DarkBERT\n","authors":["Sharana Dharshikgan Suresh Dass","Hrishav Bakul Barua","Ganesh Krishnasamy","Raveendran Paramesran","Raphael C. -W. Phan"],"pdf_url":"https://arxiv.org/pdf/2502.03724v1.pdf","comment":null}]},"2025-02-05T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2502.03465v1","updated":"2025-02-05T18:59:52Z","published":"2025-02-05T18:59:52Z","title":"Seeing World Dynamics in a Nutshell","summary":"  We consider the problem of efficiently representing casually captured\nmonocular videos in a spatially- and temporally-coherent manner. While existing\napproaches predominantly rely on 2D/2.5D techniques treating videos as\ncollections of spatiotemporal pixels, they struggle with complex motions,\nocclusions, and geometric consistency due to absence of temporal coherence and\nexplicit 3D structure. Drawing inspiration from monocular video as a projection\nof the dynamic 3D world, we explore representing videos in their intrinsic 3D\nform through continuous flows of Gaussian primitives in space-time. In this\npaper, we propose NutWorld, a novel framework that efficiently transforms\nmonocular videos into dynamic 3D Gaussian representations in a single forward\npass. At its core, NutWorld introduces a structured spatial-temporal aligned\nGaussian (STAG) representation, enabling optimization-free scene modeling with\neffective depth and flow regularization. Through comprehensive experiments, we\ndemonstrate that NutWorld achieves high-fidelity video reconstruction quality\nwhile enabling various downstream applications in real-time. Demos and code\nwill be available at https://github.com/Nut-World/NutWorld.\n","authors":["Qiuhong Shen","Xuanyu Yi","Mingbao Lin","Hanwang Zhang","Shuicheng Yan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2502.03465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10121v2","updated":"2025-02-05T16:54:42Z","published":"2024-05-16T14:21:33Z","title":"Distilling Implicit Multimodal Knowledge into Large Language Models for\n  Zero-Resource Dialogue Generation","summary":"  Integrating multimodal knowledge into large language models (LLMs) represents\na significant advancement in dialogue generation capabilities. However, the\neffective incorporation of such knowledge in zero-resource scenarios remains a\nsubstantial challenge due to the scarcity of diverse, high-quality dialogue\ndatasets. To address this, we propose the Visual Implicit Knowledge\nDistillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs\nfor enriched dialogue generation in zero-resource contexts by leveraging\nimplicit multimodal knowledge. VIKDF comprises two main stages: knowledge\ndistillation, using an Implicit Query Transformer to extract and encode visual\nimplicit knowledge from image-text pairs into knowledge vectors; and knowledge\nintegration, employing a novel Bidirectional Variational Information Fusion\ntechnique to seamlessly integrate these distilled vectors into LLMs. This\nenables the LLMs to generate dialogues that are not only coherent and engaging\nbut also exhibit a deep understanding of the context through implicit\nmultimodal cues, effectively overcoming the limitations of zero-resource\nscenarios. Our extensive experimentation across two dialogue datasets shows\nthat VIKDF outperforms existing state-of-the-art models in generating\nhigh-quality dialogues. The code is available at\nhttps://github.com/zhangbo-nlp/VIKDF.\n","authors":["Bo Zhang","Hui Ma","Jian Ding","Jian Wang","Bo Xu","Hongfei Lin"],"pdf_url":"https://arxiv.org/pdf/2405.10121v2.pdf","comment":"Accepted by Information Fusion. The code is available at\n  https://github.com/zhangbo-nlp/VIKDF"},{"id":"http://arxiv.org/abs/2502.03230v1","updated":"2025-02-05T14:45:09Z","published":"2025-02-05T14:45:09Z","title":"Efficient Vision Language Model Fine-tuning for Text-based Person\n  Anomaly Search","summary":"  This paper presents the HFUT-LMC team's solution to the WWW 2025 challenge on\nText-based Person Anomaly Search (TPAS). The primary objective of this\nchallenge is to accurately identify pedestrians exhibiting either normal or\nabnormal behavior within a large library of pedestrian images. Unlike\ntraditional video analysis tasks, TPAS significantly emphasizes understanding\nand interpreting the subtle relationships between text descriptions and visual\ndata. The complexity of this task lies in the model's need to not only match\nindividuals to text descriptions in massive image datasets but also accurately\ndifferentiate between search results when faced with similar descriptions. To\novercome these challenges, we introduce the Similarity Coverage Analysis (SCA)\nstrategy to address the recognition difficulty caused by similar text\ndescriptions. This strategy effectively enhances the model's capacity to manage\nsubtle differences, thus improving both the accuracy and reliability of the\nsearch. Our proposed solution demonstrated excellent performance in this\nchallenge.\n","authors":["Jiayi He","Shengeng Tang","Ao Liu","Lechao Cheng","Jingjing Wu","Yanyan Wei"],"pdf_url":"https://arxiv.org/pdf/2502.03230v1.pdf","comment":"Accepted by 2025 WWW Workshop on MORE"},{"id":"http://arxiv.org/abs/2403.10020v3","updated":"2025-02-05T08:11:40Z","published":"2024-03-15T05:06:21Z","title":"Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs","summary":"  The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread usage of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks, such as paraphrasing or translation. In this paper, we introduce\nwatermark collision as a novel and general philosophy for watermark attacks,\naimed at enhancing attack performance on top of any other attacking methods. We\nalso provide a comprehensive demonstration that watermark collision poses a\nthreat to all logit-based watermark algorithms, impacting not only specific\nattack scenarios but also downstream applications.\n","authors":["Yiyang Luo","Ke Lin","Chao Gu","Jiahui Hou","Lijie Wen","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2403.10020v3.pdf","comment":"Long Paper, 9 pages, accepted at NAACL 2025 Findings"}]},"2025-02-04T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2405.12221v3","updated":"2025-02-04T20:00:25Z","published":"2024-05-20T17:59:59Z","title":"Images that Sound: Composing Images and Sounds on a Single Canvas","summary":"  Spectrograms are 2D representations of sound that look very different from\nthe images found in our visual world. And natural images, when played as\nspectrograms, make unnatural sounds. In this paper, we show that it is possible\nto synthesize spectrograms that simultaneously look like natural images and\nsound like natural audio. We call these visual spectrograms images that sound.\nOur approach is simple and zero-shot, and it leverages pre-trained\ntext-to-image and text-to-spectrogram diffusion models that operate in a shared\nlatent space. During the reverse process, we denoise noisy latents with both\nthe audio and image diffusion models in parallel, resulting in a sample that is\nlikely under both models. Through quantitative evaluations and perceptual\nstudies, we find that our method successfully generates spectrograms that align\nwith a desired audio prompt while also taking the visual appearance of a\ndesired image prompt. Please see our project page for video results:\nhttps://ificl.github.io/images-that-sound/\n","authors":["Ziyang Chen","Daniel Geng","Andrew Owens"],"pdf_url":"https://arxiv.org/pdf/2405.12221v3.pdf","comment":"Accepted to NeurIPS 2024. Project site:\n  https://ificl.github.io/images-that-sound/"},{"id":"http://arxiv.org/abs/2501.17011v2","updated":"2025-02-04T16:14:38Z","published":"2025-01-28T15:17:36Z","title":"MIDI-GPT: A Controllable Generative Model for Computer-Assisted\n  Multitrack Music Composition","summary":"  We present and release MIDI-GPT, a generative system based on the Transformer\narchitecture that is designed for computer-assisted music composition\nworkflows. MIDI-GPT supports the infilling of musical material at the track and\nbar level, and can condition generation on attributes including: instrument\ntype, musical style, note density, polyphony level, and note duration. In order\nto integrate these features, we employ an alternative representation for\nmusical material, creating a time-ordered sequence of musical events for each\ntrack and concatenating several tracks into a single sequence, rather than\nusing a single time-ordered sequence where the musical events corresponding to\ndifferent tracks are interleaved. We also propose a variation of our\nrepresentation allowing for expressiveness. We present experimental results\nthat demonstrate that MIDI-GPT is able to consistently avoid duplicating the\nmusical material it was trained on, generate music that is stylistically\nsimilar to the training dataset, and that attribute controls allow enforcing\nvarious constraints on the generated material. We also outline several\nreal-world applications of MIDI-GPT, including collaborations with industry\npartners that explore the integration and evaluation of MIDI-GPT into\ncommercial products, as well as several artistic works produced using it.\n","authors":["Philippe Pasquier","Jeff Ens","Nathan Fradet","Paul Triana","Davide Rizzotti","Jean-Baptiste Rolland","Maryam Safi"],"pdf_url":"https://arxiv.org/pdf/2501.17011v2.pdf","comment":"AAAI 25"},{"id":"http://arxiv.org/abs/2502.02441v1","updated":"2025-02-04T16:08:48Z","published":"2025-02-04T16:08:48Z","title":"LLMER: Crafting Interactive Extended Reality Worlds with JSON Data\n  Generated by Large Language Models","summary":"  The integration of Large Language Models (LLMs) like GPT-4 with Extended\nReality (XR) technologies offers the potential to build truly immersive XR\nenvironments that interact with human users through natural language, e.g.,\ngenerating and animating 3D scenes from audio inputs. However, the complexity\nof XR environments makes it difficult to accurately extract relevant contextual\ndata and scene/object parameters from an overwhelming volume of XR artifacts.\nIt leads to not only increased costs with pay-per-use models, but also elevated\nlevels of generation errors. Moreover, existing approaches focusing on coding\nscript generation are often prone to generation errors, resulting in flawed or\ninvalid scripts, application crashes, and ultimately a degraded user\nexperience. To overcome these challenges, we introduce LLMER, a novel framework\nthat creates interactive XR worlds using JSON data generated by LLMs. Unlike\nprior approaches focusing on coding script generation, LLMER translates natural\nlanguage inputs into JSON data, significantly reducing the likelihood of\napplication crashes and processing latency. It employs a multi-stage strategy\nto supply only the essential contextual information adapted to the user's\nrequest and features multiple modules designed for various XR tasks. Our\npreliminary user study reveals the effectiveness of the proposed system, with\nover 80% reduction in consumed tokens and around 60% reduction in task\ncompletion time compared to state-of-the-art approaches. The analysis of users'\nfeedback also illuminates a series of directions for further optimization.\n","authors":["Jiangong Chen","Xiaoyi Wu","Tian Lan","Bin Li"],"pdf_url":"https://arxiv.org/pdf/2502.02441v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02225v1","updated":"2025-02-04T11:04:36Z","published":"2025-02-04T11:04:36Z","title":"Exploring the latent space of diffusion models directly through singular\n  value decomposition","summary":"  Despite the groundbreaking success of diffusion models in generating\nhigh-fidelity images, their latent space remains relatively under-explored,\neven though it holds significant promise for enabling versatile and\ninterpretable image editing capabilities. The complicated denoising trajectory\nand high dimensionality of the latent space make it extremely challenging to\ninterpret. Existing methods mainly explore the feature space of U-Net in\nDiffusion Models (DMs) instead of the latent space itself. In contrast, we\ndirectly investigate the latent space via Singular Value Decomposition (SVD)\nand discover three useful properties that can be used to control generation\nresults without the requirements of data collection and maintain identity\nfidelity generated images. Based on these properties, we propose a novel image\nediting framework that is capable of learning arbitrary attributes from one\npair of latent codes destined by text prompts in Stable Diffusion Models. To\nvalidate our approach, extensive experiments are conducted to demonstrate its\neffectiveness and flexibility in image editing. We will release our codes soon\nto foster further research and applications in this area.\n","authors":["Li Wang","Boyan Gao","Yanran Li","Zhao Wang","Xiaosong Yang","David A. Clifton","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.02225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02172v1","updated":"2025-02-04T09:45:52Z","published":"2025-02-04T09:45:52Z","title":"EditIQ: Automated Cinematic Editing of Static Wide-Angle Videos via\n  Dialogue Interpretation and Saliency Cues","summary":"  We present EditIQ, a completely automated framework for cinematically editing\nscenes captured via a stationary, large field-of-view and high-resolution\ncamera. From the static camera feed, EditIQ initially generates multiple\nvirtual feeds, emulating a team of cameramen. These virtual camera shots termed\nrushes are subsequently assembled using an automated editing algorithm, whose\nobjective is to present the viewer with the most vivid scene content. To\nunderstand key scene elements and guide the editing process, we employ a\ntwo-pronged approach: (1) a large language model (LLM)-based dialogue\nunderstanding module to analyze conversational flow, coupled with (2) visual\nsaliency prediction to identify meaningful scene elements and camera shots\ntherefrom. We then formulate cinematic video editing as an energy minimization\nproblem over shot selection, where cinematic constraints determine shot\nchoices, transitions, and continuity. EditIQ synthesizes an aesthetically and\nvisually compelling representation of the original narrative while maintaining\ncinematic coherence and a smooth viewing experience. Efficacy of EditIQ against\ncompeting baselines is demonstrated via a psychophysical study involving twenty\nparticipants on the BBC Old School dataset plus eleven theatre performance\nvideos. Video samples from EditIQ can be found at\nhttps://editiq-ave.github.io/.\n","authors":["Rohit Girmaji","Bhav Beri","Ramanathan Subramanian","Vineet Gandhi"],"pdf_url":"https://arxiv.org/pdf/2502.02172v1.pdf","comment":"Accepted at 30th International Conference on Intelligent User\n  Interfaces (IUI 25)"},{"id":"http://arxiv.org/abs/2405.00574v2","updated":"2025-02-04T07:57:52Z","published":"2024-05-01T15:25:54Z","title":"EALD-MLLM: Emotion Analysis in Long-sequential and De-identity videos\n  with Multi-modal Large Language Model","summary":"  Emotion AI is the ability of computers to understand human emotional states.\nExisting works have achieved promising progress, but two limitations remain to\nbe solved: 1) Previous studies have been more focused on short sequential video\nemotion analysis while overlooking long sequential video. However, the emotions\nin short sequential videos only reflect instantaneous emotions, which may be\ndeliberately guided or hidden. In contrast, long sequential videos can reveal\nauthentic emotions; 2) Previous studies commonly utilize various signals such\nas facial, speech, and even sensitive biological signals (e.g.,\nelectrocardiogram). However, due to the increasing demand for privacy,\ndeveloping Emotion AI without relying on sensitive signals is becoming\nimportant. To address the aforementioned limitations, in this paper, we\nconstruct a dataset for Emotion Analysis in Long-sequential and De-identity\nvideos called EALD by collecting and processing the sequences of athletes'\npost-match interviews. In addition to providing annotations of the overall\nemotional state of each video, we also provide the Non-Facial Body Language\n(NFBL) annotations for each player. NFBL is an inner-driven emotional\nexpression and can serve as an identity-free clue to understanding the\nemotional state. Moreover, we provide a simple but effective baseline for\nfurther research. More precisely, we evaluate the Multimodal Large Language\nModels (MLLMs) with de-identification signals (e.g., visual, speech, and NFBLs)\nto perform emotion analysis. Our experimental results demonstrate that: 1)\nMLLMs can achieve comparable, even better performance than the supervised\nsingle-modal models, even in a zero-shot scenario; 2) NFBL is an important cue\nin long sequential emotion analysis. EALD will be available on the open-source\nplatform.\n","authors":["Deng Li","Xin Liu","Bohao Xing","Baiqiang Xia","Yuan Zong","Bihan Wen","Heikki K√§lvi√§inen"],"pdf_url":"https://arxiv.org/pdf/2405.00574v2.pdf","comment":null}]},"2025-02-03T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2502.01699v1","updated":"2025-02-03T07:58:22Z","published":"2025-02-03T07:58:22Z","title":"Multimodal Inverse Attention Network with Intrinsic Discriminant Feature\n  Exploitation for Fake News Detection","summary":"  Multimodal fake news detection has garnered significant attention due to its\nprofound implications for social security. While existing approaches have\ncontributed to understanding cross-modal consistency, they often fail to\nleverage modal-specific representations and explicit discrepant features. To\naddress these limitations, we propose a Multimodal Inverse Attention Network\n(MIAN), a novel framework that explores intrinsic discriminative features based\non news content to advance fake news detection. Specifically, MIAN introduces a\nhierarchical learning module that captures diverse intra-modal relationships\nthrough local-to-global and local-to-local interactions, thereby generating\nenhanced unimodal representations to improve the identification of fake news at\nthe intra-modal level. Additionally, a cross-modal interaction module employs a\nco-attention mechanism to establish and model dependencies between the refined\nunimodal representations, facilitating seamless semantic integration across\nmodalities. To explicitly extract inconsistency features, we propose an inverse\nattention mechanism that effectively highlights the conflicting patterns and\nsemantic deviations introduced by fake news in both intra- and inter-modality.\nExtensive experiments on benchmark datasets demonstrate that MIAN significantly\noutperforms state-of-the-art methods, underscoring its pivotal contribution to\nadvancing social security through enhanced multimodal fake news detection.\n","authors":["Tianlin Zhang","En Yu","Yi Shao","Shuai Li","Sujuan Hou","Jiande Sun"],"pdf_url":"https://arxiv.org/pdf/2502.01699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01080v1","updated":"2025-02-03T05:41:41Z","published":"2025-02-03T05:41:41Z","title":"BC-GAN: A Generative Adversarial Network for Synthesizing a Batch of\n  Collocated Clothing","summary":"  Collocated clothing synthesis using generative networks has become an\nemerging topic in the field of fashion intelligence, as it has significant\npotential economic value to increase revenue in the fashion industry. In\nprevious studies, several works have attempted to synthesize\nvisually-collocated clothing based on a given clothing item using generative\nadversarial networks (GANs) with promising results. These works, however, can\nonly accomplish the synthesis of one collocated clothing item each time.\nNevertheless, users may require different clothing items to meet their multiple\nchoices due to their personal tastes and different dressing scenarios. To\naddress this limitation, we introduce a novel batch clothing generation\nframework, named BC-GAN, which is able to synthesize multiple\nvisually-collocated clothing images simultaneously. In particular, to further\nimprove the fashion compatibility of synthetic results, BC-GAN proposes a new\nfashion compatibility discriminator in a contrastive learning perspective by\nfully exploiting the collocation relationship among all clothing items. Our\nmodel was examined in a large-scale dataset with compatible outfits constructed\nby ourselves. Extensive experiment results confirmed the effectiveness of our\nproposed BC-GAN in comparison to state-of-the-art methods in terms of\ndiversity, visual authenticity, and fashion compatibility.\n","authors":["Dongliang Zhou","Haijun Zhang","Jianghong Ma","Jianyang Shi"],"pdf_url":"https://arxiv.org/pdf/2502.01080v1.pdf","comment":"This paper was accepted by IEEE TCSVT"},{"id":"http://arxiv.org/abs/2502.00992v1","updated":"2025-02-03T02:18:09Z","published":"2025-02-03T02:18:09Z","title":"FCBoost-Net: A Generative Network for Synthesizing Multiple Collocated\n  Outfits via Fashion Compatibility Boosting","summary":"  Outfit generation is a challenging task in the field of fashion technology,\nin which the aim is to create a collocated set of fashion items that complement\na given set of items. Previous studies in this area have been limited to\ngenerating a unique set of fashion items based on a given set of items, without\nproviding additional options to users. This lack of a diverse range of choices\nnecessitates the development of a more versatile framework. However, when the\ntask of generating collocated and diversified outfits is approached with\nmultimodal image-to-image translation methods, it poses a challenging problem\nin terms of non-aligned image translation, which is hard to address with\nexisting methods. In this research, we present FCBoost-Net, a new framework for\noutfit generation that leverages the power of pre-trained generative models to\nproduce multiple collocated and diversified outfits. Initially, FCBoost-Net\nrandomly synthesizes multiple sets of fashion items, and the compatibility of\nthe synthesized sets is then improved in several rounds using a novel fashion\ncompatibility booster. This approach was inspired by boosting algorithms and\nallows the performance to be gradually improved in multiple steps. Empirical\nevidence indicates that the proposed strategy can improve the fashion\ncompatibility of randomly synthesized fashion items as well as maintain their\ndiversity. Extensive experiments confirm the effectiveness of our proposed\nframework with respect to visual authenticity, diversity, and fashion\ncompatibility.\n","authors":["Dongliang Zhou","Haijun Zhang","Jianghong Ma","Jicong Fan","Zhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.00992v1.pdf","comment":"This paper has been accepted for presentation at ACM Multimedia 2023"},{"id":"http://arxiv.org/abs/2502.02610v1","updated":"2025-02-03T01:25:47Z","published":"2025-02-03T01:25:47Z","title":"Secure & Personalized Music-to-Video Generation via CHARCHA","summary":"  Music is a deeply personal experience and our aim is to enhance this with a\nfully-automated pipeline for personalized music video generation. Our work\nallows listeners to not just be consumers but co-creators in the music video\ngeneration process by creating personalized, consistent and context-driven\nvisuals based on lyrics, rhythm and emotion in the music. The pipeline combines\nmultimodal translation and generation techniques and utilizes low-rank\nadaptation on listeners' images to create immersive music videos that reflect\nboth the music and the individual. To ensure the ethical use of users'\nidentity, we also introduce CHARCHA (patent pending), a facial identity\nverification protocol that protects people against unauthorized use of their\nface while at the same time collecting authorized images from users for\npersonalizing their videos. This paper thus provides a secure and innovative\nframework for creating deeply personalized music videos.\n","authors":["Mehul Agarwal","Gauri Agarwal","Santiago Benoit","Andrew Lippman","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2502.02610v1.pdf","comment":"NeurIPS 2024 Creative AI Track"}]},"2025-02-02T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2410.08435v2","updated":"2025-02-02T19:01:07Z","published":"2024-10-11T00:41:46Z","title":"Efficient Fine-Grained Guidance for Diffusion-Based Symbolic Music\n  Generation","summary":"  Developing generative models to create or conditionally create symbolic music\npresents unique challenges due to the combination of limited data availability\nand the need for high precision in note pitch. To address these challenges, we\nintroduce an efficient Fine-Grained Guidance (FGG) approach within diffusion\nmodels. FGG guides the diffusion models to generate music that aligns more\nclosely with the control and intent of expert composers, which is critical to\nimprove the accuracy, listenability, and quality of generated music. This\napproach empowers diffusion models to excel in advanced applications such as\nimprovisation, and interactive music creation. We derive theoretical\ncharacterizations for both the challenges in symbolic music generation and the\neffects of the FGG approach. We provide numerical experiments and subjective\nevaluation to demonstrate the effectiveness of our approach. We have published\na demo page to showcase performances, as one of the first in the symbolic music\nliterature's demo pages that enables real-time interactive generation.\n","authors":["Tingyu Zhu","Haoyu Liu","Ziyu Wang","Zhimin Jiang","Zeyu Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.08435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14170v2","updated":"2025-02-02T06:35:42Z","published":"2024-10-18T04:20:46Z","title":"Personalized Image Generation with Large Multimodal Models","summary":"  Personalized content filtering, such as recommender systems, has become a\ncritical infrastructure to alleviate information overload. However, these\nsystems merely filter existing content and are constrained by its limited\ndiversity, making it difficult to meet users' varied content needs. To address\nthis limitation, personalized content generation has emerged as a promising\ndirection with broad applications. Nevertheless, most existing research focuses\non personalized text generation, with relatively little attention given to\npersonalized image generation. The limited work in personalized image\ngeneration faces challenges in accurately capturing users' visual preferences\nand needs from noisy user-interacted images and complex multimodal\ninstructions. Worse still, there is a lack of supervised data for training\npersonalized image generation models.\n  To overcome the challenges, we propose a Personalized Image Generation\nFramework named Pigeon, which adopts exceptional large multimodal models with\nthree dedicated modules to capture users' visual preferences and needs from\nnoisy user history and multimodal instructions. To alleviate the data scarcity,\nwe introduce a two-stage preference alignment scheme, comprising masked\npreference reconstruction and pairwise preference alignment, to align Pigeon\nwith the personalized image generation task. We apply Pigeon to personalized\nsticker and movie poster generation, where extensive quantitative results and\nhuman evaluation highlight its superiority over various generative baselines.\n","authors":["Yiyan Xu","Wenjie Wang","Yang Zhang","Biao Tang","Peng Yan","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2410.14170v2.pdf","comment":"Accepted for publication in WWW'25"}]},"2025-02-01T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2502.00377v1","updated":"2025-02-01T09:29:21Z","published":"2025-02-01T09:29:21Z","title":"When End-to-End is Overkill: Rethinking Cascaded Speech-to-Text\n  Translation","summary":"  Though end-to-end speech-to-text translation has been a great success, we\nargue that the cascaded speech-to-text translation model still has its place,\nwhich is usually criticized for the error propagation between automatic speech\nrecognition (ASR) and machine translation (MT) models. In this paper, we\nexplore the benefits of incorporating multiple candidates from ASR and\nself-supervised speech features into MT. Our analysis reveals that the primary\ncause of cascading errors stems from the increased divergence between similar\nsamples in the speech domain when mapped to the text domain. By including\nmultiple candidates and self-supervised speech features, our approach allows\nthe machine translation model to choose the right words and ensure precise\ntranslation using various speech samples. This strategy minimizes error spread\nand takes advantage of large ASR and MT datasets, along with pre-trained ASR/MT\nmodels, while addressing associated issues.\n","authors":["Anna Min","Chenxu Hu","Yi Ren","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.00377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00374v1","updated":"2025-02-01T09:24:32Z","published":"2025-02-01T09:24:32Z","title":"A Unit-based System and Dataset for Expressive Direct Speech-to-Speech\n  Translation","summary":"  Current research in speech-to-speech translation (S2ST) primarily\nconcentrates on translation accuracy and speech naturalness, often overlooking\nkey elements like paralinguistic information, which is essential for conveying\nemotions and attitudes in communication. To address this, our research\nintroduces a novel, carefully curated multilingual dataset from various movie\naudio tracks. Each dataset pair is precisely matched for paralinguistic\ninformation and duration. We enhance this by integrating multiple prosody\ntransfer techniques, aiming for translations that are accurate,\nnatural-sounding, and rich in paralinguistic details. Our experimental results\nconfirm that our model retains more paralinguistic information from the source\nspeech while maintaining high standards of translation accuracy and\nnaturalness.\n","authors":["Anna Min","Chenxu Hu","Yi Ren","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.00374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.00358v1","updated":"2025-02-01T07:40:29Z","published":"2025-02-01T07:40:29Z","title":"Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?","summary":"  Unlike traditional visual segmentation, audio-visual segmentation (AVS)\nrequires the model not only to identify and segment objects but also to\ndetermine whether they are sound sources. Recent AVS approaches, leveraging\ntransformer architectures and powerful foundation models like SAM, have\nachieved impressive performance on standard benchmarks. Yet, an important\nquestion remains: Do these models genuinely integrate audio-visual cues to\nsegment sounding objects? In this paper, we systematically investigate this\nissue in the context of robust AVS. Our study reveals a fundamental bias in\ncurrent methods: they tend to generate segmentation masks based predominantly\non visual salience, irrespective of the audio context. This bias results in\nunreliable predictions when sounds are absent or irrelevant. To address this\nchallenge, we introduce AVSBench-Robust, a comprehensive benchmark\nincorporating diverse negative audio scenarios including silence, ambient\nnoise, and off-screen sounds. We also propose a simple yet effective approach\ncombining balanced training with negative samples and classifier-guided\nsimilarity learning. Our extensive experiments show that state-of-theart AVS\nmethods consistently fail under negative audio conditions, demonstrating the\nprevalence of visual bias. In contrast, our approach achieves remarkable\nimprovements in both standard metrics and robustness measures, maintaining\nnear-perfect false positive rates while preserving highquality segmentation\nperformance.\n","authors":["Jia Li","Wenjie Zhao","Ziru Huang","Yunhui Guo","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2502.00358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10292v3","updated":"2025-02-01T01:56:33Z","published":"2024-04-16T05:29:14Z","title":"From Data Deluge to Data Curation: A Filtering-WoRA Paradigm for\n  Efficient Text-based Person Search","summary":"  In text-based person search endeavors, data generation has emerged as a\nprevailing practice, addressing concerns over privacy preservation and the\narduous task of manual annotation. Although the number of synthesized data can\nbe infinite in theory, the scientific conundrum persists that how much\ngenerated data optimally fuels subsequent model training. We observe that only\na subset of the data in these constructed datasets plays a decisive role.\nTherefore, we introduce a new Filtering-WoRA paradigm, which contains a\nfiltering algorithm to identify this crucial data subset and WoRA (Weighted\nLow-Rank Adaptation) learning strategy for light fine-tuning. The filtering\nalgorithm is based on the cross-modality relevance to remove the lots of coarse\nmatching synthesis pairs. As the number of data decreases, we do not need to\nfine-tune the entire model. Therefore, we propose a WoRA learning strategy to\nefficiently update a minimal portion of model parameters. WoRA streamlines the\nlearning process, enabling heightened efficiency in extracting knowledge from\nfewer, yet potent, data instances. Extensive experimentation validates the\nefficacy of pretraining, where our model achieves advanced and efficient\nretrieval performance on challenging real-world benchmarks. Notably, on the\nCUHK-PEDES dataset, we have achieved a competitive mAP of 67.02% while reducing\nmodel training time by 19.82%.\n","authors":["Jintao Sun","Hao Fei","Zhedong Zheng","Gangyi Ding"],"pdf_url":"https://arxiv.org/pdf/2404.10292v3.pdf","comment":"11 pages, 8 figures, Proceedings of the ACM Web Conference 2025 (WWW\n  '25)"}]},"2025-01-31T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2501.11403v2","updated":"2025-01-31T10:08:36Z","published":"2025-01-20T11:06:05Z","title":"Verifying Cross-modal Entity Consistency in News using Vision-language\n  Models","summary":"  The web has become a crucial source of information, but it is also used to\nspread disinformation, often conveyed through multiple modalities like images\nand text. The identification of inconsistent cross-modal information, in\nparticular entities such as persons, locations, and events, is critical to\ndetect disinformation. Previous works either identify out-of-context\ndisinformation by assessing the consistency of images to the whole document,\nneglecting relations of individual entities, or focus on generic entities that\nare not relevant to news. So far, only few approaches have addressed the task\nof validating entity consistency between images and text in news. However, the\npotential of large vision-language models (LVLMs) has not been explored yet. In\nthis paper, we propose an LVLM-based framework for verifying Cross-modal Entity\nConsistency~(LVLM4CEC), to assess whether persons, locations and events in news\narticles are consistent across both modalities. We suggest effective prompting\nstrategies for LVLMs for entity verification that leverage reference images\ncrawled from web. Moreover, we extend three existing datasets for the task of\nentity verification in news providing manual ground-truth data. Our results\nshow the potential of LVLMs for automating cross-modal entity verification,\nshowing improved accuracy in identifying persons and events when using evidence\nimages. Moreover, our method outperforms a baseline for location and event\nverification in documents. The datasets and source code are available on GitHub\nat https://github.com/TIBHannover/LVLM4CEC.\n","authors":["Sahar Tahmasebi","David Ernst","Eric M√ºller-Budack","Ralph Ewerth"],"pdf_url":"https://arxiv.org/pdf/2501.11403v2.pdf","comment":"Accepted for publication in: European Conference on Information\n  Retrieval (ECIR) 2025"}]}}