<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-13T00:00:00Z">2025-02-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">135</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embed Any NeRF: Graph Meta-Networks for Neural Tasks on Arbitrary NeRF
  Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Ballerini, Pierluigi Zama Ramirez, Samuele Salti, Luigi Di Stefano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRFs) have emerged as a groundbreaking paradigm for
representing 3D objects and scenes by encoding shape and appearance information
into the weights of a neural network. Recent works have shown how such weights
can be used as input to frameworks processing them to solve deep learning
tasks. Yet, these frameworks can only process NeRFs with a specific, predefined
architecture. In this paper, we present the first framework that can ingest
NeRFs with multiple architectures and perform inference on architectures unseen
at training time. We achieve this goal by training a Graph Meta-Network in a
representation learning framework. Moreover, we show how a contrastive
objective is conducive to obtaining an architecture-agnostic latent space. In
experiments on both MLP-based and tri-planar NeRFs, our approach demonstrates
robust performance in classification and retrieval tasks that either matches or
exceeds that of existing frameworks constrained to single architectures, thus
providing the first architecture-agnostic method to perform tasks on NeRFs by
processing their weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MME-CoT: Benchmarking Chain-of-Thought in Large Multimodal Models for
  Reasoning Quality, Robustness, and Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering questions with Chain-of-Thought (CoT) has significantly enhanced
the reasoning capabilities of Large Language Models (LLMs), yet its impact on
Large Multimodal Models (LMMs) still lacks a systematic assessment and in-depth
investigation. In this paper, we introduce MME-CoT, a specialized benchmark
evaluating the CoT reasoning performance of LMMs, spanning six domains: math,
science, OCR, logic, space-time, and general scenes. As the first comprehensive
study in this area, we propose a thorough evaluation suite incorporating three
novel metrics that assess the reasoning quality, robustness, and efficiency at
a fine-grained level. Leveraging curated high-quality data and a unique
evaluation strategy, we conduct an in-depth analysis of state-of-the-art LMMs,
uncovering several key insights: 1) Models with reflection mechanism
demonstrate a superior CoT quality, with Kimi k1.5 outperforming GPT-4o and
demonstrating the highest quality results; 2) CoT prompting often degrades LMM
performance on perception-heavy tasks, suggesting a potentially harmful
overthinking behavior; and 3) Although the CoT quality is high, LMMs with
reflection exhibit significant inefficiency in both normal response and
self-correction phases. We hope MME-CoT serves as a foundation for advancing
multimodal reasoning in LMMs. Project Page: https://mmecot.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://mmecot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Potential of Encoder-free Architectures in 3D LMMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Tang, Zoey Guo, Zhuhao Wang, Ray Zhang, Qizhi Chen, Junli Liu, Delin Qu, Zhigang Wang, Dong Wang, Xuelong Li, Bin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encoder-free architectures have been preliminarily explored in the 2D visual
domain, yet it remains an open question whether they can be effectively applied
to 3D understanding scenarios. In this paper, we present the first
comprehensive investigation into the potential of encoder-free architectures to
overcome the challenges of encoder-based 3D Large Multimodal Models (LMMs).
These challenges include the failure to adapt to varying point cloud
resolutions and the point features from the encoder not meeting the semantic
needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to
remove the encoder and enable the LLM to assume the role of the 3D encoder: 1)
We propose the LLM-embedded Semantic Encoding strategy in the pre-training
stage, exploring the effects of various point cloud self-supervised losses. And
we present the Hybrid Semantic Loss to extract high-level semantics. 2) We
introduce the Hierarchical Geometry Aggregation strategy in the instruction
tuning stage. This incorporates inductive bias into the LLM early layers to
focus on the local details of the point clouds. To the end, we present the
first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current
state-of-the-art model, ShapeLLM-13B, achieving 55.0%, 50.92%, and 42.7% on the
classification, captioning, and VQA tasks, respectively. Our results
demonstrate that the encoder-free architecture is highly promising for
replacing encoder-based architectures in the field of 3D understanding. The
code is released at https://github.com/Ivan-Tang-3D/ENEL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is released at https://github.com/Ivan-Tang-3D/ENEL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Kahana, Or Nathan, Eliahu Horwitz, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing numbers of publicly available models, there are probably
pretrained, online models for most tasks users require. However, current model
search methods are rudimentary, essentially a text-based search in the
documentation, thus users cannot find the relevant models. This paper presents
ProbeLog, a method for retrieving classification models that can recognize a
target concept, such as "Dog", without access to model metadata or training
data. Differently from previous probing methods, ProbeLog computes a descriptor
for each output dimension (logit) of each model, by observing its responses on
a fixed set of inputs (probes). Our method supports both logit-based retrieval
("find more logits like this") and zero-shot, text-based retrieval ("find all
logits corresponding to dogs"). As probing-based representations require
multiple costly feedforward passes through the model, we develop a method,
based on collaborative filtering, that reduces the cost of encoding
repositories by 3x. We demonstrate that ProbeLog achieves high retrieval
accuracy, both in real-world and fine-grained search tasks and is scalable to
full-size repositories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIFe-GoM: Generalizable <span class="highlight-title">Human</span> Rendering with Learned Iterative Feedback
  Over Multi-Resolution Gaussians-on-Mesh <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Wen, Alexander G. Schwing, Shenlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizable rendering of an animatable human avatar from sparse inputs
relies on data priors and inductive biases extracted from training on large
data to avoid scene-specific optimization and to enable fast reconstruction.
This raises two main challenges: First, unlike iterative gradient-based
adjustment in scene-specific optimization, generalizable methods must
reconstruct the human shape representation in a single pass at inference time.
Second, rendering is preferably computationally efficient yet of high
resolution. To address both challenges we augment the recently proposed dual
shape representation, which combines the benefits of a mesh and Gaussian
points, in two ways. To improve reconstruction, we propose an iterative
feedback update framework, which successively improves the canonical human
shape representation during reconstruction. To achieve computationally
efficient yet high-resolution rendering, we study a coupled-multi-resolution
Gaussians-on-Mesh representation. We evaluate the proposed approach on the
challenging THuman2.0, XHuman and AIST++ data. Our approach reconstructs an
animatable representation from sparse inputs in less than 1s, renders views
with 95.1FPS at $1024 \times 1024$, and achieves PSNR/LPIPS*/FID of
24.65/110.82/51.27 on THuman2.0, outperforming the state-of-the-art in
rendering quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025; Project page: https://wenj.github.io/LIFe-GoM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Rectified Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengsheng Guo, Alexander G. Schwing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study Variational Rectified Flow Matching, a framework that enhances
classic rectified flow matching by modeling multi-modal velocity vector-fields.
At inference time, classic rectified flow matching 'moves' samples from a
source distribution to the target distribution by solving an ordinary
differential equation via integration along a velocity vector-field. At
training time, the velocity vector-field is learnt by linearly interpolating
between coupled samples one drawn from the source and one drawn from the target
distribution randomly. This leads to ''ground-truth'' velocity vector-fields
that point in different directions at the same location, i.e., the velocity
vector-fields are multi-modal/ambiguous. However, since training uses a
standard mean-squared-error loss, the learnt velocity vector-field averages
''ground-truth'' directions and isn't multi-modal. In contrast, variational
rectified flow matching learns and samples from multi-modal flow directions. We
show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational
rectified flow matching leads to compelling results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DexTrack: Towards Generalizable Neural Tracking <span class="highlight-title">Control</span> for Dexterous
  Manipulation from <span class="highlight-title">Human</span> References <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyi Liu, Jianibieke Adalibieke, Qianwei Han, Yuzhe Qin, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of developing a generalizable neural tracking
controller for dexterous manipulation from human references. This controller
aims to manage a dexterous robot hand to manipulate diverse objects for various
purposes defined by kinematic human-object interactions. Developing such a
controller is complicated by the intricate contact dynamics of dexterous
manipulation and the need for adaptivity, generalizability, and robustness.
Current reinforcement learning and trajectory optimization methods often fall
short due to their dependence on task-specific rewards or precise system
models. We introduce an approach that curates large-scale successful robot
tracking demonstrations, comprising pairs of human references and robot
actions, to train a neural controller. Utilizing a data flywheel, we
iteratively enhance the controller's performance, as well as the number and
quality of successful tracking demonstrations. We exploit available tracking
demonstrations and carefully integrate reinforcement learning and imitation
learning to boost the controller's performance in dynamic environments. At the
same time, to obtain high-quality tracking demonstrations, we individually
optimize per-trajectory tracking by leveraging the learned tracking controller
in a homotopy optimization method. The homotopy optimization, mimicking
chain-of-thought, aids in solving challenging trajectory tracking problems to
increase demonstration diversity. We showcase our success by training a
generalizable neural controller and evaluating it in both simulation and real
world. Our method achieves over a 10% improvement in success rates compared to
leading baselines. The project website with animated results is available at
https://meowuu7.github.io/DexTrack/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/
  Code: https://github.com/Meowuu7/DexTrack/ Video:
  https://youtu.be/zru1Z-DaiWE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, Zifan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present RigAnything, a novel autoregressive transformer-based model, which
makes 3D assets rig-ready by probabilistically generating joints, skeleton
topologies, and assigning skinning weights in a template-free manner. Unlike
most existing auto-rigging methods, which rely on predefined skeleton template
and are limited to specific categories like humanoid, RigAnything approaches
the rigging problem in an autoregressive manner, iteratively predicting the
next joint based on the global input shape and the previous prediction. While
autoregressive models are typically used to generate sequential data,
RigAnything extends their application to effectively learn and represent
skeletons, which are inherently tree structures. To achieve this, we organize
the joints in a breadth-first search (BFS) order, enabling the skeleton to be
defined as a sequence of 3D locations and the parent index. Furthermore, our
model improves the accuracy of position prediction by leveraging diffusion
modeling, ensuring precise and consistent placement of joints within the
hierarchy. This formulation allows the autoregressive model to efficiently
capture both spatial and hierarchical relationships within the skeleton.
Trained end-to-end on both RigNet and Objaverse datasets, RigAnything
demonstrates state-of-the-art performance across diverse object types,
including humanoids, quadrupeds, marine creatures, insects, and many more,
surpassing prior methods in quality, robustness, generalizability, and
efficiency. Please check our website for more details:
https://www.liuisabella.com/RigAnything.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.liuisabella.com/RigAnything</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Radiance Fields with 3D-aware 2D Representations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyi Zhou, Xi Liu, Feng Luo, Siyu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent 3D reconstruction has shown great promise in empowering 3D semantic
understanding and 3D generation by distilling 2D features into the 3D space.
However, existing approaches struggle with the domain gap between 2D feature
space and 3D representations, resulting in degraded rendering performance. To
address this challenge, we propose a novel framework that integrates 3D
awareness into the 2D latent space. The framework consists of three stages: (1)
a correspondence-aware autoencoding method that enhances the 3D consistency of
2D latent representations, (2) a latent radiance field (LRF) that lifts these
3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field
(VAE-RF) alignment strategy that improves image decoding from the rendered 2D
representations. Extensive experiments demonstrate that our method outperforms
the state-of-the-art latent 3D reconstruction approaches in terms of synthesis
performance and cross-dataset generalizability across diverse indoor and
outdoor scenes. To our knowledge, this is the first work showing the radiance
field representations constructed from 2D latent representations can yield
photorealistic 3D reconstruction performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025; Project page:
  https://latent-radiance-field.github.io/LRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing a Conditional Prior Distribution for Flow-Based Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Issachar, Mohammad Salama, Raanan Fattal, Sagie Benaim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flow-based generative models have recently shown impressive performance for
conditional generation tasks, such as text-to-image generation. However,
current methods transform a general unimodal noise distribution to a specific
mode of the target data distribution. As such, every point in the initial
source distribution can be mapped to every point in the target distribution,
resulting in long average paths. To this end, in this work, we tap into a
non-utilized property of conditional flow-based models: the ability to design a
non-trivial prior distribution. Given an input condition, such as a text
prompt, we first map it to a point lying in data space, representing an
``average" data point with the minimal average distance to all data points of
the same conditional mode (e.g., class). We then utilize the flow matching
formulation to map samples from a parametric distribution centered around this
point to the conditional target distribution. Experimentally, our method
significantly improves training times and generation efficiency (FID, KID and
CLIP alignment scores) compared to baselines, producing high quality samples
using fewer sampling steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance Segmentation of Scene Sketches Using Natural Image Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mia Tang, Yael Vinker, Chuan Yan, Lvmin Zhang, Maneesh Agrawala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sketch segmentation involves grouping pixels within a sketch that belong to
the same object or instance. It serves as a valuable tool for sketch editing
tasks, such as moving, scaling, or removing specific components. While image
segmentation models have demonstrated remarkable capabilities in recent years,
sketches present unique challenges for these models due to their sparse nature
and wide variation in styles. We introduce SketchSeg, a method for instance
segmentation of raster scene sketches. Our approach adapts state-of-the-art
image segmentation and object detection models to the sketch domain by
employing class-agnostic fine-tuning and refining segmentation masks using
depth cues. Furthermore, our method organizes sketches into sorted layers,
where occluded instances are inpainted, enabling advanced sketch editing
applications. As existing datasets in this domain lack variation in sketch
styles, we construct a synthetic scene sketch segmentation dataset featuring
sketches with diverse brush strokes and varying levels of detail. We use this
dataset to demonstrate the robustness of our approach and will release it to
promote further research in the field.
  Project webpage: https://sketchseg.github.io/sketch-seg/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAIA: A Global, Multi-modal, Multi-scale Vision-Language <span class="highlight-title">Dataset</span> for
  Remote Sensing Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelos Zavras, Dimitrios Michail, Xiao Xiang Zhu, Begüm Demir, Ioannis Papoutsis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The continuous operation of Earth-orbiting satellites generates vast and
ever-growing archives of Remote Sensing (RS) images. Natural language presents
an intuitive interface for accessing, querying, and interpreting the data from
such archives. However, existing Vision-Language Models (VLMs) are
predominantly trained on web-scraped, noisy image-text data, exhibiting limited
exposure to the specialized domain of RS. This deficiency results in poor
performance on RS-specific tasks, as commonly used datasets often lack
detailed, scientifically accurate textual descriptions and instead emphasize
solely on attributes like date and location. To bridge this critical gap, we
introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and
multi-modal RS image analysis. GAIA comprises of 205,150 meticulously curated
RS image-text pairs, representing a diverse range of RS modalities associated
to different spatial resolutions. Unlike existing vision-language datasets in
RS, GAIA specifically focuses on capturing a diverse range of RS applications,
providing unique information about environmental changes, natural disasters,
and various other dynamic phenomena. The dataset provides a spatially and
temporally balanced distribution, spanning across the globe, covering the last
25 years with a balanced temporal distribution of observations. GAIA's
construction involved a two-stage process: (1) targeted web-scraping of images
and accompanying text from reputable RS-related sources, and (2) generation of
five high-quality, scientifically grounded synthetic captions for each image
using carefully crafted prompts that leverage the advanced vision-language
capabilities of GPT-4o. Our extensive experiments, including fine-tuning of
CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance
on RS image classification, cross-modal retrieval and image captioning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing GPT for <span class="highlight-title">Video</span> Understanding: Zero-Shot Performance and Prompt
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Beliaev, Victor Yang, Madhura Raju, Jiachen Sun, Xinghai Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we tackle industry challenges in video content classification
by exploring and optimizing GPT-based models for zero-shot classification
across seven critical categories of video quality. We contribute a novel
approach to improving GPT's performance through prompt optimization and policy
refinement, demonstrating that simplifying complex policies significantly
reduces false negatives. Additionally, we introduce a new
decomposition-aggregation-based prompt engineering technique, which outperforms
traditional single-prompt methods. These experiments, conducted on real
industry problems, show that thoughtful prompt design can substantially enhance
GPT's performance without additional finetuning, offering an effective and
scalable solution for improving video classification systems across various
domains in industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusing DeBias: a Recipe for Turning a Bug into a Feature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimiliano Ciranni, Vito Paolo Pastore, Roberto Di Via, Enzo Tartaglione, Francesca Odone, Vittorio Murino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning model effectiveness in classification tasks is often challenged
by the quality and quantity of training data which, whenever containing strong
spurious correlations between specific attributes and target labels, can result
in unrecoverable biases in model predictions. Tackling these biases is crucial
in improving model generalization and trust, especially in real-world
scenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting
as a plug-in for common methods in model debiasing while exploiting the
inherent bias-learning tendency of diffusion models. Our approach leverages
conditional diffusion models to generate synthetic bias-aligned images, used to
train a bias amplifier model, to be further employed as an auxiliary method in
different unsupervised debiasing approaches. Our proposed method, which also
tackles the common issue of training set memorization typical of this type of
tech- niques, beats current state-of-the-art in multiple benchmark datasets by
significant margins, demonstrating its potential as a versatile and effective
tool for tackling dataset bias in deep learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 Pages, 12 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Calibrating Gaussian Splatting for Large Field of View
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youming Deng, Wenqi Xian, Guandao Yang, Leonidas Guibas, Gordon Wetzstein, Steve Marschner, Paul Debevec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a self-calibrating framework that jointly optimizes
camera parameters, lens distortion and 3D Gaussian representations, enabling
accurate and efficient scene reconstruction. In particular, our technique
enables high-quality scene reconstruction from Large field-of-view (FOV)
imagery taken with wide-angle lenses, allowing the scene to be modeled from a
smaller number of images. Our approach introduces a novel method for modeling
complex lens distortions using a hybrid network that combines invertible
residual networks with explicit grids. This design effectively regularizes the
optimization process, achieving greater accuracy than conventional camera
models. Additionally, we propose a cubemap-based resampling strategy to support
large FOV images without sacrificing resolution or introducing distortion
artifacts. Our method is compatible with the fast rasterization of Gaussian
Splatting, adaptable to a wide variety of camera lens distortion, and
demonstrates state-of-the-art performance on both synthetic and real-world
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://denghilbert.github.io/self-cali/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language
  Models for Vision-Driven Embodied Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging Multi-modal Large Language Models (MLLMs) to create embodied
agents offers a promising avenue for tackling real-world tasks. While
language-centric embodied agents have garnered substantial attention,
MLLM-based embodied agents remain underexplored due to the lack of
comprehensive evaluation frameworks. To bridge this gap, we introduce
EmbodiedBench, an extensive benchmark designed to evaluate vision-driven
embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing
tasks across four environments, ranging from high-level semantic tasks (e.g.,
household) to low-level tasks involving atomic actions (e.g., navigation and
manipulation); and (2) six meticulously curated subsets evaluating essential
agent capabilities like commonsense reasoning, complex instruction
understanding, spatial awareness, visual perception, and long-term planning.
Through extensive experiments, we evaluated 13 leading proprietary and
open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel
at high-level tasks but struggle with low-level manipulation, with the best
model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a
multifaceted standardized evaluation platform that not only highlights existing
challenges but also offers valuable insights to advance MLLM-based embodied
agents. Our code is available at https://embodiedbench.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Term TalkingFace Generation via <span class="highlight-title">Motion</span>-Prior Conditional Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Shen, Cong Wang, Junyao Gao, Qin Guo, Jisheng Dang, Jinhui Tang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in conditional diffusion models have shown promise for
generating realistic TalkingFace videos, yet challenges persist in achieving
consistent head movement, synchronized facial expressions, and accurate lip
synchronization over extended generations. To address these, we introduce the
\textbf{M}otion-priors \textbf{C}onditional \textbf{D}iffusion \textbf{M}odel
(\textbf{MCDM}), which utilizes both archived and current clip motion priors to
enhance motion prediction and ensure temporal consistency. The model consists
of three key elements: (1) an archived-clip motion-prior that incorporates
historical frames and a reference frame to preserve identity and context; (2) a
present-clip motion-prior diffusion model that captures multimodal causality
for accurate predictions of head movements, lip sync, and expressions; and (3)
a memory-efficient temporal attention mechanism that mitigates error
accumulation by dynamically storing and updating motion features. We also
release the \textbf{TalkingFace-Wild} dataset, a multilingual collection of
over 200 hours of footage across 10 languages. Experimental results demonstrate
the effectiveness of MCDM in maintaining identity and motion continuity for
long-term TalkingFace generation. Code, models, and datasets will be publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteROI-D: System Design and Mapping for Stereo Depth Inference on
  Regions of Interest 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Erhardt, Ziang Li, Reid Pinkham, Andrew Berkovich, Zhengya Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning algorithms have enabled high quality stereo depth estimation
to run on Augmented and Virtual Reality (AR/VR) devices. However, high energy
consumption across the full image processing stack prevents stereo depth
algorithms from running effectively on battery-limited devices. This paper
introduces SteROI-D, a full stereo depth system paired with a mapping
methodology. SteROI-D exploits Region-of-Interest (ROI) and temporal sparsity
at the system level to save energy. SteROI-D's flexible and heterogeneous
compute fabric supports diverse ROIs. Importantly, we introduce a systematic
mapping methodology to effectively handle dynamic ROIs, thereby maximizing
energy savings. Using these techniques, our 28nm prototype SteROI-D design
achieves up to 4.35x reduction in total system energy compared to a baseline
ASIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper by the 2025 EDGE AI FOUNDATION Austin</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SQ-GAN: Semantic Image Communications Using Masked Vector Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Pezone, Sergio Barbarossa, Giuseppe Caire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces Semantically Masked VQ-GAN (SQ-GAN), a novel approach
integrating generative models to optimize image compression for
semantic/task-oriented communications. SQ-GAN employs off-the-shelf semantic
semantic segmentation and a new specifically developed semantic-conditioned
adaptive mask module (SAMM) to selectively encode semantically significant
features of the images. SQ-GAN outperforms state-of-the-art image compression
schemes such as JPEG2000 and BPG across multiple metrics, including perceptual
quality and semantic segmentation accuracy on the post-decoding reconstructed
image, at extreme low compression rates expressed in bits per pixel.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When and How Does CLIP Enable Domain and Compositional Generalization? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Kempf, Simon Schrodi, Max Argus, Thomas Brox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable generalization performance of contrastive vision-language
models like CLIP is often attributed to the diversity of their training
distributions. However, key questions remain unanswered: Can CLIP generalize to
an entirely unseen domain when trained on a diverse mixture of domains (domain
generalization)? Can it generalize to unseen classes within partially seen
domains (compositional generalization)? What factors affect such
generalization? To answer these questions, we trained CLIP models on
systematically constructed training distributions with controlled domain
diversity and object class exposure. Our experiments show that domain diversity
is essential for both domain and compositional generalization, yet
compositional generalization can be surprisingly weaker than domain
generalization when the training distribution contains a suboptimal subset of
the test domain. Through data-centric and mechanistic analyses, we find that
successful generalization requires learning of shared representations already
in intermediate layers and shared circuitry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prior-Constrained Association Learning for Fine-Grained Generalized
  Category Discovery <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menglin Wang, Zhun Zhong, Xiaojin Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses generalized category discovery (GCD), the task of
clustering unlabeled data from potentially known or unknown categories with the
help of labeled instances from each known category. Compared to traditional
semi-supervised learning, GCD is more challenging because unlabeled data could
be from novel categories not appearing in labeled data. Current
state-of-the-art methods typically learn a parametric classifier assisted by
self-distillation. While being effective, these methods do not make use of
cross-instance similarity to discover class-specific semantics which are
essential for representation learning and category discovery. In this paper, we
revisit the association-based paradigm and propose a Prior-constrained
Association Learning method to capture and learn the semantic relations within
data. In particular, the labeled data from known categories provides a unique
prior for the association of unlabeled data. Unlike previous methods that only
adopts the prior as a pre or post-clustering refinement, we fully incorporate
the prior into the association process, and let it constrain the association
towards a reliable grouping outcome. The estimated semantic groups are utilized
through non-parametric prototypical contrast to enhance the representation
learning. A further combination of both parametric and non-parametric
classification complements each other and leads to a model that outperforms
existing methods by a significant margin. On multiple GCD benchmarks, we
perform extensive experiments and validate the effectiveness of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Standardisation of Convex Ultrasound Data Through Geometric Analysis and
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Weld, Giovanni Faoro, Luke Dixon, Sophie Camp, Arianna Menciassi, Stamatia Giannarou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of ultrasound in healthcare has seen increased diversity and
importance. Unlike other medical imaging modalities, ultrasound research and
development has historically lagged, particularly in the case of applications
with data-driven algorithms. A significant issue with ultrasound is the extreme
variability of the images, due to the number of different machines available
and the possible combination of parameter settings. One outcome of this is the
lack of standardised and benchmarking ultrasound datasets. The method proposed
in this article is an approach to alleviating this issue of disorganisation.
For this purpose, the issue of ultrasound data sparsity is examined and a novel
perspective, approach, and solution is proposed; involving the extraction of
the underlying ultrasound plane within the image and representing it using
annulus sector geometry. An application of this methodology is proposed, which
is the extraction of scan lines and the linearisation of convex planes.
Validation of the robustness of the proposed method is performed on both
private and public data. The impact of deformation and the invertibility of
augmentation using the estimated annulus sector parameters is also studied.
Keywords: Ultrasound, Annulus Sector, Augmentation, Linearisation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation
  Networks for Quantitative Nanomaterial Analysis through Differentiable
  Rendering and Generative Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Possart, Leonid Mill, Florian Vollnhals, Tor Hildebrand, Peter Suter, Mathis Hoffmann, Jonas Utz, Daniel Augsburger, Mareike Thies, Mingxuan Wu, Fabian Wagner, George Sarau, Silke Christiansen, Katharina Breininger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanomaterials exhibit distinctive properties governed by parameters such as
size, shape, and surface characteristics, which critically influence their
applications and interactions across technological, biological, and
environmental contexts. Accurate quantification and understanding of these
materials are essential for advancing research and innovation. In this regard,
deep learning segmentation networks have emerged as powerful tools that enable
automated insights and replace subjective methods with precise quantitative
analysis. However, their efficacy depends on representative annotated datasets,
which are challenging to obtain due to the costly imaging of nanoparticles and
the labor-intensive nature of manual annotations. To overcome these
limitations, we introduce DiffRenderGAN, a novel generative model designed to
produce annotated synthetic data. By integrating a differentiable renderer into
a Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes
textural rendering parameters to generate realistic, annotated nanoparticle
images from non-annotated real microscopy images. This approach reduces the
need for manual intervention and enhances segmentation performance compared to
existing synthetic data methods by generating diverse and realistic data.
Tested on multiple ion and electron microscopy cases, including titanium
dioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),
DiffRenderGAN bridges the gap between synthetic and real data, advancing the
quantification and understanding of complex nanomaterial systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wholly-WOOD: Wholly Leveraging Diversified-quality Labels for
  Weakly-supervised Oriented Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yu, Xue Yang, Yansheng Li, Zhenjun Han, Feipeng Da, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the orientation of visual objects with compact rotated
bounding boxes (RBoxes) has become a prominent demand, which challenges
existing object detection paradigms that only use horizontal bounding boxes
(HBoxes). To equip the detectors with orientation awareness, supervised
regression/classification modules have been introduced at the high cost of
rotation annotation. Meanwhile, some existing datasets with oriented objects
are already annotated with horizontal boxes or even single points. It becomes
attractive yet remains open for effectively utilizing weaker single point and
horizontal annotations to train an oriented object detector (OOD). We develop
Wholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging
various labeling forms (Points, HBoxes, RBoxes, and their combination) in a
unified fashion. By only using HBox for training, our Wholly-WOOD achieves
performance very close to that of the RBox-trained counterpart on remote
sensing and other areas, significantly reducing the tedious efforts on
labor-intensive annotation for oriented objects. The source codes are available
at https://github.com/VisionXLab/whollywood (PyTorch-based) and
https://github.com/VisionXLab/whollywood-jittor (Jittor-based).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures, 9 tables, accepted by TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metamorphic Testing for <span class="highlight-title">Pose</span> Estimation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Duran, Thomas Laurent, Ellen Rushe, Anthony Ventresque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose estimation systems are used in a variety of fields, from sports
analytics to livestock care. Given their potential impact, it is paramount to
systematically test their behaviour and potential for failure. This is a
complex task due to the oracle problem and the high cost of manual labelling
necessary to build ground truth keypoints. This problem is exacerbated by the
fact that different applications require systems to focus on different subjects
(e.g., human versus animal) or landmarks (e.g., only extremities versus whole
body and face), which makes labelled test data rarely reusable. To combat these
problems we propose MET-POSE, a metamorphic testing framework for pose
estimation systems that bypasses the need for manual annotation while assessing
the performance of these systems under different circumstances. MET-POSE thus
allows users of pose estimation systems to assess the systems in conditions
that more closely relate to their application without having to label an ad-hoc
test dataset or rely only on available datasets, which may not be adapted to
their application domain. While we define MET-POSE in general terms, we also
present a non-exhaustive list of metamorphic rules that represent common
challenges in computer vision applications, as well as a specific way to
evaluate these rules. We then experimentally show the effectiveness of MET-POSE
by applying it to Mediapipe Holistic, a state of the art human pose estimation
system, with the FLIC and PHOENIX datasets. With these experiments, we outline
numerous ways in which the outputs of MET-POSE can uncover faults in pose
estimation systems at a similar or higher rate than classic testing using hand
labelled data, and show that users can tailor the rule set they use to the
faults and level of accuracy relevant to their application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at 2025 IEEE Conference on Software Testing,
  Verification and Validation (ICST)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel-Level Reasoning Segmentation via Multi-turn Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dexian Cai, Xiaocui Yang, Yongkang Liu, Daling Wang, Shi Feng, Yifei Zhang, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing visual perception systems focus on region-level segmentation in
single-turn dialogues, relying on complex and explicit query instructions. Such
systems cannot reason at the pixel level and comprehend dynamic user intent
that changes over interaction. Our work tackles this issue by introducing a
novel task, Pixel-level Reasoning Segmentation (Pixel-level RS) based on
multi-turn conversations, tracking evolving user intent via multi-turn
interactions for fine-grained segmentation. To establish a benchmark for this
novel task, we build a Pixel-level ReasonIng Segmentation Dataset Based on
Multi-Turn Conversations (PRIST), comprising 24k utterances from 8.3k
multi-turn conversational scenarios with segmentation targets. Building on
PRIST, we further propose MIRAS, a Multi-turn Interactive ReAsoning
Segmentation framework, integrates pixel-level segmentation with robust
multi-turn conversation understanding, generating pixel-grounded explanations
aligned with user intent. The PRIST dataset and MIRSA framework fill the gap in
pixel-level reasoning segmentation. Experimental results on the PRIST dataset
demonstrate that our method outperforms current segmentation-specific baselines
in terms of segmentation and LLM-based reasoning metrics. The code and data are
available at: https://github.com/ccccai239/PixelRIST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Redistribute Ensemble Training for Mitigating Memorization in Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoliu Guan, Yu Wu, Huayang Huang, Xiao Liu, Jiaxu Miao, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models, known for their tremendous ability to generate high-quality
samples, have recently raised concerns due to their data memorization behavior,
which poses privacy risks. Recent methods for memory mitigation have primarily
addressed the issue within the context of the text modality in cross-modal
generation tasks, restricting their applicability to specific conditions. In
this paper, we propose a novel method for diffusion models from the perspective
of visual modality, which is more generic and fundamental for mitigating
memorization. Directly exposing visual data to the model increases memorization
risk, so we design a framework where models learn through proxy model
parameters instead. Specially, the training dataset is divided into multiple
shards, with each shard training a proxy model, then aggregated to form the
final model. Additionally, practical analysis of training losses illustrates
that the losses for easily memorable images tend to be obviously lower. Thus,
we skip the samples with abnormally low loss values from the current mini-batch
to avoid memorizing. However, balancing the need to skip memorization-prone
samples while maintaining sufficient training data for high-quality image
generation presents a key challenge. Thus, we propose IET-AGC+, which
redistributes highly memorizable samples between shards, to mitigate these
samples from over-skipping. Furthermore, we dynamically augment samples based
on their loss values to further reduce memorization. Extensive experiments and
analysis on four datasets show that our method successfully reduces memory
capacity while maintaining performance. Moreover, we fine-tune the pre-trained
diffusion models, e.g., Stable Diffusion, and decrease the memorization score
by 46.7\%, demonstrating the effectiveness of our method. Code is available in:
https://github.com/liuxiao-guan/IET_AGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,9 figures. arXiv admin note: substantial text overlap with
  arXiv:2407.15328</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A 3D <span class="highlight-title">Facial</span> Reconstruction Evaluation Methodology: Comparing Smartphone
  Scans with Deep Learning Based Methods Using Geometry and Morphometry
  Criteria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Álvaro Heredia-Lidón, Alejandro Moñux-Bernal, Alejandro González, Luis M. Echeverry-Quiceno, Max Rubert, Aroa Casado, María Esther Esteban, Mireia Andreu-Montoriol, Susanna Gallardo, Cristina Ruffo, Neus Martínez-Abadías, Xavier Sevillano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional (3D) facial shape analysis has gained interest due to its
potential clinical applications. However, the high cost of advanced 3D facial
acquisition systems limits their widespread use, driving the development of
low-cost acquisition and reconstruction methods. This study introduces a novel
evaluation methodology that goes beyond traditional geometry-based benchmarks
by integrating morphometric shape analysis techniques, providing a statistical
framework for assessing facial morphology preservation. As a case study, we
compare smartphone-based 3D scans with state-of-the-art deep learning
reconstruction methods from 2D images, using high-end stereophotogrammetry
models as ground truth. This methodology enables a quantitative assessment of
global and local shape differences, offering a biologically meaningful
validation approach for low-cost 3D facial acquisition and reconstruction
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rotem Shalev-Arkushin, Rinon Gal, Amit H. Bermano, Ohad Fried
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models enable high-quality and diverse visual content synthesis.
However, they struggle to generate rare or unseen concepts. To address this
challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with
image generation models. We propose ImageRAG, a method that dynamically
retrieves relevant images based on a given text prompt, and uses them as
context to guide the generation process. Prior approaches that used retrieved
images to improve generation, trained models specifically for retrieval-based
generation. In contrast, ImageRAG leverages the capabilities of existing image
conditioning models, and does not require RAG-specific training. Our approach
is highly adaptable and can be applied across different model types, showing
significant improvement in generating rare and fine-grained concepts using
different base models.
  Our project page is available at: https://rotem-shalev.github.io/ImageRAG
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Galileo: Learning Global and Local Features in Pretrained Remote Sensing
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Tseng, Anthony Fuller, Marlena Reil, Henry Herzog, Patrick Beukema, Favyen Bastani, James R. Green, Evan Shelhamer, Hannah Kerner, David Rolnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From crop mapping to flood detection, machine learning in remote sensing has
a wide range of societally beneficial applications. The commonalities between
remote sensing data in these applications present an opportunity for pretrained
machine learning models tailored to remote sensing to reduce the labeled data
and effort required to solve individual tasks. However, such models must be:
(i) flexible enough to ingest input data of varying sensor modalities and
shapes (i.e., of varying spatial and temporal dimensions), and (ii) able to
model Earth surface phenomena of varying scales and types. To solve this gap,
we present Galileo, a family of pretrained remote sensing models designed to
flexibly process multimodal remote sensing data. We also introduce a novel and
highly effective self-supervised learning approach to learn both large- and
small-scale features, a challenge not addressed by previous models. Our Galileo
models obtain state-of-the-art results across diverse remote sensing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wasserstein distributional adversarial training for deep neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Bai, Guangyi He, Yifan Jiang, Jan Obloj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Design of adversarial attacks for deep neural networks, as well as methods of
adversarial training against them, are subject of intense research. In this
paper, we propose methods to train against distributional attack threats,
extending the TRADES method used for pointwise attacks. Our approach leverages
recent contributions and relies on sensitivity analysis for Wasserstein
distributionally robust optimization problems. We introduce an efficient
fine-tuning method which can be deployed on a previously trained model. We test
our methods on a range of pre-trained models on RobustBench. These experimental
results demonstrate the additional training enhances Wasserstein distributional
robustness, while maintaining original levels of pointwise robustness, even for
already very successful networks. The improvements are less marked for models
pre-trained using huge synthetic datasets of 20-100M images. However,
remarkably, sometimes our methods are still able to improve their performance
even when trained using only the original training dataset (50k images).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Benchmark for Crime Surveillance <span class="highlight-title">Video</span> Analysis with Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Chen, Dong Yi, Moyan Cao, Chensen Huang, Guibo Zhu, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly analysis in surveillance videos is a crucial topic in computer
vision. In recent years, multimodal large language models (MLLMs) have
outperformed task-specific models in various domains. Although MLLMs are
particularly versatile, their abilities to understand anomalous concepts and
details are insufficiently studied because of the outdated benchmarks of this
field not providing MLLM-style QAs and efficient algorithms to assess the
model's open-ended text responses. To fill this gap, we propose a benchmark for
crime surveillance video analysis with large models denoted as UCVL, including
1,829 videos and reorganized annotations from the UCF-Crime and UCF-Crime
Annotation datasets. We design six types of questions and generate diverse QA
pairs. Then we develop detailed instructions and use OpenAI's GPT-4o for
accurate assessment. We benchmark eight prevailing MLLMs ranging from 0.5B to
40B parameters, and the results demonstrate the reliability of this bench.
Moreover, we finetune LLaVA-OneVision on UCVL's training set. The improvement
validates our data's high quality for video anomaly analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating the Impact of Prominent Position Shift in Drone-based RGBT
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhang, Wen Yang, Chang Xu, Qian Hu, Fang Xu, Gui-Song Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drone-based RGBT object detection plays a crucial role in many
around-the-clock applications. However, real-world drone-viewed RGBT data
suffers from the prominent position shift problem, i.e., the position of a tiny
object differs greatly in different modalities. For instance, a slight
deviation of a tiny object in the thermal modality will induce it to drift from
the main body of itself in the RGB modality. Considering RGBT data are usually
labeled on one modality (reference), this will cause the unlabeled modality
(sensed) to lack accurate supervision signals and prevent the detector from
learning a good representation. Moreover, the mismatch of the corresponding
feature point between the modalities will make the fused features confusing for
the detection head. In this paper, we propose to cast the cross-modality box
shift issue as the label noise problem and address it on the fly via a novel
Mean Teacher-based Cross-modality Box Correction head ensemble (CBC). In this
way, the network can learn more informative representations for both
modalities. Furthermore, to alleviate the feature map mismatch problem in RGBT
fusion, we devise a Shifted Window-Based Cascaded Alignment (SWCA) module. SWCA
mines long-range dependencies between the spatially unaligned features inside
shifted windows and cascaded aligns the sensed features with the reference
ones. Extensive experiments on two drone-based RGBT object detection datasets
demonstrate that the correction results are both visually and quantitatively
favorable, thereby improving the detection performance. In particular, our CBC
module boosts the precision of the sensed modality ground truth by 25.52 aSim
points. Overall, the proposed detector achieves an mAP_50 of 43.55 points on
RGBTDronePerson and surpasses a state-of-the-art method by 8.6 mAP50 on a shift
subset of DroneVehicle dataset. The code and data will be made publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Physics-Informed Deep Learning Model for MRI Brain <span class="highlight-title">Motion</span> Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojtaba Safari, Shansong Wang, Zach Eidex, Richard Qiu, Chih-Wei Chang, David S. Yu, Xiaofeng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: MRI is crucial for brain imaging but is highly susceptible to
motion artifacts due to long acquisition times. This study introduces
PI-MoCoNet, a physics-informed motion correction network that integrates
spatial and k-space information to remove motion artifacts without explicit
motion parameter estimation, enhancing image fidelity and diagnostic
reliability. Materials and Methods: PI-MoCoNet consists of a motion detection
network (U-net with spatial averaging) to identify corrupted k-space lines and
a motion correction network (U-net with Swin Transformer blocks) to reconstruct
motion-free images. The correction is guided by three loss functions:
reconstruction (L1), perceptual (LPIPS), and data consistency (Ldc). Motion
artifacts were simulated via rigid phase encoding perturbations and evaluated
on IXI and MR-ART datasets against Pix2Pix, CycleGAN, and U-net using PSNR,
SSIM, and NMSE. Results: PI-MoCoNet significantly improved image quality. On
IXI, for minor artifacts, PSNR increased from 34.15 dB to 45.95 dB, SSIM from
0.87 to 1.00, and NMSE reduced from 0.55% to 0.04%. For moderate artifacts,
PSNR improved from 30.23 dB to 42.16 dB, SSIM from 0.80 to 0.99, and NMSE from
1.32% to 0.09%. For heavy artifacts, PSNR rose from 27.99 dB to 36.01 dB, SSIM
from 0.75 to 0.97, and NMSE decreased from 2.21% to 0.36%. On MR-ART,
PI-MoCoNet achieved PSNR gains of ~10 dB and SSIM improvements of up to 0.20,
with NMSE reductions of ~6%. Ablation studies confirmed the importance of data
consistency and perceptual losses, yielding a 1 dB PSNR gain and 0.17% NMSE
reduction. Conclusions: PI-MoCoNet effectively mitigates motion artifacts in
brain MRI, outperforming existing methods. Its ability to integrate spatial and
k-space information makes it a promising tool for clinical use in motion-prone
settings. Code: https://github.com/mosaf/PI-MoCoNet.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoAssist: E<span class="highlight-title">motion</span>al Assistant for Visual Impairment Community 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Qi, He Li, Linjie Li, Zhenyu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large multi-modality models (LMMs) has significantly
propelled the integration of artificial intelligence into practical
applications. Visual Question Answering (VQA) systems, which can process
multi-modal data including vision, text, and audio, hold great potential for
assisting the Visual Impairment (VI) community in navigating complex and
dynamic real-world environments. However, existing VI assistive LMMs overlook
the emotional needs of VI individuals, and current benchmarks lack emotional
evaluation of these LMMs. To address these gaps, this paper introduces the
EmoAssist Benchmark, a comprehensive benchmark designed to evaluate the
assistive performance of LMMs for the VI community. To the best of our
knowledge, this is the first benchmark that incorporates emotional intelligence
as a key consideration. Furthermore, we propose the EmoAssist Model, an
Emotion-Assistive LMM specifically designed for the VI community. The EmoAssist
Model utilizes Direct Preference Optimization (DPO) to align outputs with human
emotional preferences. Experiment results demonstrate that the EmoAssist Model
significantly enhances the recognition of implicit emotions and intentions of
VI users, delivers empathetic responses, and provides actionable guidance.
Specifically, it shows respective improvements of 147.8% and 89.7% in the
Empathy and Suggestion metrics on the EmoAssist Benchmark, compared to the
pre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FE-LWS: Refined Image-Text Representations via Decoder Stacking and
  Fused Encodings for Remote Sensing Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swadhin Das, Raksha Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing image captioning aims to generate descriptive text from remote
sensing images, typically employing an encoder-decoder framework. In this
setup, a convolutional neural network (CNN) extracts feature representations
from the input image, which then guide the decoder in a sequence-to-sequence
caption generation process. Although much research has focused on refining the
decoder, the quality of image representations from the encoder remains crucial
for accurate captioning. This paper introduces a novel approach that integrates
features from two distinct CNN based encoders, capturing complementary
information to enhance caption generation. Additionally, we propose a weighted
averaging technique to combine the outputs of all GRUs in the stacked decoder.
Furthermore, a comparison-based beam search strategy is incorporated to refine
caption selection. The results demonstrate that our fusion-based approach,
along with the enhanced stacked decoder, significantly outperforms both the
transformer-based state-of-the-art model and other LSTM-based baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConsistentDreamer: View-Consistent Meshes Through Balanced Multi-View
  Gaussian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Onat Şahin, Mohammad Altillawi, George Eskandar, Carlos Carbone, Ziyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion models have significantly improved 3D
generation, enabling the use of assets generated from an image for embodied AI
simulations. However, the one-to-many nature of the image-to-3D problem limits
their use due to inconsistent content and quality across views. Previous models
optimize a 3D model by sampling views from a view-conditioned diffusion prior,
but diffusion models cannot guarantee view consistency. Instead, we present
ConsistentDreamer, where we first generate a set of fixed multi-view prior
images and sample random views between them with another diffusion model
through a score distillation sampling (SDS) loss. Thereby, we limit the
discrepancies between the views guided by the SDS loss and ensure a consistent
rough shape. In each iteration, we also use our generated multi-view prior
images for fine-detail reconstruction. To balance between the rough shape and
the fine-detail optimizations, we introduce dynamic task-dependent weights
based on homoscedastic uncertainty, updated automatically in each iteration.
Additionally, we employ opacity, depth distortion, and normal alignment losses
to refine the surface for mesh extraction. Our method ensures better view
consistency and visual quality compared to the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript accepted by Pattern Recognition Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLARES: Fast and Accurate LiDAR Multi-Range Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Yang, Alexandru Paul Condurache
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D scene understanding is a critical yet challenging task in autonomous
driving, primarily due to the irregularity and sparsity of LiDAR data, as well
as the computational demands of processing large-scale point clouds. Recent
methods leverage the range-view representation to improve processing
efficiency. To mitigate the performance drop caused by information loss
inherent to the "many-to-one" problem, where multiple nearby 3D points are
mapped to the same 2D grids and only the closest is retained, prior works tend
to choose a higher azimuth resolution for range-view projection. However, this
can bring the drawback of reducing the proportion of pixels that carry
information and heavier computation within the network. We argue that it is not
the optimal solution and show that, in contrast, decreasing the resolution is
more advantageous in both efficiency and accuracy. In this work, we present a
comprehensive re-design of the workflow for range-view-based LiDAR semantic
segmentation. Our approach addresses data representation, augmentation, and
post-processing methods for improvements. Through extensive experiments on two
public datasets, we demonstrate that our pipeline significantly enhances the
performance of various network architectures over their baselines, paving the
way for more effective LiDAR-based perception in autonomous systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory-based Ensemble Learning in CMR Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Liu, Ziyi Wu, Liang Zhong, Linyi Wen, Yuankai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing models typically segment either the entire 3D frame or 2D slices
independently to derive clinical functional metrics from ventricular
segmentation in cardiac cine sequences. While performing well overall, they
struggle at the end slices. To address this, we leverage spatial continuity to
extract global uncertainty from segmentation variance and use it as memory in
our ensemble learning method, Streaming, for classifier weighting, balancing
overall and end-slice performance. Additionally, we introduce the End
Coefficient (EC) to quantify end-slice accuracy. Experiments on ACDC and M\&Ms
datasets show that our framework achieves near-state-of-the-art Dice Similarity
Coefficient (DSC) and outperforms all models on end-slice performance,
improving patient-specific segmentation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynSegNet:Dynamic Architecture Adjustment for Adversarial Learning in
  Segmenting Hemorrhagic Lesions from Fundus Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesheng Li, Minwen Liao, Haoran Chen, Yan Su, Chengchang Pan, Honggang Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The hemorrhagic lesion segmentation plays a critical role in ophthalmic
diagnosis, directly influencing early disease detection, treatment planning,
and therapeutic efficacy evaluation. However, the task faces significant
challenges due to lesion morphological variability, indistinct boundaries, and
low contrast with background tissues. To improve diagnostic accuracy and
treatment outcomes, developing advanced segmentation techniques remains
imperative. This paper proposes an adversarial learning-based dynamic
architecture adjustment approach that integrates hierarchical U-shaped
encoder-decoder, residual blocks, attention mechanisms, and ASPP modules. By
dynamically optimizing feature fusion, our method enhances segmentation
performance. Experimental results demonstrate a Dice coefficient of 0.6802, IoU
of 0.5602, Recall of 0.766, Precision of 0.6525, and Accuracy of 0.9955,
effectively addressing the challenges in fundus image hemorrhage
segmentation.[* Corresponding author.]
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Graph Question Answering with ASP and LLMs for Language Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Johannes Bauer, Thomas Eiter, Nelson Higuera Ruiz, Johannes Oetsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) is a challenging problem that requires to
process multimodal input. Answer-Set Programming (ASP) has shown great
potential in this regard to add interpretability and explainability to modular
VQA architectures. In this work, we address the problem of how to integrate ASP
with modules for vision and natural language processing to solve a new and
demanding VQA variant that is concerned with images of graphs (not graphs in
symbolic form). Images containing graph-based structures are an ubiquitous and
popular form of visualisation. Here, we deal with the particular problem of
graphs inspired by transit networks, and we introduce a novel dataset that
amends an existing one by adding images of graphs that resemble metro lines.
Our modular neuro-symbolic approach combines optical graph recognition for
graph parsing, a pretrained optical character recognition neural network for
parsing labels, Large Language Models (LLMs) for language processing, and ASP
for reasoning. This method serves as a first baseline and achieves an overall
average accuracy of 73% on the dataset. Our evaluation provides further
evidence of the potential of modular neuro-symbolic systems, in particular with
pretrained models that do not involve any further training and logic
programming for reasoning, to solve complex VQA tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings ICLP 2024, arXiv:2502.08453. This work was partially
  funded from the Bosch Center for AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster than real-time detection of shot boundaries, sampling structure
  and dynamic keyframes in <span class="highlight-title">video</span> <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Fassold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of shot boundaries (hardcuts and short dissolves), sampling
structure (progressive / interlaced / pulldown) and dynamic keyframes in a
video are fundamental video analysis tasks which have to be done before any
further high-level analysis tasks. We present a novel algorithm which does all
these analysis tasks in an unified way, by utilizing a combination of
inter-frame and intra-frame measures derived from the motion field and
normalized cross correlation. The algorithm runs four times faster than
real-time due to sparse and selective calculation of these measures. An initial
evaluation furthermore shows that the proposed algorithm is extremely robust
even for challenging content showing large camera or object motion,
flashlights, flicker or low contrast / noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ICISPC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot
  Object Customization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung X. Pham, Zhang Kang, Ji Woo Hong, Xuran Zheng, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose E-MD3C ($\underline{E}$fficient $\underline{M}$asked
$\underline{D}$iffusion Transformer with Disentangled $\underline{C}$onditions
and $\underline{C}$ompact $\underline{C}$ollector), a highly efficient
framework for zero-shot object image customization. Unlike prior works reliant
on resource-intensive Unet architectures, our approach employs lightweight
masked diffusion transformers operating on latent patches, offering
significantly improved computational efficiency. The framework integrates three
core components: (1) an efficient masked diffusion transformer for processing
autoencoder latents, (2) a disentangled condition design that ensures
compactness while preserving background alignment and fine details, and (3) a
learnable Conditions Collector that consolidates multiple inputs into a compact
representation for efficient denoising and learning. E-MD3C outperforms the
existing approach on the VITON-HD dataset across metrics such as PSNR, FID,
SSIM, and LPIPS, demonstrating clear advantages in parameters, memory
efficiency, and inference speed. With only $\frac{1}{4}$ of the parameters, our
Transformer-based 468M model delivers $2.5\times$ faster inference and uses
$\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent
diffusion model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shortcut Learning Susceptibility in Vision Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pirzada Suhail, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortcut learning, where machine learning models exploit spurious
correlations in data instead of capturing meaningful features, poses a
significant challenge to building robust and generalizable models. This
phenomenon is prevalent across various machine learning applications, including
vision, natural language processing, and speech recognition, where models may
find unintended cues that minimize training loss but fail to capture the
underlying structure of the data. Vision classifiers such as Convolutional
Neural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers
(ViTs) leverage distinct architectural principles to process spatial and
structural information, making them differently susceptible to shortcut
learning. In this study, we systematically evaluate these architectures by
introducing deliberate shortcuts into the dataset that are positionally
correlated with class labels, creating a controlled setup to assess whether
models rely on these artificial cues or learn actual distinguishing features.
We perform both quantitative evaluation by training on the shortcut-modified
dataset and testing them on two different test sets -- one containing the same
shortcuts and another without them -- to determine the extent of reliance on
shortcuts. Additionally, qualitative evaluation is performed by using network
inversion-based reconstruction techniques to analyze what the models
internalize in their weights, aiming to reconstruct the training data as
perceived by the classifiers. We evaluate shortcut learning behavior across
multiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and
CIFAR-10, to compare the susceptibility of different vision classifier
architectures to shortcut reliance and assess their varying degrees of
sensitivity to spurious correlations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal HIE Lesion Segmentation in Neonates: A Comparative Study of
  Loss Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annayah Usman, Abdul Haseeb, Tahir Syed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of Hypoxic-Ischemic Encephalopathy (HIE) lesions in neonatal MRI
is a crucial but challenging task due to diffuse multifocal lesions with
varying volumes and the limited availability of annotated HIE lesion datasets.
Using the BONBID-HIE dataset, we implemented a 3D U-Net with optimized
preprocessing, augmentation, and training strategies to overcome data
constraints. The goal of this study is to identify the optimal loss function
specifically for the HIE lesion segmentation task. To this end, we evaluated
various loss functions, including Dice, Dice-Focal, Tversky, Hausdorff Distance
(HausdorffDT) Loss, and two proposed compound losses -- Dice-Focal-HausdorffDT
and Tversky-HausdorffDT -- to enhance segmentation performance. The results
show that different loss functions predict distinct segmentation masks, with
compound losses outperforming standalone losses. Tversky-HausdorffDT Loss
achieves the highest Dice and Normalized Surface Dice scores, while
Dice-Focal-HausdorffDT Loss minimizes Mean Surface Distance. This work
underscores the significance of task-specific loss function optimization,
demonstrating that combining region-based and boundary-aware losses leads to
more accurate HIE lesion segmentation, even with limited training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-based Graph Attention Networks Improve Online Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adjovi Sim, Zhengkui Wang, Aik Beng Ng, Shalini De Mello, Simon See, Wonmin Byeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online continual learning for image classification is crucial for models to
adapt to new data while retaining knowledge of previously learned tasks. This
capability is essential to address real-world challenges involving dynamic
environments and evolving data distributions. Traditional approaches
predominantly employ Convolutional Neural Networks, which are limited to
processing images as grids and primarily capture local patterns rather than
relational information. Although the emergence of transformer architectures has
improved the ability to capture relationships, these models often require
significantly larger resources. In this paper, we present a novel online
continual learning framework based on Graph Attention Networks (GATs), which
effectively capture contextual relationships and dynamically update the
task-specific representation via learned attention weights. Our approach
utilizes a pre-trained feature extractor to convert images into graphs using
hierarchical feature maps, representing information at varying levels of
granularity. These graphs are then processed by a GAT and incorporate an
enhanced global pooling strategy to improve classification performance for
continual learning. In addition, we propose the rehearsal memory duplication
technique that improves the representation of the previous tasks while
maintaining the memory budget. Comprehensive evaluations on benchmark datasets,
including SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the
superiority of our method compared to the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Replay-free Online Continual Learning with Self-Supervised MultiPatches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Cignoni, Andrea Cossu, Alex Gomez-Villa, Joost van de Weijer, Antonio Carta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Continual Learning (OCL) methods train a model on a non-stationary
data stream where only a few examples are available at a time, often leveraging
replay strategies. However, usage of replay is sometimes forbidden, especially
in applications with strict privacy regulations. Therefore, we propose
Continual MultiPatches (CMP), an effective plug-in for existing OCL
self-supervised learning strategies that avoids the use of replay samples. CMP
generates multiple patches from a single example and projects them into a
shared feature space, where patches coming from the same example are pushed
together without collapsing into a single point. CMP surpasses replay and other
SSL-based strategies on OCL streams, challenging the role of replay as a go-to
solution for self-supervised OCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ESANN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Pruning via Structured Lasso with Class-wise Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Liu, Mingchen Li, Xia Li, Leigang Qu, Zifan Peng, Yijun Song, Zemin Liu, Linshan Jiang, Jialin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most pruning methods concentrate on unimportant filters of neural networks.
However, they face the loss of statistical information due to a lack of
consideration for class-wise data. In this paper, from the perspective of
leveraging precise class-wise information for model pruning, we utilize
structured lasso with guidance from Information Bottleneck theory. Our approach
ensures that statistical information is retained during the pruning process.
With these techniques, we introduce two innovative adaptive network pruning
schemes: sparse graph-structured lasso pruning with Information Bottleneck
(\textbf{sGLP-IB}) and sparse tree-guided lasso pruning with Information
Bottleneck (\textbf{sTLP-IB}). The key aspect is pruning model filters using
sGLP-IB and sTLP-IB to better capture class-wise relatedness. Compared to
multiple state-of-the-art methods, our approaches demonstrate superior
performance across three datasets and six model architectures in extensive
experiments. For instance, using the VGG16 model on the CIFAR-10 dataset, we
achieve a parameter reduction of 85%, a decrease in FLOPs by 61%, and maintain
an accuracy of 94.10% (0.14% higher than the original model); we reduce the
parameters by 55% with the accuracy at 76.12% using the ResNet architecture on
ImageNet (only drops 0.03%). In summary, we successfully reduce model size and
computational resource usage while maintaining accuracy. Our codes are at
https://anonymous.4open.science/r/IJCAI-8104.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Deep Regression with Tightness <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Zhang, Yuguang Yan, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For deep regression, preserving the ordinality of the targets with respect to
the feature representation improves performance across various tasks. However,
a theoretical explanation for the benefits of ordinality is still lacking. This
work reveals that preserving ordinality reduces the conditional entropy
$H(Z|Y)$ of representation $Z$ conditional on the target $Y$. However, our
findings reveal that typical regression losses do little to reduce $H(Z|Y)$,
even though it is vital for generalization performance. With this motivation,
we introduce an optimal transport-based regularizer to preserve the similarity
relationships of targets in the feature space to reduce $H(Z|Y)$. Additionally,
we introduce a simple yet efficient strategy of duplicating the regressor
targets, also with the aim of reducing $H(Z|Y)$. Experiments on three
real-world regression tasks verify the effectiveness of our strategies to
improve deep regression. Code:
https://github.com/needylove/Regression_tightness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025, Code: https://github.com/needylove/Regression_tightness</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DenseSplat: Densifying Gaussian Splatting SLAM with Neural Radiance
  Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Shuhong Liu, Tianchen Deng, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian SLAM systems excel in real-time rendering and fine-grained
reconstruction compared to NeRF-based systems. However, their reliance on
extensive keyframes is impractical for deployment in real-world robotic
systems, which typically operate under sparse-view conditions that can result
in substantial holes in the map. To address these challenges, we introduce
DenseSplat, the first SLAM system that effectively combines the advantages of
NeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for
initializing primitives that densely populate maps and seamlessly fill gaps. It
also implements geometry-aware primitive sampling and pruning strategies to
manage granularity and enhance rendering efficiency. Moreover, DenseSplat
integrates loop closure and bundle adjustment, significantly enhancing
frame-to-frame tracking accuracy. Extensive experiments on multiple large-scale
datasets demonstrate that DenseSplat achieves superior performance in tracking
and mapping compared to current state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pulling Back the Curtain: Unsupervised Adversarial Detection via
  Contrastive Auxiliary Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eylon Mizrahi, Raz Lapid, Moshe Sipper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are widely employed in safety-critical applications yet
remain susceptible to adversarial attacks -- imperceptible perturbations that
can significantly degrade model performance. Conventional defense mechanisms
predominantly focus on either enhancing model robustness or detecting
adversarial inputs independently. In this work, we propose an Unsupervised
adversarial detection via Contrastive Auxiliary Networks (U-CAN) to uncover
adversarial behavior within auxiliary feature representations, without the need
for adversarial examples. U-CAN is embedded within selected intermediate layers
of the target model. These auxiliary networks, comprising projection layers and
ArcFace-based linear layers, refine feature representations to more effectively
distinguish between benign and adversarial inputs. Comprehensive experiments
across multiple datasets (CIFAR-10, Mammals, and a subset of ImageNet) and
architectures (ResNet-50, VGG-16, and ViT) demonstrate that our method
surpasses existing unsupervised adversarial detection techniques, achieving
superior F1 scores against four distinct attack methods. The proposed framework
provides a scalable and effective solution for enhancing the security and
reliability of deep learning systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Visuals to Vocabulary: Establishing Equivalence Between Image and
  Text Token Through Autoregressive Pre-training in MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxiao Li, Fang Qu, Zhanpeng Chen, Na Su, Zhizhou Zhong, Ziyang Chen, Nan Du, Xiaolong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While MLLMs perform well on perceptual tasks, they lack precise multimodal
alignment, limiting performance. To address this challenge, we propose Vision
Dynamic Embedding-Guided Pretraining (VDEP), a hybrid autoregressive training
paradigm for MLLMs. Utilizing dynamic embeddings from the MLP following the
visual encoder, this approach supervises image hidden states and integrates
image tokens into autoregressive training. Existing MLLMs primarily focused on
recovering information from textual inputs, often neglecting the effective
processing of image data. In contrast, the key improvement of this work is the
reinterpretation of multimodal alignment as a process of recovering information
from input data, with particular emphasis on reconstructing detailed visual
features.The proposed method seamlessly integrates into standard models without
architectural changes. Experiments on 13 benchmarks show VDEP outperforms
baselines, surpassing existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Anomaly Detection on Implicit Shape representations for
  Sarcopenia Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louise Piecuch, Jeremie Huet, Antoine Frouin, Antoine Nordez, Anne-Sophie Boureau, Diana Mateus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sarcopenia is an age-related progressive loss of muscle mass and strength
that significantly impacts daily life. A commonly studied criterion for
characterizing the muscle mass has been the combination of 3D imaging and
manual segmentations. In this paper, we instead study the muscles' shape. We
rely on an implicit neural representation (INR) to model normal muscle shapes.
We then introduce an unsupervised anomaly detection method to identify
sarcopenic muscles based on the reconstruction error of the implicit model.
Relying on a conditional INR with an auto-decoding strategy, we also learn a
latent representation of the muscles that clearly separates normal from
abnormal muscles in an unsupervised fashion. Experimental results on a dataset
of 103 segmented volumes indicate that our double anomaly detection strategy
effectively discriminates sarcopenic and non-sarcopenic muscles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian
  Primitives for Weakly-Supervised Cross-View Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiwei Wang, Shaoxun Wu, Yujiao Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of weakly supervised cross-view
localization, where the goal is to estimate the pose of a ground camera
relative to a satellite image with noisy ground truth annotations. A common
approach to bridge the cross-view domain gap for pose estimation is Bird's-Eye
View (BEV) synthesis. However, existing methods struggle with height ambiguity
due to the lack of depth information in ground images and satellite height
maps. Previous solutions either assume a flat ground plane or rely on complex
models, such as cross-view transformers. We propose BevSplat, a novel method
that resolves height ambiguity by using feature-based Gaussian primitives. Each
pixel in the ground image is represented by a 3D Gaussian with semantic and
spatial features, which are synthesized into a BEV feature map for relative
pose estimation. Additionally, to address challenges with panoramic query
images, we introduce an icosphere-based supervision strategy for the Gaussian
primitives. We validate our method on the widely used KITTI and VIGOR datasets,
which include both pinhole and panoramic query images. Experimental results
show that BevSplat significantly improves localization accuracy over prior
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PTZ-Calib: Robust Pan-Tilt-Zoom Camera Calibration <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhui Guo, Lubin Fan, Bojian Wu, Jiaqi Gu, Shen Cao, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present PTZ-Calib, a robust two-stage PTZ camera
calibration method, that efficiently and accurately estimates camera parameters
for arbitrary viewpoints. Our method includes an offline and an online stage.
In the offline stage, we first uniformly select a set of reference images that
sufficiently overlap to encompass a complete 360{\deg} view. We then utilize
the novel PTZ-IBA (PTZ Incremental Bundle Adjustment) algorithm to
automatically calibrate the cameras within a local coordinate system.
Additionally, for practical application, we can further optimize camera
parameters and align them with the geographic coordinate system using extra
global reference 3D information. In the online stage, we formulate the
calibration of any new viewpoints as a relocalization problem. Our approach
balances the accuracy and computational efficiency to meet real-world demands.
Extensive evaluations demonstrate our robustness and superior performance over
state-of-the-art methods on various real and synthetic datasets. Datasets and
source code can be accessed online at https://github.com/gjgjh/PTZ-Calib
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleBlend: Enhancing Style-Specific Content Creation in Text-to-Image
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichong Chen, Shijin Wang, Yang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing visually impressive images that seamlessly align both text
prompts and specific artistic styles remains a significant challenge in
Text-to-Image (T2I) diffusion models. This paper introduces StyleBlend, a
method designed to learn and apply style representations from a limited set of
reference images, enabling content synthesis of both text-aligned and
stylistically coherent. Our approach uniquely decomposes style into two
components, composition and texture, each learned through different strategies.
We then leverage two synthesis branches, each focusing on a corresponding style
component, to facilitate effective style blending through shared features
without affecting content generation. StyleBlend addresses the common issues of
text misalignment and weak style representation that previous methods have
struggled with. Extensive qualitative and quantitative comparisons demonstrate
the superiority of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Eurographics 2025. Project page:
  https://zichongc.github.io/StyleBlend/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language In-Context Learning Driven Few-Shot Visual Inspection
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiryu Ueno, Yoshikazu Hayashi, Shunsuke Nakatsuka, Yusei Yamada, Hiroaki Aizawa, Kunihito Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose general visual inspection model using Vision-Language Model~(VLM)
with few-shot images of non-defective or defective products, along with
explanatory texts that serve as inspection criteria. Although existing VLM
exhibit high performance across various tasks, they are not trained on specific
tasks such as visual inspection. Thus, we construct a dataset consisting of
diverse images of non-defective and defective products collected from the web,
along with unified formatted output text, and fine-tune VLM. For new products,
our method employs In-Context Learning, which allows the model to perform
inspections with an example of non-defective or defective image and the
corresponding explanatory texts with visual prompts. This approach eliminates
the need to collect a large number of training samples and re-train the model
for each product. The experimental results show that our method achieves high
performance, with MCC of 0.804 and F1-score of 0.950 on MVTec AD in a one-shot
manner. Our code is available
at~https://github.com/ia-gu/Vision-Language-In-Context-Learning-Driven-Few-Shot-Visual-Inspection-Model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VISAPP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIDE: Agentically Improve Visual Language Model with Domain Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming-Chang Chiu, Fuxiao Liu, Karan Sapra, Andrew Tao, Yaser Jacoob, Xuezhe Ma, Zhiding Yu, Guilin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The enhancement of Visual Language Models (VLMs) has traditionally relied on
knowledge distillation from larger, more capable models. This dependence
creates a fundamental bottleneck for improving state-of-the-art systems,
particularly when no superior models exist. We introduce AIDE (Agentic
Improvement through Domain Experts), a novel framework that enables VLMs to
autonomously enhance their capabilities by leveraging specialized domain expert
models. AIDE operates through a four-stage process: (1) identifying instances
for refinement, (2) engaging domain experts for targeted analysis, (3)
synthesizing expert outputs with existing data, and (4) integrating enhanced
instances into the training pipeline. Experiments on multiple benchmarks,
including MMMU, MME, MMBench, etc., demonstrate AIDE's ability to achieve
notable performance gains without relying on larger VLMs nor human supervision.
Our framework provides a scalable, resource-efficient approach to continuous
VLM improvement, addressing critical limitations in current methodologies,
particularly valuable when larger models are unavailable to access.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evolution of Data-driven Single- and Multi-Hazard Susceptibility Mapping
  and Emergence of Deep Learning Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaya Sreevalsan-Nair, Aswathi Mundayatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven susceptibility mapping of natural hazards has harnessed the
advances in classification methods used on heterogeneous sources represented as
raster images. Susceptibility mapping is an important step towards risk
assessment for any natural hazard. Increasingly, multiple hazards co-occur
spatially, temporally, or both, which calls for an in-depth study on
multi-hazard susceptibility mapping. In recent years, single-hazard
susceptibility mapping algorithms have become well-established and have been
extended to multi-hazard susceptibility mapping. Deep learning is also emerging
as a promising method for single-hazard susceptibility mapping. Here, we
discuss the evolution of methods for a single hazard, their extensions to
multi-hazard maps as a late fusion of decisions, and the use of deep learning
methods in susceptibility mapping. We finally propose a vision for adapting
data fusion strategies in multimodal deep learning to multi-hazard
susceptibility mapping. From the background study of susceptibility methods, we
demonstrate that deep learning models are promising, untapped methods for
multi-hazard susceptibility mapping. Data fusion strategies provide a larger
space of deep learning models applicable to multi-hazard susceptibility
mapping.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Images are Gaussians: High-Quality Large Image Representation with
  Levels of 2D Gaussian Splatting <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingting Zhu, Guying Lin, Jinnan Chen, Xinjie Zhang, Zhenchao Jin, Zhao Wang, Lequan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Implicit Neural Representations (INRs) have demonstrated significant
success in image representation, they are often hindered by large training
memory and slow decoding speed. Recently, Gaussian Splatting (GS) has emerged
as a promising solution in 3D reconstruction due to its high-quality novel view
synthesis and rapid rendering capabilities, positioning it as a valuable tool
for a broad spectrum of applications. In particular, a GS-based representation,
2DGS, has shown potential for image fitting. In our work, we present
\textbf{L}arge \textbf{I}mages are \textbf{G}aussians (\textbf{LIG}), which
delves deeper into the application of 2DGS for image representations,
addressing the challenge of fitting large images with 2DGS in the situation of
numerous Gaussian points, through two distinct modifications: 1) we adopt a
variant of representation and optimization strategy, facilitating the fitting
of a large number of Gaussian points; 2) we propose a Level-of-Gaussian
approach for reconstructing both coarse low-frequency initialization and fine
high-frequency details. Consequently, we successfully represent large images as
Gaussian points and achieve high-quality large image representation,
demonstrating its efficacy across various types of large images. Code is
available at
{\href{https://github.com/HKU-MedAI/LIG}{https://github.com/HKU-MedAI/LIG}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 39th Annual AAAI Conference on Artificial Intelligence
  (AAAI 2025). 10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Billet Number Recognition Based on Test-Time Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Wei, Xiuzhuang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the steel billet production process, it is essential to recognize
machine-printed or manually written billet numbers on moving billets in
real-time. To address the issue of low recognition accuracy for existing scene
text recognition methods, caused by factors such as image distortions and
distribution differences between training and test data, we propose a billet
number recognition method that integrates test-time adaptation with prior
knowledge. First, we introduce a test-time adaptation method into a model that
uses the DB network for text detection and the SVTR network for text
recognition. By minimizing the model's entropy during the testing phase, the
model can adapt to the distribution of test data without the need for
supervised fine-tuning. Second, we leverage the billet number encoding rules as
prior knowledge to assess the validity of each recognition result. Invalid
results, which do not comply with the encoding rules, are replaced. Finally, we
introduce a validation mechanism into the CTC algorithm using prior knowledge
to address its limitations in recognizing damaged characters. Experimental
results on real datasets, including both machine-printed billet numbers and
handwritten billet numbers, show significant improvements in evaluation
metrics, validating the effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EventSTR: A Benchmark <span class="highlight-title">Dataset</span> and Baselines for Event Stream based Scene
  Text Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Wang, Jingtao Jiang, Dong Li, Futian Wang, Lin Zhu, Yaowei Wang, Yongyong Tian, Jin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mainstream Scene Text Recognition (STR) algorithms are developed based on RGB
cameras which are sensitive to challenging factors such as low illumination,
motion blur, and cluttered backgrounds. In this paper, we propose to recognize
the scene text using bio-inspired event cameras by collecting and annotating a
large-scale benchmark dataset, termed EventSTR. It contains 9,928
high-definition (1280 * 720) event samples and involves both Chinese and
English characters. We also benchmark multiple STR algorithms as the baselines
for future works to compare. In addition, we propose a new event-based scene
text recognition framework, termed SimC-ESTR. It first extracts the event
features using a visual encoder and projects them into tokens using a Q-former
module. More importantly, we propose to augment the vision tokens based on a
memory mechanism before feeding into the large language models. A
similarity-based error correction mechanism is embedded within the large
language model to correct potential minor errors fundamentally based on
contextual information. Extensive experiments on the newly proposed EventSTR
dataset and two simulation STR datasets fully demonstrate the effectiveness of
our proposed model. We believe that the dataset and algorithmic model can
innovatively propose an event-based STR task and are expected to accelerate the
application of event cameras in various industries. The source code and
pre-trained models will be released on https://github.com/Event-AHU/EventSTR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Peer Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Concept Bottleneck Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shin'ya Yamaguchi, Kosuke Nishida, Daiki Chijiwa, Yasutoshi Ida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept bottleneck models (CBMs) are inherently interpretable and
intervenable neural network models, which explain their final label prediction
by the intermediate prediction of high-level semantic concepts. However, they
require target task training to learn input-to-concept and concept-to-label
mappings, incurring target dataset collections and training resources. In this
paper, we present \textit{zero-shot concept bottleneck models} (Z-CBMs), which
predict concepts and labels in a fully zero-shot manner without training neural
networks. Z-CBMs utilize a large-scale concept bank, which is composed of
millions of vocabulary extracted from the web, to describe arbitrary input in
various domains. For the input-to-concept mapping, we introduce concept
retrieval, which dynamically finds input-related concepts by the cross-modal
search on the concept bank. In the concept-to-label inference, we apply concept
regression to select essential concepts from the retrieved concepts by sparse
linear regression. Through extensive experiments, we confirm that our Z-CBMs
provide interpretable and intervenable concepts without any additional
training. Code will be available at https://github.com/yshinya6/zcbm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Residual Transformer Fusion Network for Salt and Pepper Image Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bintang Pradana Erlangga Putra, Heri Prasetyo, Esti Suryani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Network (CNN) has been widely used in unstructured
datasets, one of which is image denoising. Image denoising is a noisy image
reconstruction process that aims to reduce additional noise that occurs from
the noisy image with various strategies. Image denoising has a problem, namely
that some image denoising methods require some prior knowledge of information
about noise. To overcome this problem, a combined architecture of Convolutional
Vision Transformer (CvT) and Residual Networks (ResNet) is used which is called
the Residual Transformer Fusion Network (RTF-Net). In general, the process in
this architecture can be divided into two parts, Noise Suppression Network
(NSN) and Structure Enhancement Network (SEN). Residual Block is used in the
Noise Suppression Network and is used to learn the noise map in the image,
while the CvT is used in the Structure Enhancement Network and is used to learn
the details that need to be added to the image processed by the Noise
Suppression Network. The model was trained using the DIV2K Training Set
dataset, and validation using the DIV2K Validation Set. After doing the
training, the model was tested using Lena, Bridge, Pepper, and BSD300 images
with noise levels ranging from 30%, 50%, and 70% and the PSNR results were
compared with the DBA, NASNLM, PARIGI, NLSF, NLSF-MLP and NLSF-CNN methods. The
test results show that the proposed method is superior in all cases except for
Pepper's image with a noise level of 30%, where NLSF-CNN is superior with a
PSNR value of 32.99 dB, while the proposed method gets a PSNR value of 31.70
dB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Vision Transformer with Prototypes for Interpretable
  Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luisa Gallée, Catharina Silvia Lisson, Meinrad Beer, Michael Götz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability is a highly demanded requirement for applications in high-risk
areas such as medicine. Vision Transformers have mainly been limited to
attention extraction to provide insight into the model's reasoning. Our
approach combines the high performance of Vision Transformers with the
introduction of new explainability capabilities. We present HierViT, a Vision
Transformer that is inherently interpretable and adapts its reasoning to that
of humans. A hierarchical structure is used to process domain-specific features
for prediction. It is interpretable by design, as it derives the target output
with human-defined features that are visualized by exemplary images
(prototypes). By incorporating domain knowledge about these decisive features,
the reasoning is semantically similar to human reasoning and therefore
intuitive. Moreover, attention heatmaps visualize the crucial regions for
identifying each feature, thereby providing HierViT with a versatile tool for
validating predictions. Evaluated on two medical benchmark datasets, LIDC-IDRI
for lung nodule assessment and derm7pt for skin lesion classification, HierViT
achieves superior and comparable prediction accuracy, respectively, while
offering explanations that align with human reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latents of latents to delineate pixels: hybrid Matryoshka
  autoencoder-to-U-Net pairing for segmenting large medical images in GPU-poor
  and low-data regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tahir Syed, Ariba Khan, Sawera Hanif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images are often high-resolution and lose important detail if
downsampled, making pixel-level methods such as semantic segmentation much less
efficient if performed on a low-dimensional image. We propose a low-rank
Matryoshka projection and a hybrid segmenting architecture that preserves
important information while retaining sufficient pixel geometry for pixel-level
tasks. We design the Matryoshka Autoencoder (MatAE-U-Net) which combines the
hierarchical encoding of the Matryoshka Autoencoder with the spatial
reconstruction capabilities of a U-Net decoder, leveraging multi-scale feature
extraction and skip connections to enhance accuracy and generalisation. We
apply it to the problem of segmenting the left ventricle (LV) in
echocardiographic images using the Stanford EchoNet-D dataset, including 1,000
standardised video-mask pairs of cardiac ultrasound videos resized to 112x112
pixels. The MatAE-UNet model achieves a Mean IoU of 77.68\%, Mean Pixel
Accuracy of 97.46\%, and Dice Coefficient of 86.91\%, outperforming the
baseline U-Net, which attains a Mean IoU of 74.70\%, Mean Pixel Accuracy of
97.31\%, and Dice Coefficient of 85.20\%. The results highlight the potential
of using the U-Net in the recursive Matroshka latent space for imaging problems
with low-contrast such as echocardiographic analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-driven 3D <span class="highlight-title">Human</span> Generation via Contrastive Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Zhou, Xukun Shen, Yong Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Score Distillation Sampling (SDS) have improved 3D human
generation from textual descriptions. However, existing methods still face
challenges in accurately aligning 3D models with long and complex textual
inputs. To address this challenge, we propose a novel framework that introduces
contrastive preferences, where human-level preference models, guided by both
positive and negative prompts, assist SDS for improved alignment. Specifically,
we design a preference optimization module that integrates multiple models to
comprehensively capture the full range of textual features. Furthermore, we
introduce a negation preference module to mitigate over-optimization of
irrelevant details by leveraging static-dynamic negation prompts, effectively
preventing ``reward hacking". Extensive experiments demonstrate that our method
achieves state-of-the-art results, significantly enhancing texture realism and
visual alignment with textual descriptions, particularly for long and complex
inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topo2Seq: Enhanced Topology Reasoning via Topology Sequence Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Yang, Yueru Luo, Bingkun He, Erlong Li, Zhipeng Cao, Chao Zheng, Shuqi Mei, Zhen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting lane topology from perspective views (PV) is crucial for planning
and control in autonomous driving. This approach extracts potential drivable
trajectories for self-driving vehicles without relying on high-definition (HD)
maps. However, the unordered nature and weak long-range perception of the
DETR-like framework can result in misaligned segment endpoints and limited
topological prediction capabilities. Inspired by the learning of contextual
relationships in language models, the connectivity relations in roads can be
characterized as explicit topology sequences. In this paper, we introduce
Topo2Seq, a novel approach for enhancing topology reasoning via topology
sequences learning. The core concept of Topo2Seq is a randomized order
prompt-to-sequence learning between lane segment decoder and topology sequence
decoder. The dual-decoder branches simultaneously learn the lane topology
sequences extracted from the Directed Acyclic Graph (DAG) and the lane graph
containing geometric information. Randomized order prompt-to-sequence learning
extracts unordered key points from the lane graph predicted by the lane segment
decoder, which are then fed into the prompt design of the topology sequence
decoder to reconstruct an ordered and complete lane graph. In this way, the
lane segment decoder learns powerful long-range perception and accurate
topological reasoning from the topology sequence decoder. Notably, topology
sequence decoder is only introduced during training and does not affect the
inference efficiency. Experimental evaluations on the OpenLane-V2 dataset
demonstrate the state-of-the-art performance of Topo2Seq in topology reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of
  Physical Concept Understanding <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mo Yu, Lemao Liu, Junjie Wu, Tsz Ting Chung, Shunchi Zhang, Jiangnan Li, Dit-Yan Yeung, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a systematic way, we investigate a widely asked question: Do LLMs really
understand what they say?, which relates to the more familiar term Stochastic
Parrot. To this end, we propose a summative assessment over a carefully
designed physical concept understanding task, PhysiCo. Our task alleviates the
memorization issue via the usage of grid-format inputs that abstractly describe
physical phenomena. The grids represents varying levels of understanding, from
the core phenomenon, application examples to analogies to other abstract
patterns in the grid world. A comprehensive study on our task demonstrates: (1)
state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag
behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,
as they fail on our grid task but can describe and recognize the same concepts
well in natural language; (3) our task challenges the LLMs due to intrinsic
difficulties rather than the unfamiliar grid format, as in-context learning and
fine-tuning on same formatted data added little to their performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Main Conference. First 5 authors contributed equally.
  Project page: https://physico-benchmark.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding Why Data Augmentation Improves Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Li, Jiachun Pan, Kim-Chuan Toh, Pan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is a cornerstone technique in deep learning, widely used to
improve model generalization. Traditional methods like random cropping and
color jittering, as well as advanced techniques such as CutOut, Mixup, and
CutMix, have achieved notable success across various domains. However, the
mechanisms by which data augmentation improves generalization remain poorly
understood, and existing theoretical analyses typically focus on individual
techniques without a unified explanation. In this work, we present a unified
theoretical framework that elucidates how data augmentation enhances
generalization through two key effects: partial semantic feature removal and
feature mixing. Partial semantic feature removal reduces the model's reliance
on individual feature, promoting diverse feature learning and better
generalization. Feature mixing, by scaling down original semantic features and
introducing noise, increases training complexity, driving the model to develop
more robust features. Advanced methods like CutMix integrate both effects,
achieving complementary benefits. Our theoretical insights are further
supported by experimental results, validating the effectiveness of this unified
perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Promise for Assurance of Differentiable Neurosymbolic Reasoning
  Paradigms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke E. Richards, Jessie Yaros, Jasen Babcock, Coung Ly, Robin Cosbey, Timothy Doster, Cynthia Matuszek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To create usable and deployable Artificial Intelligence (AI) systems, there
requires a level of assurance in performance under many different conditions.
Many times, deployed machine learning systems will require more classic logic
and reasoning performed through neurosymbolic programs jointly with artificial
neural network sensing. While many prior works have examined the assurance of a
single component of the system solely with either the neural network alone or
entire enterprise systems, very few works have examined the assurance of
integrated neurosymbolic systems. Within this work, we assess the assurance of
end-to-end fully differentiable neurosymbolic systems that are an emerging
method to create data-efficient and more interpretable models. We perform this
investigation using Scallop, an end-to-end neurosymbolic library, across
classification and reasoning tasks in both the image and audio domains. We
assess assurance across adversarial robustness, calibration, user performance
parity, and interpretability of solutions for catching misaligned solutions. We
find end-to-end neurosymbolic methods present unique opportunities for
assurance beyond their data efficiency through our empirical results but not
across the board. We find that this class of neurosymbolic models has higher
assurance in cases where arithmetic operations are defined and where there is
high dimensionality to the input space, where fully neural counterparts
struggle to learn robust reasoning operations. We identify the relationship
between neurosymbolic models' interpretability to catch shortcuts that later
result in increased adversarial vulnerability despite performance parity.
Finally, we find that the promise of data efficiency is typically only in the
case of class imbalanced reasoning problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic watermarks in images generated by diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhuo Chen, Naveed Akhtar, Nur Al Hasan Haldar, Ajmal Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-fidelity text-to-image diffusion models have revolutionized visual
content generation, but their widespread use raises significant ethical
concerns, including intellectual property protection and the misuse of
synthetic media. To address these challenges, we propose a novel multi-stage
watermarking framework for diffusion models, designed to establish copyright
and trace generated images back to their source. Our multi-stage watermarking
technique involves embedding: (i) a fixed watermark that is localized in the
diffusion model's learned noise distribution and, (ii) a human-imperceptible,
dynamic watermark in generates images, leveraging a fine-tuned decoder. By
leveraging the Structural Similarity Index Measure (SSIM) and cosine
similarity, we adapt the watermark's shape and color to the generated content
while maintaining robustness. We demonstrate that our method enables reliable
source verification through watermark classification, even when the dynamic
watermark is adjusted for content-specific variations. Source model
verification is enabled through watermark classification. o support further
research, we generate a dataset of watermarked images and introduce a
methodology to evaluate the statistical impact of watermarking on generated
content.Additionally, we rigorously test our framework against various attack
scenarios, demonstrating its robustness and minimal impact on image quality.
Our work advances the field of AI-generated content security by providing a
scalable solution for model ownership verification and misuse prevention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Malicious Concepts Without Image Generation in AIGC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Xu, Yushu Zhang, Shuren Qi, Tao Wang, Wenying Wen, Yuming Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text-to-image generation has achieved tremendous success in
practice, with emerging concept generation models capable of producing highly
personalized and customized content. Fervor for concept generation is
increasing rapidly among users, and platforms for concept sharing have sprung
up. The concept owners may upload malicious concepts and disguise them with
non-malicious text descriptions and example images to deceive users into
downloading and generating malicious content. The platform needs a quick method
to determine whether a concept is malicious to prevent the spread of malicious
concepts. However, simply relying on concept image generation to judge whether
a concept is malicious requires time and computational resources. Especially,
as the number of concepts uploaded and downloaded on the platform continues to
increase, this approach becomes impractical and poses a risk of generating
malicious content. In this paper, we propose Concept QuickLook, the first
systematic work to incorporate malicious concept detection into research, which
performs detection based solely on concept files without generating any images.
We define malicious concepts and design two work modes for detection: concept
matching and fuzzy detection. Extensive experiments demonstrate that the
proposed Concept QuickLook can detect malicious concepts and demonstrate
practicality in concept sharing platforms. We also design robustness
experiments to further validate the effectiveness of the solution. We hope this
work can initiate malicious concept detection tasks and provide some
inspiration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic
  Decision-Making Applied to Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Ghezloo, Mehmet Saygin Seyfioglu, Rustin Soraki, Wisdom O. Ikezogwo, Beibin Li, Tejoram Vivekanandan, Joann G. Elmore, Ranjay Krishna, Linda Shapiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diagnosing diseases through histopathology whole slide images (WSIs) is
fundamental in modern pathology but is challenged by the gigapixel scale and
complexity of WSIs. Trained histopathologists overcome this challenge by
navigating the WSI, looking for relevant patches, taking notes, and compiling
them to produce a final holistic diagnostic. Traditional AI approaches, such as
multiple instance learning and transformer-based models, fail short of such a
holistic, iterative, multi-scale diagnostic procedure, limiting their adoption
in the real-world. We introduce PathFinder, a multi-modal, multi-agent
framework that emulates the decision-making process of expert pathologists.
PathFinder integrates four AI agents, the Triage Agent, Navigation Agent,
Description Agent, and Diagnosis Agent, that collaboratively navigate WSIs,
gather evidence, and provide comprehensive diagnoses with natural language
explanations. The Triage Agent classifies the WSI as benign or risky; if risky,
the Navigation and Description Agents iteratively focus on significant regions,
generating importance maps and descriptive insights of sampled patches.
Finally, the Diagnosis Agent synthesizes the findings to determine the
patient's diagnostic classification. Our Experiments show that PathFinder
outperforms state-of-the-art methods in skin melanoma diagnosis by 8% while
offering inherent explainability through natural language descriptions of
diagnostically relevant patches. Qualitative analysis by pathologists shows
that the Description Agent's outputs are of high quality and comparable to
GPT-4o. PathFinder is also the first AI-based system to surpass the average
performance of pathologists in this challenging melanoma classification task by
9%, setting a new record for efficient, accurate, and interpretable AI-assisted
diagnostics in pathology. Data, code and models available at
https://pathfinder-dx.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models Through a Global Lens: Are They Culturally Inclusive? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Bayramli, Ayhan Suleymanzade, Na Min An, Huzama Ahmad, Eunsu Kim, Junyeong Park, James Thorne, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently enabled the creation of visually
compelling, detailed images from textual prompts. However, their ability to
accurately represent various cultural nuances remains an open question. In our
work, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion
models whether they can generate culturally specific images spanning ten
countries. We show that these models often fail to generate cultural artifacts
in architecture, clothing, and food, especially for underrepresented country
regions, by conducting a fine-grained analysis of different similarity aspects,
revealing significant disparities in cultural relevance, description fidelity,
and realism compared to real-world reference images. With the collected human
evaluations, we develop a neural-based image-image similarity metric, namely,
CultDiff-S, to predict human judgment on real and generated images with
cultural artifacts. Our work highlights the need for more inclusive generative
AI systems and equitable dataset representation over a wide range of cultures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 17 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffoRA: Enabling Parameter-Efficient LLM Fine-Tuning via Differential
  Low-Rank Matrix Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tangyu Jiang, Haodi Wang, Chun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Parameter-Efficient Fine-Tuning (PEFT) methods have been extensively
researched for large language models in the downstream tasks. Among all the
existing approaches, the Low-Rank Adaptation (LoRA) has gained popularity for
its streamlined design by incorporating low-rank matrices into existing
pre-trained models. Though effective, LoRA allocates every module an identical
low-rank matrix, which ignores the varying properties and contributions across
different components. Moreover, the existing adaptive LoRA solutions rely
highly on intuitive importance scoring indicators to adjust the interior rank
of the decomposition matrices. In this paper, we propose a new PEFT scheme
called DiffoRA, which is theoretically grounded and enables module-wise
adoption of LoRA. At the core of our DiffoRA lies a Differential Adaptation
Matrix (DAM) to determine which module is the most suitable and essential for
fine-tuning. We explain how the designed matrix impacts the convergence rate
and generalization capability of a pre-trained model. Furthermore, we construct
the DAM via continuous relaxation and discretization with weight-sharing
optimizations. We fully implement our DiffoRA and design comprehensive
experiments to evaluate its performance. The experimental results demonstrate
that our approach achieves the best model accuracy over all the
state-of-the-art baselines across various benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoL3D: Collaborative Learning of Single-view Depth and Camera Intrinsics
  for Metric 3D Shape Recovery <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghao Zhang, Lubin Fan, Shen Cao, Bojian Wu, Jieping Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering the metric 3D shape from a single image is particularly relevant
for robotics and embodied intelligence applications, where accurate spatial
understanding is crucial for navigation and interaction with environments.
Usually, the mainstream approaches achieve it through monocular depth
estimation. However, without camera intrinsics, the 3D metric shape can not be
recovered from depth alone. In this study, we theoretically demonstrate that
depth serves as a 3D prior constraint for estimating camera intrinsics and
uncover the reciprocal relations between these two elements. Motivated by this,
we propose a collaborative learning framework for jointly estimating depth and
camera intrinsics, named CoL3D, to learn metric 3D shapes from single images.
Specifically, CoL3D adopts a unified network and performs collaborative
optimization at three levels: depth, camera intrinsics, and 3D point clouds.
For camera intrinsics, we design a canonical incidence field mechanism as a
prior that enables the model to learn the residual incident field for enhanced
calibration. Additionally, we incorporate a shape similarity measurement loss
in the point cloud space, which improves the quality of 3D shapes essential for
robotic applications. As a result, when training and testing on a single
dataset with in-domain settings, CoL3D delivers outstanding performance in both
depth estimation and camera calibration across several indoor and outdoor
benchmark datasets, which leads to remarkable 3D shape quality for the
perception capabilities of robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShapeLib: designing a library of procedural 3D shape abstractions with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Kenny Jones, Paul Guerrero, Niloy J. Mitra, Daniel Ritchie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural representations are desirable, versatile, and popular shape
encodings. Authoring them, either manually or using data-driven procedures,
remains challenging, as a well-designed procedural representation should be
compact, intuitive, and easy to manipulate. A long-standing problem in shape
analysis studies how to discover a reusable library of procedural functions,
with semantically aligned exposed parameters, that can explain an entire shape
family. We present ShapeLib as the first method that leverages the priors of
frontier LLMs to design a library of 3D shape abstraction functions. Our system
accepts two forms of design intent: text descriptions of functions to include
in the library and a seed set of exemplar shapes. We discover procedural
abstractions that match this design intent by proposing, and then validating,
function applications and implementations. The discovered shape functions in
the library are not only expressive but also generalize beyond the seed set to
a full family of shapes. We train a recognition network that learns to infer
shape programs based on our library from different visual modalities
(primitives, voxels, point clouds). Our shape functions have parameters that
are semantically interpretable and can be modified to produce plausible shape
variations. We show that this allows inferred programs to be successfully
manipulated by an LLM given a text prompt. We evaluate ShapeLib on different
datasets and show clear advantages over existing methods and alternative
formulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Vision Models for Time Series Analysis: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingchao Ni, Ziming Zhao, ChengAo Shen, Hanghang Tong, Dongjin Song, Wei Cheng, Dongsheng Luo, Haifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series analysis has witnessed the inspiring development from traditional
autoregressive models, deep learning models, to recent Transformers and Large
Language Models (LLMs). Efforts in leveraging vision models for time series
analysis have also been made along the way but are less visible to the
community due to the predominant research on sequence modeling in this domain.
However, the discrepancy between continuous time series and the discrete token
space of LLMs, and the challenges in explicitly modeling the correlations of
variates in multivariate time series have shifted some research attentions to
the equally successful Large Vision Models (LVMs) and Vision Language Models
(VLMs). To fill the blank in the existing literature, this survey discusses the
advantages of vision models over LLMs in time series analysis. It provides a
comprehensive and in-depth overview of the existing methods, with dual views of
detailed taxonomy that answer the key research questions including how to
encode time series as images and how to model the imaged time series for
various tasks. Additionally, we address the challenges in the pre- and
post-processing steps involved in this framework and outline future directions
to further advance time series analysis with vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Opening Articulated Objects in the Real World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What does it take to build mobile manipulation systems that can competently
operate on previously unseen objects in previously unseen environments? This
work answers this question using opening of articulated objects as a mobile
manipulation testbed. Specifically, our focus is on the end-to-end performance
on this task without any privileged information, i.e. the robot starts at a
location with the novel target articulated object in view, and has to approach
the object and successfully open it. We first develop a system for this task,
and then conduct 100+ end-to-end system tests across 13 real world test sites.
Our large-scale study reveals a number of surprising findings: a) modular
systems outperform end-to-end learned systems for this task, even when the
end-to-end learned systems are trained on 1000+ demonstrations, b) perception,
and not precise end-effector control, is the primary bottleneck to task
success, and c) state-of-the-art articulation parameter estimation models
developed in isolation struggle when faced with robot-centric viewpoints.
Overall, our findings highlight the limitations of developing components of the
pipeline in isolation and underscore the need for system-level research,
providing a pragmatic roadmap for building generalizable mobile manipulation
systems. Videos, code, and models are available on the project website:
https://arjung128.github.io/opening-articulated-objects/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage:
  https://arjung128.github.io/opening-articulated-objects/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heuristical Comparison of Vision Transformers Against Convolutional
  Neural Networks for Semantic Segmentation on Remote Sensing Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashim Dahal, Saydul Akbar Murad, Nick Rahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViT) have recently brought a new wave of research in the
field of computer vision. These models have performed particularly well in
image classification and segmentation. Research on semantic and instance
segmentation has accelerated with the introduction of the new architecture,
with over 80% of the top 20 benchmarks for the iSAID dataset based on either
the ViT architecture or the attention mechanism behind its success. This paper
focuses on the heuristic comparison of three key factors of using (or not
using) ViT for semantic segmentation of remote sensing aerial images on the
iSAID dataset. The experimental results observed during this research were
analyzed based on three objectives. First, we studied the use of a weighted
fused loss function to maximize the mean Intersection over Union (mIoU) score
and Dice score while minimizing entropy or class representation loss. Second,
we compared transfer learning on Meta's MaskFormer, a ViT-based semantic
segmentation model, against a generic UNet Convolutional Neural Network (CNN)
based on mIoU, Dice scores, training efficiency, and inference time. Third, we
examined the trade-offs between the two models in comparison to current
state-of-the-art segmentation models. We show that the novel combined weighted
loss function significantly boosts the CNN model's performance compared to
transfer learning with ViT. The code for this implementation can be found at:
https://github.com/ashimdahal/ViT-vs-CNN-Image-Segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of
  Images and <span class="highlight-title">Video</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, Ming-Hsuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents Sa2VA, the first unified model for dense grounded
understanding of both images and videos. Unlike existing multi-modal large
language models, which are often limited to specific modalities and tasks,
Sa2VA supports a wide range of image and video tasks, including referring
segmentation and conversation, with minimal one-shot instruction tuning. Sa2VA
combines SAM-2, a foundation video segmentation model, with LLaVA, an advanced
vision-language model, and unifies text, image, and video into a shared LLM
token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2
in producing precise masks, enabling a grounded, multi-modal understanding of
both static and dynamic visual content. Additionally, we introduce Ref-SAV, an
auto-labeled dataset containing over 72k object expressions in complex video
scenes, designed to boost model performance. We also manually validate 2k video
objects in the Ref-SAV datasets to benchmark referring video object
segmentation in complex environments. Experiments show that Sa2VA achieves
state-of-the-art across multiple tasks, particularly in referring video object
segmentation, highlighting its potential for complex real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lxtgh.github.io/project/sa2va</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for
  Remote Sensing Community 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiancheng Pan, Yanxing Liu, Yuqian Fu, Muyuan Ma, Jiahao Li, Danda Pani Paudel, Luc Van Gool, Xiaomeng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection, particularly open-vocabulary object detection, plays a
crucial role in Earth sciences, such as environmental monitoring, natural
disaster assessment, and land-use planning. However, existing open-vocabulary
detectors, primarily trained on natural-world images, struggle to generalize to
remote sensing images due to a significant data domain gap. Thus, this paper
aims to advance the development of open-vocabulary object detection in remote
sensing community. To achieve this, we first reformulate the task as Locate
Anything on Earth (LAE) with the goal of detecting any novel concepts on Earth.
We then developed the LAE-Label Engine which collects, auto-annotates, and
unifies up to 10 remote sensing datasets creating the LAE-1M - the first
large-scale remote sensing object detection dataset with broad category
coverage. Using the LAE-1M, we further propose and train the novel LAE-DINO
Model, the first open-vocabulary foundation object detector for the LAE task,
featuring Dynamic Vocabulary Construction (DVC) and Visual-Guided Text Prompt
Learning (VisGT) modules. DVC dynamically constructs vocabulary for each
training batch, while VisGT maps visual features to semantic space, enhancing
text features. We comprehensively conduct experiments on established remote
sensing benchmark DIOR, DOTAv2.0, as well as our newly introduced 80-class
LAE-80C benchmark. Results demonstrate the advantages of the LAE-1M dataset and
the effectiveness of the LAE-DINO method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArthroPhase: A Novel <span class="highlight-title">Dataset</span> and Method for Phase Recognition in
  Arthroscopic <span class="highlight-title">Video</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Bahari Malayeri, Matthias Seibold, Nicola Cavalcanti, Jonas Hein, Sascha Jecklin, Lazaros Vlachopoulos, Sandro Fucentese, Sandro Hodel, Philipp Furnstahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to advance surgical phase recognition in arthroscopic
procedures, specifically Anterior Cruciate Ligament (ACL) reconstruction, by
introducing the first arthroscopy dataset and developing a novel
transformer-based model. We aim to establish a benchmark for arthroscopic
surgical phase recognition by leveraging spatio-temporal features to address
the specific challenges of arthroscopic videos including limited field of view,
occlusions, and visual distortions. We developed the ACL27 dataset, comprising
27 videos of ACL surgeries, each labeled with surgical phases. Our model
employs a transformer-based architecture, utilizing temporal-aware frame-wise
feature extraction through a ResNet-50 and transformer layers. This approach
integrates spatio-temporal features and introduces a Surgical Progress Index
(SPI) to quantify surgery progression. The model's performance was evaluated
using accuracy, precision, recall, and Jaccard Index on the ACL27 and Cholec80
datasets. The proposed model achieved an overall accuracy of 72.91% on the
ACL27 dataset. On the Cholec80 dataset, the model achieved a comparable
performance with the state-of-the-art methods with an accuracy of 92.4%. The
SPI demonstrated an output error of 10.6% and 9.86% on ACL27 and Cholec80
datasets respectively, indicating reliable surgery progression estimation. This
study introduces a significant advancement in surgical phase recognition for
arthroscopy, providing a comprehensive dataset and a robust transformer-based
model. The results validate the model's effectiveness and generalizability,
highlighting its potential to improve surgical training, real-time assistance,
and operational efficiency in orthopedic surgery. The publicly available
dataset and code will facilitate future research and development in this
critical field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surface Vision Mamba: Leveraging Bidirectional State Space Model for
  Efficient Spherical Manifold Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14679v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14679v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhao He, Weihao Zheng, Leilei Zhao, Ying Wang, Dalin Zhu, Dan Wu, Bin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based methods have demonstrated exceptional performance in
modelling long-range dependencies on spherical cortical surfaces, surpassing
traditional Geometric Deep Learning (GDL) models. However, their extensive
inference time and high memory demands pose challenges for application to large
datasets with limited computing resources. Inspired by the state space model in
computer vision, we introduce the attention-free Vision Mamba (Vim) to
spherical surfaces, presenting a domain-agnostic architecture for analyzing
data on spherical manifolds. Our method achieves surface patching by
representing spherical data as a sequence of triangular patches derived from a
subdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on
multiple neurodevelopmental phenotype regression tasks using cortical surface
metrics from neonatal brains. Experimental results demonstrate that SiM
outperforms both attention- and GDL-based methods, delivering 4.8 times faster
inference and achieving 91.7% lower memory consumption compared to the Surface
Vision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity
analysis further underscores the potential of SiM to identify subtle cognitive
developmental patterns. The code is available at
https://github.com/Rongzhao-He/surface-vision-mamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sitcom-Crafter: A Plot-Driven <span class="highlight-title">Human</span> <span class="highlight-title">Motion</span> Generation System in 3D
  Scenes <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqi Chen, Panwen Hu, Xiaojun Chang, Zhenwei Shi, Michael Kampffmeyer, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in human motion synthesis have focused on specific types
of motions, such as human-scene interaction, locomotion or human-human
interaction, however, there is a lack of a unified system capable of generating
a diverse combination of motion types. In response, we introduce
Sitcom-Crafter, a comprehensive and extendable system for human motion
generation in 3D space, which can be guided by extensive plot contexts to
enhance workflow efficiency for anime and game designers. The system is
comprised of eight modules, three of which are dedicated to motion generation,
while the remaining five are augmentation modules that ensure consistent fusion
of motion sequences and system functionality. Central to the generation modules
is our novel 3D scene-aware human-human interaction module, which addresses
collision issues by synthesizing implicit 3D Signed Distance Function (SDF)
points around motion spaces, thereby minimizing human-scene collisions without
additional data collection costs. Complementing this, our locomotion and
human-scene interaction modules leverage existing methods to enrich the
system's motion generation capabilities. Augmentation modules encompass plot
comprehension for command generation, motion synchronization for seamless
integration of different motion types, hand pose retrieval to enhance motion
realism, motion collision revision to prevent human collisions, and 3D
retargeting to ensure visual fidelity. Experimental evaluations validate the
system's ability to generate high-quality, diverse, and physically realistic
motions, underscoring its potential for advancing creative workflows. Project
page: https://windvchen.github.io/Sitcom-Crafter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. Project Page:
  https://windvchen.github.io/Sitcom-Crafter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 4-LEGS: 4D Language Embedded Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10719v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10719v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gal Fiebelman, Tamir Cohen, Ayellet Morgenstern, Peter Hedman, Hadar Averbuch-Elor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of neural representations has revolutionized our means for
digitally viewing a wide range of 3D scenes, enabling the synthesis of
photorealistic images rendered from novel views. Recently, several techniques
have been proposed for connecting these low-level representations with the
high-level semantics understanding embodied within the scene. These methods
elevate the rich semantic understanding from 2D imagery to 3D representations,
distilling high-dimensional spatial features onto 3D space. In our work, we are
interested in connecting language with a dynamic modeling of the world. We show
how to lift spatio-temporal features to a 4D representation based on 3D
Gaussian Splatting. This enables an interactive interface where the user can
spatiotemporally localize events in the video from text prompts. We demonstrate
our system on public 3D video datasets of people and animals performing various
actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Eurographics 2025. Project webpage:
  https://tau-vailab.github.io/4-LEGS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Importance of Backbone to the Adversarial Robustness of Object
  Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Li, Hang Chen, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection is a critical component of various security-sensitive
applications, such as autonomous driving and video surveillance. However,
existing object detectors are vulnerable to adversarial attacks, which poses a
significant challenge to their reliability and security. Through experiments,
first, we found that existing works on improving the adversarial robustness of
object detectors give a false sense of security. Second, we found that
adversarially pre-trained backbone networks were essential for enhancing the
adversarial robustness of object detectors. We then proposed a simple yet
effective recipe for fast adversarial fine-tuning on object detectors with
adversarially pre-trained backbones. Without any modifications to the structure
of object detectors, our recipe achieved significantly better adversarial
robustness than previous works. Finally, we explored the potential of different
modern object detector designs for improving adversarial robustness with our
recipe and demonstrated interesting findings, which inspired us to design
state-of-the-art (SOTA) robust detectors. Our empirical results set a new
milestone for adversarially robust object detection. Code and trained
checkpoints are available at https://github.com/thu-ml/oddefense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TIFS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongru Yan, Yu Zheng, Yueqi Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skins wrapping around our bodies, leathers covering over the sofa, sheet
metal coating the car - it suggests that objects are enclosed by a series of
continuous surfaces, which provides us with informative geometry prior for
objectness deduction. In this paper, we propose Gaussian-Det which leverages
Gaussian Splatting as surface representation for multi-view based 3D object
detection. Unlike existing monocular or NeRF-based methods which depict the
objects via discrete positional data, Gaussian-Det models the objects in a
continuous manner by formulating the input Gaussians as feature descriptors on
a mass of partial surfaces. Furthermore, to address the numerous outliers
inherently introduced by Gaussian splatting, we accordingly devise a Closure
Inferring Module (CIM) for the comprehensive surface-based objectness
deduction. CIM firstly estimates the probabilistic feature residuals for
partial surfaces given the underdetermined nature of Gaussian Splatting, which
are then coalesced into a holistic representation on the overall surface
closure of the object proposal. In this way, the surface information
Gaussian-Det exploits serves as the prior on the quality and reliability of
objectness and the information basis of proposal refinement. Experiments on
both synthetic and real-world datasets demonstrate that Gaussian-Det
outperforms various existing approaches, in terms of both average precision and
recall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ADBM: Adversarial diffusion bridge model for reliable adversarial
  purification <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00315v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00315v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Li, Wenxuan Sun, Huanran Chen, Qiongxiu Li, Yining Liu, Yingzhe He, Jie Shi, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently Diffusion-based Purification (DiffPure) has been recognized as an
effective defense method against adversarial examples. However, we find
DiffPure which directly employs the original pre-trained diffusion models for
adversarial purification, to be suboptimal. This is due to an inherent
trade-off between noise purification performance and data recovery quality.
Additionally, the reliability of existing evaluations for DiffPure is
questionable, as they rely on weak adaptive attacks. In this work, we propose a
novel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs
a reverse bridge from the diffused adversarial data back to its original clean
examples, enhancing the purification capabilities of the original diffusion
models. Through theoretical analysis and experimental validation across various
scenarios, ADBM has proven to be a superior and robust defense mechanism,
offering significant promise for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Transformer Policy: Scaling Diffusion Transformer for
  Generalist Vision-Language-Action Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15959v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15959v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Hou, Tianyi Zhang, Yuwen Xiong, Hengjun Pu, Chengyang Zhao, Ronglei Tong, Yu Qiao, Jifeng Dai, Yuntao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large vision-language action models pretrained on diverse robot
datasets have demonstrated the potential for generalizing to new environments
with a few in-domain data. However, those approaches usually predict individual
discretized or continuous action by a small action head, which limits the
ability in handling diverse action spaces. In contrast, we model the continuous
action sequence with a large multi-modal diffusion transformer, dubbed as
Diffusion Transformer Policy, in which we directly denoise action chunks by a
large transformer model rather than a small action head for action embedding.
By leveraging the scaling capability of transformers, the proposed approach can
effectively model continuous end-effector actions across large diverse robot
datasets, and achieve better generalization performance. Extensive experiments
demonstrate the effectiveness and generalization of Diffusion Transformer
Policy on Maniskill2, Libero, Calvin and SimplerEnv, as well as the real-world
Franka arm, achieving consistent better performance on Real-to-Sim benchmark
SimplerEnv, real-world Franka Arm and Libero compared to OpenVLA and Octo.
Specifically, without bells and whistles, the proposed approach achieves
state-of-the-art performance with only a single third-view camera stream in the
Calvin task ABC->D, improving the average number of tasks completed in a row of
5 to 3.6, and the pretraining stage significantly facilitates the success
sequence length on the Calvin by over 1.2. Project Page:
https://zhihou7.github.io/dit_policy_vla/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhance-A-<span class="highlight-title">Video</span>: Better Generated <span class="highlight-title">Video</span> for Free 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Luo, Xuanlei Zhao, Mengzhao Chen, Kaipeng Zhang, Wenqi Shao, Kai Wang, Zhangyang Wang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DiT-based video generation has achieved remarkable results, but research into
enhancing existing models remains relatively unexplored. In this work, we
introduce a training-free approach to enhance the coherence and quality of
DiT-based generated videos, named Enhance-A-Video. The core idea is enhancing
the cross-frame correlations based on non-diagonal temporal attention
distributions. Thanks to its simple design, our approach can be easily applied
to most DiT-based video generation frameworks without any retraining or
fine-tuning. Across various DiT-based video generation models, our approach
demonstrates promising improvements in both temporal consistency and visual
quality. We hope this research can inspire future explorations in video
generation enhancement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moment of Untruth: Dealing with Negative Queries in <span class="highlight-title">Video</span> Moment
  Retrieval <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Flanagan, Dima Damen, Michael Wray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Moment Retrieval is a common task to evaluate the performance of
visual-language models - it involves localising start and end times of moments
in videos from query sentences. The current task formulation assumes that the
queried moment is present in the video, resulting in false positive moment
predictions when irrelevant query sentences are provided. In this paper we
propose the task of Negative-Aware Video Moment Retrieval (NA-VMR), which
considers both moment retrieval accuracy and negative query rejection accuracy.
We make the distinction between In-Domain and Out-of-Domain negative queries
and provide new evaluation benchmarks for two popular video moment retrieval
datasets: QVHighlights and Charades-STA. We analyse the ability of current SOTA
video moment retrieval approaches to adapt to Negative-Aware Video Moment
Retrieval and propose UniVTG-NA, an adaptation of UniVTG designed to tackle
NA-VMR. UniVTG-NA achieves high negative rejection accuracy (avg. $98.4\%$)
scores while retaining moment retrieval scores to within $3.87\%$ Recall@1.
Dataset splits and code are available at
https://github.com/keflanagan/MomentofUntruth
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures. Accepted at WACV 2025. Paper webpage:
  https://keflanagan.github.io/Moment-of-Untruth</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Illegal Waste Detection in Remote Sensing Images: A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Gibellini, Piero Fraternali, Giacomo Boracchi, Luca Morandini, Andrea Diecidue, Simona Malegori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental crime currently represents the third largest criminal activity
worldwide while threatening ecosystems as well as human health. Among the
crimes related to this activity, improper waste management can nowadays be
countered more easily thanks to the increasing availability and decreasing cost
of Very-High-Resolution Remote Sensing images, which enable semi-automatic
territory scanning in search of illegal landfills. This paper proposes a
pipeline, developed in collaboration with professionals from a local
environmental agency, for detecting candidate illegal dumping sites leveraging
a classifier of Remote Sensing images. To identify the best configuration for
such classifier, an extensive set of experiments was conducted and the impact
of diverse image characteristics and training settings was thoroughly analyzed.
The local environmental agency was then involved in an experimental exercise
where outputs from the developed classifier were integrated in the experts'
everyday work, resulting in time savings with respect to manual
photo-interpretation. The classifier was eventually run with valuable results
on a location outside of the training area, highlighting potential for
cross-border applicability of the proposed pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CANeRV: Content Adaptive Neural Representation for <span class="highlight-title">Video</span> Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lv Tang, Jun Zhu, Xinfeng Zhang, Li Zhang, Siwei Ma, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in video compression introduce implicit neural representation
(INR) based methods, which effectively capture global dependencies and
characteristics of entire video sequences. Unlike traditional and deep learning
based approaches, INR-based methods optimize network parameters from a global
perspective, resulting in superior compression potential. However, most current
INR methods utilize a fixed and uniform network architecture across all frames,
limiting their adaptability to dynamic variations within and between video
sequences. This often leads to suboptimal compression outcomes as these methods
struggle to capture the distinct nuances and transitions in video content. To
overcome these challenges, we propose Content Adaptive Neural Representation
for Video Compression (CANeRV), an innovative INR-based video compression
network that adaptively conducts structure optimisation based on the specific
content of each video sequence. To better capture dynamic information across
video sequences, we propose a dynamic sequence-level adjustment (DSA).
Furthermore, to enhance the capture of dynamics between frames within a
sequence, we implement a dynamic frame-level adjustment (DFA). {Finally, to
effectively capture spatial structural information within video frames, thereby
enhancing the detail restoration capabilities of CANeRV, we devise a structure
level hierarchical structural adaptation (HSA).} Experimental results
demonstrate that CANeRV can outperform both H.266/VVC and state-of-the-art
INR-based video compression techniques across diverse video datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image and Point-cloud Classification for Jet Analysis in High-Energy
  Physics: A <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11934v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11934v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Kheddar, Yassine Himeur, Abbes Amira, Rachik Soualah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, there has been a growing trend in the field of high-energy physics
(HEP), in both its experimental and phenomenological studies, to incorporate
machine learning (ML) and its specialized branch, deep learning (DL). This
review paper provides a thorough illustration of these applications using
different ML and DL approaches. The first part of the paper examines the basics
of various particle physics types and establishes guidelines for assessing
particle physics alongside the available learning models. Next, a detailed
classification is provided for representing Jets that are reconstructed in
high-energy collisions, mainly in proton-proton collisions at well-defined beam
energies. This section covers various datasets, preprocessing techniques, and
feature extraction and selection methods. The presented techniques can be
applied to future hadron-hadron colliders (HHC), such as the high-luminosity
LHC (HL-LHC) and the future circular collider - hadron-hadron (FCChh). The
authors then explore several AI techniques analyses designed specifically for
both image and point-cloud (PC) data in HEP. Additionally, a closer look is
taken at the classification associated with Jet tagging in hadron collisions.
In this review, various state-of-the-art (SOTA) techniques in ML and DL are
examined, with a focus on their implications for HEP demands. More precisely,
this discussion addresses various applications in extensive detail, such as Jet
tagging, Jet tracking, particle classification, and more. The review concludes
with an analysis of the current state of HEP using DL methodologies. It
highlights the challenges and potential areas for future research, which are
illustrated for each application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted paper in Frontier of Physics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Mi, Hanqing Wang, Tai Wang, Yilun Chen, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding (3DVG) is challenging because of the requirement of
understanding on visual information, language and spatial relationships. While
supervised approaches have achieved superior performance, they are constrained
by the scarcity and high cost of 3D vision-language datasets. On the other
hand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for
training data. However, these methods incur prohibitive time and token costs
during inference. To address the challenges, we introduce a novel training-free
symbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual
Grounder, that offers significantly reduced inference costs compared to
previous agent-based methods while maintaining comparable performance. EaSe
uses LLM generated codes to compute on spatial relationships. EaSe also
implements an automatic pipeline to evaluate and optimize the quality of these
codes and integrate VLMs to assist in the grounding process. Experimental
results demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2%
Acc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover,
it substantially reduces the inference time and cost, offering a balanced
trade-off between performance and efficiency. Codes are available at
https://github.com/OpenRobotLab/EaSe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NBM: an Open <span class="highlight-title">Dataset</span> for the Acoustic Monitoring of Nocturnal Migratory
  Birds in Europe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03633v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03633v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Airale, Adrien Pajot, Juliette Linossier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The persisting threats on migratory bird populations highlight the urgent
need for effective monitoring techniques that could assist in their
conservation. Among these, passive acoustic monitoring is an essential tool,
particularly for nocturnal migratory species that are difficult to track
otherwise. This work presents the Nocturnal Bird Migration (NBM) dataset, a
collection of 13,359 annotated vocalizations from 117 species of the Western
Palearctic. The dataset includes precise time and frequency annotations,
gathered by dozens of bird enthusiasts across France, enabling novel downstream
acoustic analysis. In particular, we prove the utility of this database by
training an original two-stage deep object detection model tailored for the
processing of audio data. While allowing the precise localization of bird calls
in spectrograms, this model shows competitive accuracy on the 45 main species
of the dataset with state-of-the-art systems trained on much larger audio
collections. These results highlight the interest of fostering similar
open-science initiatives to acquire costly but valuable fine-grained
annotations of audio files. All data and code are made openly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dream-in-Style: Text-to-3D Generation Using Stylized Score Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hubert Kompanowski, Binh-Son Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method to generate 3D objects in styles. Our method takes a text
prompt and a style reference image as input and reconstructs a neural radiance
field to synthesize a 3D model with the content aligning with the text prompt
and the style following the reference image. To simultaneously generate the 3D
object and perform style transfer in one go, we propose a stylized score
distillation loss to guide a text-to-3D optimization process to output visually
plausible geometry and appearance. Our stylized score distillation is based on
a combination of an original pretrained text-to-image model and its modified
sibling with the key and value features of self-attention layers manipulated to
inject styles from the reference image. Comparisons with state-of-the-art
methods demonstrated the strong visual performance of our method, further
supported by the quantitative results from our user study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Model for Compressed Sensing MRI Across Undersampling Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16290v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16290v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armeet Singh Jatyani, Jiayun Wang, Aditi Chandrashekar, Zihui Wu, Miguel Liu-Schiaffini, Bahareh Tolooshams, Anima Anandkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compressed Sensing MRI reconstructs images of the body's internal anatomy
from undersampled measurements, thereby reducing the scan time - the time
subjects need to remain still. Recently, deep neural networks have shown great
potential for reconstructing high-fidelity images from highly undersampled
measurements in the frequency space. However, one needs to train multiple
models for different undersampling patterns and desired output image
resolutions, since most networks operate on a fixed discretization. Such
approaches are highly impractical in clinical settings, where undersampling
patterns and image resolutions are frequently changed to accommodate different
real-time imaging and diagnostic requirements.
  We propose a unified model robust to different measurement undersampling
patterns and image resolutions in compressed sensing MRI. Our model is based on
neural operators, a discretization-agnostic architecture. Neural operators are
employed in both image and measurement space, which capture local and global
image features for MRI reconstruction. Empirically, we achieve consistent
performance across different undersampling rates and patterns, with an average
11 percent SSIM and 4dB PSNR improvement over a state-of-the-art CNN,
End-to-End VarNet. For efficiency, our inference speed is also 1,400x faster
than diffusion methods. The resolution-agnostic design also enhances zero-shot
super-resolution and extended field of view in reconstructed images. Our
unified model offers a versatile solution for MRI, adapting seamlessly to
various measurement undersampling and imaging resolutions, making it highly
effective for flexible and reliable clinical imaging. Our code is available at
https://armeet.ca/nomri.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Unsupervised Dynamic MRI Reconstruction via Diffeo-Temporal
  Equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Wang, Mike Davies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing dynamic MRI image sequences from undersampled accelerated
measurements is crucial for faster and higher spatiotemporal resolution
real-time imaging of cardiac motion, free breathing motion and many other
applications. Classical paradigms, such as gated cine MRI, assume periodicity,
disallowing imaging of true motion. Supervised deep learning methods are
fundamentally flawed as, in dynamic imaging, ground truth fully-sampled videos
are impossible to truly obtain. We propose an unsupervised framework to learn
to reconstruct dynamic MRI sequences from undersampled measurements alone by
leveraging natural geometric spatiotemporal equivariances of MRI. Dynamic
Diffeomorphic Equivariant Imaging (DDEI) significantly outperforms
state-of-the-art unsupervised methods such as SSDU on highly accelerated
dynamic cardiac imaging. Our method is agnostic to the underlying neural
network architecture and can be used to adapt the latest models and
post-processing approaches. Our code and video demos are at
https://github.com/Andrewwango/ddei.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference paper at ISBI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NanoVLMs: How small can we go and still make coherent Vision Language
  Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mukund Agarwalla, Himanshu Kumar, Raj Dandekar, Rajat Dandekar, Sreedath Panat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs), such as GPT-4V and Llama 3.2 vision, have
garnered significant research attention for their ability to leverage Large
Language Models (LLMs) in multimodal tasks. However, their potential is
constrained by inherent challenges, including proprietary restrictions,
substantial computational demands, and limited accessibility. Smaller models,
such as GIT and BLIP, exhibit marked limitations, often failing to generate
coherent and consistent text beyond a few tokens, even with extensive training.
This underscores a pivotal inquiry: how small can a VLM be and still produce
fluent and consistent text? Drawing inspiration from the exceptional learning
process of 3-4 year old children, who rely heavily on visual cues for
understanding and communication, we introduce two novel datasets: ShortDesc
(featuring concise image descriptions) and LongDesc (containing more detailed
image descriptions). These datasets consist of image-text pairs where the text
is restricted to the simple vocabulary and syntax typically used by young
children, generated with a scaled- down model, GPT-4o. Using these datasets, we
demonstrate that it is possible to train VLMs that are significantly smaller,
up to 10 times smaller than state of the art(SOTA) small VLMs while maintaining
architectural simplicity. To evaluate the outputs, we leverage GPT-4o to grade
the text, as if stories written by students, on creativity, meaningfulness, and
consistency, assigning scores out of 10. This method addresses limitations of
standard benchmarks by accommodating unstructured outputs and providing a
multidimensional evaluation of the model capabilities. Our findings contribute
to the development of lightweight, accessible multimodal models for resource
constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance
  Segmentation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed El Amine Boudjoghra, Angela Dai, Jean Lahoud, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works on open-vocabulary 3D instance segmentation show strong promise,
but at the cost of slow inference speed and high computation requirements. This
high computation cost is typically due to their heavy reliance on 3D clip
features, which require computationally expensive 2D foundation models like
Segment Anything (SAM) and CLIP for multi-view aggregation into 3D. As a
consequence, this hampers their applicability in many real-world applications
that require both fast and accurate predictions. To this end, we propose a fast
yet accurate open-vocabulary 3D instance segmentation approach, named Open-YOLO
3D, that effectively leverages only 2D object detection from multi-view RGB
images for open-vocabulary 3D instance segmentation. We address this task by
generating class-agnostic 3D masks for objects in the scene and associating
them with text prompts. We observe that the projection of class-agnostic 3D
point cloud instances already holds instance information; thus, using SAM might
only result in redundancy that unnecessarily increases the inference time. We
empirically find that a better performance of matching text prompts to 3D masks
can be achieved in a faster fashion with a 2D object detector. We validate our
Open-YOLO 3D on two benchmarks, ScanNet200 and Replica, under two scenarios:
(i) with ground truth masks, where labels are required for given object
proposals, and (ii) with class-agnostic 3D proposals generated from a 3D
proposal network. Our Open-YOLO 3D achieves state-of-the-art performance on
both datasets while obtaining up to $\sim$16$\times$ speedup compared to the
best existing method in literature. On ScanNet200 val. set, our Open-YOLO 3D
achieves mean average precision (mAP) of 24.7\% while operating at 22 seconds
per scene. Code and model are available at github.com/aminebdj/OpenYOLO3D.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSP-IR: Semantic and Structure Priors for Diffusion-based Realistic
  Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhong Zhang, Hengsheng Zhang, Zhengxue Cheng, Rong Xie, Li Song, Wenjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic image restoration is a crucial task in computer vision, and
diffusion-based models for image restoration have garnered significant
attention due to their ability to produce realistic results. Restoration can be
seen as a controllable generation conditioning on priors. However, due to the
severity of image degradation, existing diffusion-based restoration methods
cannot fully exploit priors from low-quality images and still have many
challenges in perceptual quality, semantic fidelity, and structure accuracy.
Based on the challenges, we introduce a novel image restoration method, SSP-IR.
Our approach aims to fully exploit semantic and structure priors from
low-quality images to guide the diffusion model in generating semantically
faithful and structurally accurate natural restoration results. Specifically,
we integrate the visual comprehension capabilities of Multimodal Large Language
Models (explicit) and the visual representations of the original image
(implicit) to acquire accurate semantic prior. To extract
degradation-independent structure prior, we introduce a Processor with RGB and
FFT constraints to extract structure prior from the low-quality images, guiding
the diffusion model and preventing the generation of unreasonable artifacts.
Lastly, we employ a multi-level attention mechanism to integrate the acquired
semantic and structure priors. The qualitative and quantitative results
demonstrate that our method outperforms other state-of-the-art methods overall
on both synthetic and real-world datasets. Our project page is
https://zyhrainbow.github.io/projects/SSP-IR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in IEEE TCSVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenVid-1M: A Large-Scale High-Quality <span class="highlight-title">Dataset</span> for Text-to-<span class="highlight-title">video</span>
  Generation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02371v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02371v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, Ying Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video (T2V) generation has recently garnered significant attention
thanks to the large multi-modality model Sora. However, T2V generation still
faces two important challenges: 1) Lacking a precise open sourced high-quality
dataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,
are either with low quality or too large for most research institutions.
Therefore, it is challenging but crucial to collect a precise high-quality
text-video pairs for T2V generation. 2) Ignoring to fully utilize textual
information. Recent T2V methods have focused on vision transformers, using a
simple cross attention module for video generation, which falls short of
thoroughly extracting semantic information from text prompt. To address these
issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive
captions. This open-scenario dataset contains over 1 million text-video pairs,
facilitating research on T2V generation. Furthermore, we curate 433K 1080p
videos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition
video generation. Additionally, we propose a novel Multi-modal Video Diffusion
Transformer (MVDiT) capable of mining both structure information from visual
tokens and semantic information from text tokens. Extensive experiments and
ablation studies verify the superiority of OpenVid-1M over previous datasets
and the effectiveness of our MVDiT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 15 figures, Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grid Jigsaw Representation with CLIP: A New Perspective on Image
  Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Song, Zhenzhen Hu, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised representation learning for image clustering is essential in
computer vision. Although the advancement of visual models has improved image
clustering with efficient visual representations, challenges still remain.
Firstly, existing features often lack the ability to represent the internal
structure of images, hindering the accurate clustering of visually similar
images. Secondly, finer-grained semantic labels are often missing, limiting the
ability to capture nuanced differences and similarities between images. In this
paper, we propose a new perspective on image clustering, the pretrain-based
Grid Jigsaw Representation (pGJR). Inspired by human jigsaw puzzle processing,
we modify the traditional jigsaw learning to gain a more sequential and
incremental understanding of image structure. We also leverage the pretrained
CLIP to extract the prior features which can benefit from the enhanced
cross-modal representation for richer and more nuanced semantic information and
label level differentiation. Our experiments demonstrate that using the
pretrained model as a feature extractor can accelerate the convergence of
clustering. We append the GJR module to pGJR and observe significant
improvements on common-use benchmark datasets. The experimental results
highlight the effectiveness of our approach in the clustering task, as
evidenced by improvements in the ACC, NMI, and ARI metrics, as well as the
super-fast convergence speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An <span class="highlight-title">Overview</span> of Prototype Formulations for Interpretable Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08925v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08925v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Xiling Li, Korbinian Franz Rudolf, Nils Blank, Rudolf Lioutikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prototypical part networks offer interpretable alternatives to black-box deep
learning models. However, many of these networks rely on Euclidean prototypes,
which may limit their flexibility. This work provides a comprehensive overview
of various prototype formulations. Experiments conducted on the CUB-200-2011,
Stanford Cars, and Oxford Flowers datasets demonstrate the effectiveness and
versatility of these different formulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal Contribution of M.X.Li and K.F.Rudolf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining Explainability: Recommendations for Effective Use of Concept
  Activation Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angus Nicolson, Lisa Schut, J. Alison Noble, Yarin Gal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-based explanations translate the internal representations of deep
learning models into a language that humans are familiar with: concepts. One
popular method for finding concepts is Concept Activation Vectors (CAVs), which
are learnt using a probe dataset of concept exemplars. In this work, we
investigate three properties of CAVs: (1) inconsistency across layers, (2)
entanglement with other concepts, and (3) spatial dependency. Each property
provides both challenges and opportunities in interpreting models. We introduce
tools designed to detect the presence of these properties, provide insight into
how each property can lead to misleading explanations, and provide
recommendations to mitigate their impact. To demonstrate practical
applications, we apply our recommendations to a melanoma classification task,
showing how entanglement can lead to uninterpretable results and that the
choice of negative probe set can have a substantial impact on the meaning of a
CAV. Further, we show that understanding these properties can be used to our
advantage. For example, we introduce spatially dependent CAVs to test if a
model is translation invariant with respect to a specific concept and class.
Our experiments are performed on natural images (ImageNet), skin lesions (ISIC
2019), and a new synthetic dataset, Elements. Elements is designed to capture a
known ground truth relationship between concepts and classes. We release this
dataset to facilitate further research in understanding and evaluating
interpretability methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Transactions on Machine Learning Research (02/2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Naturally Aggregated Appearance for Efficient 3D Editing <span class="chip">3DV
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ka Leong Cheng, Qiuyu Wang, Zifan Shi, Kecheng Zheng, Yinghao Xu, Hao Ouyang, Qifeng Chen, Yujun Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields, which represent a 3D scene as a color field and a
density field, have demonstrated great progress in novel view synthesis yet are
unfavorable for editing due to the implicitness. This work studies the task of
efficient 3D editing, where we focus on editing speed and user interactivity.
To this end, we propose to learn the color field as an explicit 2D appearance
aggregation, also called canonical image, with which users can easily customize
their 3D editing via 2D image processing. We complement the canonical image
with a projection field that maps 3D points onto 2D pixels for texture query.
This field is initialized with a pseudo canonical camera model and optimized
with offset regularity to ensure the naturalness of the canonical image.
Extensive experiments on different datasets suggest that our representation,
dubbed AGAP, well supports various ways of 3D editing (e.g., stylization,
instance segmentation, and interactive drawing). Our approach demonstrates
remarkable efficiency by being at least 20 times faster per edit compared to
existing NeRF-based editing methods. Project page is available at
https://felixcheng97.github.io/AGAP/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://felixcheng97.github.io/AGAP/; accepted to 3DV
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMI3D: MLLM-based 3D Perception from a Single 2D Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang, Sicheng Zhao, Yanhao Zhang, Hui Chen, Haonan Lu, Jungong Han, Guiguang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in autonomous driving, augmented reality, robotics, and
embodied intelligence have necessitated 3D perception algorithms. However,
current 3D perception methods, especially specialized small models, exhibit
poor generalization in open scenarios. On the other hand, multimodal large
language models (MLLMs) excel in general capacity but underperform in 3D tasks,
due to weak 3D local spatial object perception, poor text-based geometric
numerical output, and inability to handle camera focal variations. To address
these challenges, we propose the following solutions: Spatial-Enhanced Local
Feature Mining for better spatial feature extraction, 3D Query Token-Derived
Info Decoding for precise geometric regression, and Geometry Projection-Based
3D Reasoning for handling camera focal length variations. We employ
parameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a
powerful 3D perception MLLM. Additionally, we have constructed the IG3D
dataset, which provides fine-grained descriptions and question-answer
annotations. Extensive experiments demonstrate that our LLMI3D achieves
state-of-the-art performance, outperforming other methods by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When do they StOP?: A First Step Towards Automatically Identifying Team
  Communication in the Operating Room 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08299v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08299v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keqi Chen, Lilien Schewski, Vinkle Srivastav, Joël Lavanchy, Didier Mutter, Guido Beldi, Sandra Keller, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Surgical performance depends not only on surgeons' technical skills
but also on team communication within and across the different professional
groups present during the operation. Therefore, automatically identifying team
communication in the OR is crucial for patient safety and advances in the
development of computer-assisted surgical workflow analysis and intra-operative
support systems. To take the first step, we propose a new task of detecting
communication briefings involving all OR team members, i.e. the team Time-out
and the StOP?-protocol, by localizing their start and end times in video
recordings of surgical operations. Methods: We generate an OR dataset of real
surgeries, called Team-OR, with more than one hundred hours of surgical videos
captured by the multi-view camera system in the OR. The dataset contains
temporal annotations of 33 Time-out and 22 StOP?-protocol activities in total.
We then propose a novel group activity detection approach, where we encode both
scene context and action features, and use an efficient neural network model to
output the results. Results: The experimental results on the Team-OR dataset
show that our approach outperforms existing state-of-the-art temporal action
detection approaches. It also demonstrates the lack of research on group
activities in the OR, proving the significance of our dataset. Conclusion: We
investigate the Team Time-Out and the StOP?-protocol in the OR, by presenting
the first OR dataset with temporal annotations of group activities protocols,
and introducing a novel group activity detection approach that outperforms
existing approaches. Code is available at
https://github.com/CAMMA-public/Team-OR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EventZoom: A Progressive Approach to Event-Based Data Augmentation for
  Enhanced Neuromorphic Vision <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18880v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18880v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiting Dong, Xiang He, Guobin Shen, Dongcheng Zhao, Yang Li, Yi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Vision Sensors (DVS) capture event data with high temporal resolution
and low power consumption, presenting a more efficient solution for visual
processing in dynamic and real-time scenarios compared to conventional video
capture methods. Event data augmentation serve as an essential method for
overcoming the limitation of scale and diversity in event datasets. Our
comparative experiments demonstrate that the two factors, spatial integrity and
temporal continuity, can significantly affect the capacity of event data
augmentation, which are guarantee for maintaining the sparsity and high dynamic
range characteristics unique to event data. However, existing augmentation
methods often neglect the preservation of spatial integrity and temporal
continuity. To address this, we developed a novel event data augmentation
strategy EventZoom, which employs a temporal progressive strategy, embedding
transformed samples into the original samples through progressive scaling and
shifting. The scaling process avoids the spatial information loss associated
with cropping, while the progressive strategy prevents interruptions or abrupt
changes in temporal information. We validated EventZoom across various
supervised learning frameworks. The experimental results show that EventZoom
consistently outperforms existing event data augmentation methods with SOTA
performance. For the first time, we have concurrently employed Semi-supervised
and Unsupervised learning to verify feasibility on event augmentation
algorithms, demonstrating the applicability and effectiveness of EventZoom as a
powerful event-based data augmentation tool in handling real-world scenes with
high dynamics and variability environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiFloodSynth: Multi-Annotated Flood Synthetic <span class="highlight-title">Dataset</span> Generation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03966v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03966v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YoonJe Kang, Yonghoon Jung, Wonseop Shin, Bumsoo Kim, Sanghyun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present synthetic data generation framework for flood
hazard detection system. For high fidelity and quality, we characterize several
real-world properties into virtual world and simulate the flood situation by
controlling them. For the sake of efficiency, recent generative models in
image-to-3D and urban city synthesis are leveraged to easily composite flood
environments so that we avoid data bias due to the hand-crafted manner. Based
on our framework, we build the flood synthetic dataset with 5 levels, dubbed
MultiFloodSynth which contains rich annotation types like normal map,
segmentation, 3D bounding box for a variety of downstream task. In experiments,
our dataset demonstrate the enhanced performance of flood hazard detection with
on-par realism compared with real dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures. Accepted as Oral Presentation to AAAI 2025
  Workshop on Good-Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13310v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13310v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Yiwen Tang, Xuanzhuo Xu, Ziteng Cui, Yu Qiao, Peng Gao, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D object detection has long been a challenging task in autonomous
driving. Most existing methods follow conventional 2D detectors to first
localize object centers, and then predict 3D attributes by neighboring
features. However, only using local visual features is insufficient to
understand the scene-level 3D spatial structures and ignores the long-range
inter-object depth relations. In this paper, we introduce the first DETR
framework for Monocular DEtection with a depth-guided TRansformer, named
MonoDETR. We modify the vanilla transformer to be depth-aware and guide the
whole detection process by contextual depth cues. Specifically, concurrent to
the visual encoder that captures object appearances, we introduce to predict a
foreground depth map, and specialize a depth encoder to extract non-local depth
embeddings. Then, we formulate 3D object candidates as learnable queries and
propose a depth-guided decoder to conduct object-scene depth interactions. In
this way, each object query estimates its 3D attributes adaptively from the
depth-guided regions on the image and is no longer constrained to local visual
features. On KITTI benchmark with monocular images as input, MonoDETR achieves
state-of-the-art performance and requires no extra dense depth annotations.
Besides, our depth-guided modules can also be plug-and-play to enhance
multi-view 3D object detectors on nuScenes dataset, demonstrating our superior
generalization capacity. Code is available at
https://github.com/ZrrSkywalker/MonoDETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023. Code is available at
  https://github.com/ZrrSkywalker/MonoDETR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers
  for Open-Domain Sound Generation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung X. Pham, Tri Ton, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MDSGen, a novel framework for vision-guided open-domain sound
generation optimized for model parameter size, memory consumption, and
inference speed. This framework incorporates two key innovations: (1) a
redundant video feature removal module that filters out unnecessary visual
information, and (2) a temporal-aware masking strategy that leverages temporal
context for enhanced audio generation accuracy. In contrast to existing
resource-heavy Unet-based models, \texttt{MDSGen} employs denoising masked
diffusion transformers, facilitating efficient generation without reliance on
pre-trained diffusion models. Evaluated on the benchmark VGGSound dataset, our
smallest model (5M parameters) achieves $97.9$% alignment accuracy, using
$172\times$ fewer parameters, $371$% less memory, and offering $36\times$
faster inference than the current 860M-parameter state-of-the-art model
($93.9$% accuracy). The larger model (131M parameters) reaches nearly $99$%
accuracy while requiring $6.5\times$ fewer parameters. These results highlight
the scalability and effectiveness of our approach. The code is available at
https://bit.ly/mdsgen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColorSense: A Study on Color Vision in Machine Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming-Chang Chiu, Yingfei Wang, Derrick Eui Gyu Kim, Pin-Yu Chen, Xuezhe Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color vision is essential for human visual perception, but its impact on
machine perception is still underexplored. There has been an intensified demand
for understanding its role in machine perception for safety-critical tasks such
as assistive driving and surgery but lacking suitable datasets. To fill this
gap, we curate multipurpose datasets ColorSense, by collecting 110,000
non-trivial human annotations of foreground and background color labels from
popular visual recognition benchmarks. To investigate the impact of color
vision on machine perception, we assign each image a color discrimination level
based on its dominant foreground and background colors and use it to study the
impact of color vision on machine perception. We validate the use of our
datasets by demonstrating that the level of color discrimination has a
dominating effect on the performance of mainstream machine perception models.
Specifically, we examine the perception ability of machine vision by
considering key factors such as model architecture, training objective, model
size, training data, and task complexity. Furthermore, to investigate how color
and environmental factors affect the robustness of visual recognition in
machine perception, we integrate our ColorSense datasets with image corruptions
and perform a more comprehensive visual perception evaluation. Our findings
suggest that object recognition tasks such as classification and localization
are susceptible to color vision bias, especially for high-stakes cases such as
vehicle classes, and advanced mitigation techniques such as data augmentation
and so on only give marginal improvement. Our analyses highlight the need for
new approaches toward the performance evaluation of machine perception models
in real-world applications. Lastly, we present various potential applications
of ColorSense such as studying spurious correlations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 figures, Accepted at Secure and Trustworthy Machine
  Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing <span class="highlight-title">Video</span>-LLM Reasoning via Agent-of-Thoughts Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudi Shi, Shangzhe Di, Qirui Chen, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the problem of video question answering (VideoQA), a task
that often requires multi-step reasoning and a profound understanding of
spatial-temporal dynamics. While large video-language models perform well on
benchmarks, they often lack explainability and spatial-temporal grounding. In
this paper, we propose Agent-of-Thoughts Distillation (AoTD), a method that
enhances models by incorporating automatically generated Chain-of-Thoughts
(CoTs) into the instruction-tuning process. Specifically, we leverage an
agent-based system to decompose complex questions into sub-tasks, and address
them with specialized vision models, the intermediate results are then treated
as reasoning chains. We also introduce a verification mechanism using a large
language model (LLM) to ensure the reliability of generated CoTs. Extensive
experiments demonstrate that AoTD improves the performance on multiple-choice
and open-ended benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-level Asymmetric Contrastive Learning for Volumetric Medical Image
  Segmentation Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11876v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11876v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Zeng, Lei Zhu, Xinliang Zhang, Micky C Nnamdi, Wenqi Shi, J Ben Tamo, Qian Chen, Hangzhou He, Lujia Jin, Zifeng Tian, Qiushi Ren, Zhaoheng Xie, Yanye Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is a fundamental yet challenging task due to the
arduous process of acquiring large volumes of high-quality labeled data from
experts. Contrastive learning offers a promising but still problematic solution
to this dilemma. Firstly existing medical contrastive learning strategies focus
on extracting image-level representation, which ignores abundant multi-level
representations. Furthermore they underutilize the decoder either by random
initialization or separate pre-training from the encoder, thereby neglecting
the potential collaboration between the encoder and decoder. To address these
issues, we propose a novel multi-level asymmetric contrastive learning
framework named MACL for volumetric medical image segmentation pre-training.
Specifically, we design an asymmetric contrastive learning structure to
pre-train encoder and decoder simultaneously to provide better initialization
for segmentation models. Moreover, we develop a multi-level contrastive
learning strategy that integrates correspondences across feature-level,
image-level, and pixel-level representations to ensure the encoder and decoder
capture comprehensive details from representations of varying scales and
granularities during the pre-training phase. Finally, experiments on 8 medical
image datasets indicate our MACL framework outperforms existing 11 contrastive
learning strategies. i.e. Our MACL achieves a superior performance with more
precise predictions from visualization figures and 1.72%, 7.87%, 2.49% and
1.48% Dice higher than previous best results on ACDC, MMWHS, HVSMR and CHAOS
with 10% labeled data, respectively. And our MACL also has a strong
generalization ability among 5 variant U-Net backbones. Our code will be
released at https://github.com/stevezs315/MACL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIIS: Visible and Infrared Information Synthesis for Severe Low-light
  Image Enhancement <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhao, Mengyuan Yu, Fan Yang, Peiguang Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Images captured in severe low-light circumstances often suffer from
significant information absence. Existing singular modality image enhancement
methods struggle to restore image regions lacking valid information. By
leveraging light-impervious infrared images, visible and infrared image fusion
methods have the potential to reveal information hidden in darkness. However,
they primarily emphasize inter-modal complementation but neglect intra-modal
enhancement, limiting the perceptual quality of output images. To address these
limitations, we propose a novel task, dubbed visible and infrared information
synthesis (VIIS), which aims to achieve both information enhancement and fusion
of the two modalities. Given the difficulty in obtaining ground truth in the
VIIS task, we design an information synthesis pretext task (ISPT) based on
image augmentation. We employ a diffusion model as the framework and design a
sparse attention-based dual-modalities residual (SADMR) conditioning mechanism
to enhance information interaction between the two modalities. This mechanism
enables features with prior knowledge from both modalities to adaptively and
iteratively attend to each modality's information during the denoising process.
Our extensive experiments demonstrate that our model qualitatively and
quantitatively outperforms not only the state-of-the-art methods in relevant
fields but also the newly designed baselines capable of both information
enhancement and fusion. The code is available at
https://github.com/Chenz418/VIIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Segment Anything Model Towards Open-Vocabulary Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xumeng Han, Longhui Wei, Xuehui Yu, Zhiyang Dou, Xin He, Kuiran Wang, Yingfei Sun, Zhenjun Han, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent Segment Anything Model (SAM) has emerged as a new paradigmatic
vision foundation model, showcasing potent zero-shot generalization and
flexible prompting. Despite SAM finding applications and adaptations in various
domains, its primary limitation lies in the inability to grasp object
semantics. In this paper, we present Sambor to seamlessly integrate SAM with
the open-vocabulary object detector in an end-to-end framework. While retaining
all the remarkable capabilities inherent to SAM, we boost it to detect
arbitrary objects from human inputs like category names or reference
expressions. Building upon the SAM image encoder, we introduce a novel
SideFormer module designed to acquire SAM features adept at perceiving objects
and inject comprehensive semantic information for recognition. In addition, we
devise an Open-set RPN that leverages SAM proposals to assist in finding
potential objects. Consequently, Sambor enables the open-vocabulary detector to
equally focus on generalizing both localization and classification sub-tasks.
Our approach demonstrates superior zero-shot performance across benchmarks,
including COCO and LVIS, proving highly competitive against previous
state-of-the-art methods. We aspire for this work to serve as a meaningful
endeavor in endowing SAM to recognize diverse object categories and advancing
open-vocabulary learning with the support of vision foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Omni<span class="highlight-title">Human</span>-1: Rethinking the Scaling-Up of One-Stage Conditioned <span class="highlight-title">Human</span>
  <span class="highlight-title">Animation</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, Chao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end human animation, such as audio-driven talking human generation,
has undergone notable advancements in the recent few years. However, existing
methods still struggle to scale up as large general video generation models,
limiting their potential in real applications. In this paper, we propose
OmniHuman, a Diffusion Transformer-based framework that scales up data by
mixing motion-related conditions into the training phase. To this end, we
introduce two training principles for these mixed conditions, along with the
corresponding model architecture and inference strategy. These designs enable
OmniHuman to fully leverage data-driven motion generation, ultimately achieving
highly realistic human video generation. More importantly, OmniHuman supports
various portrait contents (face close-up, portrait, half-body, full-body),
supports both talking and singing, handles human-object interactions and
challenging body poses, and accommodates different image styles. Compared to
existing end-to-end audio-driven methods, OmniHuman not only produces more
realistic videos, but also offers greater flexibility in inputs. It also
supports multiple driving modalities (audio-driven, video-driven and combined
driving signals). Video samples are provided on the ttfamily project page
(https://omnihuman-lab.github.io)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://omnihuman-lab.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PulseCheck457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large
  Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingrui Wang, Wufei Ma, Tiezheng Zhang, Celso M de Melo, Jieneng Chen, Alan Yuille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large multimodal models (LMMs) have demonstrated remarkable
capabilities in visual scene interpretation and reasoning, their capacity for
complex and precise 3-dimensional spatial reasoning remains uncertain. Existing
benchmarks focus predominantly on 2D spatial understanding and lack a framework
to comprehensively evaluate 6D spatial reasoning across varying complexities.
To address this limitation, we present PulseCheck457, a scalable and unbiased
synthetic dataset designed with 4 key capability for spatial reasoning:
multi-object recognition, 2D location, 3D location, and 3D orientation. We
develop a cascading evaluation structure, constructing 7 question types across
5 difficulty levels that range from basic single object recognition to our new
proposed complex 6D spatial reasoning tasks. We evaluated various large
multimodal models (LMMs) on PulseCheck457, observing a general decline in
performance as task complexity increases, particularly in 3D reasoning and 6D
spatial tasks. To quantify these challenges, we introduce the Relative
Performance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning
capabilities. Leveraging the unbiased attribute design of our dataset, we also
uncover prediction biases across different attributes, with similar patterns
observed in real-world image settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial
  Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Sun, Jianhao Li, Xueqian Zhang, Zhongdao Wang, Bailan Feng, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies point cloud perception within outdoor environments.
Existing methods face limitations in recognizing objects located at a distance
or occluded, due to the sparse nature of outdoor point clouds. In this work, we
observe a significant mitigation of this problem by accumulating multiple
temporally consecutive LiDAR sweeps, resulting in a remarkable improvement in
perception accuracy. However, the computation cost also increases, hindering
previous approaches from utilizing a large number of LiDAR sweeps. To tackle
this challenge, we find that a considerable portion of points in the
accumulated point cloud is redundant, and discarding these points has minimal
impact on perception accuracy. We introduce a simple yet effective Gumbel
Spatial Pruning (GSP) layer that dynamically prunes points based on a learned
end-to-end sampling. The GSP layer is decoupled from other network components
and thus can be seamlessly integrated into existing point cloud network
architectures. Without incurring additional computational overhead, we increase
the number of LiDAR sweeps from 10, a common practice, to as many as 40.
Consequently, there is a significant enhancement in perception performance. For
instance, in nuScenes 3D object detection and BEV map segmentation tasks, our
pruning strategy improves the vanilla TransL baseline and other baseline
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DM-Mamba: Dual-domain Multi-scale Mamba for MRI reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucong Meng, Zhiwei Yang, Zhijian Song, Yonghong Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accelerated MRI reconstruction poses a challenging ill-posed inverse
problem due to the significant undersampling in k-space. Deep neural networks,
such as CNNs and ViT, have shown substantial performance improvements for this
task while encountering the dilemma between global receptive fields and
efficient computation. To this end, this paper pioneers exploring Mamba, a new
paradigm for long-range dependency modeling with linear complexity, for
efficient and effective MRI reconstruction. However, directly applying Mamba to
MRI reconstruction faces three significant issues: (1) Mamba's row-wise and
column-wise scanning disrupts k-space's unique spectrum, leaving its potential
in k-space learning unexplored. (2) Existing Mamba methods unfold feature maps
with multiple lengthy scanning paths, leading to long-range forgetting and high
computational burden. (3) Mamba struggles with spatially-varying contents,
resulting in limited diversity of local representations. To address these, we
propose a dual-domain multi-scale Mamba for MRI reconstruction from the
following perspectives: (1) We pioneer vision Mamba in k-space learning. A
circular scanning is customized for spectrum unfolding, benefiting the global
modeling of k-space. (2) We propose a multi-scale Mamba with an efficient
scanning strategy in both image and k-space domains. It mitigates long-range
forgetting and achieves a better trade-off between efficiency and performance.
(3) We develop a local diversity enhancement module to improve the
spatially-varying representation of Mamba. Extensive experiments are conducted
on three public datasets for MRI reconstruction under various undersampling
patterns. Comprehensive results demonstrate that our method significantly
outperforms state-of-the-art methods with lower computational cost.
Implementation code will be available at
https://github.com/XiaoMengLiLiLi/DM-Mamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Feature-based Image Stitching for Endoscopic <span class="highlight-title">Video</span>s in
  Pediatric Eosinophilic Esophagitis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juming Xiong, Muyang Li, Ruining Deng, Tianyuan Yao, Shunxing Bao, Regina N Tyree, Girish Hiremath, Yuankai Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video endoscopy represents a major advance in the investigation of
gastrointestinal diseases. Reviewing endoscopy videos often involves frequent
adjustments and reorientations to piece together a complete view, which can be
both time-consuming and prone to errors. Image stitching techniques address
this issue by providing a continuous and complete visualization of the examined
area. However, endoscopic images, particularly those of the esophagus, present
unique challenges. The smooth surface, lack of distinct feature points, and
non-horizontal orientation complicate the stitching process, rendering
traditional feature-based methods often ineffective for these types of images.
In this paper, we propose a novel preprocessing pipeline designed to enhance
endoscopic image stitching through advanced computational techniques. Our
approach converts endoscopic video data into continuous 2D images by following
four key steps: (1) keyframe selection, (2) image rotation adjustment to
correct distortions, (3) surface unwrapping using polar coordinate
transformation to generate a flat image, and (4) feature point matching
enhanced by Adaptive Histogram Equalization for improved feature detection. We
evaluate stitching quality through the assessment of valid feature point match
pairs. Experiments conducted on 20 pediatric endoscopy videos demonstrate that
our method significantly improves image alignment and stitching quality
compared to traditional techniques, laying a robust foundation for more
effective panoramic image creation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RenderWorld: World Model with Self-Supervised 3D Label <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Yan, Wenzhen Dong, Yihua Shao, Yuhang Lu, Liu Haiyang, Jingwen Liu, Haozhe Wang, Zhe Wang, Yan Wang, Fabio Remondino, Yuexin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving with vision-only is not only more
cost-effective compared to LiDAR-vision fusion but also more reliable than
traditional methods. To achieve a economical and robust purely visual
autonomous driving system, we propose RenderWorld, a vision-only end-to-end
autonomous driving framework, which generates 3D occupancy labels using a
self-supervised gaussian-based Img2Occ Module, then encodes the labels by
AM-VAE, and uses world model for forecasting and planning. RenderWorld employs
Gaussian Splatting to represent 3D scenes and render 2D images greatly improves
segmentation accuracy and reduces GPU memory consumption compared with
NeRF-based methods. By applying AM-VAE to encode air and non-air separately,
RenderWorld achieves more fine-grained scene element representation, leading to
state-of-the-art performance in both 4D occupancy forecasting and motion
planning from autoregressive world model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 2025 IEEE International Conference on Robotics and
  Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImDy: <span class="highlight-title">Human</span> Inverse Dynamics from Imitated Observations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpeng Liu, Junxuan Liang, Zili Lin, Haowen Hou, Yong-Lu Li, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse dynamics (ID), which aims at reproducing the driven torques from
human kinematic observations, has been a critical tool for gait analysis.
However, it is hindered from wider application to general motion due to its
limited scalability. Conventional optimization-based ID requires expensive
laboratory setups, restricting its availability. To alleviate this problem, we
propose to exploit the recently progressive human motion imitation algorithms
to learn human inverse dynamics in a data-driven manner. The key insight is
that the human ID knowledge is implicitly possessed by motion imitators, though
not directly applicable. In light of this, we devise an efficient data
collection pipeline with state-of-the-art motion imitation algorithms and
physics simulators, resulting in a large-scale human inverse dynamics benchmark
as Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint
torque and full-body ground reaction force data. With ImDy, we train a
data-driven human inverse dynamics solver ImDyS(olver) in a fully supervised
manner, which conducts ID and ground reaction force estimation simultaneously.
Experiments on ImDy and real-world data demonstrate the impressive competency
of ImDyS in human inverse dynamics and ground reaction force estimation.
Moreover, the potential of ImDy(-S) as a fundamental motion analysis tool is
exhibited with downstream applications. The project page is
https://foruck.github.io/ImDy/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICLR 2025. Yong-Lu Li and Cewu Lu are the corresponding
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What if Eye...? Computationally Recreating Vision Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushagra Tiwary, Aaron Young, Zaid Tasneem, Tzofi Klinghoffer, Akshat Dave, Tomaso Poggio, Dan-Eric Nilsson, Brian Cheung, Ramesh Raskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision systems in nature show remarkable diversity, from simple
light-sensitive patches to complex camera eyes with lenses. While natural
selection has produced these eyes through countless mutations over millions of
years, they represent just one set of realized evolutionary paths. Testing
hypotheses about how environmental pressures shaped eye evolution remains
challenging since we cannot experimentally isolate individual factors.
Computational evolution offers a way to systematically explore alternative
trajectories. Here we show how environmental demands drive three fundamental
aspects of visual evolution through an artificial evolution framework that
co-evolves both physical eye structure and neural processing in embodied
agents. First, we demonstrate computational evidence that task specific
selection drives bifurcation in eye evolution - orientation tasks like
navigation in a maze leads to distributed compound-type eyes while an object
discrimination task leads to the emergence of high-acuity camera-type eyes.
Second, we reveal how optical innovations like lenses naturally emerge to
resolve fundamental tradeoffs between light collection and spatial precision.
Third, we uncover systematic scaling laws between visual acuity and neural
processing, showing how task complexity drives coordinated evolution of sensory
and computational capabilities. Our work introduces a novel paradigm that
illuminates evolutionary principles shaping vision by creating targeted
single-player games where embodied agents must simultaneously evolve visual
systems and learn complex behaviors. Through our unified genetic encoding
framework, these embodied agents serve as next-generation hypothesis testing
machines while providing a foundation for designing manufacturable bio-inspired
vision systems. Website: http://eyes.mit.edu/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: http://eyes.mit.edu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE
  Solvers <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In applications of diffusion models, controllable generation is of practical
significance, but is also challenging. Current methods for controllable
generation primarily focus on modifying the score function of diffusion models,
while Mean Reverting (MR) Diffusion directly modifies the structure of the
stochastic differential equation (SDE), making the incorporation of image
conditions simpler and more natural. However, current training-free fast
samplers are not directly applicable to MR Diffusion. And thus MR Diffusion
requires hundreds of NFEs (number of function evaluations) to obtain
high-quality samples. In this paper, we propose a new algorithm named MRS (MR
Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time
SDE and the probability flow ordinary differential equation (PF-ODE) associated
with MR Diffusion, and derive semi-analytical solutions. The solutions consist
of an analytical function and an integral parameterized by a neural network.
Based on this solution, we can generate high-quality samples in fewer steps.
Our approach does not require training and supports all mainstream
parameterizations, including noise prediction, data prediction and velocity
prediction. Extensive experiments demonstrate that MR Sampler maintains high
sampling quality with a speedup of 10 to 20 times across ten different image
restoration tasks. Our algorithm accelerates the sampling procedure of MR
Diffusion, making it more practical in controllable generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework
  with Interactive Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Lin, Yingjing Xu, Xuanwen Bao, Zhou Zhao, Zhouyang Wang, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous advancement of vision language models (VLMs) technology,
remarkable research achievements have emerged in the dermatology field, the
fourth most prevalent human disease category. However, despite these
advancements, VLM still faces explainable problems to user in diagnosis due to
the inherent complexity of dermatological conditions, existing tools offer
relatively limited support for user comprehension. We propose SkinGEN, a
diagnosis-to-generation framework that leverages the stable diffusion(SD) model
to generate reference demonstrations from diagnosis results provided by VLM,
thereby enhancing the visual explainability for users. Through extensive
experiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for
skin condition image generation. We conduct a user study with 32 participants
evaluating both the system performance and explainability. Results demonstrate
that SkinGEN significantly improves users' comprehension of VLM predictions and
fosters increased trust in the diagnostic process. This work paves the way for
more transparent and user-centric VLM applications in dermatology and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Semi-Supervised 2D <span class="highlight-title">Human</span> <span class="highlight-title">Pose</span> Estimation by Revisiting Data
  Augmentation and Consistency Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11566v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11566v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayi Zhou, Mukun Luo, Fei Jiang, Yue Ding, Hongtao Lu, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 2D human pose estimation (HPE) is a basic visual problem. However, its
supervised learning requires massive keypoint labels, which is labor-intensive
to collect. Thus, we aim at boosting a pose estimator by excavating extra
unlabeled data with semi-supervised learning (SSL). Most previous SSHPE methods
are consistency-based and strive to maintain consistent outputs for differently
augmented inputs. Under this genre, we find that SSHPE can be boosted from two
cores: advanced data augmentations and concise consistency training ways.
Specifically, for the first core, we discover the synergistic effects of
existing augmentations, and reveal novel paradigms for conveniently producing
new superior HPE-oriented augmentations which can more effectively add noise on
unlabeled samples. We can therefore establish paired easy-hard augmentations
with larger difficulty gaps. For the second core, we propose to repeatedly
augment unlabeled images with diverse hard augmentations, and generate
multi-path predictions sequentially for optimizing multi-losses in a single
network. This simple and compact design is interpretable, and easily benefits
from newly found augmentations. Comparing to state-of-the-art SSL approaches,
our method brings substantial improvements on public datasets. And we
extensively validate the superiority and versatility of our approach on
conventional human body images, overhead fisheye images, and human hand images.
The code is released in https://github.com/hnuzhy/MultiAugs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review. Semi-Supervised 2D Human Pose Estimation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00626v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00626v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko, Bryan A. Plummer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Typographic attacks, adding misleading text to images, can deceive
vision-language models (LVLMs). The susceptibility of recent large LVLMs like
GPT4-V to such attacks is understudied, raising concerns about amplified
misinformation in personal assistant applications. Previous attacks use simple
strategies, such as random misleading words, which don't fully exploit LVLMs'
language reasoning abilities. We introduce an experimental setup for testing
typographic attacks on LVLMs and propose two novel self-generated attacks: (1)
Class-based attacks, where the model identifies a similar class to deceive
itself, and (2) Reasoned attacks, where an advanced LVLM suggests an attack
combining a deceiving class and description. Our experiments show these attacks
significantly reduce classification performance by up to 60\% and are effective
across different models, including InstructBLIP and MiniGPT4. Code:
https://github.com/mqraitem/Self-Gen-Typo-Attack
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ For Better or For Worse? Learning Minimum Variance Features With Label
  Augmentation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06855v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06855v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muthu Chidambaram, Rong Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation has been pivotal in successfully training deep learning
models on classification tasks over the past decade. An important subclass of
data augmentation techniques - which includes both label smoothing and Mixup -
involves modifying not only the input data but also the input label during
model training. In this work, we analyze the role played by the label
augmentation aspect of such methods. We first prove that linear models on
binary classification data trained with label augmentation learn only the
minimum variance features in the data, while standard training (which includes
weight decay) can learn higher variance features. We then use our techniques to
show that even for nonlinear models and general data distributions, the label
smoothing and Mixup losses are lower bounded by a function of the model output
variance. Lastly, we demonstrate empirically that this aspect of label
smoothing and Mixup can be a positive and a negative. On the one hand, we show
that the strong performance of label smoothing and Mixup on image
classification benchmarks is correlated with learning low variance hidden
representations. On the other hand, we show that Mixup and label smoothing can
be more susceptible to low variance spurious correlations in the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025, 25 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Cognitive Evaluation Benchmark of Image Reasoning and Description for
  Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18409v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18409v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiujie Song, Mengyue Wu, Kenny Q. Zhu, Chunhao Zhang, Yanyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs), despite their recent success, are
hardly comprehensively tested for their cognitive abilities. Inspired by the
prevalent use of the Cookie Theft task in human cognitive tests, we propose a
novel evaluation benchmark to evaluate high-level cognitive abilities of LVLMs
using images with rich semantics. The benchmark consists of 251 images along
with comprehensive annotations. It defines eight reasoning capabilities and
comprises an image description task and a visual question answering task. Our
evaluation of well-known LVLMs shows that there is still a significant gap in
cognitive abilities between LVLMs and humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UEMM-Air: Make Unmanned Aerial Vehicles Perform More Multi-modal Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06230v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06230v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Yao, Fan Liu, Shengxiang Xu, Chuanyi Zhang, Xing Ma, Jianyu Jiang, Zequan Wang, Shimin Di, Jun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of multi-modal learning for Unmanned Aerial Vehicles (UAVs)
typically relies on a large amount of pixel-aligned multi-modal image data.
However, existing datasets face challenges such as limited modalities, high
construction costs, and imprecise annotations. To this end, we propose a
synthetic multi-modal UAV-based multi-task dataset, UEMM-Air. Specifically, we
simulate various UAV flight scenarios and object types using the Unreal Engine
(UE). Then we design the UAV's flight logic to automatically collect data from
different scenarios, perspectives, and altitudes. Furthermore, we propose a
novel heuristic automatic annotation algorithm to generate accurate object
detection labels. Finally, we utilize labels to generate text descriptions of
images to make our UEMM-Air support more cross-modality tasks. In total, our
UEMM-Air consists of 120k pairs of images with 6 modalities and precise
annotations. Moreover, we conduct numerous experiments and establish new
benchmark results on our dataset. We also found that models pre-trained on
UEMM-Air exhibit better performance on downstream tasks compared to other
similar datasets. The dataset is publicly available
(https://github.com/1e12Leon/UEMM-Air) to support the research of multi-modal
tasks on UAVs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">240</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Benefit and Limitation of Diffusion Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guhao Feng, Yihan Geng, Jian Guan, Wei Wu, Liwei Wang, Di He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion language models have emerged as a promising approach for text
generation. One would naturally expect this method to be an efficient
replacement for autoregressive models since multiple tokens can be sampled in
parallel during each diffusion step. However, its efficiency-accuracy trade-off
is not yet well understood. In this paper, we present a rigorous theoretical
analysis of a widely used type of diffusion language model, the Masked
Diffusion Model (MDM), and find that its effectiveness heavily depends on the
target evaluation metric. Under mild conditions, we prove that when using
perplexity as the metric, MDMs can achieve near-optimal perplexity in sampling
steps regardless of sequence length, demonstrating that efficiency can be
achieved without sacrificing performance. However, when using the sequence
error rate--which is important for understanding the "correctness" of a
sequence, such as a reasoning chain--we show that the required sampling steps
must scale linearly with sequence length to obtain "correct" sequences, thereby
eliminating MDM's efficiency advantage over autoregressive models. Our analysis
establishes the first theoretical foundation for understanding the benefits and
limitations of MDMs. All theoretical findings are supported by empirical
studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Kahana, Or Nathan, Eliahu Horwitz, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing numbers of publicly available models, there are probably
pretrained, online models for most tasks users require. However, current model
search methods are rudimentary, essentially a text-based search in the
documentation, thus users cannot find the relevant models. This paper presents
ProbeLog, a method for retrieving classification models that can recognize a
target concept, such as "Dog", without access to model metadata or training
data. Differently from previous probing methods, ProbeLog computes a descriptor
for each output dimension (logit) of each model, by observing its responses on
a fixed set of inputs (probes). Our method supports both logit-based retrieval
("find more logits like this") and zero-shot, text-based retrieval ("find all
logits corresponding to dogs"). As probing-based representations require
multiple costly feedforward passes through the model, we develop a method,
based on collaborative filtering, that reduces the cost of encoding
repositories by 3x. We demonstrate that ProbeLog achieves high retrieval
accuracy, both in real-world and fine-grained search tasks and is scalable to
full-size repositories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Rectified Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengsheng Guo, Alexander G. Schwing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study Variational Rectified Flow Matching, a framework that enhances
classic rectified flow matching by modeling multi-modal velocity vector-fields.
At inference time, classic rectified flow matching 'moves' samples from a
source distribution to the target distribution by solving an ordinary
differential equation via integration along a velocity vector-field. At
training time, the velocity vector-field is learnt by linearly interpolating
between coupled samples one drawn from the source and one drawn from the target
distribution randomly. This leads to ''ground-truth'' velocity vector-fields
that point in different directions at the same location, i.e., the velocity
vector-fields are multi-modal/ambiguous. However, since training uses a
standard mean-squared-error loss, the learnt velocity vector-field averages
''ground-truth'' directions and isn't multi-modal. In contrast, variational
rectified flow matching learns and samples from multi-modal flow directions. We
show on synthetic data, MNIST, CIFAR-10, and ImageNet that variational
rectified flow matching leads to compelling results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DexTrack: Towards Generalizable Neural Tracking <span class="highlight-title">Control</span> for Dexterous
  Manipulation from <span class="highlight-title">Human</span> References <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyi Liu, Jianibieke Adalibieke, Qianwei Han, Yuzhe Qin, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of developing a generalizable neural tracking
controller for dexterous manipulation from human references. This controller
aims to manage a dexterous robot hand to manipulate diverse objects for various
purposes defined by kinematic human-object interactions. Developing such a
controller is complicated by the intricate contact dynamics of dexterous
manipulation and the need for adaptivity, generalizability, and robustness.
Current reinforcement learning and trajectory optimization methods often fall
short due to their dependence on task-specific rewards or precise system
models. We introduce an approach that curates large-scale successful robot
tracking demonstrations, comprising pairs of human references and robot
actions, to train a neural controller. Utilizing a data flywheel, we
iteratively enhance the controller's performance, as well as the number and
quality of successful tracking demonstrations. We exploit available tracking
demonstrations and carefully integrate reinforcement learning and imitation
learning to boost the controller's performance in dynamic environments. At the
same time, to obtain high-quality tracking demonstrations, we individually
optimize per-trajectory tracking by leveraging the learned tracking controller
in a homotopy optimization method. The homotopy optimization, mimicking
chain-of-thought, aids in solving challenging trajectory tracking problems to
increase demonstration diversity. We showcase our success by training a
generalizable neural controller and evaluating it in both simulation and real
world. Our method achieves over a 10% improvement in success rates compared to
leading baselines. The project website with animated results is available at
https://meowuu7.github.io/DexTrack/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Website: https://meowuu7.github.io/DexTrack/
  Code: https://github.com/Meowuu7/DexTrack/ Video:
  https://youtu.be/zru1Z-DaiWE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing a Conditional Prior Distribution for Flow-Based Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Issachar, Mohammad Salama, Raanan Fattal, Sagie Benaim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flow-based generative models have recently shown impressive performance for
conditional generation tasks, such as text-to-image generation. However,
current methods transform a general unimodal noise distribution to a specific
mode of the target data distribution. As such, every point in the initial
source distribution can be mapped to every point in the target distribution,
resulting in long average paths. To this end, in this work, we tap into a
non-utilized property of conditional flow-based models: the ability to design a
non-trivial prior distribution. Given an input condition, such as a text
prompt, we first map it to a point lying in data space, representing an
``average" data point with the minimal average distance to all data points of
the same conditional mode (e.g., class). We then utilize the flow matching
formulation to map samples from a parametric distribution centered around this
point to the conditional target distribution. Experimentally, our method
significantly improves training times and generation efficiency (FID, KID and
CLIP alignment scores) compared to baselines, producing high quality samples
using fewer sampling steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Score-of-Mixture Training: Training One-Step Generative Models Made
  Simple 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejas Jayashankar, J. Jon Ryu, Gregory Wornell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Score-of-Mixture Training (SMT), a novel framework for training
one-step generative models by minimizing a class of divergences called the
$\alpha$-skew Jensen-Shannon divergence. At its core, SMT estimates the score
of mixture distributions between real and fake samples across multiple noise
levels. Similar to consistency models, our approach supports both training from
scratch (SMT) and distillation using a pretrained diffusion model, which we
call Score-of-Mixture Distillation (SMD). It is simple to implement, requires
minimal hyperparameter tuning, and ensures stable training. Experiments on
CIFAR-10 and ImageNet 64x64 show that SMT/SMD are competitive with and can even
outperform existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Human</span>-LLM Coevolution: Evidence from Academic Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingmeng Geng, Roberto Trotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With a statistical analysis of arXiv paper abstracts, we report a marked drop
in the frequency of several words previously identified as overused by ChatGPT,
such as "delve", starting soon after they were pointed out in early 2024. The
frequency of certain other words favored by ChatGPT, such as "significant", has
instead kept increasing. These phenomena suggest that some authors of academic
papers have adapted their use of large language models (LLMs), for example, by
selecting outputs or applying modifications to the LLM-generated content. Such
coevolution and cooperation of humans and LLMs thus introduce additional
challenges to the detection of machine-generated text in real-world scenarios.
Estimating the impact of LLMs on academic writing by examining word frequency
remains feasible, and more attention should be paid to words that were already
frequently employed, including those that have decreased in frequency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfCite: Self-Supervised Alignment for Context Attribution in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James Glass, Shang-Wen Li, Wen-tau Yih
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SelfCite, a novel self-supervised approach that aligns LLMs to
generate high-quality, fine-grained, sentence-level citations for the
statements in their generated responses. Instead of only relying on costly and
labor-intensive annotations, SelfCite leverages a reward signal provided by the
LLM itself through context ablation: If a citation is necessary, removing the
cited text from the context should prevent the same response; if sufficient,
retaining the cited text alone should preserve the same response. This reward
can guide the inference-time best-of-N sampling strategy to improve citation
quality significantly, as well as be used in preference optimization to
directly fine-tune the models for generating better citations. The
effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3
points on the LongBench-Cite benchmark across five long-form question answering
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Implementation available at https://github.com/voidism/SelfCite</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do LLMs Recognize Your Preferences? Evaluating Personalized Preference
  Following in LLMs <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, Kaixiang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly used as chatbots, yet their
ability to personalize responses to user preferences remains limited. We
introduce PrefEval, a benchmark for evaluating LLMs' ability to infer, memorize
and adhere to user preferences in a long-context conversational setting.
PrefEval comprises 3,000 manually curated user preference and query pairs
spanning 20 topics. PrefEval contains user personalization or preference
information in both explicit and implicit forms, and evaluates LLM performance
using a generation and a classification task. With PrefEval, we evaluated the
aforementioned preference following capabilities of 10 open-source and
proprietary LLMs in multi-session conversations with varying context lengths up
to 100k tokens. We benchmark with various prompting, iterative feedback, and
retrieval-augmented generation methods. Our benchmarking effort reveals that
state-of-the-art LLMs face significant challenges in proactively following
users' preferences during conversations. In particular, in zero-shot settings,
preference following accuracy falls below 10% at merely 10 turns (~3k tokens)
across most evaluated models. Even with advanced prompting and retrieval
methods, preference following still deteriorates in long-context conversations.
Furthermore, we show that fine-tuning on PrefEval significantly improves
performance. We believe PrefEval serves as a valuable resource for measuring,
understanding, and enhancing LLMs' preference following abilities, paving the
way for personalized conversational agents. Our code and dataset are available
at https://prefeval.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025 as oral presentation. Code and data at:
  https://prefeval.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Censor Dependent Variational Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanhui Liu, Xiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a comprehensive analysis of variational inference in
latent variable models for survival analysis, emphasizing the distinctive
challenges associated with applying variational methods to survival data. We
identify a critical weakness in the existing methodology, demonstrating how a
poorly designed variational distribution may hinder the objective of survival
analysis tasks--modeling time-to-event distributions. We prove that the optimal
variational distribution, which perfectly bounds the log-likelihood, may depend
on the censoring mechanism. To address this issue, we propose censor-dependent
variational inference (CDVI), tailored for latent variable models in survival
analysis. More practically, we introduce CD-CVAE, a V-structure Variational
Autoencoder (VAE) designed for the scalable implementation of CDVI. Further
discussion extends some existing theories and training techniques to survival
analysis. Extensive experiments validate our analysis and demonstrate
significant improvements in the estimation of individual survival
distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rolling Ahead Diffusion for Traffic Scene Simulation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Liu, Matthew Niedoba, William Harvey, Adam Scibior, Berend Zwartsenberg, Frank Wood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic driving simulation requires that NPCs not only mimic natural
driving behaviors but also react to the behavior of other simulated agents.
Recent developments in diffusion-based scenario generation focus on creating
diverse and realistic traffic scenarios by jointly modelling the motion of all
the agents in the scene. However, these traffic scenarios do not react when the
motion of agents deviates from their modelled trajectories. For example, the
ego-agent can be controlled by a stand along motion planner. To produce
reactive scenarios with joint scenario models, the model must regenerate the
scenario at each timestep based on new observations in a Model Predictive
Control (MPC) fashion. Although reactive, this method is time-consuming, as one
complete possible future for all NPCs is generated per simulation step.
Alternatively, one can utilize an autoregressive model (AR) to predict only the
immediate next-step future for all NPCs. Although faster, this method lacks the
capability for advanced planning. We present a rolling diffusion based traffic
scene generation model which mixes the benefits of both methods by predicting
the next step future and simultaneously predicting partially noised further
future steps at the same time. We show that such model is efficient compared to
diffusion model based AR, achieving a beneficial compromise between reactivity
and computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Workshop on Machine Learning for Autonomous Driving at
  AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Coordinate with Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad H. Danesh, Tu Trinh, Benjamin Plaut, Nguyen X. Khanh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When deployed in dynamic environments, AI agents will inevitably encounter
challenges that exceed their individual capabilities. Leveraging assistance
from expert agents-whether human or AI-can significantly enhance safety and
performance in such situations. However, querying experts is often costly,
necessitating the development of agents that can efficiently request and
utilize expert guidance. In this paper, we introduce a fundamental coordination
problem called Learning to Yield and Request Control (YRC), where the objective
is to learn a strategy that determines when to act autonomously and when to
seek expert assistance. We consider a challenging practical setting in which an
agent does not interact with experts during training but must adapt to novel
environmental changes and expert interventions at test time. To facilitate
empirical research, we introduce YRC-Bench, an open-source benchmark featuring
diverse domains. YRC-Bench provides a standardized Gym-like API, simulated
experts, evaluation pipeline, and implementation of competitive baselines.
Towards tackling the YRC problem, we propose a novel validation approach and
investigate the performance of various learning methods across diverse
environments, yielding insights that can guide future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing GPT for <span class="highlight-title">Video</span> Understanding: Zero-Shot Performance and Prompt
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Beliaev, Victor Yang, Madhura Raju, Jiachen Sun, Xinghai Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we tackle industry challenges in video content classification
by exploring and optimizing GPT-based models for zero-shot classification
across seven critical categories of video quality. We contribute a novel
approach to improving GPT's performance through prompt optimization and policy
refinement, demonstrating that simplifying complex policies significantly
reduces false negatives. Additionally, we introduce a new
decomposition-aggregation-based prompt engineering technique, which outperforms
traditional single-prompt methods. These experiments, conducted on real
industry problems, show that thoughtful prompt design can substantially enhance
GPT's performance without additional finetuning, offering an effective and
scalable solution for improving video classification systems across various
domains in industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Montgomery Bohde, Mrunali Manjrekar, Runzhong Wang, Shuiwang Ji, Connor W. Coley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mass spectrometry plays a fundamental role in elucidating the structures of
unknown molecules and subsequent scientific discoveries. One formulation of the
structure elucidation task is the conditional $\textit{de novo}$ generation of
molecular structure given a mass spectrum. Toward a more accurate and efficient
scientific discovery pipeline for small molecules, we present DiffMS, a
formula-restricted encoder-decoder generative network that achieves
state-of-the-art performance on this task. The encoder utilizes a transformer
architecture and models mass spectra domain knowledge such as peak formulae and
neutral losses, and the decoder is a discrete graph diffusion model restricted
by the heavy-atom composition of a known chemical formula. To develop a robust
decoder that bridges latent embeddings and molecular structures, we pretrain
the diffusion decoder with fingerprint-structure pairs, which are available in
virtually infinite quantities, compared to structure-spectrum pairs that number
in the tens of thousands. Extensive experiments on established benchmarks show
that DiffMS outperforms existing models on $\textit{de novo}$ molecule
generation. We provide several ablations to demonstrate the effectiveness of
our diffusion and pretraining approaches and show consistent performance
scaling with increasing pretraining dataset size. DiffMS code is publicly
available at https://github.com/coleygroup/DiffMS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing the Utility of Higher-Order Information in Relational Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Pellegrin, Lukas Fesser, Melanie Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Higher-order information is crucial for relational learning in many domains
where relationships extend beyond pairwise interactions. Hypergraphs provide a
natural framework for modeling such relationships, which has motivated recent
extensions of graph neural net- work architectures to hypergraphs. However,
comparisons between hypergraph architectures and standard graph-level models
remain limited. In this work, we systematically evaluate a selection of
hypergraph-level and graph-level architectures, to determine their
effectiveness in leveraging higher-order information in relational learning.
Our results show that graph-level architectures applied to hypergraph
expansions often outperform hypergraph- level ones, even on inputs that are
naturally parametrized as hypergraphs. As an alternative approach for
leveraging higher-order information, we propose hypergraph-level encodings
based on classical hypergraph characteristics. While these encodings do not
significantly improve hypergraph architectures, they yield substantial
performance gains when combined with graph-level models. Our theoretical
analysis shows that hypergraph-level encodings provably increase the
representational power of message-passing graph neural networks beyond that of
their graph-level counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot generation of synthetic neurosurgical data with large language
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin A. Barr, Eddie Guo, Emre Sezgin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical data is fundamental to advance neurosurgical research, but access is
often constrained by data availability, small sample sizes, privacy
regulations, and resource-intensive preprocessing and de-identification
procedures. Synthetic data offers a potential solution to challenges associated
with accessing and using real-world data (RWD). This study aims to evaluate the
capability of zero-shot generation of synthetic neurosurgical data with a large
language model (LLM), GPT-4o, by benchmarking with the conditional tabular
generative adversarial network (CTGAN). Synthetic datasets were compared to
real-world neurosurgical data to assess fidelity (means, proportions,
distributions, and bivariate correlations), utility (ML classifier performance
on RWD), and privacy (duplication of records from RWD). The GPT-4o-generated
datasets matched or exceeded CTGAN performance, despite no fine-tuning or
access to RWD for pre-training. Datasets demonstrated high univariate and
bivariate fidelity to RWD without directly exposing any real patient records,
even at amplified sample size. Training an ML classifier on GPT-4o-generated
data and testing on RWD for a binary prediction task showed an F1 score (0.706)
with comparable performance to training on the CTGAN data (0.705) for
predicting postoperative functional status deterioration. GPT-4o demonstrated a
promising ability to generate high-fidelity synthetic neurosurgical data. These
findings also indicate that data synthesized with GPT-4o can effectively
augment clinical data with small sample sizes, and train ML models for
prediction of neurosurgical outcomes. Further investigation is necessary to
improve the preservation of distributional characteristics and boost classifier
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusing DeBias: a Recipe for Turning a Bug into a Feature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimiliano Ciranni, Vito Paolo Pastore, Roberto Di Via, Enzo Tartaglione, Francesca Odone, Vittorio Murino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning model effectiveness in classification tasks is often challenged
by the quality and quantity of training data which, whenever containing strong
spurious correlations between specific attributes and target labels, can result
in unrecoverable biases in model predictions. Tackling these biases is crucial
in improving model generalization and trust, especially in real-world
scenarios. This paper presents Diffusing DeBias (DDB), a novel approach acting
as a plug-in for common methods in model debiasing while exploiting the
inherent bias-learning tendency of diffusion models. Our approach leverages
conditional diffusion models to generate synthetic bias-aligned images, used to
train a bias amplifier model, to be further employed as an auxiliary method in
different unsupervised debiasing approaches. Our proposed method, which also
tackles the common issue of training set memorization typical of this type of
tech- niques, beats current state-of-the-art in multiple benchmark datasets by
significant margins, demonstrating its potential as a versatile and effective
tool for tackling dataset bias in deep learning applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 Pages, 12 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyntheticPop: Attacking Speaker Verification Systems With Synthetic
  VoicePops 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshaq Jamdar, Amith Kamath Belman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice Authentication (VA), also known as Automatic Speaker Verification
(ASV), is a widely adopted authentication method, particularly in automated
systems like banking services, where it serves as a secondary layer of user
authentication. Despite its popularity, VA systems are vulnerable to various
attacks, including replay, impersonation, and the emerging threat of deepfake
audio that mimics the voice of legitimate users. To mitigate these risks,
several defense mechanisms have been proposed. One such solution, Voice Pops,
aims to distinguish an individual's unique phoneme pronunciations during the
enrollment process. While promising, the effectiveness of VA+VoicePop against a
broader range of attacks, particularly logical or adversarial attacks, remains
insufficiently explored. We propose a novel attack method, which we refer to as
SyntheticPop, designed to target the phoneme recognition capabilities of the
VA+VoicePop system. The SyntheticPop attack involves embedding synthetic "pop"
noises into spoofed audio samples, significantly degrading the model's
performance. We achieve an attack success rate of over 95% while poisoning 20%
of the training dataset. Our experiments demonstrate that VA+VoicePop achieves
69% accuracy under normal conditions, 37% accuracy when subjected to a baseline
label flipping attack, and just 14% accuracy under our proposed SyntheticPop
attack, emphasizing the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Tensor Completion via Approximate Richardson Iteration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Ghadiri, Matthew Fahrbach, Yunbum Kook, Ali Jadbabaie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study tensor completion (TC) through the lens of low-rank tensor
decomposition (TD). Many TD algorithms use fast alternating minimization
methods, which solve highly structured linear regression problems at each step
(e.g., for CP, Tucker, and tensor-train decompositions). However, such
algebraic structure is lost in TC regression problems, making direct extensions
unclear. To address this, we propose a lifting approach that approximately
solves TC regression problems using structured TD regression algorithms as
blackbox subroutines, enabling sublinear-time methods. We theoretically analyze
the convergence rate of our approximate Richardson iteration based algorithm,
and we demonstrate on real-world tensors that its running time can be 100x
faster than direct methods for CP completion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Learning of Multi-index Models via Iterative Subspace
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Diakonikolas, Giannis Iakovidis, Daniel M. Kane, Nikos Zarifis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the task of learning Multi-Index Models (MIMs) with label noise
under the Gaussian distribution. A $K$-MIM is any function $f$ that only
depends on a $K$-dimensional subspace. We focus on well-behaved MIMs with
finite ranges that satisfy certain regularity properties. Our main contribution
is a general robust learner that is qualitatively optimal in the Statistical
Query (SQ) model. Our algorithm iteratively constructs better approximations to
the defining subspace by computing low-degree moments conditional on the
projection to the subspace computed thus far, and adding directions with
relatively large empirical moments. This procedure efficiently finds a subspace
$V$ so that $f(\mathbf{x})$ is close to a function of the projection of
$\mathbf{x}$ onto $V$. Conversely, for functions for which these conditional
moments do not help, we prove an SQ lower bound suggesting that no efficient
learner exists.
  As applications, we provide faster robust learners for the following concept
classes:
  * {\bf Multiclass Linear Classifiers} We give a constant-factor approximate
agnostic learner with sample complexity $N = O(d)
2^{\mathrm{poly}(K/\epsilon)}$ and computational complexity $\mathrm{poly}(N
,d)$. This is the first constant-factor agnostic learner for this class whose
complexity is a fixed-degree polynomial in $d$.
  * {\bf Intersections of Halfspaces} We give an approximate agnostic learner
for this class achieving 0-1 error $K \tilde{O}(\mathrm{OPT}) + \epsilon$ with
sample complexity $N=O(d^2) 2^{\mathrm{poly}(K/\epsilon)}$ and computational
complexity $\mathrm{poly}(N ,d)$. This is the first agnostic learner for this
class with near-linear error dependence and complexity a fixed-degree
polynomial in $d$.
  Furthermore, we show that in the presence of random classification noise, the
complexity of our algorithm scales polynomially with $1/\epsilon$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models for Molecules: A <span class="highlight-title">Survey</span> of Methods and Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Chao Song, Zhiyuan Liu, Yu Rong, Qiang Liu, Shu Wu, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative tasks about molecules, including but not limited to molecule
generation, are crucial for drug discovery and material design, and have
consistently attracted significant attention. In recent years, diffusion models
have emerged as an impressive class of deep generative models, sparking
extensive research and leading to numerous studies on their application to
molecular generative tasks. Despite the proliferation of related work, there
remains a notable lack of up-to-date and systematic surveys in this area.
Particularly, due to the diversity of diffusion model formulations, molecular
data modalities, and generative task types, the research landscape is
challenging to navigate, hindering understanding and limiting the area's
growth. To address this, this paper conducts a comprehensive survey of
diffusion model-based molecular generative methods. We systematically review
the research from the perspectives of methodological formulations, data
modalities, and task types, offering a novel taxonomy. This survey aims to
facilitate understanding and further flourishing development in this area. The
relevant papers are summarized at:
https://github.com/AzureLeon1/awesome-molecular-diffusion-models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EQ-VAE: Equivariance Regularized Latent Space for Improved Generative
  Image Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, Nikos Komodakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent generative models have emerged as a leading approach for high-quality
image synthesis. These models rely on an autoencoder to compress images into a
latent space, followed by a generative model to learn the latent distribution.
We identify that existing autoencoders lack equivariance to semantic-preserving
transformations like scaling and rotation, resulting in complex latent spaces
that hinder generative performance. To address this, we propose EQ-VAE, a
simple regularization approach that enforces equivariance in the latent space,
reducing its complexity without degrading reconstruction quality. By finetuning
pre-trained autoencoders with EQ-VAE, we enhance the performance of several
state-of-the-art generative models, including DiT, SiT, REPA and MaskGIT,
achieving a 7 speedup on DiT-XL/2 with only five epochs of SD-VAE fine-tuning.
EQ-VAE is compatible with both continuous and discrete autoencoders, thus
offering a versatile enhancement for a wide range of latent generative models.
Project page and code: https://eq-vae.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When and How Does CLIP Enable Domain and Compositional Generalization? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Kempf, Simon Schrodi, Max Argus, Thomas Brox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable generalization performance of contrastive vision-language
models like CLIP is often attributed to the diversity of their training
distributions. However, key questions remain unanswered: Can CLIP generalize to
an entirely unseen domain when trained on a diverse mixture of domains (domain
generalization)? Can it generalize to unseen classes within partially seen
domains (compositional generalization)? What factors affect such
generalization? To answer these questions, we trained CLIP models on
systematically constructed training distributions with controlled domain
diversity and object class exposure. Our experiments show that domain diversity
is essential for both domain and compositional generalization, yet
compositional generalization can be surprisingly weaker than domain
generalization when the training distribution contains a suboptimal subset of
the test domain. Through data-centric and mechanistic analyses, we find that
successful generalization requires learning of shared representations already
in intermediate layers and shared circuitry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AttentionSmithy: A Modular Framework for Rapid Transformer Development
  and Customization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caleb Cranney, Jesse G. Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer architectures have transformed AI applications but remain complex
to customize for domain experts lacking low-level implementation expertise. We
introduce AttentionSmithy, a modular software package that simplifies
transformer innovation by breaking down key components into reusable building
blocks: attention modules, feed-forward networks, normalization layers, and
positional encodings. Users can rapidly prototype and evaluate transformer
variants without extensive coding. Our framework supports four positional
encoding strategies and integrates with neural architecture search for
automated design. We validate AttentionSmithy by replicating the original
transformer under resource constraints and optimizing translation performance
by combining positional encodings. Additionally, we demonstrate its
adaptability in gene-specific modeling, achieving over 95% accuracy in cell
type classification. These case studies highlight AttentionSmithy's potential
to accelerate research across diverse fields by removing framework
implementation barriers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable First-order Method for Certifying Optimal k-Sparse GLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachang Liu, Soroosh Shafiee, Andrea Lodi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problem of certifying optimality for sparse
generalized linear models (GLMs), where sparsity is enforced through an
$\ell_0$ cardinality constraint. While branch-and-bound (BnB) frameworks can
certify optimality by pruning nodes using dual bounds, existing methods for
computing these bounds are either computationally intensive or exhibit slow
convergence, limiting their scalability to large-scale problems. To address
this challenge, we propose a first-order proximal gradient algorithm designed
to solve the perspective relaxation of the problem within a BnB framework.
Specifically, we formulate the relaxed problem as a composite optimization
problem and demonstrate that the proximal operator of the non-smooth component
can be computed exactly in log-linear time complexity, eliminating the need to
solve a computationally expensive second-order cone program. Furthermore, we
introduce a simple restart strategy that enhances convergence speed while
maintaining low per-iteration complexity. Extensive experiments on synthetic
and real-world datasets show that our approach significantly accelerates dual
bound computations and is highly effective in providing optimality certificates
for large-scale problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Eidetic Learning: an Efficient and Provable Solution to Catastrophic
  Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Dronen, Randall Balestriero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catastrophic forgetting -- the phenomenon of a neural network learning a task
t1 and losing the ability to perform it after being trained on some other task
t2 -- is a long-standing problem for neural networks [McCloskey and Cohen,
1989]. We present a method, Eidetic Learning, that provably solves catastrophic
forgetting. A network trained with Eidetic Learning -- here, an EideticNet --
requires no rehearsal or replay. We consider successive discrete tasks and show
how at inference time an EideticNet automatically routes new instances without
auxiliary task information. An EideticNet bears a family resemblance to the
sparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network
capacity is partitioned across tasks and the network itself performs
data-conditional routing. An EideticNet is easy to implement and train, is
efficient, and has time and space complexity linear in the number of
parameters. The guarantee of our method holds for normalization layers of
modern neural networks during both pre-training and fine-tuning. We show with a
variety of network architectures and sets of tasks that EideticNets are immune
to forgetting. While the practical benefits of EideticNets are substantial, we
believe they can be benefit practitioners and theorists alike. The code for
training EideticNets is available at
\href{https://github.com/amazon-science/eideticnet-training}{this https URL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures; code is available at
  https://github.com/amazon-science/eideticnet-training</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Agnostic PAC Learning in the Small Error Regime 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Asilis, Mikael Møller Høgsgaard, Grigoris Velegkas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary classification in the classic PAC model exhibits a curious phenomenon:
Empirical Risk Minimization (ERM) learners are suboptimal in the realizable
case yet optimal in the agnostic case. Roughly speaking, this owes itself to
the fact that non-realizable distributions $\mathcal{D}$ are simply more
difficult to learn than realizable distributions -- even when one discounts a
learner's error by $\mathrm{err}(h^*_{\mathcal{D}})$, the error of the best
hypothesis in $\mathcal{H}$ for $\mathcal{D}$. Thus, optimal agnostic learners
are permitted to incur excess error on (easier-to-learn) distributions
$\mathcal{D}$ for which $\tau = \mathrm{err}(h^*_{\mathcal{D}})$ is small.
  Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS `24) addresses this
shortcoming by including $\tau$ itself as a parameter in the agnostic error
term. In this more fine-grained model, they demonstrate tightness of the error
lower bound $\tau + \Omega \left(\sqrt{\frac{\tau (d + \log(1 / \delta))}{m}} +
\frac{d + \log(1 / \delta)}{m} \right)$ in a regime where $\tau > d/m$, and
leave open the question of whether there may be a higher lower bound when $\tau
\approx d/m$, with $d$ denoting $\mathrm{VC}(\mathcal{H})$. In this work, we
resolve this question by exhibiting a learner which achieves error $c \cdot
\tau + O \left(\sqrt{\frac{\tau (d + \log(1 / \delta))}{m}} + \frac{d + \log(1
/ \delta)}{m} \right)$ for a constant $c \leq 2.1$, thus matching the lower
bound when $\tau \approx d/m$. Further, our learner is computationally
efficient and is based upon careful aggregations of ERM classifiers, making
progress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS
`24). We leave open the interesting question of whether our approach can be
refined to lower the constant from 2.1 to 1, which would completely settle the
complexity of agnostic learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cracking the Code: Enhancing Development finance understanding with
  artificial intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Beaucoral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing development projects is crucial for understanding donors aid
strategies, recipients priorities, and to assess development finance capacity
to adress development issues by on-the-ground actions. In this area, the
Organisation for Economic Co-operation and Developments (OECD) Creditor
Reporting System (CRS) dataset is a reference data source. This dataset
provides a vast collection of project narratives from various sectors
(approximately 5 million projects). While the OECD CRS provides a rich source
of information on development strategies, it falls short in informing project
purposes due to its reporting process based on donors self-declared main
objectives and pre-defined industrial sectors. This research employs a novel
approach that combines Machine Learning (ML) techniques, specifically Natural
Language Processing (NLP), an innovative Python topic modeling technique called
BERTopic, to categorise (cluster) and label development projects based on their
narrative descriptions. By revealing existing yet hidden topics of development
finance, this application of artificial intelligence enables a better
understanding of donor priorities and overall development funding and provides
methods to analyse public and private projects narratives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communicating Likelihoods with Normalising Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Y. Araz, Anja Beck, Méril Reboud, Michael Spannowsky, Danny van Dyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a machine-learning-based workflow to model an unbinned likelihood
from its samples. A key advancement over existing approaches is the validation
of the learned likelihood using rigorous statistical tests of the joint
distribution, such as the Kolmogorov-Smirnov test of the joint distribution.
Our method enables the reliable communication of experimental and
phenomenological likelihoods for subsequent analyses. We demonstrate its
effectiveness through three case studies in high-energy physics. To support
broader adoption, we provide an open-source reference implementation, nabu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages + references, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse Design with Dynamic Mode Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Zhu, Liangliang Cheng, Anping Jing, Hanyu Huo, Ziqiang Lang, Bo Zhang, J. Nathan Kutz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a computationally efficient method for the automation of inverse
design in science and engineering. Based on simple least-square regression, the
underlying dynamic mode decomposition algorithm can be used to construct a
low-rank subspace spanning multiple experiments in parameter space. The
proposed inverse design dynamic mode composition (ID-DMD) algorithm leverages
the computed low-dimensional subspace to enable fast digital design and
optimization on laptop-level computing, including the potential to prescribe
the dynamics themselves. Moreover, the method is robust to noise, physically
interpretable, and can provide uncertainty quantification metrics. The
architecture can also efficiently scale to large-scale design problems using
randomized algorithms in the ID-DMD. The simplicity of the method and its
implementation are highly attractive in practice, and the ID-DMD has been
demonstrated to be an order of magnitude more accurate than competing methods
while simultaneously being 3-5 orders faster on challenging engineering design
problems ranging from structural vibrations to fluid dynamics. Due to its
speed, robustness, interpretability, and ease-of-use, ID-DMD in comparison with
other leading machine learning methods represents a significant advancement in
data-driven methods for inverse design and optimization, promising a paradigm
shift in how to approach inverse design in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Objective quantification of mood states using large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Onysk, Quentin Huys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotional states influence human behaviour and cognition, leading to diverse
thought trajectories. Similarly, Large Language Models (LLMs) showcase an
excellent level of response consistency across wide-ranging contexts (prompts).
We leverage these parallels to establish a framework for quantifying mental
states. Our approach utilises self-report questionnaires that reliably assess
these states due to their inherent sensitivity to patterns of co-occurring
responses. Specifically, we recruited a large sample of participants (N=422) to
investigate how well an LLM (Mistral-7B-OpenOrca) quantifies a heterogenous set
of depressive mood states measured with participants' open-ended responses to a
depression questionnaire. We show LLM responses to held-out multiple-choice
questions, given participants' open-ended answers, correlate strongly (r:
0.52-0.84) with true questionnaire scores, demonstrating LLM's generalisation
from mood representations. We explore a link between these representations and
factor analysis. Using ridge regression, we find depression-related subspaces
within LLM hidden states. We show these subspaces to be predictive of
participants' "Depression" and "Somatic & Emotional Distress" factor scores, as
well as suicidality severity. Overall, LLMs can provide quantitative measures
of mental states. The reliability of these hinges upon how informative the
questions we ask participants are. Used correctly, this approach could
supplement mental state assessment in a variety of settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main text - 9 pages, 5 figures;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing Generative AI value in a public sector context: evidence from
  a field experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trevor Fitzpatrick, Seamus Kelly, Patrick Carey, David Walsh, Ruairi Nugent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Generative AI (Gen AI) has motivated an interest in
understanding how it could be used to enhance productivity across various
tasks. We add to research results for the performance impact of Gen AI on
complex knowledge-based tasks in a public sector setting. In a pre-registered
experiment, after establishing a baseline level of performance, we find mixed
evidence for two types of composite tasks related to document understanding and
data analysis. For the Documents task, the treatment group using Gen AI had a
17% improvement in answer quality scores (as judged by human evaluators) and a
34% improvement in task completion time compared to a control group. For the
Data task, we find the Gen AI treatment group experienced a 12% reduction in
quality scores and no significant difference in mean completion time compared
to the control group. These results suggest that the benefits of Gen AI may be
task and potentially respondent dependent. We also discuss field notes and
lessons learned, as well as supplementary insights from a post-trial survey and
feedback workshop with participants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffRenderGAN: Addressing Training Data Scarcity in Deep Segmentation
  Networks for Quantitative Nanomaterial Analysis through Differentiable
  Rendering and Generative Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Possart, Leonid Mill, Florian Vollnhals, Tor Hildebrand, Peter Suter, Mathis Hoffmann, Jonas Utz, Daniel Augsburger, Mareike Thies, Mingxuan Wu, Fabian Wagner, George Sarau, Silke Christiansen, Katharina Breininger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanomaterials exhibit distinctive properties governed by parameters such as
size, shape, and surface characteristics, which critically influence their
applications and interactions across technological, biological, and
environmental contexts. Accurate quantification and understanding of these
materials are essential for advancing research and innovation. In this regard,
deep learning segmentation networks have emerged as powerful tools that enable
automated insights and replace subjective methods with precise quantitative
analysis. However, their efficacy depends on representative annotated datasets,
which are challenging to obtain due to the costly imaging of nanoparticles and
the labor-intensive nature of manual annotations. To overcome these
limitations, we introduce DiffRenderGAN, a novel generative model designed to
produce annotated synthetic data. By integrating a differentiable renderer into
a Generative Adversarial Network (GAN) framework, DiffRenderGAN optimizes
textural rendering parameters to generate realistic, annotated nanoparticle
images from non-annotated real microscopy images. This approach reduces the
need for manual intervention and enhances segmentation performance compared to
existing synthetic data methods by generating diverse and realistic data.
Tested on multiple ion and electron microscopy cases, including titanium
dioxide (TiO$_2$), silicon dioxide (SiO$_2$)), and silver nanowires (AgNW),
DiffRenderGAN bridges the gap between synthetic and real data, advancing the
quantification and understanding of complex nanomaterial systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Predict Global Atrial Fibrillation Dynamics from Sparse
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Jenkins, Andrea Cini, Joseph Barker, Alexander Sharp, Arunashis Sau, Varun Valentine, Srushti Valasang, Xinyang Li, Tom Wong, Timothy Betts, Danilo Mandic, Cesare Alippi, Fu Siong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catheter ablation of Atrial Fibrillation (AF) consists of a one-size-fits-all
treatment with limited success in persistent AF. This may be due to our
inability to map the dynamics of AF with the limited resolution and coverage
provided by sequential contact mapping catheters, preventing effective patient
phenotyping for personalised, targeted ablation. Here we introduce FibMap, a
graph recurrent neural network model that reconstructs global AF dynamics from
sparse measurements. Trained and validated on 51 non-contact whole atria
recordings, FibMap reconstructs whole atria dynamics from 10% surface coverage,
achieving a 210% lower mean absolute error and an order of magnitude higher
performance in tracking phase singularities compared to baseline methods.
Clinical utility of FibMap is demonstrated on real-world contact mapping
recordings, achieving reconstruction fidelity comparable to non-contact
mapping. FibMap's state-spaces and patient-specific parameters offer insights
for electrophenotyping AF. Integrating FibMap into clinical practice could
enable personalised AF care and improve outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Differentiable Rank-Based Objective For Better Feature Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krunoslav Lehman Pavasovic, David Lopez-Paz, Giulio Biroli, Levent Sagun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we leverage existing statistical methods to better understand
feature learning from data. We tackle this by modifying the model-free variable
selection method, Feature Ordering by Conditional Independence (FOCI), which is
introduced in \cite{azadkia2021simple}. While FOCI is based on a non-parametric
coefficient of conditional dependence, we introduce its parametric,
differentiable approximation. With this approximate coefficient of correlation,
we present a new algorithm called difFOCI, which is applicable to a wider range
of machine learning problems thanks to its differentiable nature and learnable
parameters. We present difFOCI in three contexts: (1) as a variable selection
method with baseline comparisons to FOCI, (2) as a trainable model parametrized
with a neural network, and (3) as a generic, widely applicable neural network
regularizer, one that improves feature learning with better management of
spurious correlations. We evaluate difFOCI on increasingly complex problems
ranging from basic variable selection in toy examples to saliency map
comparisons in convolutional networks. We then show how difFOCI can be
incorporated in the context of fairness to facilitate classifications without
relying on sensitive data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Conformal Prediction for Correlated Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Cini, Alexander Jenkins, Danilo Mandic, Cesare Alippi, Filippo Maria Bianchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of uncertainty quantification in time series
forecasting by exploiting observations at correlated sequences. Relational deep
learning methods leveraging graph representations are among the most effective
tools for obtaining point estimates from spatiotemporal data and correlated
time series. However, the problem of exploiting relational structures to
estimate the uncertainty of such predictions has been largely overlooked in the
same context. To this end, we propose a novel distribution-free approach based
on the conformal prediction framework and quantile regression. Despite the
recent applications of conformal prediction to sequential data, existing
methods operate independently on each target time series and do not account for
relationships among them when constructing the prediction interval. We fill
this void by introducing a novel conformal prediction method based on graph
deep learning operators. Our method, named Conformal Relational Prediction
(CoRel), does not require the relational structure (graph) to be known as a
prior and can be applied on top of any pre-trained time series predictor.
Additionally, CoRel includes an adaptive component to handle non-exchangeable
data and changes in the input time series. Our approach provides accurate
coverage and archives state-of-the-art uncertainty quantification in relevant
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Formulation for Non-Rectangular Lp Robust Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navdeep Kumar, Adarsh Gupta, Maxence Mohamed Elfatihi, Giorgia Ramponi, Kfir Yehuda Levy, Shie Mannor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study robust Markov decision processes (RMDPs) with non-rectangular
uncertainty sets, which capture interdependencies across states unlike
traditional rectangular models. While non-rectangular robust policy evaluation
is generally NP-hard, even in approximation, we identify a powerful class of
$L_p$-bounded uncertainty sets that avoid these complexity barriers due to
their structural simplicity. We further show that this class can be decomposed
into infinitely many \texttt{sa}-rectangular $L_p$-bounded sets and leverage
its structural properties to derive a novel dual formulation for $L_p$ RMDPs.
This formulation provides key insights into the adversary's strategy and
enables the development of the first robust policy evaluation algorithms for
non-rectangular RMDPs. Empirical results demonstrate that our approach
significantly outperforms brute-force methods, establishing a promising
foundation for future investigation into non-rectangular robust MDPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On multi-token prediction for efficient LLM inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somesh Mehra, Javier Alonso Garcia, Lukas Mauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We systematically investigate multi-token prediction (MTP) capabilities
within LLMs pre-trained for next-token prediction (NTP). We first show that
such models inherently possess MTP capabilities via numerical marginalization
over intermediate token probabilities, though performance is data-dependent and
improves with model scale. Furthermore, we explore the challenges of
integrating MTP heads into frozen LLMs and find that their hidden layers are
strongly specialized for NTP, making adaptation non-trivial. Finally, we show
that while joint training of MTP heads with the backbone improves performance,
it cannot fully overcome this barrier, prompting further research in this
direction. Our findings provide a deeper understanding of MTP applied to
pretrained LLMs, informing strategies for accelerating inference through
parallel token prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Reinforcement Learning for Optimization in Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Farooq, Kamran Iqbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has become a critical tool for optimization
challenges within automation, leading to significant advancements in several
areas. This review article examines the current landscape of RL within
automation, with a particular focus on its roles in manufacturing, energy
systems, and robotics. It discusses state-of-the-art methods, major challenges,
and upcoming avenues of research within each sector, highlighting RL's capacity
to solve intricate optimization challenges. The paper reviews the advantages
and constraints of RL-driven optimization methods in automation. It points out
prevalent challenges encountered in RL optimization, including issues related
to sample efficiency and scalability; safety and robustness; interpretability
and trustworthiness; transfer learning and meta-learning; and real-world
deployment and integration. It further explores prospective strategies and
future research pathways to navigate these challenges. Additionally, the survey
includes a comprehensive list of relevant research papers, making it an
indispensable guide for scholars and practitioners keen on exploring this
domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 tables, and 1 figure. Accepted at IEEE 20th International
  Conference on Automation Science and Engineering (CASE) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A hierarchical approach for assessing the vulnerability of tree-based
  classification models to membership inference attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard J. Preen, Jim Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models can inadvertently expose confidential properties of
their training data, making them vulnerable to membership inference attacks
(MIA). While numerous evaluation methods exist, many require computationally
expensive processes, such as training multiple shadow models. This article
presents two new complementary approaches for efficiently identifying
vulnerable tree-based models: an ante-hoc analysis of hyperparameter choices
and a post-hoc examination of trained model structure. While these new methods
cannot certify whether a model is safe from MIA, they provide practitioners
with a means to significantly reduce the number of models that need to undergo
expensive MIA assessment through a hierarchical filtering approach.
  More specifically, it is shown that the rank order of disclosure risk for
different hyperparameter combinations remains consistent across datasets,
enabling the development of simple, human-interpretable rules for identifying
relatively high-risk models before training. While this ante-hoc analysis
cannot determine absolute safety since this also depends on the specific
dataset, it allows the elimination of unnecessarily risky configurations during
hyperparameter tuning. Additionally, computationally inexpensive structural
metrics serve as indicators of MIA vulnerability, providing a second filtering
stage to identify risky models after training but before conducting expensive
attacks. Empirical results show that hyperparameter-based risk prediction rules
can achieve high accuracy in predicting the most at risk combinations of
hyperparameters across different tree-based model types, while requiring no
model training. Moreover, target model accuracy is not seen to correlate with
privacy risk, suggesting opportunities to optimise model configurations for
both performance and privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot Pouring: Identifying Causes of Spillage and Selecting Alternative
  Action Parameters Using Probabilistic Actual Causation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaime Maldonado, Jonas Krumme, Christoph Zetzsche, Vanessa Didelez, Kerstin Schill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a
large variety of objects and goals. When confronted with an unexpected or
unwanted outcome, we take corrective actions and try again until achieving the
desired result. The reasoning performed to identify a cause of the observed
outcome and to select an appropriate corrective action is a crucial aspect of
human reasoning for successful task execution. Central to this reasoning is the
assumption that a factor is responsible for producing the observed outcome. In
this paper, we investigate the use of probabilistic actual causation to
determine whether a factor is the cause of an observed undesired outcome.
Furthermore, we show how the actual causation probabilities can be used to find
alternative actions to change the outcome. We apply the probabilistic actual
causation analysis to a robot pouring task. When spillage occurs, the analysis
indicates whether a task parameter is the cause and how it should be changed to
avoid spillage. The analysis requires a causal graph of the task and the
corresponding conditional probability distributions. To fulfill these
requirements, we perform a complete causal modeling procedure (i.e., task
analysis, definition of variables, determination of the causal graph structure,
and estimation of conditional probability distributions) using data from a
realistic simulation of the robot pouring task, covering a large combinatorial
space of task parameters. Based on the results, we discuss the implications of
the variables' representation and how the alternative actions suggested by the
actual causation analysis would compare to the alternative solutions proposed
by a human observer. The practical use of the analysis of probabilistic actual
causation to select alternative action parameters is demonstrated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SQuARE: Sequential Question Answering Reasoning Engine for Enhanced
  Chain-of-Thought in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Fleischer, Moshe Berchansky, Gad Markovits, Moshe Wasserblat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of Natural Language Processing, Large Language
Models (LLMs) are tasked with increasingly complex reasoning challenges.
Traditional methods like chain-of-thought prompting have shown promise but
often fall short in fully leveraging a model's reasoning capabilities. This
paper introduces SQuARE (Sequential Question Answering Reasoning Engine), a
novel prompting technique designed to improve reasoning through a
self-interrogation paradigm. Building upon CoT frameworks, SQuARE prompts
models to generate and resolve multiple auxiliary questions before tackling the
main query, promoting a more thorough exploration of various aspects of a
topic. Our expansive evaluations, conducted with Llama 3 and GPT-4o models
across multiple question-answering datasets, demonstrate that SQuARE
significantly surpasses traditional CoT prompts and existing
rephrase-and-respond methods. By systematically decomposing queries, SQuARE
advances LLM capabilities in reasoning tasks. The code is publicly available at
https://github.com/IntelLabs/RAG-FiT/tree/square.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoRA Training Provably Converges to a Low-Rank Global Minimum or It
  Fails Loudly (But it Probably Won't Fail) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsu Kim, Jaeyeon Kim, Ernest K. Ryu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank adaptation (LoRA) has become a standard approach for fine-tuning
large foundation models. However, our theoretical understanding of LoRA remains
limited as prior analyses of LoRA's training dynamics either rely on
linearization arguments or consider highly simplified setups. In this work, we
analyze the LoRA loss landscape without such restrictive assumptions. We define
two regimes: a ``special regime'', which includes idealized setups where
linearization arguments hold, and a ``generic regime'' representing more
realistic setups where linearization arguments do not hold. In the generic
regime, we show that LoRA training converges to a global minimizer with low
rank and small magnitude, or a qualitatively distinct solution with high rank
and large magnitude. Finally, we argue that the zero-initialization and weight
decay in LoRA training induce an implicit bias toward the low-rank,
small-magnitude region of the parameter space -- where global minima lie --
thus shedding light on why LoRA training usually succeeds in finding global
minima.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating multiple single-event upsets during deep neural network
  inference using fault-aware training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toon Vinck, Naïn Jonckers, Gert Dekkers, Jeffrey Prinzie, Peter Karsmakers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are increasingly used in safety-critical
applications. Reliable fault analysis and mitigation are essential to ensure
their functionality in harsh environments that contain high radiation levels.
This study analyses the impact of multiple single-bit single-event upsets in
DNNs by performing fault injection at the level of a DNN model. Additionally, a
fault aware training (FAT) methodology is proposed that improves the DNNs'
robustness to faults without any modification to the hardware. Experimental
results show that the FAT methodology improves the tolerance to faults up to a
factor 3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, Topical Workshop on Electronics for Particle
  Physics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Agents as Digital Representatives in Collective Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Jarrett, Miruna Pîslar, Michiel A. Bakker, Michael Henry Tessler, Raphael Köster, Jan Balaguer, Romuald Elie, Christopher Summerfield, Andrea Tacchetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider the process of collective decision-making, in which a group of
individuals interactively select a preferred outcome from among a universe of
alternatives. In this context, "representation" is the activity of making an
individual's preferences present in the process via participation by a proxy
agent -- i.e. their "representative". To this end, learned models of human
behavior have the potential to fill this role, with practical implications for
multi-agent scenario studies and mechanism design. In this work, we investigate
the possibility of training \textit{language agents} to behave in the capacity
of representatives of human agents, appropriately expressing the preferences of
those individuals whom they stand for. First, we formalize the setting of
\textit{collective decision-making} -- as the episodic process of interaction
between a group of agents and a decision mechanism. On this basis, we then
formalize the problem of \textit{digital representation} -- as the simulation
of an agent's behavior to yield equivalent outcomes from the mechanism.
Finally, we conduct an empirical case study in the setting of
\textit{consensus-finding} among diverse humans, and demonstrate the
feasibility of fine-tuning large language models to act as digital
representatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple Path Structural Encoding for Graph Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louis Airale, Antonio Longa, Mattia Rigon, Andrea Passerini, Roberto Passerone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph transformers extend global self-attention to graph-structured data,
achieving notable success in graph learning. Recently, random walk structural
encoding (RWSE) has been found to further enhance their predictive power by
encoding both structural and positional information into the edge
representation. However, RWSE cannot always distinguish between edges that
belong to different local graph patterns, which reduces its ability to capture
the full structural complexity of graphs. This work introduces Simple Path
Structural Encoding (SPSE), a novel method that utilizes simple path counts for
edge encoding. We show theoretically and experimentally that SPSE overcomes the
limitations of RWSE, providing a richer representation of graph structures,
particularly for capturing local cyclic patterns. To make SPSE computationally
tractable, we propose an efficient approximate algorithm for simple path
counting. SPSE demonstrates significant performance improvements over RWSE on
various benchmarks, including molecular and long-range graph datasets,
achieving statistically significant gains in discriminative tasks. These
results pose SPSE as a powerful edge encoding alternative for enhancing the
expressivity of graph transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Accuracy Cost of Weakness: A Theoretical Analysis of Fixed-Segment
  Weak Labeling for Events in Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Martinsson, Olof Mogren, Tuomas Virtanen, Maria Sandsten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate labels are critical for deriving robust machine learning models.
Labels are used to train supervised learning models and to evaluate most
machine learning paradigms. In this paper, we model the accuracy and cost of a
common weak labeling process where annotators assign presence or absence labels
to fixed-length data segments for a given event class. The annotator labels a
segment as "present" if it sufficiently covers an event from that class, e.g.,
a birdsong sound event in audio data. We analyze how the segment length affects
the label accuracy and the required number of annotations, and compare this
fixed-length labeling approach with an oracle method that uses the true event
activations to construct the segments. Furthermore, we quantify the gap between
these methods and verify that in most realistic scenarios the oracle method is
better than the fixed-length labeling method in both accuracy and cost. Our
findings provide a theoretical justification for adaptive weak labeling
strategies that mimic the oracle process, and a foundation for optimizing weak
labeling processes in sequence labeling tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wasserstein distributional adversarial training for deep neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Bai, Guangyi He, Yifan Jiang, Jan Obloj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Design of adversarial attacks for deep neural networks, as well as methods of
adversarial training against them, are subject of intense research. In this
paper, we propose methods to train against distributional attack threats,
extending the TRADES method used for pointwise attacks. Our approach leverages
recent contributions and relies on sensitivity analysis for Wasserstein
distributionally robust optimization problems. We introduce an efficient
fine-tuning method which can be deployed on a previously trained model. We test
our methods on a range of pre-trained models on RobustBench. These experimental
results demonstrate the additional training enhances Wasserstein distributional
robustness, while maintaining original levels of pointwise robustness, even for
already very successful networks. The improvements are less marked for models
pre-trained using huge synthetic datasets of 20-100M images. However,
remarkably, sometimes our methods are still able to improve their performance
even when trained using only the original training dataset (50k images).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning for modelling unstructured grid data in computational
  physics: a <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sibo Cheng, Marc Bocquet, Weiping Ding, Tobias Sebastian Finn, Rui Fu, Jinlong Fu, Yike Guo, Eleda Johnson, Siyi Li, Che Liu, Eric Newton Moro, Jie Pan, Matthew Piggott, Cesar Quilodran, Prakhar Sharma, Kun Wang, Dunhui Xiao, Xiao Xue, Yong Zeng, Mingrui Zhang, Hao Zhou, Kewei Zhu, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unstructured grid data are essential for modelling complex geometries and
dynamics in computational physics. Yet, their inherent irregularity presents
significant challenges for conventional machine learning (ML) techniques. This
paper provides a comprehensive review of advanced ML methodologies designed to
handle unstructured grid data in high-dimensional dynamical systems. Key
approaches discussed include graph neural networks, transformer models with
spatial attention mechanisms, interpolation-integrated ML methods, and meshless
techniques such as physics-informed neural networks. These methodologies have
proven effective across diverse fields, including fluid dynamics and
environmental simulations. This review is intended as a guidebook for
computational scientists seeking to apply ML approaches to unstructured grid
data in their domains, as well as for ML researchers looking to address
challenges in computational physics. It places special focus on how ML methods
can overcome the inherent limitations of traditional numerical techniques and,
conversely, how insights from computational physics can inform ML development.
To support benchmarking, this review also provides a summary of open-access
datasets of unstructured grid data in computational physics. Finally, emerging
directions such as generative models with unstructured data, reinforcement
learning for mesh generation, and hybrid physics-data-driven paradigms are
discussed to inspire future advancements in this evolving field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Spatiotemporal Point Processes: Trends and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumantrak Mukherjee, Mouad Elhamdi, George Mohler, David A. Selby, Yao Xie, Sebastian Vollmer, Gerrit Grossmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal point processes (STPPs) are probabilistic models for events
occurring in continuous space and time. Real-world event data often exhibit
intricate dependencies and heterogeneous dynamics. By incorporating modern deep
learning techniques, STPPs can model these complexities more effectively than
traditional approaches. Consequently, the fusion of neural methods with STPPs
has become an active and rapidly evolving research area. In this review, we
categorize existing approaches, unify key design choices, and explain the
challenges of working with this data modality. We further highlight emerging
trends and diverse application domains. Finally, we identify open challenges
and gaps in the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ This looks like what? Challenges and Future Research Directions for
  Part-Prototype Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khawla Elhadri, Tomasz Michalski, Adam Wróbel, Jörg Schlötterer, Bartosz Zieliński, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing interest in eXplainable Artificial Intelligence (XAI) has
prompted research into models with built-in interpretability, the most
prominent of which are part-prototype models. Part-Prototype Models (PPMs) make
decisions by comparing an input image to a set of learned prototypes, providing
human-understandable explanations in the form of ``this looks like that''.
Despite their inherent interpretability, PPMS are not yet considered a valuable
alternative to post-hoc models. In this survey, we investigate the reasons for
this and provide directions for future research. We analyze papers from 2019 to
2024, and derive a taxonomy of the challenges that current PPMS face. Our
analysis shows that the open challenges are quite diverse. The main concern is
the quality and quantity of prototypes. Other concerns are the lack of
generalization to a variety of tasks and contexts, and general methodological
issues, including non-standardized evaluation. We provide ideas for future
research in five broad directions: improving predictive performance, developing
novel architectures grounded in theory, establishing frameworks for human-AI
collaboration, aligning models with humans, and establishing metrics and
benchmarks for evaluation. We hope that this survey will stimulate research and
promote intrinsically interpretable models for application domains. Our list of
surveyed papers is available at https://github.com/aix-group/ppm-survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Diffusion Network for Drug-Gene Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Wu, Wensheng Gan, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting drug-gene associations is crucial for drug development and disease
treatment. While graph neural networks (GNN) have shown effectiveness in this
task, they face challenges with data sparsity and efficient contrastive
learning implementation. We introduce a graph diffusion network for drug-gene
prediction (GDNDGP), a framework that addresses these limitations through two
key innovations. First, it employs meta-path-based homogeneous graph learning
to capture drug-drug and gene-gene relationships, ensuring similar entities
share embedding spaces. Second, it incorporates a parallel diffusion network
that generates hard negative samples during training, eliminating the need for
exhaustive negative sample retrieval. Our model achieves superior performance
on the DGIdb 4.0 dataset and demonstrates strong generalization capability on
tripartite drug-gene-disease networks. Results show significant improvements
over existing methods in drug-gene prediction tasks, particularly in handling
complex heterogeneous relationships. The source code is publicly available at
https://github.com/csjywu1/GDNDGP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/ACM TCBB. 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Full Swap Regret and Discretized Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxwell Fishelson, Robert Kleinberg, Princewill Okoroafor, Renato Paes Leme, Jon Schneider, Yifeng Teng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of minimizing swap regret in structured normal-form
games. Players have a very large (potentially infinite) number of pure actions,
but each action has an embedding into $d$-dimensional space and payoffs are
given by bilinear functions of these embeddings. We provide an efficient
learning algorithm for this setting that incurs at most
$\tilde{O}(T^{(d+1)/(d+3)})$ swap regret after $T$ rounds.
  To achieve this, we introduce a new online learning problem we call
\emph{full swap regret minimization}. In this problem, a learner repeatedly
takes a (randomized) action in a bounded convex $d$-dimensional action set
$\mathcal{K}$ and then receives a loss from the adversary, with the goal of
minimizing their regret with respect to the \emph{worst-case} swap function
mapping $\mathcal{K}$ to $\mathcal{K}$. For varied assumptions about the
convexity and smoothness of the loss functions, we design algorithms with full
swap regret bounds ranging from $O(T^{d/(d+2)})$ to $O(T^{(d+1)/(d+2)})$.
  Finally, we apply these tools to the problem of online forecasting to
minimize calibration error, showing that several notions of calibration can be
viewed as specific instances of full swap regret. In particular, we design
efficient algorithms for online forecasting that guarantee at most $O(T^{1/3})$
$\ell_2$-calibration error and $O(\max(\sqrt{\epsilon T}, T^{1/3}))$
\emph{discretized-calibration} error (when the forecaster is restricted to
predicting multiples of $\epsilon$).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimization for Simultaneous Selection of Machine Learning
  Algorithms and Hyperparameters on Shared Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Ishikawa, Ryota Ozaki, Yohei Kanzaki, Ichiro Takeuchi, Masayuki Karasuyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting the optimal combination of a machine learning (ML) algorithm and
its hyper-parameters is crucial for the development of high-performance ML
systems. However, since the combination of ML algorithms and hyper-parameters
is enormous, the exhaustive validation requires a significant amount of time.
Many existing studies use Bayesian optimization (BO) for accelerating the
search. On the other hand, a significant difficulty is that, in general, there
exists a different hyper-parameter space for each one of candidate ML
algorithms. BO-based approaches typically build a surrogate model independently
for each hyper-parameter space, by which sufficient observations are required
for all candidate ML algorithms. In this study, our proposed method embeds
different hyper-parameter spaces into a shared latent space, in which a
surrogate multi-task model for BO is estimated. This approach can share
information of observations from different ML algorithms by which efficient
optimization is expected with a smaller number of total observations. We
further propose the pre-training of the latent space embedding with an
adversarial regularization, and a ranking model for selecting an effective
pre-trained embedding for a given target dataset. Our empirical study
demonstrates effectiveness of the proposed method through datasets from OpenML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth-Bounds for Neural Networks via the Braid Arrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Grillo, Christoph Hertrich, Georg Loho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We contribute towards resolving the open question of how many hidden layers
are required in ReLU networks for exactly representing all continuous and
piecewise linear functions on $\mathbb{R}^d$. While the question has been
resolved in special cases, the best known lower bound in general is still 2. We
focus on neural networks that are compatible with certain polyhedral complexes,
more precisely with the braid fan. For such neural networks, we prove a
non-constant lower bound of $\Omega(\log\log d)$ hidden layers required to
exactly represent the maximum of $d$ numbers. Additionally, under our
assumption, we provide a combinatorial proof that 3 hidden layers are necessary
to compute the maximum of 5 numbers; this had only been verified with an
excessive computation so far. Finally, we show that a natural generalization of
the best known upper bound to maxout networks is not tight, by demonstrating
that a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to
represent the maximum of 7 numbers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Jensen Gap for Max-Min Group Fairness Optimization in
  Recommendation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Yuxin Li, Wenjie Wang, Liang Pang, Jun Xu, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group max-min fairness (MMF) is commonly used in fairness-aware recommender
systems (RS) as an optimization objective, as it aims to protect marginalized
item groups and ensures a fair competition platform. However, our theoretical
analysis indicates that integrating MMF constraint violates the assumption of
sample independence during optimization, causing the loss function to deviate
from linear additivity. Such nonlinearity property introduces the Jensen gap
between the model's convergence point and the optimal point if mini-batch
sampling is applied. Both theoretical and empirical studies show that as the
mini-batch size decreases and the group size increases, the Jensen gap will
widen accordingly. Some methods using heuristic re-weighting or debiasing
strategies have the potential to bridge the Jensen gap. However, they either
lack theoretical guarantees or suffer from heavy computational costs. To
overcome these limitations, we first theoretically demonstrate that the
MMF-constrained objective can be essentially reformulated as a group-weighted
optimization objective. Then we present an efficient and effective algorithm
named FairDual, which utilizes a dual optimization technique to minimize the
Jensen gap. Our theoretical analysis demonstrates that FairDual can achieve a
sub-linear convergence rate to the globally optimal solution and the Jensen gap
can be well bounded under a mini-batch sampling strategy with random shuffle.
Extensive experiments conducted using six large-scale RS backbone models on
three publicly available datasets demonstrate that FairDual outperforms all
baselines in terms of both accuracy and fairness. Our data and codes are shared
at https://github.com/XuChen0427/FairDual.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SigGate: Enhancing Recurrent Neural Networks with Signature-Based Gating
  Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémi Genet, Hugo Inzirillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach that enhances recurrent neural
networks (RNNs) by incorporating path signatures into their gating mechanisms.
Our method modifies both Long Short-Term Memory (LSTM) and Gated Recurrent Unit
(GRU) architectures by replacing their forget and reset gates, respectively,
with learnable path signatures. These signatures, which capture the geometric
features of the entire path history, provide a richer context for controlling
information flow through the network's memory. This modification allows the
networks to make memory decisions based on the full historical context rather
than just the current input and state. Through experimental studies, we
demonstrate that our Signature-LSTM (SigLSTM) and Signature-GRU (SigGRU) models
outperform their traditional counterparts across various sequential learning
tasks. By leveraging path signatures in recurrent architectures, this method
offers new opportunities to enhance performance in time series analysis and
forecasting applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-asymptotic Analysis of Diffusion Annealed Langevin Monte Carlo for
  Generative Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paula Cordero-Encinar, O. Deniz Akyildiz, Andrew B. Duncan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the theoretical properties of general diffusion
(interpolation) paths and their Langevin Monte Carlo implementation, referred
to as diffusion annealed Langevin Monte Carlo (DALMC), under weak conditions on
the data distribution. Specifically, we analyse and provide non-asymptotic
error bounds for the annealed Langevin dynamics where the path of distributions
is defined as Gaussian convolutions of the data distribution as in diffusion
models. We then extend our results to recently proposed heavy-tailed (Student's
t) diffusion paths, demonstrating their theoretical properties for heavy-tailed
data distributions for the first time. Our analysis provides theoretical
guarantees for a class of score-based generative models that interpolate
between a simple distribution (Gaussian or Student's t) and the data
distribution in finite time. This approach offers a broader perspective
compared to standard score-based diffusion approaches, which are typically
based on a forward Ornstein-Uhlenbeck (OU) noising process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Seamless Hierarchical Federated Learning under Intermittent
  Client Participation: A Stagewise Decision-Making Methodology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Wu, Minghui Liwang, Yuhan Su, Li Li, Seyyedali Hosseinalipour, Xianbin Wang, Huaiyu Dai, Zhenzhen Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a pioneering distributed learning paradigm
that enables devices/clients to build a shared global model. This global model
is obtained through frequent model transmissions between clients and a central
server, which may cause high latency, energy consumption, and congestion over
backhaul links. To overcome these drawbacks, Hierarchical Federated Learning
(HFL) has emerged, which organizes clients into multiple clusters and utilizes
edge nodes (e.g., edge servers) for intermediate model aggregations between
clients and the central server. Current research on HFL mainly focus on
enhancing model accuracy, latency, and energy consumption in scenarios with a
stable/fixed set of clients. However, addressing the dynamic availability of
clients -- a critical aspect of real-world scenarios -- remains underexplored.
This study delves into optimizing client selection and client-to-edge
associations in HFL under intermittent client participation so as to minimize
overall system costs (i.e., delay and energy), while achieving fast model
convergence. We unveil that achieving this goal involves solving a complex
NP-hard problem. To tackle this, we propose a stagewise methodology that splits
the solution into two stages, referred to as Plan A and Plan B. Plan A focuses
on identifying long-term clients with high chance of participation in
subsequent model training rounds. Plan B serves as a backup, selecting
alternative clients when long-term clients are unavailable during model
training rounds. This stagewise methodology offers a fresh perspective on
client selection that can enhance both HFL and conventional FL via enabling
low-overhead decision-making processes. Through evaluations on MNIST and
CIFAR-10 datasets, we show that our methodology outperforms existing benchmarks
in terms of model accuracy and system costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures,5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convex Is Back: Solving Belief MDPs With Convexity-Informed Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Koutas, Daniel Hettegger, Kostas G. Papakonstantinou, Daniel Straub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method for Deep Reinforcement Learning (DRL),
incorporating the convex property of the value function over the belief space
in Partially Observable Markov Decision Processes (POMDPs). We introduce hard-
and soft-enforced convexity as two different approaches, and compare their
performance against standard DRL on two well-known POMDP environments, namely
the Tiger and FieldVisionRockSample problems. Our findings show that including
the convexity feature can substantially increase performance of the agents, as
well as increase robustness over the hyperparameter space, especially when
testing on out-of-distribution domains. The source code for this work can be
found at https://github.com/Dakout/Convex_DRL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When do neural networks learn world models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianren Zhang, Guanyu Chen, Feng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans develop world models that capture the underlying generation process of
data. Whether neural networks can learn similar world models remains an open
problem. In this work, we provide the first theoretical results for this
problem, showing that in a multi-task setting, models with a low-degree bias
provably recover latent data-generating variables under mild assumptions --
even if proxy tasks involve complex, non-linear functions of the latents.
However, such recovery is also sensitive to model architecture. Our analysis
leverages Boolean models of task solutions via the Fourier-Walsh transform and
introduces new techniques for analyzing invertible Boolean transforms, which
may be of independent interest. We illustrate the algorithmic implications of
our results and connect them to related research areas, including
self-supervised learning, out-of-distribution generalization, and the linear
representation hypothesis in large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Attention Mechanism Learning to Facilitate Opto-physiological
  Monitoring during Physical Activity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zheng, Sijung Hu, Vincent Dwyer, Mahsa Derakhshani, Laura Barrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opto-physiological monitoring is a non-contact technique for measuring
cardiac signals, i.e., photoplethysmography (PPG). Quality PPG signals directly
lead to reliable physiological readings. However, PPG signal acquisition
procedures are often accompanied by spurious motion artefacts (MAs), especially
during low-to-high-intensity physical activity. This study proposes a practical
adversarial learning approach for opto-physiological monitoring by using a
generative adversarial network with an attention mechanism (AM-GAN) to model
motion noise and to allow MA removal. The AM-GAN learns an MA-resistant mapping
from raw and noisy signals to clear PPG signals in an adversarial manner,
guided by an attention mechanism to directly translate the motion reference of
triaxial acceleration to the MAs appearing in the raw signal. The AM-GAN was
experimented with three various protocols engaged with 39 subjects in various
physical activities. The average absolute error for heart rate (HR) derived
from the MA-free PPG signal via the AM-GAN, is 1.81 beats/min for the IEEE-SPC
dataset and 3.86 beats/min for the PPGDalia dataset. The same procedure applied
to an in-house LU dataset resulted in average absolute errors for HR and
respiratory rate (RR) of less than 1.37 beats/min and 2.49 breaths/min,
respectively. The study demonstrates the robustness and resilience of AM-GAN,
particularly during low-to-high-intensity physical activities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Rolling Horizon Optimization for Network-Constrained V2X Value
  Stacking of Electric Vehicles Under Uncertainties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Canchen Jiang, Ariel Liebman, Bo Jie, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electric vehicle (EV) coordination can provide significant benefits through
vehicle-to-everything (V2X) by interacting with the grid, buildings, and other
EVs. This work aims to develop a V2X value-stacking framework, including
vehicle-to-building (V2B), vehicle-to-grid (V2G), and energy trading, to
maximize economic benefits for residential communities while maintaining
distribution voltage. This work also seeks to quantify the impact of prediction
errors related to building load, renewable energy, and EV arrivals. A dynamic
rolling-horizon optimization (RHO) method is employed to leverage multiple
revenue streams and maximize the potential of EV coordination. To address
energy uncertainties, including hourly local building load, local photovoltaic
(PV) generation, and EV arrivals, this work develops a Transformer-based
forecasting model named Gated Recurrent Units-Encoder-Temporal Fusion Decoder
(GRU-EN-TFD). The simulation results, using real data from Australia's National
Electricity Market, and the Independent System Operators in New England and New
York in the US, reveal that V2X value stacking can significantly reduce energy
costs. The proposed GRU-EN-TFD model outperforms the benchmark forecast model.
Uncertainties in EV arrivals have a more substantial impact on value-stacking
performance, highlighting the significance of its accurate forecast. This work
provides new insights into the dynamic interactions among residential
communities, unlocking the full potential of EV batteries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, accepted by Renewable Energy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Uncertainty Principle for Linear Recurrent Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre François, Antonio Orvieto, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider linear recurrent neural networks, which have become a key
building block of sequence modeling due to their ability for stable and
effective long-range modeling. In this paper, we aim at characterizing this
ability on a simple but core copy task, whose goal is to build a linear filter
of order $S$ that approximates the filter that looks $K$ time steps in the past
(which we refer to as the shift-$K$ filter), where $K$ is larger than $S$.
Using classical signal models and quadratic cost, we fully characterize the
problem by providing lower bounds of approximation, as well as explicit filters
that achieve this lower bound up to constants. The optimal performance
highlights an uncertainty principle: the optimal filter has to average values
around the $K$-th time step in the past with a range~(width) that is
proportional to $K/S$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FE-LWS: Refined Image-Text Representations via Decoder Stacking and
  Fused Encodings for Remote Sensing Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Swadhin Das, Raksha Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing image captioning aims to generate descriptive text from remote
sensing images, typically employing an encoder-decoder framework. In this
setup, a convolutional neural network (CNN) extracts feature representations
from the input image, which then guide the decoder in a sequence-to-sequence
caption generation process. Although much research has focused on refining the
decoder, the quality of image representations from the encoder remains crucial
for accurate captioning. This paper introduces a novel approach that integrates
features from two distinct CNN based encoders, capturing complementary
information to enhance caption generation. Additionally, we propose a weighted
averaging technique to combine the outputs of all GRUs in the stacked decoder.
Furthermore, a comparison-based beam search strategy is incorporated to refine
caption selection. The results demonstrate that our fusion-based approach,
along with the enhanced stacked decoder, significantly outperforms both the
transformer-based state-of-the-art model and other LSTM-based baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via
  Subgraph Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlun Zhang, Enyan Dai, Kentaro Yoshioka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in
modeling data with graph structures, yet recent research reveals their
susceptibility to adversarial attacks. Traditional attack methodologies, which
rely on manipulating the original graph or adding links to artificially created
nodes, often prove impractical in real-world settings. This paper introduces a
novel adversarial scenario involving the injection of an isolated subgraph to
deceive both the link recommender and the node classifier within a GNN system.
Specifically, the link recommender is mislead to propose links between targeted
victim nodes and the subgraph, encouraging users to unintentionally establish
connections and that would degrade the node classification accuracy, thereby
facilitating a successful attack. To address this, we present the LiSA
framework, which employs a dual surrogate model and bi-level optimization to
simultaneously meet two adversarial objectives. Extensive experiments on
real-world datasets demonstrate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GEVRM: Goal-Expressive <span class="highlight-title">Video</span> Generation Model For Robust Visual
  Manipulation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of embodied artificial intelligence, significant
progress has been made in vision-language-action (VLA) models for general robot
decision-making. However, the majority of existing VLAs fail to account for the
inevitable external perturbations encountered during deployment. These
perturbations introduce unforeseen state information to the VLA, resulting in
inaccurate actions and consequently, a significant decline in generalization
performance. The classic internal model control (IMC) principle demonstrates
that a closed-loop system with an internal model that includes external input
signals can accurately track the reference input and effectively offset the
disturbance. We propose a novel closed-loop VLA method GEVRM that integrates
the IMC principle to enhance the robustness of robot visual manipulation. The
text-guided video generation model in GEVRM can generate highly expressive
future visual planning goals. Simultaneously, we evaluate perturbations by
simulating responses, which are called internal embeddings and optimized
through prototype contrastive learning. This allows the model to implicitly
infer and distinguish perturbations from the external environment. The proposed
GEVRM achieves state-of-the-art performance on both standard and perturbed
CALVIN benchmarks and shows significant improvements in realistic robot tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking the Potential of Classic GNNs for Graph-level Tasks: Simple
  Architectures Meet Excellence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuankai Luo, Lei Shi, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message-passing Graph Neural Networks (GNNs) are often criticized for their
limited expressiveness, issues like over-smoothing and over-squashing, and
challenges in capturing long-range dependencies, while Graph Transformers (GTs)
are considered superior due to their global attention mechanisms. Literature
frequently suggests that GTs outperform GNNs, particularly in graph-level tasks
such as graph classification and regression. In this study, we explore the
untapped potential of GNNs through an enhanced framework, GNN+, which
integrates six widely used techniques: edge feature integration, normalization,
dropout, residual connections, feed-forward networks, and positional encoding,
to effectively tackle graph-level tasks. We conduct a systematic evaluation of
three classic GNNs, namely GCN, GIN, and GatedGCN, enhanced by the GNN+
framework across 14 well-known graph-level datasets. Our results show that,
contrary to the prevailing belief, classic GNNs excel in graph-level tasks,
securing top three rankings across all datasets and achieving first place in
eight, while also demonstrating greater efficiency than GTs. This highlights
the potential of simple GNN architectures, challenging the belief that complex
mechanisms in GTs are essential for superior graph-level performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bandit Multiclass List Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liad Erez, Tomer Koren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of multiclass list classification with (semi-)bandit
feedback, where input examples are mapped into subsets of size $m$ of a
collection of $K$ possible labels, and the feedback consists of the predicted
labels which lie in the set of true labels of the given example. Our main
result is for the $(\varepsilon,\delta)$-PAC variant of the problem for which
we design an algorithm that returns an $\varepsilon$-optimal hypothesis with
high probability using a sample complexity of $O \big( (\mathrm{poly}(K/m) + sm
/ \varepsilon^2) \log (|H|/\delta) \big)$ where $H$ is the underlying (finite)
hypothesis class and $s$ is an upper bound on the number of true labels for a
given example. This bound improves upon known bounds for combinatorial
semi-bandits whenever $s \ll K$. Moreover, in the regime where $s = O(1)$ the
leading terms in our bound match the corresponding full-information rates,
implying that bandit feedback essentially comes at no cost. Our PAC learning
algorithm is also computationally efficient given access to an ERM oracle for
$H$. Additionally, we consider the regret minimization setting where data can
be generated adversarially, and establish a regret bound of $\widetilde O(|H| +
\sqrt{smT \log |H|})$. Our results generalize and extend those of Erez et al.
(2024) who consider the simpler single-label setting corresponding to $s=m=1$,
and in fact hold for the more general contextual combinatorial semi-bandit
problem with $s$-sparse rewards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnomalyGFM: Graph Foundation Model for Zero/Few-shot Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hezhe Qiao, Chaoxi Niu, Ling Chen, Guansong Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph anomaly detection (GAD) aims to identify abnormal nodes that differ
from the majority of the nodes in a graph, which has been attracting
significant attention in recent years. Existing generalist graph models have
achieved remarkable success in different graph tasks but struggle to generalize
to the GAD task. This limitation arises from their difficulty in learning
generalized knowledge for capturing the inherently infrequent, irregular and
heterogeneous abnormality patterns in graphs from different domains. To address
this challenge, we propose AnomalyGFM, a GAD-oriented graph foundation model
that supports zero-shot inference and few-shot prompt tuning for GAD in diverse
graph datasets. One key insight is that graph-agnostic representations for
normal and abnormal classes are required to support effective zero/few-shot GAD
across different graphs. Motivated by this, AnomalyGFM is pre-trained to align
data-independent, learnable normal and abnormal class prototypes with node
representation residuals (i.e., representation deviation of a node from its
neighbors). The residual features essentially project the node information into
a unified feature space where we can effectively measure the abnormality of
nodes from different graphs in a consistent way. This provides a driving force
for the learning of graph-agnostic, discriminative prototypes for the normal
and abnormal classes, which can be used to enable zero-shot GAD on new graphs,
including very large-scale graphs. If there are few-shot labeled normal nodes
available in the new graphs, AnomalyGFM can further support prompt tuning to
leverage these nodes for better adaptation. Comprehensive experiments on 11
widely-used GAD datasets with real anomalies, demonstrate that AnomalyGFM
significantly outperforms state-of-the-art competing methods under both zero-
and few-shot GAD settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Importance of Embedding Norms in Self-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Draganov, Sharvaree Vadgama, Sebastian Damrich, Jan Niklas Böhm, Lucas Maes, Dmitry Kobak, Erik Bekkers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) allows training data representations without a
supervised signal and has become an important paradigm in machine learning.
Most SSL methods employ the cosine similarity between embedding vectors and
hence effectively embed data on a hypersphere. While this seemingly implies
that embedding norms cannot play any role in SSL, a few recent works have
suggested that embedding norms have properties related to network convergence
and confidence. In this paper, we resolve this apparent contradiction and
systematically establish the embedding norm's role in SSL training. Using
theoretical analysis, simulations, and experiments, we show that embedding
norms (i) govern SSL convergence rates and (ii) encode network confidence, with
smaller norms corresponding to unexpected samples. Additionally, we show that
manipulating embedding norms can have large effects on convergence speed. Our
findings demonstrate that SSL embedding norms are integral to understanding and
optimizing network behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ You Do Not Fully Utilize Transformer's Representation Capacity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Gerasimov, Yaroslav Aksenov, Nikita Balagansky, Viacheslav Sinii, Daniil Gavrilov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to RNNs, which compress previous tokens into a single hidden
state, Transformers can attend to all previous tokens directly. However,
standard Transformers only use representations from the immediately preceding
layer. In this paper, we show that this design choice causes representation
collapse and leads to suboptimal performance. To address this issue, we
introduce Layer-Integrated Memory (LIMe), a simple yet powerful approach that
preserves the model's overall memory footprint while expanding its
representational capacity by allowing access to hidden states from earlier
layers. Through extensive experiments across various architectures and
different lookup mechanisms, we demonstrate consistent performance improvements
on a wide range of tasks. Moreover, our analysis of the learned representation
dynamics and our exploration of depthwise circuits reveal how LIMe integrates
information across layers, pointing to promising directions for future
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abduction of Domain Relationships from Data for VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Al Mehdi Saadat Chowdhury, Paulo Shakarian, Gerardo I. Simari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of visual question answering (VQA) where
the image and query are represented by ASP programs that lack domain data. We
provide an approach that is orthogonal and complementary to existing knowledge
augmentation techniques where we abduce domain relationships of image
constructs from past examples. After framing the abduction problem, we provide
a baseline approach, and an implementation that significantly improves the
accuracy of query answering yet requires few examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings ICLP 2024, arXiv:2502.08453</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-Symbolic Contrastive Learning for Cross-domain Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyue Liu, Ryo Ueda, Zhen Wan, Katsumi Inoue, Chris G. Willcocks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) have made significant advances in natural
language inference (NLI) tasks, however their sensitivity to textual
perturbations and dependence on large datasets indicate an over-reliance on
shallow heuristics. In contrast, inductive logic programming (ILP) excels at
inferring logical relationships across diverse, sparse and limited datasets,
but its discrete nature requires the inputs to be precisely specified, which
limits their application. This paper proposes a bridge between the two
approaches: neuro-symbolic contrastive learning. This allows for smooth and
differentiable optimisation that improves logical accuracy across an otherwise
discrete, noisy, and sparse topological space of logical functions. We show
that abstract logical relationships can be effectively embedded within a
neuro-symbolic paradigm, by representing data as logic programs and sets of
logic rules. The embedding space captures highly varied textual information
with similar semantic logical relations, but can also separate similar textual
relations that have dissimilar logical relations. Experimental results
demonstrate that our approach significantly improves the inference capabilities
of the models in terms of generalisation and reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings ICLP 2024, arXiv:2502.08453</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Euclidean Alignment for Transfer Learning in EEG-Based
  Brain-Computer Interfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongrui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the non-stationarity and large individual differences of EEG signals,
EEG-based brain-computer interfaces (BCIs) usually need subject-specific
calibration to tailor the decoding algorithm for each new subject, which is
time-consuming and user-unfriendly, hindering their real-world applications.
Transfer learning (TL) has been extensively used to expedite the calibration,
by making use of EEG data from other subjects/sessions. An important
consideration in TL for EEG-based BCIs is to reduce the data distribution
discrepancies among different subjects/session, to avoid negative transfer.
Euclidean alignment (EA) was proposed in 2020 to address this challenge.
Numerous experiments from 10 different BCI paradigms demonstrated its
effectiveness and efficiency. This paper revisits the EA, explaining its
procedure and correct usage, introducing its applications and extensions, and
pointing out potential new research directions. It should be very helpful to
BCI researchers, especially those who are working on EEG signal decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding High-Dimensional Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Papenmeier, Matthias Poloczek, Luigi Nardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work reported that simple Bayesian optimization methods perform well
for high-dimensional real-world tasks, seemingly contradicting prior work and
tribal knowledge. This paper investigates the 'why'. We identify fundamental
challenges that arise in high-dimensional Bayesian optimization and explain why
recent methods succeed. Our analysis shows that vanishing gradients caused by
Gaussian process initialization schemes play a major role in the failures of
high-dimensional Bayesian optimization and that methods that promote local
search behaviors are better suited for the task. We find that maximum
likelihood estimation of Gaussian process length scales suffices for
state-of-the-art performance. Based on this, we propose a simple variant of
maximum likelihood estimation called MSR that leverages these findings to
achieve state-of-the-art performance on a comprehensive set of real-world
applications. We also present targeted experiments to illustrate and confirm
our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizability through Explainability: Countering Overfitting with
  Counterfactual Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavio Giorgi, Fabiano Veglianti, Fabrizio Silvestri, Gabriele Tolomei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overfitting is a well-known issue in machine learning that occurs when a
model struggles to generalize its predictions to new, unseen data beyond the
scope of its training set. Traditional techniques to mitigate overfitting
include early stopping, data augmentation, and regularization. In this work, we
demonstrate that the degree of overfitting of a trained model is correlated
with the ability to generate counterfactual examples. The higher the
overfitting, the easier it will be to find a valid counterfactual example for a
randomly chosen input data point. Therefore, we introduce CF-Reg, a novel
regularization term in the training loss that controls overfitting by ensuring
enough margin between each instance and its corresponding counterfactual.
Experiments conducted across multiple datasets and models show that our
counterfactual regularizer generally outperforms existing regularization
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Stage Representation Learning for Analyzing Movement Behavior
  Dynamics in People Living with Dementia <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Cui, Alexander Capstick, Payam Barnaghi, Gregory Scott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In remote healthcare monitoring, time series representation learning reveals
critical patient behavior patterns from high-frequency data. This study
analyzes home activity data from individuals living with dementia by proposing
a two-stage, self-supervised learning approach tailored to uncover low-rank
structures. The first stage converts time-series activities into text sequences
encoded by a pre-trained language model, providing a rich, high-dimensional
latent state space using a PageRank-based method. This PageRank vector captures
latent state transitions, effectively compressing complex behaviour data into a
succinct form that enhances interpretability. This low-rank representation not
only enhances model interpretability but also facilitates clustering and
transition analysis, revealing key behavioral patterns correlated with
clinicalmetrics such as MMSE and ADAS-COG scores. Our findings demonstrate the
framework's potential in supporting cognitive status prediction, personalized
care interventions, and large-scale health monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025 Workshop on Large Language Models and Generative AI for
  Health</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOB-Bench: Benchmarking Generative AI for Finance - an Application to
  Limit Order Book Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peer Nagy, Sascha Frey, Kang Li, Bidipta Sarkar, Svitlana Vyetrenko, Stefan Zohren, Ani Calinescu, Jakob Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While financial data presents one of the most challenging and interesting
sequence modelling tasks due to high noise, heavy tails, and strategic
interactions, progress in this area has been hindered by the lack of consensus
on quantitative evaluation paradigms. To address this, we present LOB-Bench, a
benchmark, implemented in python, designed to evaluate the quality and realism
of generative message-by-order data for limit order books (LOB) in the LOBSTER
format. Our framework measures distributional differences in conditional and
unconditional statistics between generated and real LOB data, supporting
flexible multivariate statistical evaluation. The benchmark also includes
features commonly used LOB statistics such as spread, order book volumes, order
imbalance, and message inter-arrival times, along with scores from a trained
discriminator network. Lastly, LOB-Bench contains "market impact metrics", i.e.
the cross-correlations and price response functions for specific events in the
data. We benchmark generative autoregressive state-space models, a (C)GAN, as
well as a parametric LOB model and find that the autoregressive GenAI approach
beats traditional model classes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot
  Object Customization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung X. Pham, Zhang Kang, Ji Woo Hong, Xuran Zheng, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose E-MD3C ($\underline{E}$fficient $\underline{M}$asked
$\underline{D}$iffusion Transformer with Disentangled $\underline{C}$onditions
and $\underline{C}$ompact $\underline{C}$ollector), a highly efficient
framework for zero-shot object image customization. Unlike prior works reliant
on resource-intensive Unet architectures, our approach employs lightweight
masked diffusion transformers operating on latent patches, offering
significantly improved computational efficiency. The framework integrates three
core components: (1) an efficient masked diffusion transformer for processing
autoencoder latents, (2) a disentangled condition design that ensures
compactness while preserving background alignment and fine details, and (3) a
learnable Conditions Collector that consolidates multiple inputs into a compact
representation for efficient denoising and learning. E-MD3C outperforms the
existing approach on the VITON-HD dataset across metrics such as PSNR, FID,
SSIM, and LPIPS, demonstrating clear advantages in parameters, memory
efficiency, and inference speed. With only $\frac{1}{4}$ of the parameters, our
Transformer-based 468M model delivers $2.5\times$ faster inference and uses
$\frac{2}{3}$ of the GPU memory compared to an 1720M Unet-based latent
diffusion model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vertical Federated Continual Learning via Evolving Prototype Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Wang, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertical Federated Learning (VFL) has garnered significant attention as a
privacy-preserving machine learning framework for sample-aligned feature
federation. However, traditional VFL approaches do not address the challenges
of class and feature continual learning, resulting in catastrophic forgetting
of knowledge from previous tasks. To address the above challenge, we propose a
novel vertical federated continual learning method, named Vertical Federated
Continual Learning via Evolving Prototype Knowledge (V-LETO), which primarily
facilitates the transfer of knowledge from previous tasks through the evolution
of prototypes. Specifically, we propose an evolving prototype knowledge method,
enabling the global model to retain both previous and current task knowledge.
Furthermore, we introduce a model optimization technique that mitigates the
forgetting of previous task knowledge by restricting updates to specific
parameters of the local model, thereby enhancing overall performance. Extensive
experiments conducted in both CIL and FIL settings demonstrate that our method,
V-LETO, outperforms the other state-of-the-art methods. For example, our method
outperforms the state-of-the-art method by 10.39% and 35.15% for CIL and FIL
tasks, respectively. Our code is available at
https://anonymous.4open.science/r/V-LETO-0108/README.md.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Regularization can make diffusion models more efficient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahsa Taheri, Johannes Lederer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are one of the key architectures of generative AI. Their
main drawback, however, is the computational costs. This study indicates that
the concept of sparsity, well known especially in statistics, can provide a
pathway to more efficient diffusion pipelines. Our mathematical guarantees
prove that sparsity can reduce the input dimension's influence on the
computational complexity to that of a much smaller intrinsic dimension of the
data. Our empirical findings confirm that inducing sparsity can indeed lead to
better samples at a lower cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shortcut Learning Susceptibility in Vision Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pirzada Suhail, Amit Sethi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortcut learning, where machine learning models exploit spurious
correlations in data instead of capturing meaningful features, poses a
significant challenge to building robust and generalizable models. This
phenomenon is prevalent across various machine learning applications, including
vision, natural language processing, and speech recognition, where models may
find unintended cues that minimize training loss but fail to capture the
underlying structure of the data. Vision classifiers such as Convolutional
Neural Networks (CNNs), Multi-Layer Perceptrons (MLPs), and Vision Transformers
(ViTs) leverage distinct architectural principles to process spatial and
structural information, making them differently susceptible to shortcut
learning. In this study, we systematically evaluate these architectures by
introducing deliberate shortcuts into the dataset that are positionally
correlated with class labels, creating a controlled setup to assess whether
models rely on these artificial cues or learn actual distinguishing features.
We perform both quantitative evaluation by training on the shortcut-modified
dataset and testing them on two different test sets -- one containing the same
shortcuts and another without them -- to determine the extent of reliance on
shortcuts. Additionally, qualitative evaluation is performed by using network
inversion-based reconstruction techniques to analyze what the models
internalize in their weights, aiming to reconstruct the training data as
perceived by the classifiers. We evaluate shortcut learning behavior across
multiple benchmark datasets, including MNIST, Fashion-MNIST, SVHN, and
CIFAR-10, to compare the susceptibility of different vision classifier
architectures to shortcut reliance and assess their varying degrees of
sensitivity to spurious correlations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature-based Graph Attention Networks Improve Online Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adjovi Sim, Zhengkui Wang, Aik Beng Ng, Shalini De Mello, Simon See, Wonmin Byeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online continual learning for image classification is crucial for models to
adapt to new data while retaining knowledge of previously learned tasks. This
capability is essential to address real-world challenges involving dynamic
environments and evolving data distributions. Traditional approaches
predominantly employ Convolutional Neural Networks, which are limited to
processing images as grids and primarily capture local patterns rather than
relational information. Although the emergence of transformer architectures has
improved the ability to capture relationships, these models often require
significantly larger resources. In this paper, we present a novel online
continual learning framework based on Graph Attention Networks (GATs), which
effectively capture contextual relationships and dynamically update the
task-specific representation via learned attention weights. Our approach
utilizes a pre-trained feature extractor to convert images into graphs using
hierarchical feature maps, representing information at varying levels of
granularity. These graphs are then processed by a GAT and incorporate an
enhanced global pooling strategy to improve classification performance for
continual learning. In addition, we propose the rehearsal memory duplication
technique that improves the representation of the previous tasks while
maintaining the memory budget. Comprehensive evaluations on benchmark datasets,
including SVHN, CIFAR10, CIFAR100, and MiniImageNet, demonstrate the
superiority of our method compared to the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Replay-free Online Continual Learning with Self-Supervised MultiPatches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Cignoni, Andrea Cossu, Alex Gomez-Villa, Joost van de Weijer, Antonio Carta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Continual Learning (OCL) methods train a model on a non-stationary
data stream where only a few examples are available at a time, often leveraging
replay strategies. However, usage of replay is sometimes forbidden, especially
in applications with strict privacy regulations. Therefore, we propose
Continual MultiPatches (CMP), an effective plug-in for existing OCL
self-supervised learning strategies that avoids the use of replay samples. CMP
generates multiple patches from a single example and projects them into a
shared feature space, where patches coming from the same example are pushed
together without collapsing into a single point. CMP surpasses replay and other
SSL-based strategies on OCL streams, challenging the role of replay as a go-to
solution for self-supervised OCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ESANN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trust Me, I Know the Way: Predictive Uncertainty in the Presence of
  Shortcut Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Wimmer, Bernd Bischl, Ludwig Bothmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The correct way to quantify predictive uncertainty in neural networks remains
a topic of active discussion. In particular, it is unclear whether the
state-of-the art entropy decomposition leads to a meaningful representation of
model, or epistemic, uncertainty (EU) in the light of a debate that pits
ignorance against disagreement perspectives. We aim to reconcile the
conflicting viewpoints by arguing that both are valid but arise from different
learning situations. Notably, we show that the presence of shortcuts is
decisive for EU manifesting as disagreement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting and Steering Protein Language Models through Sparse
  Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edith Natalia Villegas Garcia, Alessio Ansuini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in transformer-based language models have
revolutionized natural language processing, yet understanding the internal
mechanisms of these models remains a significant challenge. This paper explores
the application of sparse autoencoders (SAE) to interpret the internal
representations of protein language models, specifically focusing on the ESM-2
8M parameter model. By performing a statistical analysis on each latent
component's relevance to distinct protein annotations, we identify potential
interpretations linked to various protein characteristics, including
transmembrane regions, binding sites, and specialized motifs.
  We then leverage these insights to guide sequence generation, shortlisting
the relevant latent components that can steer the model towards desired targets
such as zinc finger domains. This work contributes to the emerging field of
mechanistic interpretability in biological sequence models, offering new
perspectives on model steering for sequence design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finite-Time Analysis of Discrete-Time Stochastic Interpolants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Liu, Yu Chen, Rui Hu, Longbo Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The stochastic interpolant framework offers a powerful approach for
constructing generative models based on ordinary differential equations (ODEs)
or stochastic differential equations (SDEs) to transform arbitrary data
distributions. However, prior analyses of this framework have primarily focused
on the continuous-time setting, assuming a perfect solution of the underlying
equations. In this work, we present the first discrete-time analysis of the
stochastic interpolant framework, where we introduce an innovative
discrete-time sampler and derive a finite-time upper bound on its distribution
estimation error. Our result provides a novel quantification of how different
factors, including the distance between source and target distributions and
estimation accuracy, affect the convergence rate and also offers a new
principled way to design efficient schedules for convergence acceleration.
Finally, numerical experiments are conducted on the discrete-time sampler to
corroborate our theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Dialect-Aware Framework for the Classification of Arabic
  Dialects and E<span class="highlight-title">motion</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasser A Alsadhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Arabic is one of the oldest languages still in use today. As a result,
several Arabic-speaking regions have developed dialects that are unique to
them. Dialect and emotion recognition have various uses in Arabic text
analysis, such as determining an online customer's origin based on their
comments. Furthermore, intelligent chatbots that are aware of a user's emotions
can respond appropriately to the user. Current research in emotion detection in
the Arabic language lacks awareness of how emotions are exhibited in different
dialects, which motivates the work found in this study. This research addresses
the problems of dialect and emotion classification in Arabic. Specifically,
this is achieved by building a novel framework that can identify and predict
Arabic dialects and emotions from a given text. The framework consists of three
modules: A text-preprocessing module, a classification module, and a clustering
module with the novel capability of building new dialect-aware emotion
lexicons. The proposed framework generated a new emotional lexicon for
different dialects. It achieved an accuracy of 88.9% in classifying Arabic
dialects, which outperforms the state-of-the-art results by 6.45 percentage
points. Furthermore, the framework achieved 89.1-79% accuracy in detecting
emotions in the Egyptian and Gulf dialects, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Deep Regression with Tightness <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Zhang, Yuguang Yan, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For deep regression, preserving the ordinality of the targets with respect to
the feature representation improves performance across various tasks. However,
a theoretical explanation for the benefits of ordinality is still lacking. This
work reveals that preserving ordinality reduces the conditional entropy
$H(Z|Y)$ of representation $Z$ conditional on the target $Y$. However, our
findings reveal that typical regression losses do little to reduce $H(Z|Y)$,
even though it is vital for generalization performance. With this motivation,
we introduce an optimal transport-based regularizer to preserve the similarity
relationships of targets in the feature space to reduce $H(Z|Y)$. Additionally,
we introduce a simple yet efficient strategy of duplicating the regressor
targets, also with the aim of reducing $H(Z|Y)$. Experiments on three
real-world regression tasks verify the effectiveness of our strategies to
improve deep regression. Code:
https://github.com/needylove/Regression_tightness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025, Code: https://github.com/needylove/Regression_tightness</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Law for Stochastic Gradient Descent in Quadratically
  Parameterized Linear Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihong Ding, Haihan Zhang, Hanzhen Zhao, Cong Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In machine learning, the scaling law describes how the model performance
improves with the model and data size scaling up. From a learning theory
perspective, this class of results establishes upper and lower generalization
bounds for a specific learning algorithm. Here, the exact algorithm running
using a specific model parameterization often offers a crucial implicit
regularization effect, leading to good generalization. To characterize the
scaling law, previous theoretical studies mainly focus on linear models,
whereas, feature learning, a notable process that contributes to the remarkable
empirical success of neural networks, is regretfully vacant. This paper studies
the scaling law over a linear regression with the model being quadratically
parameterized. We consider infinitely dimensional data and slope ground truth,
both signals exhibiting certain power-law decay rates. We study convergence
rates for Stochastic Gradient Descent and demonstrate the learning rates for
variables will automatically adapt to the ground truth. As a result, in the
canonical linear regression, we provide explicit separations for generalization
curves between SGD with and without feature learning, and the
information-theoretical lower bound that is agnostic to parametrization method
and the algorithm. Our analysis for decaying ground truth provides a new
characterization for the learning dynamic of the model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-shot Federated Learning Methods: A Practical Guide 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Liu, Zhenheng Tang, Xia Li, Yijun Song, Sijie Ji, Zemin Liu, Bo Han, Linshan Jiang, Jialin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-shot Federated Learning (OFL) is a distributed machine learning paradigm
that constrains client-server communication to a single round, addressing
privacy and communication overhead issues associated with multiple rounds of
data exchange in traditional Federated Learning (FL). OFL demonstrates the
practical potential for integration with future approaches that require
collaborative training models, such as large language models (LLMs). However,
current OFL methods face two major challenges: data heterogeneity and model
heterogeneity, which result in subpar performance compared to conventional FL
methods. Worse still, despite numerous studies addressing these limitations, a
comprehensive summary is still lacking. To address these gaps, this paper
presents a systematic analysis of the challenges faced by OFL and thoroughly
reviews the current methods. We also offer an innovative categorization method
and analyze the trade-offs of various techniques. Additionally, we discuss the
most promising future directions and the technologies that should be integrated
into the OFL field. This work aims to provide guidance and insights for future
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Anomaly Detection on Implicit Shape representations for
  Sarcopenia Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louise Piecuch, Jeremie Huet, Antoine Frouin, Antoine Nordez, Anne-Sophie Boureau, Diana Mateus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sarcopenia is an age-related progressive loss of muscle mass and strength
that significantly impacts daily life. A commonly studied criterion for
characterizing the muscle mass has been the combination of 3D imaging and
manual segmentations. In this paper, we instead study the muscles' shape. We
rely on an implicit neural representation (INR) to model normal muscle shapes.
We then introduce an unsupervised anomaly detection method to identify
sarcopenic muscles based on the reconstruction error of the implicit model.
Relying on a conditional INR with an auto-decoding strategy, we also learn a
latent representation of the muscles that clearly separates normal from
abnormal muscles in an unsupervised fashion. Experimental results on a dataset
of 103 segmented volumes indicate that our double anomaly detection strategy
effectively discriminates sarcopenic and non-sarcopenic muscles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of Tabular Transformer Architectures for Operating System
  Fingerprinting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubén Pérez-Jove, Cristian R. Munteanu, Alejandro Pazos, Jose Vázquez-Naya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operating System (OS) fingerprinting is essential for network management and
cybersecurity, enabling accurate device identification based on network traffic
analysis. Traditional rule-based tools such as Nmap and p0f face challenges in
dynamic environments due to frequent OS updates and obfuscation techniques.
While Machine Learning (ML) approaches have been explored, Deep Learning (DL)
models, particularly Transformer architectures, remain unexploited in this
domain. This study investigates the application of Tabular Transformer
architectures-specifically TabTransformer and FT-Transformer-for OS
fingerprinting, leveraging structured network data from three publicly
available datasets. Our experiments demonstrate that FT-Transformer generally
outperforms traditional ML models, previous approaches and TabTransformer
across multiple classification levels (OS family, major, and minor versions).
The results establish a strong foundation for DL-based OS fingerprinting,
improving accuracy and adaptability in complex network environments.
Furthermore, we ensure the reproducibility of our research by providing an
open-source implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted as a preprint (not peer reviewed). 22 pages, 9 figures.
  Code and datasets available at:
  https://github.com/rubenpjove/tabularT-OS-fingerprinting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Cryptocurrency Unpredictability: A Comprehensive Study of
  Complexity and Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Puoti, Fabrizio Pittorino, Manuel Roveri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper offers a thorough examination of the univariate predictability in
cryptocurrency time-series. By exploiting a combination of complexity measure
and model predictions we explore the cryptocurrencies time-series forecasting
task focusing on the exchange rate in USD of Litecoin, Binance Coin, Bitcoin,
Ethereum, and XRP. On one hand, to assess the complexity and the randomness of
these time-series, a comparative analysis has been performed using Brownian and
colored noises as a benchmark. The results obtained from the Complexity-Entropy
causality plane and power density spectrum analysis reveal that cryptocurrency
time-series exhibit characteristics closely resembling those of Brownian noise
when analyzed in a univariate context. On the other hand, the application of a
wide range of statistical, machine and deep learning models for time-series
forecasting demonstrates the low predictability of cryptocurrencies. Notably,
our analysis reveals that simpler models such as Naive models consistently
outperform the more complex machine and deep learning ones in terms of
forecasting accuracy across different forecast horizons and time windows. The
combined study of complexity and forecasting accuracies highlights the
difficulty of predicting the cryptocurrency market. These findings provide
valuable insights into the inherent characteristics of the cryptocurrency data
and highlight the need to reassess the challenges associated with predicting
cryptocurrency's price movements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the author's accepted manuscript, modified per ACM
  self-archiving policy. The definitive Version of Record is available at
  https://doi.org/10.1145/3703412.3703420</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowAR: une plateforme uniformisée pour la reconnaissance des
  activités humaines à partir de capteurs binaires 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ncibi, Luc Bouganim, Philippe Pucheral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This demo showcases a platform for developing human activity recognition (AR)
systems, focusing on daily activities using sensor data, like binary sensors.
With a data-driven approach, this platform, named FlowAR, features a three-step
pipeline (flow): data cleaning, segmentation, and personalized classification.
Its modularity allows flexibility to test methods, datasets, and ensure
rigorous evaluations. A concrete use case demonstrates its effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in French language https://editions-rnti.fr/?inprocid=1003044</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRANE: Reasoning with constrained LLM generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debangshu Banerjee, Tarun Suresh, Shubham Ugare, Sasa Misailovic, Gagandeep Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code generation, symbolic math reasoning, and other tasks require LLMs to
produce outputs that are both syntactically and semantically correct.
Constrained LLM generation is a promising direction to enforce adherence to
formal grammar, but prior works have empirically observed that strict
enforcement of formal constraints often diminishes the reasoning capabilities
of LLMs. In this work, we first provide a theoretical explanation for why
constraining LLM outputs to very restrictive grammars that only allow
syntactically valid final answers reduces the reasoning capabilities of the
model. Second, we demonstrate that by augmenting the output grammar with
carefully designed additional rules, it is always possible to preserve the
reasoning capabilities of the LLM while ensuring syntactic and semantic
correctness in its outputs. Building on these theoretical insights, we propose
a reasoning-augmented constrained decoding algorithm, CRANE, which effectively
balances the correctness of constrained generation with the flexibility of
unconstrained generation. Experiments on multiple open-source LLMs and
benchmarks show that CRANE significantly outperforms both state-of-the-art
constrained decoding strategies and standard unconstrained decoding, showing up
to 10% points accuracy improvement over baselines on challenging symbolic
reasoning benchmarks GSM-symbolic and FOLIO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Game Theory Meets Large Language Models: A Systematic <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Sun, Yusen Wu, Yukun Cheng, Xu Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Game theory establishes a fundamental framework for analyzing strategic
interactions among rational decision-makers. The rapid advancement of large
language models (LLMs) has sparked extensive research exploring the
intersection of these two fields. Specifically, game-theoretic methods are
being applied to evaluate and enhance LLM capabilities, while LLMs themselves
are reshaping classic game models. This paper presents a comprehensive survey
of the intersection of these fields, exploring a bidirectional relationship
from three perspectives: (1) Establishing standardized game-based benchmarks
for evaluating LLM behavior; (2) Leveraging game-theoretic methods to improve
LLM performance through algorithmic innovations; (3) Characterizing the
societal impacts of LLMs through game modeling. Among these three aspects, we
also highlight how the equilibrium analysis for traditional game models is
impacted by LLMs' advanced language understanding, which in turn extends the
study of game theory. Finally, we identify key challenges and future research
directions, assessing their feasibility based on the current state of the
field. By bridging theoretical rigor with emerging AI capabilities, this survey
aims to foster interdisciplinary collaboration and drive progress in this
evolving research area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Member-Group Relations via Multi-View Graph Filtering for
  Effective Group Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chae-Hyun Kim, Yoon-Ryung Choi, Jin-Duk Park, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group recommendation aims at providing optimized recommendations tailored to
diverse groups, enabling groups to enjoy appropriate items. On the other hand,
most existing group recommendation methods are built upon deep neural network
(DNN) architectures designed to capture the intricate relationships between
member-level and group-level interactions. While these DNN-based approaches
have proven their effectiveness, they require complex and expensive training
procedures to incorporate group-level interactions in addition to member-level
interactions. To overcome such limitations, we introduce Group-GF, a new
approach for extremely fast recommendations of items to each group via
multi-view graph filtering (GF) that offers a holistic view of complex
member-group dynamics, without the need for costly model training.
Specifically, in Group-GF, we first construct three item similarity graphs
manifesting different viewpoints for GF. Then, we discover a distinct
polynomial graph filter for each similarity graph and judiciously aggregate the
three graph filters. Extensive experiments demonstrate the effectiveness of
Group-GF in terms of significantly reducing runtime and achieving
state-of-the-art recommendation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 4 tables; ACM Web Conference (WWW 2025) (to
  appear) (Please cite our conference version.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Algorithms in Linear Regression under Covariate Shift: On the
  Importance of Precondition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanshi Liu, Haihan Zhang, Qian Chen, Cong Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common pursuit in modern statistical learning is to attain satisfactory
generalization out of the source data distribution (OOD). In theory, the
challenge remains unsolved even under the canonical setting of covariate shift
for the linear model. This paper studies the foundational (high-dimensional)
linear regression where the ground truth variables are confined to an
ellipse-shape constraint and addresses two fundamental questions in this
regime: (i) given the target covariate matrix, what is the min-max
\emph{optimal} algorithm under covariate shift? (ii) for what kinds of target
classes, the commonly-used SGD-type algorithms achieve optimality? Our analysis
starts with establishing a tight lower generalization bound via a Bayesian
Cramer-Rao inequality. For (i), we prove that the optimal estimator can be
simply a certain linear transformation of the best estimator for the source
distribution. Given the source and target matrices, we show that the
transformation can be efficiently computed via a convex program. The min-max
optimal analysis for SGD leverages the idea that we recognize both the
accumulated updates of the applied algorithms and the ideal transformation as
preconditions on the learning variables. We provide sufficient conditions when
SGD with its acceleration variants attain optimality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Criteria-Aware Graph Filtering: Extremely Fast Yet Accurate
  Multi-Criteria Recommendation <span class="chip">WWW 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Duk Park, Jaemin Yoo, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-criteria (MC) recommender systems, which utilize MC rating information
for recommendation, are increasingly widespread in various e-commerce domains.
However, the MC recommendation using training-based collaborative filtering,
requiring consideration of multiple ratings compared to single-criterion
counterparts, often poses practical challenges in achieving state-of-the-art
performance along with scalable model training. To solve this problem, we
propose CA-GF, a training-free MC recommendation method, which is built upon
criteria-aware graph filtering for efficient yet accurate MC recommendations.
Specifically, first, we construct an item-item similarity graph using an MC
user-expansion graph. Next, we design CA-GF composed of the following key
components, including 1) criterion-specific graph filtering where the optimal
filter for each criterion is found using various types of polynomial low-pass
filters and 2) criteria preference-infused aggregation where the smoothed
signals from each criterion are aggregated. We demonstrate that CA-GF is (a)
efficient: providing the computational efficiency, offering the extremely fast
runtime of less than 0.2 seconds even on the largest benchmark dataset, (b)
accurate: outperforming benchmark MC recommendation methods, achieving
substantial accuracy gains up to 24% compared to the best competitor, and (c)
interpretable: providing interpretations for the contribution of each criterion
to the model prediction based on visualizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 7 tables; ACM Web Conference (WWW 2025) (to
  appear) (Please cite our conference version.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Concept Bottleneck Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shin'ya Yamaguchi, Kosuke Nishida, Daiki Chijiwa, Yasutoshi Ida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept bottleneck models (CBMs) are inherently interpretable and
intervenable neural network models, which explain their final label prediction
by the intermediate prediction of high-level semantic concepts. However, they
require target task training to learn input-to-concept and concept-to-label
mappings, incurring target dataset collections and training resources. In this
paper, we present \textit{zero-shot concept bottleneck models} (Z-CBMs), which
predict concepts and labels in a fully zero-shot manner without training neural
networks. Z-CBMs utilize a large-scale concept bank, which is composed of
millions of vocabulary extracted from the web, to describe arbitrary input in
various domains. For the input-to-concept mapping, we introduce concept
retrieval, which dynamically finds input-related concepts by the cross-modal
search on the concept bank. In the concept-to-label inference, we apply concept
regression to select essential concepts from the retrieved concepts by sparse
linear regression. Through extensive experiments, we confirm that our Z-CBMs
provide interpretable and intervenable concepts without any additional
training. Code will be available at https://github.com/yshinya6/zcbm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversity Enhances an LLM's Performance in RAG and Long-context Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhchao Wang, Bin Bi, Yanqi Luo, Sitaram Asur, Claire Na Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in large language models (LLMs) have highlighted the
challenge of context window limitations, primarily due to the quadratic time
complexity of the self-attention mechanism (\(O(N^2)\), where \(N\) denotes the
context window length). This constraint impacts tasks such as
retrieval-augmented generation (RAG) in question answering (Q\&A) and long
context summarization. A common approach involves selecting content with the
highest similarity to the query; however, this often leads to redundancy and
the exclusion of diverse yet relevant information. Building on principles from
Maximal Marginal Relevance (MMR) and Farthest Point Sampling (FPS), we
integrate diversity into the content selection process. Our findings reveal
that incorporating diversity substantially increases the recall of selecting
relevant sentences or chunks before LLM-based Q\&A and summarization. These
results highlight the importance of maintaining diversity in future LLM
applications to further improve summarization and Q\&A outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hope vs. Hate: Understanding User Interactions with LGBTQ+ News Content
  in Mainstream US News Media through the Lens of Hope Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Pofcher, Christopher M. Homan, Randall Sell, Ashiqur R. KhudaBukhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes three contributions. First, via a substantial corpus of
1,419,047 comments posted on 3,161 YouTube news videos of major US cable news
outlets, we analyze how users engage with LGBTQ+ news content. Our analyses
focus both on positive and negative content. In particular, we construct a
fine-grained hope speech classifier that detects positive (hope speech),
negative, neutral, and irrelevant content. Second, in consultation with a
public health expert specializing on LGBTQ+ health, we conduct an annotation
study with a balanced and diverse political representation and release a
dataset of 3,750 instances with fine-grained labels and detailed annotator
demographic information. Finally, beyond providing a vital resource for the
LGBTQ+ community, our annotation study and subsequent in-the-wild assessments
reveal (1) strong association between rater political beliefs and how they rate
content relevant to a marginalized community; (2) models trained on individual
political beliefs exhibit considerable in-the-wild disagreement; and (3)
zero-shot large language models (LLMs) align more with liberal raters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach
  for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Wei, Chung-Yiu Yau, Hoi-To Wai,  Yang,  Zhao, Dongyeop Kang, Youngsuk Park, Mingyi Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised fine-tuning is a standard method for adapting pre-trained large
language models (LLMs) to downstream tasks. Quantization has been recently
studied as a post-training technique for efficient LLM deployment. To obtain
quantized fine-tuned LLMs, conventional pipelines would first fine-tune the
pre-trained models, followed by post-training quantization. This often yields
suboptimal performance as it fails to leverage the synergy between fine-tuning
and quantization. To effectively realize low-bit quantization of weights,
activations, and KV caches in LLMs, we propose an algorithm named Rotated
Straight-Through-Estimator (RoSTE), which combines quantization-aware
supervised fine-tuning (QA-SFT) with an adaptive rotation strategy that
identifies an effective rotation configuration to reduce activation outliers.
We provide theoretical insights on RoSTE by analyzing its prediction error when
applied to an overparameterized least square quantized training problem. Our
findings reveal that the prediction error is directly proportional to the
quantization error of the converged weights, which can be effectively managed
through an optimized rotation configuration. Experiments on Pythia and Llama
models of different sizes demonstrate the effectiveness of RoSTE. Compared to
existing post-SFT quantization baselines, our method consistently achieves
superior performances across various tasks and different LLM architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End triplet loss based fine-tuning for network embedding in
  effective PII detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishika Kohli, Shaifu Gupta, Manoj Singh Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are many approaches in mobile data ecosystem that inspect network
traffic generated by applications running on user's device to detect personal
data exfiltration from the user's device. State-of-the-art methods rely on
features extracted from HTTP requests and in this context, machine learning
involves training classifiers on these features and making predictions using
labelled packet traces. However, most of these methods include external feature
selection before model training. Deep learning, on the other hand, typically
does not require such techniques, as it can autonomously learn and identify
patterns in the data without external feature extraction or selection
algorithms. In this article, we propose a novel deep learning based end-to-end
learning framework for prediction of exposure of personally identifiable
information (PII) in mobile packets. The framework employs a pre-trained large
language model (LLM) and an autoencoder to generate embedding of network
packets and then uses a triplet-loss based fine-tuning method to train the
model, increasing detection effectiveness using two real-world datasets. We
compare our proposed detection framework with other state-of-the-art works in
detecting PII leaks from user's device.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Hybrid Ensemble Model for Network Anomaly Detection:
  Balancing Security and Data Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaobo Liu, Zihao Zhao, Weijie He, Jiren Wang, Jing Peng, Haoyuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy-preserving network anomaly detection has become an essential area of
research due to growing concerns over the protection of sensitive data.
Traditional anomaly de- tection models often prioritize accuracy while
neglecting the critical aspect of privacy. In this work, we propose a hybrid
ensemble model that incorporates privacy-preserving techniques to address both
detection accuracy and data protection. Our model combines the strengths of
several machine learning algo- rithms, including K-Nearest Neighbors (KNN),
Support Vector Machines (SVM), XGBoost, and Artificial Neural Networks (ANN),
to create a robust system capable of identifying network anomalies while
ensuring privacy. The proposed approach in- tegrates advanced preprocessing
techniques that enhance data quality and address the challenges of small sample
sizes and imbalanced datasets. By embedding privacy measures into the model
design, our solution offers a significant advancement over existing methods,
ensuring both enhanced detection performance and strong privacy safeguards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 5th International Conference on Big Data, Artificial
  Intelligence and Internet of Things Engineering(ICBAIE 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Residual Transformer Fusion Network for Salt and Pepper Image Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bintang Pradana Erlangga Putra, Heri Prasetyo, Esti Suryani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Network (CNN) has been widely used in unstructured
datasets, one of which is image denoising. Image denoising is a noisy image
reconstruction process that aims to reduce additional noise that occurs from
the noisy image with various strategies. Image denoising has a problem, namely
that some image denoising methods require some prior knowledge of information
about noise. To overcome this problem, a combined architecture of Convolutional
Vision Transformer (CvT) and Residual Networks (ResNet) is used which is called
the Residual Transformer Fusion Network (RTF-Net). In general, the process in
this architecture can be divided into two parts, Noise Suppression Network
(NSN) and Structure Enhancement Network (SEN). Residual Block is used in the
Noise Suppression Network and is used to learn the noise map in the image,
while the CvT is used in the Structure Enhancement Network and is used to learn
the details that need to be added to the image processed by the Noise
Suppression Network. The model was trained using the DIV2K Training Set
dataset, and validation using the DIV2K Validation Set. After doing the
training, the model was tested using Lena, Bridge, Pepper, and BSD300 images
with noise levels ranging from 30%, 50%, and 70% and the PSNR results were
compared with the DBA, NASNLM, PARIGI, NLSF, NLSF-MLP and NLSF-CNN methods. The
test results show that the proposed method is superior in all cases except for
Pepper's image with a noise level of 30%, where NLSF-CNN is superior with a
PSNR value of 32.99 dB, while the proposed method gets a PSNR value of 31.70
dB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Off-Policy Evaluation for Recommendations with Missing-Not-At-Random
  Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tatsuki Takahashi, Chihiro Maru, Hiroko Shoji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unbiased recommender learning (URL) and off-policy evaluation/learning
(OPE/L) techniques are effective in addressing the data bias caused by display
position and logging policies, thereby consistently improving the performance
of recommendations. However, when both bias exits in the logged data, these
estimators may suffer from significant bias. In this study, we first analyze
the position bias of the OPE estimator when rewards are missing not at random.
To mitigate both biases, we propose a novel estimator that leverages two
probabilities of logging policies and reward observations as propensity scores.
Our experiments demonstrate that the proposed estimator achieves superior
performance compared to other estimators, even as the levels of bias in reward
observations increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task Generalization With AutoRegressive Compositional Structure: Can
  Learning From $\d$ Tasks Generalize to $\d^{T}$ Tasks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhesam Abedsoltan, Huaqing Zhang, Kaiyue Wen, Hongzhou Lin, Jingzhao Zhang, Mikhail Belkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable task generalization, solving
tasks they were never explicitly trained on with only a few demonstrations.
This raises a fundamental question: When can learning from a small set of tasks
generalize to a large task family? In this paper, we investigate task
generalization through the lens of AutoRegressive Compositional (ARC)
structure, where each task is a composition of $T$ operations, and each
operation is among a finite family of $\d$ subtasks. This yields a total class
of size~\( \d^\TT \). We first show that generalization to all \( \d^\TT \)
tasks is theoretically achievable by training on only \( \tilde{O}(\d) \)
tasks. Empirically, we demonstrate that Transformers achieve such exponential
task generalization on sparse parity functions via in-context learning (ICL)
and Chain-of-Thought (CoT) reasoning. We further demonstrate this
generalization in arithmetic and language translation, extending beyond parity
functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Force Field: Learning Generalized Physical Representation from a
  Few Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqian Li, Ruihong Shen, Chi Zhang, Yixin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physical reasoning is a remarkable human ability that enables rapid learning
and generalization from limited experience. Current AI models, despite
extensive training, still struggle to achieve similar generalization,
especially in Out-of-distribution (OOD) settings. This limitation stems from
their inability to abstract core physical principles from observations. A key
challenge is developing representations that can efficiently learn and
generalize physical dynamics from minimal data. Here we present Neural Force
Field (NFF) a modeling framework built on Neural Ordinary Differential Equation
(NODE) that learns interpretable force field representations which can be
efficiently integrated through an Ordinary Differential Equation ( ODE) solver
to predict object trajectories. Unlike existing approaches that rely on
high-dimensional latent spaces, NFF captures fundamental physical concepts such
as gravity, support, and collision in an interpretable manner. Experiments on
two challenging physical reasoning tasks demonstrate that NFF, trained with
only a few examples, achieves strong generalization to unseen scenarios. This
physics-grounded representation enables efficient forward-backward planning and
rapid adaptation through interactive refinement. Our work suggests that
incorporating physics-inspired representations into learning systems can help
bridge the gap between artificial and human physical reasoning capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline
  Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xun Wang, Zhuoran Li, Hai Zhong, Longbo Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a data-driven approach, offline MARL learns superior policies solely from
offline datasets, ideal for domains rich in historical data but with high
interaction costs and risks. However, most existing methods are task-specific,
requiring retraining for new tasks, leading to redundancy and inefficiency. To
address this issue, in this paper, we propose a task-efficient multi-task
offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL).
Unlike existing offline skill-discovery methods, SD-CQL discovers skills by
reconstructing the next observation. It then evaluates fixed and variable
actions separately and employs behavior-regularized conservative Q-learning to
execute the optimal action for each skill. This approach eliminates the need
for local-global alignment and enables strong multi-task generalization from
limited small-scale source tasks. Substantial experiments on StarCraftII
demonstrates the superior generalization performance and task-efficiency of
SD-CQL. It achieves the best performance on $\textbf{10}$ out of $14$ task
sets, with up to $\textbf{65%}$ improvement on individual task sets, and is
within $4\%$ of the best baseline on the remaining four.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What exactly has TabPFN learned to do? <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calvin McCarter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TabPFN [Hollmann et al., 2023], a Transformer model pretrained to perform
in-context learning on fresh tabular classification problems, was presented at
the last ICLR conference. To better understand its behavior, we treat it as a
black-box function approximator generator and observe its generated function
approximations on a varied selection of training datasets. Exploring its
learned inductive biases in this manner, we observe behavior that is at turns
either brilliant or baffling. We conclude this post with thoughts on how these
results might inform the development, evaluation, and application of prior-data
fitted networks (PFNs) in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Originally published in Blogposts Track at ICLR 2024. Appendix
  contains re-analysis on TabPFN-v2 [Hollmann et al., 2025]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small Molecule Drug Discovery Through Deep Learning:Progress,
  Challenges, and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Li, Yida Xiong, Hongzhi Zhang, Xiantao Cai, Bo Du, Wenbin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their excellent drug-like and pharmacokinetic properties, small
molecule drugs are widely used to treat various diseases, making them a
critical component of drug discovery. In recent years, with the rapid
development of deep learning (DL) techniques, DL-based small molecule drug
discovery methods have achieved excellent performance in prediction accuracy,
speed, and complex molecular relationship modeling compared to traditional
machine learning approaches. These advancements enhance drug screening
efficiency and optimization, and they provide more precise and effective
solutions for various drug discovery tasks. Contributing to this field's
development, this paper aims to systematically summarize and generalize the
recent key tasks and representative techniques in DL-based small molecule drug
discovery in recent years. Specifically, we provide an overview of the major
tasks in small molecule drug discovery and their interrelationships. Next, we
analyze the six core tasks, summarizing the related methods, commonly used
datasets, and technological development trends. Finally, we discuss key
challenges, such as interpretability and out-of-distribution generalization,
and offer our insights into future research directions for DL-assisted small
molecule drug discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 1 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SkyRover: A Modular Simulator for Cross-Domain Pathfinding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhui Ma, Wenhao Li, Bo Jin, Changhong Lu, Xiangfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs) and Automated Guided Vehicles (AGVs)
increasingly collaborate in logistics, surveillance, inspection tasks and etc.
However, existing simulators often focus on a single domain, limiting
cross-domain study. This paper presents the SkyRover, a modular simulator for
UAV-AGV multi-agent pathfinding (MAPF). SkyRover supports realistic agent
dynamics, configurable 3D environments, and convenient APIs for external
solvers and learning methods. By unifying ground and aerial operations, it
facilitates cross-domain algorithm design, testing, and benchmarking.
Experiments highlight SkyRover's capacity for efficient pathfinding and
high-fidelity simulations in UAV-AGV coordination. Project is available at
https://sites.google.com/view/mapf3d/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Time-evolving Causality over Data Streams <span class="chip">KDD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Chihara, Yasuko Matsubara, Ren Fujiwara, Yasushi Sakurai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given an extensive, semi-infinite collection of multivariate coevolving data
sequences (e.g., sensor/web activity streams) whose observations influence each
other, how can we discover the time-changing cause-and-effect relationships in
co-evolving data streams? How efficiently can we reveal dynamical patterns that
allow us to forecast future values? In this paper, we present a novel streaming
method, ModePlait, which is designed for modeling such causal relationships
(i.e., time-evolving causality) in multivariate co-evolving data streams and
forecasting their future values. The solution relies on characteristics of the
causal relationships that evolve over time in accordance with the dynamic
changes of exogenous variables. ModePlait has the following properties: (a)
Effective: it discovers the time-evolving causality in multivariate co-evolving
data streams by detecting the transitions of distinct dynamical patterns
adaptively. (b) Accurate: it enables both the discovery of time-evolving
causality and the forecasting of future values in a streaming fashion. (c)
Scalable: our algorithm does not depend on data stream length and thus is
applicable to very large sequences. Extensive experiments on both synthetic and
real-world datasets demonstrate that our proposed model outperforms
state-of-the-art methods in terms of discovering the time-evolving causality as
well as forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> on Imbalanced Data Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Gao, Dongting Xie, Yihang Zhang, Zhengren Wang, Conghui He, Hongzhi Yin, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the expansion of data availability, machine learning (ML) has achieved
remarkable breakthroughs in both academia and industry. However, imbalanced
data distributions are prevalent in various types of raw data and severely
hinder the performance of ML by biasing the decision-making processes. To
deepen the understanding of imbalanced data and facilitate the related research
and applications, this survey systematically analyzing various real-world data
formats and concludes existing researches for different data formats into four
distinct categories: data re-balancing, feature representation, training
strategy, and ensemble learning. This structured analysis help researchers
comprehensively understand the pervasive nature of imbalance across diverse
data format, thereby paving a clearer path toward achieving specific research
goals. we provide an overview of relevant open-source libraries, spotlight
current challenges, and offer novel insights aimed at fostering future
advancements in this critical area of study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Biologically Plausible Brain Graph Transformer <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ciyuan Peng, Yuelong Huang, Qichao Dong, Shuo Yu, Feng Xia, Chengqi Zhang, Yaochu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art brain graph analysis methods fail to fully encode the
small-world architecture of brain graphs (accompanied by the presence of hubs
and functional modules), and therefore lack biological plausibility to some
extent. This limitation hinders their ability to accurately represent the
brain's structural and functional properties, thereby restricting the
effectiveness of machine learning models in tasks such as brain disorder
detection. In this work, we propose a novel Biologically Plausible Brain Graph
Transformer (BioBGT) that encodes the small-world architecture inherent in
brain graphs. Specifically, we present a network entanglement-based node
importance encoding technique that captures the structural importance of nodes
in global information propagation during brain graph communication,
highlighting the biological properties of the brain structure. Furthermore, we
introduce a functional module-aware self-attention to preserve the functional
segregation and integration characteristics of brain graphs in the learned
representations. Experimental results on three benchmark datasets demonstrate
that BioBGT outperforms state-of-the-art models, enhancing biologically
plausible brain graph representations for various brain graph analytical tasks
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27pages, 16figures, published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrated Optimization and Game Theory Framework for Fair Cost
  Allocation in Community Microgrids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K. Victor Sam Moses Babu, Pratyush Chakraborty, Mayukha Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fair cost allocation in community microgrids remains a significant challenge
due to the complex interactions between multiple participants with varying load
profiles, distributed energy resources, and storage systems. Traditional cost
allocation methods often fail to adequately address the dynamic nature of
participant contributions and benefits, leading to inequitable distribution of
costs and reduced participant satisfaction. This paper presents a novel
framework integrating multi-objective optimization with cooperative game theory
for fair and efficient microgrid operation and cost allocation. The proposed
approach combines mixed-integer linear programming for optimal resource
dispatch with Shapley value analysis for equitable benefit distribution,
ensuring both system efficiency and participant satisfaction. The framework was
validated using real-world data across six distinct operational scenarios,
demonstrating significant improvements in both technical and economic
performance. Results show peak demand reductions ranging from 7.8% to 62.6%,
solar utilization rates reaching 114.8% through effective storage integration,
and cooperative gains of up to $1,801.01 per day. The Shapley value-based
allocation achieved balanced benefit-cost distributions, with net positions
ranging from -16.0% to +14.2% across different load categories, ensuring
sustainable participant cooperation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Graph Contrastive Pretraining for Device-level
  Integrated Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungyoung Lee, Ziyi Wang, Seunggeun Kim, Taekyun Lee, David Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised graph representation learning has driven significant
advancements in domains such as social network analysis, molecular design, and
electronics design automation (EDA). However, prior works in EDA have mainly
focused on the representation of gate-level digital circuits, failing to
capture analog and mixed-signal circuits. To address this gap, we introduce
DICE: Device-level Integrated Circuits Encoder, the first self-supervised
pretrained graph neural network (GNN) model for any circuit expressed at the
device level. DICE is a message-passing neural network (MPNN) trained through
graph contrastive learning, and its pretraining process is simulation-free,
incorporating two novel data augmentation techniques. Experimental results
demonstrate that DICE achieves substantial performance gains across three
downstream tasks, underscoring its effectiveness for both analog and digital
circuits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of
  Physical Concept Understanding <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mo Yu, Lemao Liu, Junjie Wu, Tsz Ting Chung, Shunchi Zhang, Jiangnan Li, Dit-Yan Yeung, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a systematic way, we investigate a widely asked question: Do LLMs really
understand what they say?, which relates to the more familiar term Stochastic
Parrot. To this end, we propose a summative assessment over a carefully
designed physical concept understanding task, PhysiCo. Our task alleviates the
memorization issue via the usage of grid-format inputs that abstractly describe
physical phenomena. The grids represents varying levels of understanding, from
the core phenomenon, application examples to analogies to other abstract
patterns in the grid world. A comprehensive study on our task demonstrates: (1)
state-of-the-art LLMs, including GPT-4o, o1 and Gemini 2.0 flash thinking, lag
behind humans by ~40%; (2) the stochastic parrot phenomenon is present in LLMs,
as they fail on our grid task but can describe and recognize the same concepts
well in natural language; (3) our task challenges the LLMs due to intrinsic
difficulties rather than the unfamiliar grid format, as in-context learning and
fine-tuning on same formatted data added little to their performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Main Conference. First 5 authors contributed equally.
  Project page: https://physico-benchmark.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond the Singular: The Essential Role of Multiple Generations in
  Effective Benchmark Evaluation and Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Zhang, Hengrui Cai, Wenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant utilities in
real-world applications, exhibiting impressive capabilities in natural language
processing and understanding. Benchmark evaluations are crucial for assessing
the capabilities of LLMs as they can provide a comprehensive assessment of
their strengths and weaknesses. However, current evaluation methods often
overlook the inherent randomness of LLMs by employing deterministic generation
strategies or relying on a single random sample, resulting in unaccounted
sampling variance and unreliable benchmark score estimates. In this paper, we
propose a hierarchical statistical model that provides a more comprehensive
representation of the benchmarking process by incorporating both benchmark
characteristics and LLM randomness. We show that leveraging multiple
generations improves the accuracy of estimating the benchmark score and reduces
variance. We also introduce $\mathbb P\left(\text{correct}\right)$, a
prompt-level difficulty score based on correct ratios, providing fine-grained
insights into individual prompts. Additionally, we create a data map that
visualizes difficulty and semantic prompts, enabling error detection and
quality control in benchmark construction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 table, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language in the Flow of Time: Time-Series-Paired Texts Weaved into a
  Unified Temporal Narrative 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Li, Xiao Lin, Zhining Liu, Jiaru Zou, Ziwei Wu, Lecheng Zheng, Dongqi Fu, Yada Zhu, Hendrik Hamann, Hanghang Tong, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While many advances in time series models focus exclusively on numerical
data, research on multimodal time series, particularly those involving
contextual textual information commonly encountered in real-world scenarios,
remains in its infancy. Consequently, effectively integrating the text modality
remains challenging. In this work, we highlight an intuitive yet significant
observation that has been overlooked by existing works: time-series-paired
texts exhibit periodic properties that closely mirror those of the original
time series. Building on this insight, we propose a novel framework, Texts as
Time Series (TaTS), which considers the time-series-paired texts to be
auxiliary variables of the time series. TaTS can be plugged into any existing
numerical-only time series models and enable them to handle time series data
with paired texts effectively. Through extensive experiments on both multimodal
time series forecasting and imputation tasks across benchmark datasets with
various existing time series models, we demonstrate that TaTS can enhance
predictive performance and achieve outperformance without modifying model
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Off-Policy $n$-Step TD-Learning with Linear Function
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han-Dong Lim, Donghwan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyzes multi-step temporal difference (TD)-learning algorithms
within the ``deadly triad'' scenario, characterized by linear function
approximation, off-policy learning, and bootstrapping. In particular, we prove
that $n$-step TD-learning algorithms converge to a solution as the sampling
horizon $n$ increases sufficiently. The paper is divided into two parts. In the
first part, we comprehensively examine the fundamental properties of their
model-based deterministic counterparts, including projected value iteration,
gradient descent algorithms, which can be viewed as prototype deterministic
algorithms whose analysis plays a pivotal role in understanding and developing
their model-free reinforcement learning counterparts. In particular, we prove
that these algorithms converge to meaningful solutions when $n$ is sufficiently
large. Based on these findings, in the second part, two $n$-step TD-learning
algorithms are proposed and analyzed, which can be seen as the model-free
reinforcement learning counterparts of the model-based deterministic
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2402.15781</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding Why Data Augmentation Improves Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Li, Jiachun Pan, Kim-Chuan Toh, Pan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is a cornerstone technique in deep learning, widely used to
improve model generalization. Traditional methods like random cropping and
color jittering, as well as advanced techniques such as CutOut, Mixup, and
CutMix, have achieved notable success across various domains. However, the
mechanisms by which data augmentation improves generalization remain poorly
understood, and existing theoretical analyses typically focus on individual
techniques without a unified explanation. In this work, we present a unified
theoretical framework that elucidates how data augmentation enhances
generalization through two key effects: partial semantic feature removal and
feature mixing. Partial semantic feature removal reduces the model's reliance
on individual feature, promoting diverse feature learning and better
generalization. Feature mixing, by scaling down original semantic features and
introducing noise, increases training complexity, driving the model to develop
more robust features. Advanced methods like CutMix integrate both effects,
achieving complementary benefits. Our theoretical insights are further
supported by experimental results, validating the effectiveness of this unified
perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reevaluating Policy Gradient Methods for Imperfect-Information Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Rudolph, Nathan Lichtle, Sobhan Mohammadpour, Alexandre Bayen, J. Zico Kolter, Amy Zhang, Gabriele Farina, Eugene Vinitsky, Samuel Sokota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past decade, motivated by the putative failure of naive self-play deep
reinforcement learning (DRL) in adversarial imperfect-information games,
researchers have developed numerous DRL algorithms based on fictitious play
(FP), double oracle (DO), and counterfactual regret minimization (CFR). In
light of recent results of the magnetic mirror descent algorithm, we
hypothesize that simpler generic policy gradient methods like PPO are
competitive with or superior to these FP, DO, and CFR-based DRL approaches. To
facilitate the resolution of this hypothesis, we implement and release the
first broadly accessible exact exploitability computations for four large
games. Using these games, we conduct the largest-ever exploitability comparison
of DRL algorithms for imperfect-information games. Over 5600 training runs, FP,
DO, and CFR-based approaches fail to outperform generic policy gradient
methods. Code is available at https://github.com/nathanlct/IIG-RL-Benchmark and
https://github.com/gabrfarina/exp-a-spiel .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoLike: Auditing Social Media Recommendations through User
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hieu Le, Salma Elmalaki, Zubair Shafiq, Athina Markopoulou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern social media platforms, such as TikTok, Facebook, and YouTube, rely on
recommendation systems to personalize content for users based on user
interactions with endless streams of content, such as "For You" pages. However,
these complex algorithms can inadvertently deliver problematic content related
to self-harm, mental health, and eating disorders. We introduce AutoLike, a
framework to audit recommendation systems in social media platforms for topics
of interest and their sentiments. To automate the process, we formulate the
problem as a reinforcement learning problem. AutoLike drives the recommendation
system to serve a particular type of content through interactions (e.g.,
liking). We apply the AutoLike framework to the TikTok platform as a case
study. We evaluate how well AutoLike identifies TikTok content automatically
across nine topics of interest; and conduct eight experiments to demonstrate
how well it drives TikTok's recommendation system towards particular topics and
sentiments. AutoLike has the potential to assist regulators in auditing
recommendation systems for problematic content. (Warning: This paper contains
qualitative examples that may be viewed as offensive or harmful.)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Escaping Collapse: The Strength of Weak Data for Large Language Model
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kareem Amin, Sara Babakniya, Alex Bie, Weiwei Kong, Umar Syed, Sergei Vassilvitskii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetically-generated data plays an increasingly larger role in training
large language models. However, while synthetic data has been found to be
useful, studies have also shown that without proper curation it can cause LLM
performance to plateau, or even "collapse", after many training iterations. In
this paper, we formalize this question and develop a theoretical framework to
investigate how much curation is needed in order to ensure that LLM performance
continually improves. We find that the requirements are nearly minimal. We
describe a training procedure that converges to an optimal LLM even if almost
all of the non-synthetic training data is of poor quality. Our analysis is
inspired by boosting, a classic machine learning technique that leverages a
very weak learning algorithm to produce an arbitrarily good classifier. Our
training procedure subsumes many recently proposed methods for training LLMs on
synthetic data, and thus our analysis sheds light on why they are successful,
and also suggests opportunities for future improvement. We present experiments
that validate our theory, and show that dynamically focusing labeling resources
on the most challenging examples -- in much the same way that boosting focuses
the efforts of the weak learner -- leads to improved performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without
  Compromising Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razvan-Gabriel Dumitru, Minglai Yang, Vikas Yadav, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CopySpec, an innovative technique designed to tackle the
inefficiencies LLMs face when generating responses that closely resemble
previous outputs. CopySpec identifies repeated sequences in the model's chat
history and speculates that the same tokens will follow, enabling seamless
copying without compromising output quality or requiring additional GPU memory.
To evaluate the effectiveness of our approach, we conducted experiments using
five LLMs and five datasets: MT-Bench, CNN/DM, GSM-8K, HumanEval, and our newly
created dataset, MT-Redundant. MT-Redundant, introduced in this paper,
transforms the second turn of MT-Bench into a request for variations of the
first turn's answer, simulating real-world scenarios where users request
modifications to prior responses. Our results demonstrate significant
speed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select
MT-Redundant categories, and 2.66x on the third turn of GSM-8K's
self-correction tasks. Moreover, we show that CopySpec integrates seamlessly
with speculative decoding, yielding an average 49% additional speed-up over
speculative decoding for the second turn of MT-Redundant across all eight
categories. While LLMs, even with speculative decoding, suffer from slower
inference as context sizes grow, CopySpec leverages the expanded context to
accelerate inference, making it faster as the context size increases. Our code
and dataset are publicly available at https://github.com/RazvanDu/CopySpec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 18 figures, 19 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLEAR: Cluster-based Prompt Learning on Heterogeneous Graphs <span class="chip">PAKDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiyang Wang, Zhongbao Zhang, Junda Ye, Li Sun, Jianzhong Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning has attracted increasing attention in the graph domain as a
means to bridge the gap between pretext and downstream tasks. Existing studies
on heterogeneous graph prompting typically use feature prompts to modify node
features for specific downstream tasks, which do not concern the structure of
heterogeneous graphs. Such a design also overlooks information from the
meta-paths, which are core to learning the high-order semantics of the
heterogeneous graphs. To address these issues, we propose CLEAR, a
Cluster-based prompt LEARNING model on heterogeneous graphs. We present cluster
prompts that reformulate downstream tasks as heterogeneous graph
reconstruction. In this way, we align the pretext and downstream tasks to share
the same training objective. Additionally, our cluster prompts are also
injected into the meta-paths such that the prompt learning process incorporates
high-order semantic information entailed by the meta-paths. Extensive
experiments on downstream tasks confirm the superiority of CLEAR. It
consistently outperforms state-of-the-art models, achieving up to 5%
improvement on the F1 metric for node classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by PAKDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on
  a Single GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejun Lee, Geon Park, Jaduk Suh, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern large language models (LLMs), handling very long context lengths
presents significant challenges as it causes slower inference speeds and
increased memory costs. Additionally, most existing pre-trained LLMs fail to
generalize beyond their original training sequence lengths. To enable efficient
and practical long-context utilization, we introduce InfiniteHiP, a novel, and
practical LLM inference framework that accelerates processing by dynamically
eliminating irrelevant context tokens through a modular hierarchical token
pruning algorithm. Our method also allows generalization to longer sequences by
selectively applying various RoPE adjustment methods according to the internal
attention patterns within LLMs. Furthermore, we offload the key-value cache to
host memory during inference, significantly reducing GPU memory pressure. As a
result, InfiniteHiP enables the processing of up to 3 million tokens on a
single L40s 48GB GPU -- 3x larger -- without any permanent loss of context
information. Our framework achieves an 18.95x speedup in attention decoding for
a 1 million token context without requiring additional training. We implement
our method in the SGLang framework and demonstrate its effectiveness and
practicality through extensive evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear-Time User-Level DP-SCO via Robust Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badih Ghazi, Ravi Kumar, Daogao Liu, Pasin Manurangsi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-level differentially private stochastic convex optimization (DP-SCO) has
garnered significant attention due to the paramount importance of safeguarding
user privacy in modern large-scale machine learning applications. Current
methods, such as those based on differentially private stochastic gradient
descent (DP-SGD), often struggle with high noise accumulation and suboptimal
utility due to the need to privatize every intermediate iterate. In this work,
we introduce a novel linear-time algorithm that leverages robust statistics,
specifically the median and trimmed mean, to overcome these challenges. Our
approach uniquely bounds the sensitivity of all intermediate iterates of SGD
with gradient estimation based on robust statistics, thereby significantly
reducing the gradient estimation noise for privacy purposes and enhancing the
privacy-utility trade-off. By sidestepping the repeated privatization required
by previous methods, our algorithm not only achieves an improved theoretical
privacy-utility trade-off but also maintains computational efficiency. We
complement our algorithm with an information-theoretic lower bound, showing
that our upper bound is optimal up to logarithmic factors and the dependence on
$\epsilon$. This work sets the stage for more robust and efficient
privacy-preserving techniques in machine learning, with implications for future
research and application in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2D Integrated Bayesian Tomography of Plasma Electron Density Profile for
  HL-3 Based on Gaussian Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Wang, Renjie Yang, Dong Li, Zongyu Yang, Zhijun Wang, Yixiong Wei, Jing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an integrated Bayesian model that combines line
integral measurements and point values using Gaussian Process (GP). The
proposed method leverages Gaussian Process Regression (GPR) to incorporate
point values into 2D profiles and employs coordinate mapping to integrate
magnetic flux information for 2D inversion. The average relative error of the
reconstructed profile, using the integrated Bayesian tomography model with
normalized magnetic flux, is as low as 3.60*10^(-4). Additionally, sensitivity
tests were conducted on the number of grids, the standard deviation of
synthetic diagnostic data, and noise levels, laying a solid foundation for the
application of the model to experimental data. This work not only achieves
accurate 2D inversion using the integrated Bayesian model but also provides a
robust framework for decoupling pressure information from equilibrium
reconstruction, thus making it possible to optimize equilibrium reconstruction
using inversion results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WENDy for Nonlinear-in-Parameter ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nic Rummel, Daniel A. Messenger, Stephen Becker, Vanja Dukic, David M. Bortz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Weak-form Estimation of Non-linear Dynamics (WENDy) algorithm is extended
to accommodate systems of ordinary differential equations that are
nonlinear-in-parameters (NiP). The extension rests on derived analytic
expressions for a likelihood function, its gradient and its Hessian matrix.
WENDy makes use of these to approximate a maximum likelihood estimator based on
optimization routines suited for non-convex optimization problems. The
resulting parameter estimation algorithm has better accuracy, a substantially
larger domain of convergence, and is often orders of magnitude faster than the
conventional output error least squares method (based on forward solvers).
  The WENDy.jl algorithm is efficiently implemented in Julia. We demonstrate
the algorithm's ability to accommodate the weak form optimization for both
additive normal and multiplicative log-normal noise, and present results on a
suite of benchmark systems of ordinary differential equations. In order to
demonstrate the practical benefits of our approach, we present extensive
comparisons between our method and output error methods in terms of accuracy,
precision, bias, and coverage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Sensor Fusion In Digital Twin Technology For Enhanced Capabilities
  In A Home Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Momoh, Salisu Yahaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the integration of data sensor fusion in digital twin
technology to bolster home environment capabilities, particularly in the
context of challenges brought on by the coronavirus pandemic and its economic
effects. The study underscores the crucial role of digital transformation in
not just adapting to, but also mitigating disruptions during the fourth
industrial revolution. Using the Wit Motion sensor, data was collected for
activities such as walking, working, sitting, and lying, with sensors measuring
accelerometers, gyroscopes, and magnetometers. The research integrates
Cyber-physical systems, IoT, AI, and robotics to fortify digital twin
capabilities.
  The paper compares sensor fusion methods, including feature-level fusion,
decision-level fusion, and Kalman filter fusion, alongside machine learning
models like SVM, GBoost, and Random Forest to assess model effectiveness.
Results show that sensor fusion significantly improves the accuracy and
reliability of these models, as it compensates for individual sensor
weaknesses, particularly with magnetometers. Despite higher accuracy in ideal
conditions, integrating data from multiple sensors ensures more consistent and
reliable results in real-world settings, thereby establishing a robust system
that can be confidently applied in practical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Graph-Based Semi-Supervised Learning via $p$-Conductances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sawyer Jack Robertson, Chester Holtz, Zhengchao Wan, Gal Mishne, Alexander Cloninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of semi-supervised learning on graphs in the regime
where data labels are scarce or possibly corrupted. We propose an approach
called $p$-conductance learning that generalizes the $p$-Laplace and Poisson
learning methods by introducing an objective reminiscent of $p$-Laplacian
regularization and an affine relaxation of the label constraints. This leads to
a family of probability measure mincut programs that balance sparse edge
removal with accurate distribution separation. Our theoretical analysis
connects these programs to well-known variational and probabilistic problems on
graphs (including randomized cuts, effective resistance, and Wasserstein
distance) and provides motivation for robustness when labels are diffused via
the heat kernel. Computationally, we develop a semismooth Newton-conjugate
gradient algorithm and extend it to incorporate class-size estimates when
converting the continuous solutions into label assignments. Empirical results
on computer vision and citation datasets demonstrate that our approach achieves
state-of-the-art accuracy in low label-rate, corrupted-label, and partial-label
regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When and why randomised exploration works (in linear bandits) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Abeille, David Janz, Ciara Pike-Burke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide an approach for the analysis of randomised exploration algorithms
like Thompson sampling that does not rely on forced optimism or posterior
inflation. With this, we demonstrate that in the $d$-dimensional linear bandit
setting, when the action space is smooth and strongly convex, randomised
exploration algorithms enjoy an $n$-step regret bound of the order $O(d\sqrt{n}
\log(n))$. Notably, this shows for the first time that there exist non-trivial
linear bandit settings where Thompson sampling can achieve optimal dimension
dependence in the regret.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Vision Models for Time Series Analysis: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingchao Ni, Ziming Zhao, ChengAo Shen, Hanghang Tong, Dongjin Song, Wei Cheng, Dongsheng Luo, Haifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series analysis has witnessed the inspiring development from traditional
autoregressive models, deep learning models, to recent Transformers and Large
Language Models (LLMs). Efforts in leveraging vision models for time series
analysis have also been made along the way but are less visible to the
community due to the predominant research on sequence modeling in this domain.
However, the discrepancy between continuous time series and the discrete token
space of LLMs, and the challenges in explicitly modeling the correlations of
variates in multivariate time series have shifted some research attentions to
the equally successful Large Vision Models (LVMs) and Vision Language Models
(VLMs). To fill the blank in the existing literature, this survey discusses the
advantages of vision models over LLMs in time series analysis. It provides a
comprehensive and in-depth overview of the existing methods, with dual views of
detailed taxonomy that answer the key research questions including how to
encode time series as images and how to model the imaged time series for
various tasks. Additionally, we address the challenges in the pre- and
post-processing steps involved in this framework and outline future directions
to further advance time series analysis with vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Systematic Evaluation of Generative Models on Tabular Transportation
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengen Wang, Alvaro Cardenas, Gurcan Comert, Murat Kantarcioglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sharing of large-scale transportation data is beneficial for
transportation planning and policymaking. However, it also raises significant
security and privacy concerns, as the data may include identifiable personal
information, such as individuals' home locations. To address these concerns,
synthetic data generation based on real transportation data offers a promising
solution that allows privacy protection while potentially preserving data
utility. Although there are various synthetic data generation techniques, they
are often not tailored to the unique characteristics of transportation data,
such as the inherent structure of transportation networks formed by all trips
in the datasets. In this paper, we use New York City taxi data as a case study
to conduct a systematic evaluation of the performance of widely used tabular
data generative models. In addition to traditional metrics such as distribution
similarity, coverage, and privacy preservation, we propose a novel graph-based
metric tailored specifically for transportation data. This metric evaluates the
similarity between real and synthetic transportation networks, providing
potentially deeper insights into their structural and functional alignment. We
also introduced an improved privacy metric to address the limitations of the
commonly-used one. Our experimental results reveal that existing tabular data
generative models often fail to perform as consistently as claimed in the
literature, particularly when applied to transportation data use cases.
Furthermore, our novel graph metric reveals a significant gap between synthetic
and real data. This work underscores the potential need to develop generative
models specifically tailored to take advantage of the unique characteristics of
emerging domains, such as transportation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Opening Articulated Objects in the Real World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What does it take to build mobile manipulation systems that can competently
operate on previously unseen objects in previously unseen environments? This
work answers this question using opening of articulated objects as a mobile
manipulation testbed. Specifically, our focus is on the end-to-end performance
on this task without any privileged information, i.e. the robot starts at a
location with the novel target articulated object in view, and has to approach
the object and successfully open it. We first develop a system for this task,
and then conduct 100+ end-to-end system tests across 13 real world test sites.
Our large-scale study reveals a number of surprising findings: a) modular
systems outperform end-to-end learned systems for this task, even when the
end-to-end learned systems are trained on 1000+ demonstrations, b) perception,
and not precise end-effector control, is the primary bottleneck to task
success, and c) state-of-the-art articulation parameter estimation models
developed in isolation struggle when faced with robot-centric viewpoints.
Overall, our findings highlight the limitations of developing components of the
pipeline in isolation and underscore the need for system-level research,
providing a pragmatic roadmap for building generalizable mobile manipulation
systems. Videos, code, and models are available on the project website:
https://arjung128.github.io/opening-articulated-objects/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage:
  https://arjung128.github.io/opening-articulated-objects/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformers Learn Low Sensitivity Functions: Investigations and
  Implications <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers achieve state-of-the-art accuracy and robustness across many
tasks, but an understanding of their inductive biases and how those biases
differ from other neural network architectures remains elusive. In this work,
we identify the sensitivity of the model to token-wise random perturbations in
the input as a unified metric which explains the inductive bias of transformers
across different data modalities and distinguishes them from other
architectures. We show that transformers have lower sensitivity than MLPs,
CNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show
that this low-sensitivity bias has important implications: i) lower sensitivity
correlates with improved robustness; it can also be used as an efficient
intervention to further improve the robustness of transformers; ii) it
corresponds to flatter minima in the loss landscape; and iii) it can serve as a
progress measure for grokking. We support these findings with theoretical
results showing (weak) spectral bias of transformers in the NTK regime, and
improved robustness due to the lower sensitivity. The code is available at
https://github.com/estija/sensitivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. 24 pages, 19 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Personalized Federated Prompt Learning for Multimodal
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13904v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13904v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linh Tran, Wei Sun, Stacy Patterson, Ana Milanova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (LLMs) are pivotal in revolutionizing
customer support and operations by integrating multiple modalities such as
text, images, and audio. Federated Prompt Learning (FPL) is a recently proposed
approach that combines pre-trained multimodal LLMs such as vision-language
models with federated learning to create personalized, privacy-preserving AI
systems. However, balancing the competing goals of personalization,
generalization, and privacy remains a significant challenge.
Over-personalization can lead to overfitting, reducing generalizability, while
stringent privacy measures, such as differential privacy, can hinder both
personalization and generalization. In this paper, we propose a Differentially
Private Federated Prompt Learning (DP-FPL) approach to tackle this challenge by
leveraging a low-rank factorization scheme to capture generalization while
maintaining a residual term that preserves expressiveness for personalization.
To ensure privacy, we introduce a novel method where we apply local
differential privacy to the two low-rank components of the local prompt, and
global differential privacy to the global prompt. Our approach mitigates the
impact of privacy noise on the model performance while balancing the tradeoff
between personalization and generalization. Extensive experiments demonstrate
the effectiveness of our approach over other benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OGBench: Benchmarking Offline Goal-Conditioned RL <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seohong Park, Kevin Frans, Benjamin Eysenbach, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline goal-conditioned reinforcement learning (GCRL) is a major problem in
reinforcement learning (RL) because it provides a simple, unsupervised, and
domain-agnostic way to acquire diverse behaviors and representations from
unlabeled data without rewards. Despite the importance of this setting, we lack
a standard benchmark that can systematically evaluate the capabilities of
offline GCRL algorithms. In this work, we propose OGBench, a new, high-quality
benchmark for algorithms research in offline goal-conditioned RL. OGBench
consists of 8 types of environments, 85 datasets, and reference implementations
of 6 representative offline GCRL algorithms. We have designed these challenging
and realistic environments and datasets to directly probe different
capabilities of algorithms, such as stitching, long-horizon reasoning, and the
ability to handle high-dimensional inputs and stochasticity. While
representative algorithms may rank similarly on prior benchmarks, our
experiments reveal stark strengths and weaknesses in these different
capabilities, providing a strong foundation for building new algorithms.
Project page: https://seohong.me/projects/ogbench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Universal Laws of Outlier Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aram Ebtekar, Yuhao Wang, Dominik Janzing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We argue that Algorithmic Information Theory (AIT) admits a principled way to
quantify outliers in terms of so-called randomness deficiency. For the
probability distribution generated by a causal Bayesian network, we show that
the randomness deficiency of the joint state decomposes into randomness
deficiencies of each causal mechanism, subject to the Independence of
Mechanisms Principle. Accordingly, anomalous joint observations can be
quantitatively attributed to their root causes, i.e., the mechanisms that
behaved anomalously. As an extension of Levin's law of randomness conservation,
we show that weak outliers cannot cause strong ones when Independence of
Mechanisms holds. We show how these information theoretic laws provide a better
understanding of the behaviour of outliers defined with respect to existing
scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic Normality of Generalized Low-Rank Matrix Sensing via
  Riemannian Geometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osbert Bastani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove an asymptotic normality guarantee for generalized low-rank matrix
sensing -- i.e., matrix sensing under a general convex loss $\bar\ell(\langle
X,M\rangle,y^*)$, where $M\in\mathbb{R}^{d\times d}$ is the unknown rank-$k$
matrix, $X$ is a measurement matrix, and $y^*$ is the corresponding
measurement. Our analysis relies on tools from Riemannian geometry to handle
degeneracy of the Hessian of the loss due to rotational symmetry in the
parameter space. In particular, we parameterize the manifold of low-rank
matrices by $\bar\theta\bar\theta^\top$, where
$\bar\theta\in\mathbb{R}^{d\times k}$. Then, assuming the minimizer of the
empirical loss $\bar\theta^0\in\mathbb{R}^{d\times k}$ is in a constant size
ball around the true parameters $\bar\theta^*$, we prove
$\sqrt{n}(\phi^0-\phi^*)\xrightarrow{D}N(0,(H^*)^{-1})$ as $n\to\infty$, where
$\phi^0$ and $\phi^*$ are representations of $\bar\theta^*$ and $\bar\theta^0$
in the horizontal space of the Riemannian quotient manifold
$\mathbb{R}^{d\times k}/\text{O}(k)$, and $H^*$ is the Hessian of the true loss
in the same representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TransMLA: Multi-Head Latent Attention Is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanxu Meng, Zengwei Yao, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models (LLMs) often encounter communication bottlenecks
on current hardware, rather than purely computational constraints. Multi-head
Latent Attention (MLA) tackles this challenge by using low-rank matrices in the
key-value (KV) layers, thereby allowing compressed latent KV states to be
cached. This approach significantly reduces the KV cache size relative to
traditional multi-head attention, leading to faster inference. Moreover, MLA
employs an up-projection matrix to increase expressiveness, trading additional
computation for reduced communication overhead. Although MLA has demonstrated
efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers
still rely on Group Query Attention (GQA) and have not announced any plans to
adopt MLA. In this paper, we show that GQA can always be represented by MLA
while maintaining the same KV cache overhead, but the converse does not hold.
To encourage broader use of MLA, we introduce TransMLA, a post-training method
that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,
Mixtral) into MLA-based models. After conversion, the model can undergo
additional training to boost expressiveness without increasing the KV cache
size. Furthermore, we plan to develop MLA-specific inference acceleration
techniques to preserve low latency in transformed models, thus enabling more
efficient distillation of Deepseek R1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/fxmeng/TransMLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WASP: A Weight-Space Approach to Detecting Learned Spuriousness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18970v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18970v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Daniel Păduraru, Antonio Bărbălau, Radu Filipescu, Andrei Liviu Nicolicioiu, Elena Burceanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is of crucial importance to train machine learning models such that they
clearly understand what defines each class in a given task. Though there is a
sum of works dedicated to identifying the spurious correlations featured by a
dataset that may impact the model's understanding of the classes, all current
approaches rely solely on data or error analysis. That is, they cannot point
out spurious correlations learned by the model that are not already pointed out
by the counterexamples featured in the validation or training sets. We propose
a method that transcends this limitation, switching the focus from analyzing a
model's predictions to analyzing the model's weights, the mechanism behind the
making of the decisions, which proves to be more insightful. Our proposed
Weight-space Approach to detecting Spuriousness (WASP) relies on analyzing the
weights of foundation models as they drift towards capturing various (spurious)
correlations while being fine-tuned on a given dataset. We demonstrate that
different from previous works, our method (i) can expose spurious correlations
featured by a dataset even when they are not exposed by training or validation
counterexamples, (ii) it works for multiple modalities such as image and text,
and (iii) it can uncover previously untapped spurious correlations learned by
ImageNet-1k classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 6 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HorNets: Learning from Discrete and Continuous Signals with Routing
  Neural Networks <span class="chip">ACML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boshko Koloski, Nada Lavrač, Blaž Škrlj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Construction of neural network architectures suitable for learning from both
continuous and discrete tabular data is a challenging research endeavor.
Contemporary high-dimensional tabular data sets are often characterized by a
relatively small instance count, requiring data-efficient learning. We propose
HorNets (Horn Networks), a neural network architecture with state-of-the-art
performance on synthetic and real-life data sets from scarce-data tabular
domains. HorNets are based on a clipped polynomial-like activation function,
extended by a custom discrete-continuous routing mechanism that decides which
part of the neural network to optimize based on the input's cardinality. By
explicitly modeling parts of the feature combination space or combining whole
space in a linear attention-like manner, HorNets dynamically decide which mode
of operation is the most suitable for a given piece of data with no explicit
supervision. This architecture is one of the few approaches that reliably
retrieves logical clauses (including noisy XNOR) and achieves state-of-the-art
classification performance on 14 real-life biomedical high-dimensional data
sets. HorNets are made freely available under a permissive license alongside a
synthetic generator of categorical benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ACML conference journal track with the Machine
  Learning journal. The first and the last authors share an equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed-curvature decision trees and random forests <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Chlenski, Quentin Chu, Raiyan R. Khan, Kaizhu Du, Antonio Khalil Moretti, Itsik Pe'er
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision trees (DTs) and their random forest (RF) extensions are workhorses
of classification and regression in Euclidean spaces. However, algorithms for
learning in non-Euclidean spaces are still limited. We extend DT and RF
algorithms to product manifolds: Cartesian products of several hyperbolic,
hyperspherical, or Euclidean components. Such manifolds handle heterogeneous
curvature while still factorizing neatly into simpler components, making them
compelling embedding spaces for complex datasets. Our novel angular
reformulation of DTs respects the geometry of the product manifold, yielding
splits that are geodesically convex, maximum-margin, and composable. In the
special cases of single-component manifolds, our method simplifies to its
Euclidean or hyperbolic counterparts, or introduces hyperspherical DT
algorithms, depending on the curvature. We benchmark our method on various
classification, regression, and link prediction tasks on synthetic data, graph
embeddings, mixed-curvature variational autoencoder latent spaces, and
empirical data. Compared to 7 other classifiers, product RFs ranked first on 25
out of 57 benchmarks, and placed in the top 2 for 46 out of 57. This highlights
the value of product RFs as straightforward yet powerful new tools for data
analysis in product manifolds. Code for our paper is available at
https://github.com/pchlenski/manify.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 figures. Submitted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal Predictive Portfolio Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines portfolio selection using predictive models for portfolio
returns. Portfolio selection is a fundamental task in finance, and a variety of
methods have been developed to achieve this goal. For instance, the
mean-variance approach constructs portfolios by balancing the trade-off between
the mean and variance of asset returns, while the quantile-based approach
optimizes portfolios by considering tail risk. These methods often depend on
distributional information estimated from historical data using predictive
models, each of which carries its own uncertainty. To address this, we propose
a framework for predictive portfolio selection via conformal prediction ,
called \emph{Conformal Predictive Portfolio Selection} (CPPS). Our approach
forecasts future portfolio returns, computes the corresponding prediction
intervals, and selects the portfolio of interest based on these intervals. The
framework is flexible and can accommodate a wide range of predictive models,
including autoregressive (AR) models, random forests, and neural networks. We
demonstrate the effectiveness of the CPPS framework by applying it to an AR
model and validate its performance through empirical studies, showing that it
delivers superior returns compared to simpler strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimism in the Face of Ambiguity Principle for Multi-Armed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengmeng Li, Daniel Kuhn, Bahar Taşkesen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Follow-The-Regularized-Leader (FTRL) algorithms often enjoy optimal regret
for adversarial as well as stochastic bandit problems and allow for a
streamlined analysis. Nonetheless, FTRL algorithms require the solution of an
optimization problem in every iteration and are thus computationally
challenging. In contrast, Follow-The-Perturbed-Leader (FTPL) algorithms achieve
computational efficiency by perturbing the estimates of the rewards of the
arms, but their regret analysis is cumbersome. We propose a new FTPL algorithm
that generates optimal policies for both adversarial and stochastic multi-armed
bandits. Like FTRL, our algorithm admits a unified regret analysis, and similar
to FTPL, it offers low computational costs. Unlike existing FTPL algorithms
that rely on independent additive disturbances governed by a \textit{known}
distribution, we allow for disturbances governed by an \textit{ambiguous}
distribution that is only known to belong to a given set and propose a
principle of optimism in the face of ambiguity. Consequently, our framework
generalizes existing FTPL algorithms. It also encapsulates a broad range of
FTRL methods as special cases, including several optimal ones, which appears to
be impossible with current FTPL methods. Finally, we use techniques from
discrete choice theory to devise an efficient bisection algorithm for computing
the optimistic arm sampling probabilities. This algorithm is up to $10^4$ times
faster than standard FTRL algorithms that solve an optimization problem in
every iteration. Our results not only settle existing conjectures but also
provide new insights into the impact of perturbations by mapping FTRL to FTPL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep
  Graph Networks <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Heilig, Alessio Gravina, Alessandro Trenta, Claudio Gallicchio, Davide Bacciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamics of information diffusion within graphs is a critical open issue
that heavily influences graph representation learning, especially when
considering long-range propagation. This calls for principled approaches that
control and regulate the degree of propagation and dissipation of information
throughout the neural flow. Motivated by this, we introduce (port-)Hamiltonian
Deep Graph Networks, a novel framework that models neural information flow in
graphs by building on the laws of conservation of Hamiltonian dynamical
systems. We reconcile under a single theoretical and practical framework both
non-dissipative long-range propagation and non-conservative behaviors,
introducing tools from mechanical systems to gauge the equilibrium between the
two components. Our approach can be applied to general message-passing
architectures, and it provides theoretical guarantees on information
conservation in time. Empirical results prove the effectiveness of our
port-Hamiltonian scheme in pushing simple graph convolutional architectures to
state-of-the-art performance in long-range benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025 (https://openreview.net/forum?id=03EkqSCKuO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proxy-informed Bayesian transfer learning with unknown sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabina J. Sloman, Julien Martinelli, Samuel Kaski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization outside the scope of one's training data requires leveraging
prior knowledge about the effects that transfer, and the effects that don't,
between different data sources. Transfer learning is a framework for specifying
and refining this knowledge about sets of source (training) and target
(prediction) data. A challenging open problem is addressing the empirical
phenomenon of negative transfer, whereby the transfer learner performs worse on
the target data after taking the source data into account than before. We first
introduce a Bayesian perspective on negative transfer, and then a method to
address it. The key insight from our formulation is that negative transfer can
stem from misspecified prior information about non-transferable causes of the
source data. Our proposed method, proxy-informed robust method for
probabilistic transfer learning (PROMPT), does not require prior knowledge of
the source data (the data sources may be "unknown"). PROMPT is thus applicable
when differences between tasks are unobserved, such as in the presence of
latent confounders. Moreover, the learner need not have access to observations
in the target task (cannot "fine-tune"), and instead makes use of proxy
(indirect) information. Our theoretical results show that the threat of
negative transfer does not depend on the informativeness of the proxy
information, highlighting the usefulness of PROMPT in cases where only noisy
indirect information, such as human feedback, is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bias-Correction Decentralized Stochastic Gradient Algorithm with
  Momentum Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19082v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19082v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Hu, Xi Chen, Weidong Liu, Xiaojun Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed stochastic optimization algorithms can simultaneously process
large-scale datasets, significantly accelerating model training. However, their
effectiveness is often hindered by the sparsity of distributed networks and
data heterogeneity. In this paper, we propose a momentum-accelerated
distributed stochastic gradient algorithm, termed Exact-Diffusion with Momentum
(EDM), which mitigates the bias from data heterogeneity and incorporates
momentum techniques commonly used in deep learning to enhance convergence rate.
Our theoretical analysis demonstrates that the EDM algorithm converges
sub-linearly to the neighborhood of the optimal solution, the radius of which
is irrespective of data heterogeneity, when applied to non-convex objective
functions; under the Polyak-Lojasiewicz condition, which is a weaker assumption
than strong convexity, it converges linearly to the target region. Our analysis
techniques employed to handle momentum in complex distributed parameter update
structures yield a sufficiently tight convergence upper bound, offering a new
perspective for the theoretical analysis of other momentum-based distributed
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Importance of Backbone to the Adversarial Robustness of Object
  Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Li, Hang Chen, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection is a critical component of various security-sensitive
applications, such as autonomous driving and video surveillance. However,
existing object detectors are vulnerable to adversarial attacks, which poses a
significant challenge to their reliability and security. Through experiments,
first, we found that existing works on improving the adversarial robustness of
object detectors give a false sense of security. Second, we found that
adversarially pre-trained backbone networks were essential for enhancing the
adversarial robustness of object detectors. We then proposed a simple yet
effective recipe for fast adversarial fine-tuning on object detectors with
adversarially pre-trained backbones. Without any modifications to the structure
of object detectors, our recipe achieved significantly better adversarial
robustness than previous works. Finally, we explored the potential of different
modern object detector designs for improving adversarial robustness with our
recipe and demonstrated interesting findings, which inspired us to design
state-of-the-art (SOTA) robust detectors. Our empirical results set a new
milestone for adversarially robust object detection. Code and trained
checkpoints are available at https://github.com/thu-ml/oddefense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TIFS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaron Maene, Vincent Derkinderen, Pedro Zuidberg Dos Martires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A popular approach to neurosymbolic AI involves mapping logic formulas to
arithmetic circuits (computation graphs consisting of sums and products) and
passing the outputs of a neural network through these circuits. This approach
enforces symbolic constraints onto a neural network in a principled and
end-to-end differentiable way. Unfortunately, arithmetic circuits are
challenging to run on modern AI accelerators as they exhibit a high degree of
irregular sparsity. To address this limitation, we introduce knowledge layers
(KLay), a new data structure to represent arithmetic circuits that can be
efficiently parallelized on GPUs. Moreover, we contribute two algorithms used
in the translation of traditional circuit representations to KLay and a further
algorithm that exploits parallelization opportunities during circuit
evaluations. We empirically show that KLay achieves speedups of multiple orders
of magnitude over the state of the art, thereby paving the way towards scaling
neurosymbolic AI to larger real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Galois theorem for machine learning: Functions on symmetric matrices
  and point clouds via lightweight invariant features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08097v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08097v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Blum-Smith, Ningyuan Huang, Marco Cuturi, Soledad Villar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a mathematical formulation for machine learning of
(1) functions on symmetric matrices that are invariant with respect to the
action of permutations by conjugation, and (2) functions on point clouds that
are invariant with respect to rotations, reflections, and permutations of the
points. To achieve this, we provide a general construction of generically
separating invariant features using ideas inspired by Galois theory. We
construct $O(n^2)$ invariant features derived from generators for the field of
rational functions on $n\times n$ symmetric matrices that are invariant under
joint permutations of rows and columns. We show that these invariant features
can separate all distinct orbits of symmetric matrices except for a measure
zero set; such features can be used to universally approximate invariant
functions on almost all weighted graphs. For point clouds in a fixed dimension,
we prove that the number of invariant features can be reduced, generically
without losing expressivity, to $O(n)$, where $n$ is the number of points. We
combine these invariant features with DeepSets to learn functions on symmetric
matrices and point clouds with varying sizes. We empirically demonstrate the
feasibility of our approach on molecule property regression and point cloud
distance prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ADBM: Adversarial diffusion bridge model for reliable adversarial
  purification <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00315v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00315v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Li, Wenxuan Sun, Huanran Chen, Qiongxiu Li, Yining Liu, Yingzhe He, Jie Shi, Xiaolin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently Diffusion-based Purification (DiffPure) has been recognized as an
effective defense method against adversarial examples. However, we find
DiffPure which directly employs the original pre-trained diffusion models for
adversarial purification, to be suboptimal. This is due to an inherent
trade-off between noise purification performance and data recovery quality.
Additionally, the reliability of existing evaluations for DiffPure is
questionable, as they rely on weak adaptive attacks. In this work, we propose a
novel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs
a reverse bridge from the diffused adversarial data back to its original clean
examples, enhancing the purification capabilities of the original diffusion
models. Through theoretical analysis and experimental validation across various
scenarios, ADBM has proven to be a superior and robust defense mechanism,
offering significant promise for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sable: a Performant, Efficient and Scalable Sequence Model for MARL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omayma Mahjoub, Sasha Abramowitz, Ruan de Kock, Wiem Khlifi, Simon du Toit, Jemma Daniel, Louay Ben Nessir, Louise Beyers, Claude Formanek, Liam Clark, Arnu Pretorius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As multi-agent reinforcement learning (MARL) progresses towards solving
larger and more complex problems, it becomes increasingly important that
algorithms exhibit the key properties of (1) strong performance, (2) memory
efficiency and (3) scalability. In this work, we introduce Sable, a performant,
memory efficient and scalable sequence modeling approach to MARL. Sable works
by adapting the retention mechanism in Retentive Networks to achieve
computationally efficient processing of multi-agent observations with long
context memory for temporal reasoning. Through extensive evaluations across six
diverse environments, we demonstrate how Sable is able to significantly
outperform existing state-of-the-art methods in a large number of diverse tasks
(34 out of 45 tested). Furthermore, Sable maintains performance as we scale the
number of agents, handling environments with more than a thousand agents while
exhibiting a linear increase in memory usage. Finally, we conduct ablation
studies to isolate the source of Sable's performance gains and confirm its
efficient computational memory usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Offline Imitation Learning via Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Rupf, Marco Bagatella, Nico Gürtler, Jonas Frey, Georg Martius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot imitation learning algorithms hold the promise of reproducing
unseen behavior from as little as a single demonstration at test time. Existing
practical approaches view the expert demonstration as a sequence of goals,
enabling imitation with a high-level goal selector, and a low-level
goal-conditioned policy. However, this framework can suffer from myopic
behavior: the agent's immediate actions towards achieving individual goals may
undermine long-term objectives. We introduce a novel method that mitigates this
issue by directly optimizing the occupancy matching objective that is intrinsic
to imitation learning. We propose to lift a goal-conditioned value function to
a distance between occupancies, which are in turn approximated via a learned
world model. The resulting method can learn from offline, suboptimal data, and
is capable of non-myopic, zero-shot imitation, as we demonstrate in complex,
continuous benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Embeddings with Coupled Adam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Stollenwerk, Tobias Stollenwerk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their remarkable capabilities, LLMs learn word representations that
exhibit the undesirable yet poorly understood feature of anisotropy. In this
paper, we argue that the second moment in Adam is a cause of anisotropic
embeddings, and suggest a modified optimizer called Coupled Adam to mitigate
the problem. Our experiments demonstrate that Coupled Adam significantly
improves the quality of embeddings, while also leading to better upstream and
downstream performance on large enough datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures; figures corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The LLM Language Network: A Neuroscientific Approach for Identifying
  Causally Task-Relevant Units <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badr AlKhamissi, Greta Tuckute, Antoine Bosselut, Martin Schrimpf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable capabilities on not just
language tasks, but also various tasks that are not linguistic in nature, such
as logical reasoning and social inference. In the human brain, neuroscience has
identified a core language system that selectively and causally supports
language processing. We here ask whether similar specialization for language
emerges in LLMs. We identify language-selective units within 18 popular LLMs,
using the same localization approach that is used in neuroscience. We then
establish the causal role of these units by demonstrating that ablating LLM
language-selective units -- but not random units -- leads to drastic deficits
in language tasks. Correspondingly, language-selective LLM units are more
aligned to brain recordings from the human language system than random units.
Finally, we investigate whether our localization method extends to other
cognitive domains: while we find specialized networks in some LLMs for
reasoning and social capabilities, there are substantial differences among
models. These findings provide functional and causal evidence for
specialization in large language models, and highlight parallels with the
functional organization in the brain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impact of Batch Normalization on Convolutional Network Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hermanus L. Potgieter, Coenraad Mouton, Marelie H. Davel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Batch normalization (BatchNorm) is a popular layer normalization technique
used when training deep neural networks. It has been shown to enhance the
training speed and accuracy of deep learning models. However, the mechanics by
which BatchNorm achieves these benefits is an active area of research, and
different perspectives have been proposed. In this paper, we investigate the
effect of BatchNorm on the resulting hidden representations, that is, the
vectors of activation values formed as samples are processed at each hidden
layer. Specifically, we consider the sparsity of these representations, as well
as their implicit clustering -- the creation of groups of representations that
are similar to some extent. We contrast image classification models trained
with and without batch normalization and highlight consistent differences
observed. These findings highlight that BatchNorm's effect on representational
sparsity is not a significant factor affecting generalization, while the
representations of models trained with BatchNorm tend to show more advantageous
clustering characteristics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Crime Forecasting: A Spatio-temporal Analysis with Deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Mao, Wei Du, Shuo Wen, Qi Li, Tong Zhang, Wei Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study uses deep-learning models to predict city partition crime counts
on specific days. It helps police enhance surveillance, gather intelligence,
and proactively prevent crimes. We formulate crime count prediction as a
spatiotemporal sequence challenge, where both input data and prediction targets
are spatiotemporal sequences. In order to improve the accuracy of crime
forecasting, we introduce a new model that combines Convolutional Neural
Networks (CNN) and Long Short-Term Memory (LSTM) networks. We conducted a
comparative analysis to access the effects of various data sequences, including
raw and binned data, on the prediction errors of four deep learning forecasting
models. Directly inputting raw crime data into the forecasting model causes
high prediction errors, making the model unsuitable for real - world use. The
findings indicate that the proposed CNN-LSTM model achieves optimal performance
when crime data is categorized into 10 or 5 groups. Data binning can enhance
forecasting model performance, but poorly defined intervals may reduce map
granularity. Compared to dividing into 5 bins, binning into 10 intervals
strikes an optimal balance, preserving data characteristics and surpassing raw
data in predictive modelling efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was submitted without the consent of all co-authors. The
  content of the paper is incomplete and requires substantial additional work
  before it can be considered a complete and coherent submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Federated Unsupervised Domain Adaptation for
  Regression on Small-Scale and High-Dimensional Biological Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cem Ata Baykara, Ali Burak Ünal, Nico Pfeifer, Mete Akgün
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models often struggle with generalization in small,
heterogeneous datasets due to domain shifts caused by variations in data
collection and population differences. This challenge is particularly
pronounced in biological data, where data is high-dimensional, small-scale, and
decentralized across institutions. While federated domain adaptation methods
(FDA) aim to address these challenges, most existing approaches rely on deep
learning and focus on classification tasks, making them unsuitable for
small-scale, high-dimensional applications. In this work, we propose freda, a
privacy-preserving federated method for unsupervised domain adaptation in
regression tasks. Unlike deep learning-based FDA approaches, freda is the first
method to enable the federated training of Gaussian Processes to model complex
feature relationships while ensuring complete data privacy through randomized
encoding and secure aggregation. This allows for effective domain adaptation
without direct access to raw data, making it well-suited for applications
involving high-dimensional, heterogeneous datasets. We evaluate freda on the
challenging task of age prediction from DNA methylation data, demonstrating
that it achieves performance comparable to the centralized state-of-the-art
method while preserving complete data privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret Bounds for Episodic Risk-Sensitive Linear Quadratic Regulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Xu, Xuefeng Gao, Xuedong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Risk-sensitive linear quadratic regulator is one of the most fundamental
problems in risk-sensitive optimal control. In this paper, we study online
adaptive control of risk-sensitive linear quadratic regulator in the finite
horizon episodic setting. We propose a simple least-squares greedy algorithm
and show that it achieves $\widetilde{\mathcal{O}}(\log N)$ regret under a
specific identifiability assumption, where $N$ is the total number of episodes.
If the identifiability assumption is not satisfied, we propose incorporating
exploration noise into the least-squares-based algorithm, resulting in an
algorithm with $\widetilde{\mathcal{O}}(\sqrt{N})$ regret. To our best
knowledge, this is the first set of regret bounds for episodic risk-sensitive
linear quadratic regulator. Our proof relies on perturbation analysis of
less-standard Riccati equations for risk-sensitive linear quadratic control,
and a delicate analysis of the loss in the risk-sensitive performance criterion
due to applying the suboptimal controller in the online learning process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxin Hu, Hao Li, Yihe Yuan, Jing Li, Ivor Tsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the milestones in large language models (LLMs) and multimodal
models, we have seen a surge in applying LLMs to biochemical tasks. Leveraging
graph features and molecular text representations, LLMs can tackle various
tasks, such as predicting chemical reaction outcomes and describing molecular
properties. However, most current work overlooks the *multi-level nature* of
the graph modality, even though different chemistry tasks may benefit from
different feature levels. In this work, we first study the effect of feature
granularity and reveal that even reducing all GNN-generated feature tokens to a
single one does not significantly impact model performance. We then investigate
the effect of various graph feature levels and demonstrate that both the
quality of LLM-generated molecules and model performance across different tasks
depend on different graph feature levels. Therefore, we conclude with two key
insights: (1) current molecular-related multimodal LLMs lack a comprehensive
understanding of graph features, and (2) static processing is not sufficient
for hierarchical graph feature. We share our findings in detail, with the hope
of paving the way for the community to develop more advanced multimodal LLMs
for incorporating molecular graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 tables, 1 figure, paper under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion-LAM: Probabilistic Limited Area Weather Forecasting with
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Larsson, Joel Oskarsson, Tomas Landelius, Fredrik Lindsten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods have been shown to be effective for weather
forecasting, based on the speed and accuracy compared to traditional numerical
models. While early efforts primarily concentrated on deterministic
predictions, the field has increasingly shifted toward probabilistic
forecasting to better capture the forecast uncertainty. Most machine
learning-based models have been designed for global-scale predictions, with
only limited work targeting regional or limited area forecasting, which allows
more specialized and flexible modeling for specific locations. This work
introduces Diffusion-LAM, a probabilistic limited area weather model leveraging
conditional diffusion. By conditioning on boundary data from surrounding
regions, our approach generates forecasts within a defined area. Experimental
results on the MEPS limited area dataset demonstrate the potential of
Diffusion-LAM to deliver accurate probabilistic forecasts, highlighting its
promise for limited-area weather prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noise Matters: Diffusion Model-based Urban Mobility Generation with
  Collaborative Noise Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05000v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05000v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Zhang, Yuan Yuan, Jingtao Ding, Jian Yuan, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With global urbanization, the focus on sustainable cities has largely grown,
driving research into equity, resilience, and urban planning, which often
relies on mobility data. The rise of web-based apps and mobile devices has
provided valuable user data for mobility-related research. However, real-world
mobility data is costly and raises privacy concerns. To protect privacy while
retaining key features of real-world movement, the demand for synthetic data
has steadily increased. Recent advances in diffusion models have shown great
potential for mobility trajectory generation due to their ability to model
randomness and uncertainty. However, existing approaches often directly apply
identically distributed (i.i.d.) noise sampling from image generation
techniques, which fail to account for the spatiotemporal correlations and
social interactions that shape urban mobility patterns. In this paper, we
propose CoDiffMob, a diffusion model for urban mobility generation with
collaborative noise priors, we emphasize the critical role of noise in
diffusion models for generating mobility data. By leveraging both individual
movement characteristics and population-wide dynamics, we construct novel
collaborative noise priors that provide richer and more informative guidance
throughout the generation process. Extensive experiments demonstrate the
superiority of our method, with generated data accurately capturing both
individual preferences and collective patterns, achieving an improvement of
over 32%. Furthermore, it can effectively replace web-derived mobility data to
better support downstream applications, while safeguarding user privacy and
fostering a more secure and ethical web. This highlights its tremendous
potential for applications in sustainable city-related research. The code and
data are available at https://github.com/tsinghua-fib-lab/CoDiffMob.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flow Matching: Markov Kernels, Stochastic Processes and Transport Plans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Wald, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Among generative neural models, flow matching techniques stand out for their
simple applicability and good scaling properties. Here, velocity fields of
curves connecting a simple latent and a target distribution are learned. Then
the corresponding ordinary differential equation can be used to sample from a
target distribution, starting in samples from the latent one. This paper
reviews from a mathematical point of view different techniques to learn the
velocity fields of absolutely continuous curves in the Wasserstein geometry. We
show how the velocity fields can be characterized and learned via i) transport
plans (couplings) between latent and target distributions, ii) Markov kernels
and iii) stochastic processes, where the latter two include the coupling
approach, but are in general broader. Besides this main goal, we show how flow
matching can be used for solving Bayesian inverse problems, where the
definition of conditional Wasserstein distances plays a central role. Finally,
we briefly address continuous normalizing flows and score matching techniques,
which approach the learning of velocity fields of curves from other directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tighter sparse variational Gaussian processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thang D. Bui, Matthew Ashman, Richard E. Turner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse variational Gaussian process (GP) approximations based on inducing
points have become the de facto standard for scaling GPs to large datasets,
owing to their theoretical elegance, computational efficiency, and ease of
implementation. This paper introduces a provably tighter variational
approximation by relaxing the standard assumption that the conditional
approximate posterior given the inducing points must match that in the prior.
The key innovation is to modify the conditional posterior to have smaller
variances than that of the prior at the training points. We derive the
collapsed bound for the regression case, describe how to use the proposed
approximation in large data settings, and discuss its application to handle
orthogonally structured inducing points and GP latent variable models.
Extensive experiments on regression benchmarks, classification, and latent
variable models demonstrate that the proposed approximation consistently
matches or outperforms standard sparse variational GPs while maintaining the
same computational cost. An implementation will be made available in all
popular GP packages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature contamination: Neural networks learn uncorrelated features and
  fail to generalize <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianren Zhang, Chujie Zhao, Guanyu Chen, Yizhou Jiang, Feng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations that generalize under distribution shifts is
critical for building robust machine learning models. However, despite
significant efforts in recent years, algorithmic advances in this direction
have been limited. In this work, we seek to understand the fundamental
difficulty of out-of-distribution generalization with deep neural networks. We
first empirically show that perhaps surprisingly, even allowing a neural
network to explicitly fit the representations obtained from a teacher network
that can generalize out-of-distribution is insufficient for the generalization
of the student network. Then, by a theoretical study of two-layer ReLU networks
optimized by stochastic gradient descent (SGD) under a structured feature
model, we identify a fundamental yet unexplored feature learning proclivity of
neural networks, feature contamination: neural networks can learn uncorrelated
features together with predictive features, resulting in generalization failure
under distribution shifts. Notably, this mechanism essentially differs from the
prevailing narrative in the literature that attributes the generalization
failure to spurious correlations. Overall, our results offer new insights into
the non-linear feature learning dynamics of neural networks and highlight the
necessity of considering inductive biases in out-of-distribution
generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A method of supervised learning from conflicting data with hidden
  contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.12113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.12113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianren Zhang, Yizhou Jiang, Feng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional supervised learning assumes a stable input-output relationship.
However, this assumption fails in open-ended training settings where the
input-output relationship depends on hidden contexts. In this work, we
formulate a more general supervised learning problem in which training data is
drawn from multiple unobservable domains, each potentially exhibiting distinct
input-output maps. This inherent conflict in data renders standard empirical
risk minimization training ineffective. To address this challenge, we propose a
method LEAF that introduces an allocation function, which learns to assign
conflicting data to different predictive models. We establish a connection
between LEAF and a variant of the Expectation-Maximization algorithm, allowing
us to derive an analytical expression for the allocation function. Finally, we
provide a theoretical analysis of LEAF and empirically validate its
effectiveness on both synthetic and real-world tasks involving conflicting
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Inference for Temporal Difference Learning with Linear
  Function Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichen Wu, Gen Li, Yuting Wei, Alessandro Rinaldo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical inference with finite-sample validity for the value function of a
given policy in Markov decision processes (MDPs) is crucial for ensuring the
reliability of reinforcement learning. Temporal Difference (TD) learning,
arguably the most widely used algorithm for policy evaluation, serves as a
natural framework for this purpose. In this paper, we study the consistency
properties of TD learning with Polyak-Ruppert averaging and linear function
approximation, and obtain three significant improvements over existing results.
First, we derive a novel sharp high-dimensional probability convergence
guarantee that depends explicitly on the asymptotic variance and holds under
weak conditions. We further establish refined high-dimensional Berry-Esseen
bounds over the class of convex sets that guarantee faster rates than those in
the literature. Finally, we propose a plug-in estimator for the asymptotic
covariance matrix, designed for efficient online computation. These results
enable the construction of confidence regions and simultaneous confidence
intervals for the linear parameters of the value function, with guaranteed
finite-sample coverage. We demonstrate the applicability of our theoretical
findings through numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Scheduling for LLM Inference with KV Cache Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Jaillet, Jiashuo Jiang, Chara Podimata, Zijie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) inference, where a trained model generates text
one word at a time in response to user prompts, is a computationally intensive
process requiring efficient scheduling to optimize latency and resource
utilization. A key challenge in LLM inference is the management of the
Key-Value (KV) cache, which reduces redundant computations but introduces
memory constraints. In this work, we model LLM inference with KV cache
constraints theoretically and propose novel batching and scheduling algorithms
that minimize inference latency while effectively managing the KV cache's
memory.
  We analyze both semi-online and fully online scheduling models, and our
results are threefold. First, we provide a polynomial-time algorithm that
achieves exact optimality in terms of average latency in the semi-online prompt
arrival model. Second, in the fully online case with a stochastic prompt
arrival, we introduce an efficient online scheduling algorithm with constant
regret. Third, we prove that no algorithm (deterministic or randomized) can
achieve a constant competitive ratio in fully online adversarial settings. Our
empirical evaluations on a public LLM inference dataset, using the Llama-70B
model on A100 GPUs, show that our approach significantly outperforms benchmark
algorithms used currently in practice, achieving lower latency while reducing
energy consumption. Overall, our results offer a path toward more sustainable
and cost-effective LLM deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WGFormer: An SE(3)-Transformer Driven by Wasserstein Gradient Flows for
  Molecular Ground-State Conformation Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09795v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09795v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanmeng Wang, Minjie Cheng, Hongteng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting molecular ground-state conformation (i.e., energy-minimized
conformation) is crucial for many chemical applications such as molecular
docking and property prediction. Classic energy-based simulation is
time-consuming when solving this problem while existing learning-based methods
have advantages in computational efficiency but sacrifice accuracy and
interpretability. In this work, we propose a novel and effective method to
bridge the energy-based simulation and the learning-based strategy, which
designs and learns a Wasserstein gradient flow-driven SE(3)-Transformer, called
WGFormer, for molecular ground-state conformation prediction. Specifically, our
method tackles this task within an auto-encoding framework, which encodes
low-quality conformations by the proposed WGFormer and decodes corresponding
ground-state conformations by an MLP. The architecture of WGFormer corresponds
to Wasserstein gradient flows -- it optimizes molecular conformations by
minimizing an energy function defined on the latent mixture models of atoms,
thereby significantly improving performance and interpretability. Extensive
experiments show that our method consistently outperforms state-of-the-art
competitors, providing a new and insightful paradigm to predict molecular
ground-state conformation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rendering Wireless Environments Useful for Gradient Estimators: A
  Zero-Order Stochastic Federated Learning Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elissa Mhanna, Mohamad Assaad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-device federated learning (FL) is a growing machine learning setting
whereby multiple edge devices collaborate to train a model without disclosing
their raw data. With the great number of mobile devices participating in more
FL applications via the wireless environment, the practical implementation of
these applications will be hindered due to the limited uplink capacity of
devices, causing critical bottlenecks. In this work, we propose a novel doubly
communication-efficient zero-order (ZO) method with a one-point gradient
estimator that replaces communicating long vectors with scalar values and that
harnesses the nature of the wireless communication channel, overcoming the need
to know the channel state coefficient. It is the first method that includes the
wireless channel in the learning algorithm itself instead of wasting resources
to analyze it and remove its impact. We then offer a thorough analysis of the
proposed zero-order federated learning (ZOFL) framework and prove that our
method converges \textit{almost surely}, which is a novel result in nonconvex
ZO optimization. We further prove a convergence rate of
$O(\frac{1}{\sqrt[3]{K}})$ in the nonconvex setting. We finally demonstrate the
potential of our algorithm with experimental results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Copyright in Generative Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.09266v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.09266v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-generated artworks are now part of the contemporary art scene: they
are attracting significant investments and they are presented in exhibitions
together with those created by human artists. These artworks are mainly based
on generative deep learning techniques, which have seen a formidable
development and remarkable refinement in the very recent years. Given the
inherent characteristics of these techniques, a series of novel legal problems
arise. In this article, we consider a set of key questions in the area of
generative deep learning for the arts, including the following: is it possible
to use copyrighted works as training set for generative models? How do we
legally store their copies in order to perform the training process? Who (if
someone) will own the copyright on the generated data? We try to answer these
questions considering the law in force in both the United States of America and
the European Union, and potential future alternatives. We then extend our
analysis to code generation, which is an emerging area of generative deep
learning. Finally, we also formulate a set of practical guidelines for artists
and developers working on deep learning generated art, as well as some policy
suggestions for policymakers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Data & Policy at
  https://www.cambridge.org/core/journals/data-and-policy/article/copyright-in-generative-deep-learning/C401539FDF79A6AC6CEE8C5256508B5E</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Creativity and Machine Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.02726v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.02726v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in the area of machine learning and creativity.
This survey presents an overview of the history and the state of the art of
computational creativity theories, key machine learning techniques (including
generative deep learning), and corresponding automatic evaluation methods.
After presenting a critical discussion of the key contributions in this area,
we outline the current research challenges and emerging opportunities in this
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ACM Computing Surveys at
  https://dl.acm.org/doi/10.1145/3664595</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometry-aware RL for Manipulation of Varying Shapes and Deformable
  Objects <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07005v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07005v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tai Hoang, Huy Le, Philipp Becker, Vien Anh Ngo, Gerhard Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulating objects with varying geometries and deformable objects is a
major challenge in robotics. Tasks such as insertion with different objects or
cloth hanging require precise control and effective modelling of complex
dynamics. In this work, we frame this problem through the lens of a
heterogeneous graph that comprises smaller sub-graphs, such as actuators and
objects, accompanied by different edge types describing their interactions.
This graph representation serves as a unified structure for both rigid and
deformable objects tasks, and can be extended further to tasks comprising
multiple actuators. To evaluate this setup, we present a novel and challenging
reinforcement learning benchmark, including rigid insertion of diverse objects,
as well as rope and cloth manipulation with multiple end-effectors. These tasks
present a large search space, as both the initial and target configurations are
uniformly sampled in 3D space. To address this issue, we propose a novel
graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi),
utilizing $SE(3)$ equivariant message passing networks as the main backbone to
exploit the geometric symmetry. In addition, by modeling explicit
heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous
equivariant policies in terms of average returns, sample efficiency, and
generalization to unseen objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution
  Simulations for Time-Efficient Fine-Resolution Policy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Kadokawa, Hirotaka Tahara, Takamitsu Matsubara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In earthwork and construction, excavators often encounter large rocks mixed
with various soil conditions, requiring skilled operators. This paper presents
a framework for achieving autonomous excavation using reinforcement learning
(RL) through a rock excavation simulator. In the simulation, resolution can be
defined by the particle size/number in the whole soil space. Fine-resolution
simulations closely mimic real-world behavior but demand significant
calculation time and challenging sample collection, while coarse-resolution
simulations enable faster sample collection but deviate from real-world
behavior. To combine the advantages of both resolutions, we explore using
policies developed in coarse-resolution simulations for pre-training in
fine-resolution simulations. To this end, we propose a novel policy learning
framework called Progressive-Resolution Policy Distillation (PRPD), which
progressively transfers policies through some middle-resolution simulations
with conservative policy transfer to avoid domain gaps that could lead to
policy transfer failure. Validation in a rock excavation simulator and nine
real-world rock environments demonstrated that PRPD reduced sampling time to
less than 1/7 while maintaining task success rates comparable to those achieved
through policy learning in a fine-resolution simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymmetrical estimator for training encapsulated deep photonic neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18458v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18458v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhi Wang, Minjia Chen, Chunhui Yao, Jie Ma, Ting Yan, Richard Penty, Qixiang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photonic neural networks (PNNs) are fast in-propagation and high bandwidth
paradigms that aim to popularize reproducible NN acceleration with higher
efficiency and lower cost. However, the training of PNN is known to be
challenging, where the device-to-device and system-to-system variations create
imperfect knowledge of the PNN. Despite backpropagation (BP)-based training
algorithms being the industry standard for their robustness, generality, and
fast gradient convergence for digital training, existing PNN-BP methods rely
heavily on accurate intermediate state extraction or extensive computational
resources for deep PNNs (DPNNs). The truncated photonic signal propagation and
the computation overhead bottleneck DPNN's operation efficiency and increase
system construction cost. Here, we introduce the asymmetrical training (AsyT)
method, tailored for encapsulated DPNNs, where the signal is preserved in the
analogue photonic domain for the entire structure. AsyT offers a lightweight
solution for DPNNs with minimum readouts, fast and energy-efficient operation,
and minimum system footprint. AsyT's ease of operation, error tolerance, and
generality aim to promote PNN acceleration in a widened operational scenario
despite the fabrication variations and imperfect controls. We demonstrated AsyT
for encapsulated DPNN with integrated photonic chips, repeatably enhancing the
performance from in-silico BP for different network structures and datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exogenous Matching: Learning Good Proposals for Tractable Counterfactual
  Estimation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13914v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13914v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikang Chen, Dehui Du, Lili Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an importance sampling method for tractable and efficient
estimation of counterfactual expressions in general settings, named Exogenous
Matching. By minimizing a common upper bound of counterfactual estimators, we
transform the variance minimization problem into a conditional distribution
learning problem, enabling its integration with existing conditional
distribution modeling approaches. We validate the theoretical results through
experiments under various types and settings of Structural Causal Models (SCMs)
and demonstrate the outperformance on counterfactual estimation tasks compared
to other existing importance sampling methods. We also explore the impact of
injecting structural prior knowledge (counterfactual Markov boundaries) on the
results. Finally, we apply this method to identifiable proxy SCMs and
demonstrate the unbiasedness of the estimates, empirically illustrating the
applicability of the method to practical scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 15 figures. Accepted at NeurIPS 2024, see
  https://papers.nips.cc/paper_files/paper/2024/hash/ee94bf235482e4c1f689c04c81656dbf-Abstract-Conference.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geospatial Trajectory Generation via Efficient Abduction: Deployment for
  Independent Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyagna Bavikadi, Dyuman Aditya, Devendra Parkar, Paulo Shakarian, Graham Mueller, Chad Parvis, Gerardo I. Simari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to generate artificial human movement patterns while meeting
location and time constraints is an important problem in the security
community, particularly as it enables the study of the analog problem of
detecting such patterns while maintaining privacy. We frame this problem as an
instance of abduction guided by a novel parsimony function represented as an
aggregate truth value over an annotated logic program. This approach has the
added benefit of affording explainability to an analyst user. By showing that
any subset of such a program can provide a lower bound on this parsimony
requirement, we are able to abduce movement trajectories efficiently through an
informed (i.e., A*) search. We describe how our implementation was enhanced
with the application of multiple techniques in order to be scaled and
integrated with a cloud-based software stack that included bottom-up rule
learning, geolocated knowledge graph retrieval/management, and interfaces with
government systems for independently conducted government-run tests for which
we provide results. We also report on our own experiments showing that we not
only provide exact results but also scale to very large scenarios and provide
realistic agent trajectories that can go undetected by machine learning anomaly
detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings ICLP 2024, arXiv:2502.08453</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple yet Effective DDG Predictor is An Unsupervised Antibody
  Optimizer and Explainer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lirong Wu, Yunfan Liu, Haitao Lin, Yufei Huang, Guojiang Zhao, Zhifeng Gao, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proteins that exist today have been optimized over billions of years of
natural evolution, during which nature creates random mutations and selects
them. The discovery of functionally promising mutations is challenged by the
limited evolutionary accessible regions, i.e., only a small region on the
fitness landscape is beneficial. There have been numerous priors used to
constrain protein evolution to regions of landscapes with high-fitness
variants, among which the change in binding free energy (DDG) of protein
complexes upon mutations is one of the most commonly used priors. However, the
huge mutation space poses two challenges: (1) how to improve the efficiency of
DDG prediction for fast mutation screening; and (2) how to explain mutation
preferences and efficiently explore accessible evolutionary regions. To address
these challenges, we propose a lightweight DDG predictor (Light-DDG), which
adopts a structure-aware Transformer as the backbone and enhances it by
knowledge distilled from existing powerful but computationally heavy DDG
predictors. Additionally, we augmented, annotated, and released a large-scale
dataset containing millions of mutation data for pre-training Light-DDG. We
find that such a simple yet effective Light-DDG can serve as a good
unsupervised antibody optimizer and explainer. For the target antibody, we
propose a novel Mutation Explainer to learn mutation preferences, which
accounts for the marginal benefit of each mutation per residue. To further
explore accessible evolutionary regions, we conduct preference-guided antibody
optimization and evaluate antibody candidates quickly using Light-DDG to
identify desirable mutations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Modeling of Combined Sewer Systems for Urban Sustainability:
  An Empirical Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11619v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11619v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vipin Singh, Tianheng Ling, Teodor Chiaburu, Felix Biessmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate change poses complex challenges, with extreme weather events becoming
increasingly frequent and difficult to model. Examples include the dynamics of
Combined Sewer Systems (CSS). Overburdened CSS during heavy rainfall will
overflow untreated wastewater into surface water bodies. Classical approaches
to modeling the impact of extreme rainfall events rely on physical simulations,
which are particularly challenging to create for large urban infrastructures.
Deep Learning (DL) models offer a cost-effective alternative for modeling the
complex dynamics of sewer systems. In this study, we present a comprehensive
empirical evaluation of several state-of-the-art DL time series models for
predicting sewer system dynamics in a large urban infrastructure, utilizing
three years of measurement data. We especially investigate the potential of DL
models to maintain predictive precision during network outages by comparing
global models, which have access to all variables within the sewer system, and
local models, which are limited to data from a restricted set of local sensors.
Our findings demonstrate that DL models can accurately predict the dynamics of
sewer system load, even under network outage conditions. These results suggest
that DL models can effectively aid in balancing the load redistribution in CSS,
thereby enhancing the sustainability and resilience of urban infrastructures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, accepted at 2nd Workshop on 'Public Interest AI'
  co-located with 47th German Conference on Artificial Intelligence, Wuerzburg
  23rd September 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Safety Misbehaviours in Autonomous Driving Systems using
  Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruben Grewal, Paolo Tonella, Andrea Stocco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated real-time recognition of unexpected situations plays a crucial
role in the safety of autonomous vehicles, especially in unsupported and
unpredictable scenarios. This paper evaluates different Bayesian uncertainty
quantification methods from the deep learning domain for the anticipatory
testing of safety-critical misbehaviours during system-level simulation-based
testing. Specifically, we compute uncertainty scores as the vehicle executes,
following the intuition that high uncertainty scores are indicative of
unsupported runtime conditions that can be used to distinguish safe from
failure-inducing driving behaviors. In our study, we conducted an evaluation of
the effectiveness and computational overhead associated with two Bayesian
uncertainty quantification methods, namely MC- Dropout and Deep Ensembles, for
misbehaviour avoidance. Overall, for three benchmarks from the Udacity
simulator comprising both out-of-distribution and unsafe conditions introduced
via mutation testing, both methods successfully detected a high number of
out-of-bounds episodes providing early warnings several seconds in advance,
outperforming two state-of-the-art misbehaviour prediction methods based on
autoencoders and attention maps in terms of effectiveness and efficiency.
Notably, Deep Ensembles detected most misbehaviours without any false alarms
and did so even when employing a relatively small number of models, making them
computationally feasible for real-time detection. Our findings suggest that
incorporating uncertainty quantification methods is a viable approach for
building fail-safe mechanisms in deep neural network-based autonomous vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of the 17th IEEE International Conference on Software
  Testing, Verification and Validation 2024 (ICST '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Regularization of Learnable Embeddings for Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Butera, Giovanni De Felice, Andrea Cini, Cesare Alippi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In forecasting multiple time series, accounting for the individual features
of each sequence can be challenging. To address this, modern deep learning
methods for time series analysis combine a shared (global) model with local
layers, specific to each time series, often implemented as learnable
embeddings. Ideally, these local embeddings should encode meaningful
representations of the unique dynamics of each sequence. However, when these
are learned end-to-end as parameters of a forecasting model, they may end up
acting as mere sequence identifiers. Shared processing blocks may then become
reliant on such identifiers, limiting their transferability to new contexts. In
this paper, we address this issue by investigating methods to regularize the
learning of local learnable embeddings for time series processing.
Specifically, we perform the first extensive empirical study on the subject and
show how such regularizations consistently improve performance in widely
adopted architectures. Furthermore, we show that methods attempting to prevent
the co-adaptation of local and global parameters by means of embeddings
perturbation are particularly effective in this context. In this regard, we
include in the comparison several perturbation-based regularization methods,
going as far as periodically resetting the embeddings during training. The
obtained results provide an important contribution to understanding the
interplay between learnable local parameters and shared processing layers: a
key challenge in modern time series processing models and a step toward
developing effective foundation models for time series.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning convolution operators on compact Abelian groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilia Magnani, Ernesto De Vito, Philipp Hennig, Lorenzo Rosasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning convolution operators associated to
compact Abelian groups. We study a regularization-based approach and provide
corresponding learning guarantees, discussing natural regularity condition on
the convolution kernel. More precisely, we assume the convolution kernel is a
function in a translation invariant Hilbert space and analyze a natural ridge
regression (RR) estimator. Building on existing results for RR, we characterize
the accuracy of the estimator in terms of finite sample bounds. Interestingly,
regularity assumptions which are classical in the analysis of RR, have a novel
and natural interpretation in terms of space/frequency localization.
Theoretical results are illustrated by numerical simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-free reinforcement learning with noisy actions for automated
  experimental <span class="highlight-title">control</span> in optics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lea Richtmann, Viktoria-S. Schmiesing, Dennis Wilken, Jan Heine, Aaron Tranter, Avishek Anand, Tobias J. Osborne, Michèle Heurs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Setting up and controlling optical systems is often a challenging and tedious
task. The high number of degrees of freedom to control mirrors, lenses, or
phases of light makes automatic control challenging, especially when the
complexity of the system cannot be adequately modeled due to noise or
non-linearities. Here, we show that reinforcement learning (RL) can overcome
these challenges when coupling laser light into an optical fiber, using a
model-free RL approach that trains directly on the experiment without
pre-training. By utilizing the sample-efficient algorithms Soft Actor-Critic
(SAC) or Truncated Quantile Critics (TQC), our agent learns to couple with 90%
efficiency, comparable to the human expert. We demonstrate that direct training
on an experiment can replace extensive system modeling. Our result exemplifies
RL's potential to tackle problems in optics, paving the way for more complex
applications where full noise modeling is not feasible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages + 12 pages appendices, 2 + 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An <span class="highlight-title">Overview</span> of Prototype Formulations for Interpretable Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08925v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08925v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Xiling Li, Korbinian Franz Rudolf, Nils Blank, Rudolf Lioutikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prototypical part networks offer interpretable alternatives to black-box deep
learning models. However, many of these networks rely on Euclidean prototypes,
which may limit their flexibility. This work provides a comprehensive overview
of various prototype formulations. Experiments conducted on the CUB-200-2011,
Stanford Cars, and Oxford Flowers datasets demonstrate the effectiveness and
versatility of these different formulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal Contribution of M.X.Li and K.F.Rudolf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Multi-kernel Graph Learning for Autism Prediction and
  Biomarker Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.03388v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.03388v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Liu, Junbin Mao, Hanhe Lin, Hulin Kuang, Shirui Pan, Xusheng Wu, Shan Xie, Fei Liu, Yi Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to its complexity, graph learning-based multi-modal integration and
classification is one of the most challenging obstacles for disease prediction.
To effectively offset the negative impact between modalities in the process of
multi-modal integration and extract heterogeneous information from graphs, we
propose a novel method called MMKGL (Multi-modal Multi-Kernel Graph Learning).
For the problem of negative impact between modalities, we propose a multi-modal
graph embedding module to construct a multi-modal graph. Different from
conventional methods that manually construct static graphs for all modalities,
each modality generates a separate graph by adaptive learning, where a function
graph and a supervision graph are introduced for optimization during the
multi-graph fusion embedding process. We then propose a multi-kernel graph
learning module to extract heterogeneous information from the multi-modal
graph. The information in the multi-modal graph at different levels is
aggregated by convolutional kernels with different receptive field sizes,
followed by generating a cross-kernel discovery tensor for disease prediction.
Our method is evaluated on the benchmark Autism Brain Imaging Data Exchange
(ABIDE) dataset and outperforms the state-of-the-art methods. In addition,
discriminative brain regions associated with autism are identified by our
model, providing guidance for the study of autism pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DrivAerNet++: A Large-Scale Multimodal Car <span class="highlight-title">Dataset</span> with Computational
  Fluid Dynamics Simulations and Deep Learning Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09624v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09624v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elrefaie, Florin Morar, Angela Dai, Faez Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DrivAerNet++, the largest and most comprehensive multimodal
dataset for aerodynamic car design. DrivAerNet++ comprises 8,000 diverse car
designs modeled with high-fidelity computational fluid dynamics (CFD)
simulations. The dataset includes diverse car configurations such as fastback,
notchback, and estateback, with different underbody and wheel designs to
represent both internal combustion engines and electric vehicles. Each entry in
the dataset features detailed 3D meshes, parametric models, aerodynamic
coefficients, and extensive flow and surface field data, along with segmented
parts for car classification and point cloud data. This dataset supports a wide
array of machine learning applications including data-driven design
optimization, generative modeling, surrogate model training, CFD simulation
acceleration, and geometric classification. With more than 39 TB of publicly
available engineering data, DrivAerNet++ fills a significant gap in available
resources, providing high-quality, diverse data to enhance model training,
promote generalization, and accelerate automotive design processes. Along with
rigorous dataset validation, we also provide ML benchmarking results on the
task of aerodynamic drag prediction, showcasing the breadth of applications
supported by our dataset. This dataset is set to significantly impact
automotive design and broader engineering disciplines by fostering innovation
and improving the fidelity of aerodynamic evaluations. Dataset and code
available at: https://github.com/Mohamedelrefaie/DrivAerNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining Explainability: Recommendations for Effective Use of Concept
  Activation Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angus Nicolson, Lisa Schut, J. Alison Noble, Yarin Gal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-based explanations translate the internal representations of deep
learning models into a language that humans are familiar with: concepts. One
popular method for finding concepts is Concept Activation Vectors (CAVs), which
are learnt using a probe dataset of concept exemplars. In this work, we
investigate three properties of CAVs: (1) inconsistency across layers, (2)
entanglement with other concepts, and (3) spatial dependency. Each property
provides both challenges and opportunities in interpreting models. We introduce
tools designed to detect the presence of these properties, provide insight into
how each property can lead to misleading explanations, and provide
recommendations to mitigate their impact. To demonstrate practical
applications, we apply our recommendations to a melanoma classification task,
showing how entanglement can lead to uninterpretable results and that the
choice of negative probe set can have a substantial impact on the meaning of a
CAV. Further, we show that understanding these properties can be used to our
advantage. For example, we introduce spatially dependent CAVs to test if a
model is translation invariant with respect to a specific concept and class.
Our experiments are performed on natural images (ImageNet), skin lesions (ISIC
2019), and a new synthetic dataset, Elements. Elements is designed to capture a
known ground truth relationship between concepts and classes. We release this
dataset to facilitate further research in understanding and evaluating
interpretability methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Transactions on Machine Learning Research (02/2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rhythmic sharing: A bio-inspired paradigm for zero-shot adaptation and
  learning in neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoony Kang, Wolfgang Losert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The brain can rapidly adapt to new contexts and learn from limited data, a
coveted characteristic that artificial intelligence algorithms have struggled
to mimic. Inspired by oscillatory rhythms of the mechanical structures of
neural cells, we developed a learning paradigm that is based on oscillations in
link strengths and associates learning with the coordination of these
oscillations. We find that this paradigm yields rapid adaptation and learning
in artificial neural networks. Link oscillations can rapidly change
coordination, endowing the network with the ability to sense subtle context
changes in an unsupervised manner. In other words, the network generates the
missing contextual tokens required to perform as a generalist AI architecture
capable of predicting dynamics in multiple contexts. Oscillations also allow
the network to extrapolate dynamics to never-seen-before contexts. These
capabilities make our learning paradigm a powerful starting point for novel
models of learning and cognition. Furthermore, learning through link
coordination is agnostic to the specifics of the neural network architecture,
hence our study opens the door for introducing rapid adaptation and learning
capabilities into leading AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures v.2 comments: Updated email, updated typo on
  p.11: h -> h^2 for RMSE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Value of Prediction in Identifying the Worst-Off 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Unai Fischer-Abaigar, Christoph Kern, Juan Carlos Perdomo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is increasingly used in government programs to identify and
support the most vulnerable individuals, prioritizing assistance for those at
greatest risk over optimizing aggregate outcomes. This paper examines the
welfare impacts of prediction in equity-driven contexts, and how they compare
to other policy levers, such as expanding bureaucratic capacity. Through
mathematical models and a real-world case study on long-term unemployment
amongst German residents, we develop a comprehensive understanding of the
relative effectiveness of prediction in surfacing the worst-off. Our findings
provide clear analytical frameworks and practical, data-driven tools that
empower policymakers to make principled decisions when designing these systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Operator Takeover for Visuomotor Diffusion Policy Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Ingelhag, Jesper Munkeby, Michael C. Welle, Marco Moletta, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Real-Time Operator Takeover (RTOT) paradigm enabling operators
to seamlessly take control of a live visuomotor diffusion policy, guiding the
system back into desirable states or reinforcing specific demonstrations. We
present new insights in using the Mahalonobis distance to automatically
identify undesirable states. Once the operator has intervened and redirected
the system, the control is seamlessly returned to the policy, which resumes
generating actions until further intervention is required. We demonstrate that
incorporating the targeted takeover demonstrations significantly improves
policy performance compared to training solely with an equivalent number of,
but longer, initial demonstrations. We provide an in-depth analysis of using
the Mahalanobis distance to detect out-of-distribution states, illustrating its
utility for identifying critical failure points during execution. Supporting
materials, including videos of initial and takeover demonstrations and all rice
scooping experiments, are available on the project website:
https://operator-takeover.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequential Binary Classification for Intrusion Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrihari Vasudevan, Ishan Chokshi, Raaghul Ranganathan, Nachiappan Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network Intrusion Detection Systems (IDS) have become increasingly important
as networks become more vulnerable to new and sophisticated attacks. Machine
Learning (ML)-based IDS are increasingly seen as the most effective approach to
handle this issue. However, IDS datasets suffer from high class imbalance,
which impacts the performance of standard ML models. Different from existing
data-driven techniques to handling class imbalance, this paper explores a
structural approach to handling class imbalance in multi-class classification
(MCC) problems. The proposed approach - Sequential Binary Classification (SBC),
is a hierarchical cascade of (regular) binary classifiers. Experiments on
benchmark IDS datasets demonstrate that the structural approach to handling
class-imbalance, as exemplified by SBC, is a viable approach to handling the
issue.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-learning of shared linear representations beyond well-specified
  linear regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathieu Even, Laurent Massoulié
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by multi-task and meta-learning approaches, we consider the problem
of learning structure shared by tasks or users, such as shared low-rank
representations or clustered structures. While all previous works focus on
well-specified linear regression, we consider more general convex objectives,
where the structural low-rank and cluster assumptions are expressed on the
optima of each function. We show that under mild assumptions such as
\textit{Hessian concentration} and \textit{noise concentration at the optimum},
rank and clustered regularized estimators recover such structure, provided the
number of samples per task and the number of tasks are large enough. We then
study the problem of recovering the subspace in which all the solutions lie, in
the setting where there is only a single sample per task: we show that in that
case, the rank-constrained estimator can recover the subspace, but that the
number of tasks needs to scale exponentially large with the dimension of the
subspace. Finally, we provide a polynomial-time algorithm via nuclear norm
constraints for learning a shared linear representation in the context of
convex learning objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous Autoregressive Modeling with Stochastic Monotonic Alignment
  for Speech Synthesis <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Lin, Chenghan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel autoregressive modeling approach for speech synthesis,
combining a variational autoencoder (VAE) with a multi-modal latent space and
an autoregressive model that uses Gaussian Mixture Models (GMM) as the
conditional probability distribution. Unlike previous methods that rely on
residual vector quantization, our model leverages continuous speech
representations from the VAE's latent space, greatly simplifying the training
and inference pipelines. We also introduce a stochastic monotonic alignment
mechanism to enforce strict monotonic alignments. Our approach significantly
outperforms the state-of-the-art autoregressive model VALL-E in both subjective
and objective evaluations, achieving these results with only 10.3\% of VALL-E's
parameters. This demonstrates the potential of continuous speech language
models as a more efficient alternative to existing quantization-based speech
language models. Sample audio can be found at https://tinyurl.com/gmm-lm-tts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lambda-Skip Connections: the architectural component that prevents Rank
  Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10609v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10609v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Arangath Joseph, Jerome Sieber, Melanie N. Zeilinger, Carmen Amo Alonso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rank collapse, a phenomenon where embedding vectors in sequence models
rapidly converge to a uniform token or equilibrium state, has recently gained
attention in the deep learning literature. This phenomenon leads to reduced
expressivity and potential training instabilities due to vanishing gradients.
Empirical evidence suggests that architectural components like skip
connections, LayerNorm, and MultiLayer Perceptrons (MLPs) play critical roles
in mitigating rank collapse. While this issue is well-documented for
transformers, alternative sequence models, such as State Space Models (SSMs),
which have recently gained prominence, have not been thoroughly examined for
similar vulnerabilities. This paper extends the theory of rank collapse from
transformers to SSMs using a unifying framework that captures both
architectures. We study how a parametrized version of the classic skip
connection component, which we call \emph{lambda-skip connections}, provides
guarantees for rank collapse prevention. Through analytical results, we present
a sufficient condition to guarantee prevention of rank collapse across all the
aforementioned architectures. We also study the necessity of this condition via
ablation studies and analytical examples. To our knowledge, this is the first
study that provides a general guarantee to prevent rank collapse, and that
investigates rank collapse in the context of SSMs, offering valuable
understanding for both theoreticians and practitioners. Finally, we validate
our findings with experiments demonstrating the crucial role of architectural
components such as skip connections and gating mechanisms in preventing rank
collapse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PH-VAE: A Polynomial Hierarchical Variational Autoencoder Towards
  Disentangled Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Shaofan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The variational autoencoder (VAE) is a simple and efficient generative
artificial intelligence method for modeling complex probability distributions
of various types of data, such as images and texts. However, it suffers some
main shortcomings, such as lack of interpretability in the latent variables,
difficulties in tuning hyperparameters while training, producing blurry,
unrealistic downstream outputs or loss of information due to how it calculates
loss functions and recovers data distributions, overfitting, and origin gravity
effect for small data sets, among other issues. These and other limitations
have caused unsatisfactory generation effects for the data with complex
distributions. In this work, we proposed and developed a polynomial
hierarchical variational autoencoder (PH-VAE), in which we used a polynomial
hierarchical date format to generate or to reconstruct the data distributions.
In doing so, we also proposed a novel Polynomial Divergence in the loss
function to replace or generalize the Kullback-Leibler (KL) divergence, which
results in systematic and drastic improvements in both accuracy and
reproducibility of the re-constructed distribution function as well as the
quality of re-constructed data images while keeping the dataset size the same
but capturing fine resolution of the data. Moreover, we showed that the
proposed PH-VAE has some form of disentangled representation learning ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Evolved Universal Transformer Memory <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13166v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13166v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Cetin, Qi Sun, Tianyu Zhao, Yujin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior methods propose to offset the escalating costs of modern foundation
models by dropping specific parts of their contexts with hand-designed rules,
while attempting to preserve their original performance. We overcome this
trade-off with Neural Attention Memory Models (NAMMs), introducing a learned
network for memory management that improves both the performance and efficiency
of transformers. We evolve NAMMs atop pre-trained transformers to provide
different latent contexts focusing on the most relevant information for
individual layers and attention heads. NAMMs are universally applicable to any
model using self-attention as they condition exclusively on the values in the
produced attention matrices. Learning NAMMs on a small set of problems, we
achieve substantial performance improvements across multiple long-context
benchmarks while cutting the model's input contexts up to a fraction of the
original sizes. We show the generality of our conditioning enables zero-shot
transfer of NAMMs trained only on language to entirely new transformer
architectures even across input modalities, with their benefits carrying over
to vision and reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025. Source code available at
  https://github.com/SakanaAI/evo-memory</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Small Singular Values Matter: A Random Matrix Analysis of Transformer
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Staats, Matthias Thamm, Bernd Rosenow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become increasingly central to AI
applications, understanding their inner workings is essential. In this work, we
analyze the spectra of weight matrices in pretrained transformer models through
the lens of random matrix theory (RMT) to uncover learned structures. We find
that certain regions of the weight matrix spectra deviate markedly from RMT
predictions, indicating richer feature encoding. By comparing the corresponding
singular vectors to eigenvectors of activation covariance matrices, we observe
substantial overlap precisely where the spectra deviate from RMT expectations.
Our analysis further reveals the important role of small singular values in
LLMs, showing that these values contain significant information, a claim
supported by increased perplexity when they are removed from the model.
Although these small values may appear unimportant prior to task-specific
fine-tuning, removing them afterward significantly degrades performance,
revealing that fine-tuning refines the model primarily in these spectral
regions. These results emphasize the critical role of small singular values,
suggesting that removing them in an already aligned transformer can be
detrimental, as it may compromise model alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiFloodSynth: Multi-Annotated Flood Synthetic <span class="highlight-title">Dataset</span> Generation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03966v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03966v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YoonJe Kang, Yonghoon Jung, Wonseop Shin, Bumsoo Kim, Sanghyun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present synthetic data generation framework for flood
hazard detection system. For high fidelity and quality, we characterize several
real-world properties into virtual world and simulate the flood situation by
controlling them. For the sake of efficiency, recent generative models in
image-to-3D and urban city synthesis are leveraged to easily composite flood
environments so that we avoid data bias due to the hand-crafted manner. Based
on our framework, we build the flood synthetic dataset with 5 levels, dubbed
MultiFloodSynth which contains rich annotation types like normal map,
segmentation, 3D bounding box for a variety of downstream task. In experiments,
our dataset demonstrate the enhanced performance of flood hazard detection with
on-par realism compared with real dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures. Accepted as Oral Presentation to AAAI 2025
  Workshop on Good-Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Representation Alignment: Successor Features Enable Emergent
  Compositionality in Robot Instruction Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05454v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05454v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Myers, Bill Chunyuan Zheng, Anca Dragan, Kuan Fang, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective task representations should facilitate compositionality, such that
after learning a variety of basic tasks, an agent can perform compound tasks
consisting of multiple steps simply by composing the representations of the
constituent steps together. While this is conceptually simple and appealing, it
is not clear how to automatically learn representations that enable this sort
of compositionality. We show that learning to associate the representations of
current and future states with a temporal alignment loss can improve
compositional generalization, even in the absence of any explicit subtask
planning or reinforcement learning. We evaluate our approach across diverse
robotic manipulation tasks as well as in simulation, showing substantial
improvements for tasks specified with either language or goal images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Large Language Models Know and What People Think They Know 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny, Xinyue Hu, Lukas Mayer, Padhraic Smyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As artificial intelligence (AI) systems, particularly large language models
(LLMs), become increasingly integrated into decision-making processes, the
ability to trust their outputs is crucial. To earn human trust, LLMs must be
well calibrated such that they can accurately assess and communicate the
likelihood of their predictions being correct. Whereas recent work has focused
on LLMs' internal confidence, less is understood about how effectively they
convey uncertainty to users. Here we explore the calibration gap, which refers
to the difference between human confidence in LLM-generated answers and the
models' actual confidence, and the discrimination gap, which reflects how well
humans and models can distinguish between correct and incorrect answers. Our
experiments with multiple-choice and short-answer questions reveal that users
tend to overestimate the accuracy of LLM responses when provided with default
explanations. Moreover, longer explanations increased user confidence, even
when the extra length did not improve answer accuracy. By adjusting LLM
explanations to better reflect the models' internal confidence, both the
calibration gap and the discrimination gap narrowed, significantly improving
user perception of LLM accuracy. These findings underscore the importance of
accurate uncertainty communication and highlight the effect of explanation
length in influencing user trust in AI-assisted decision-making environments.
Code and Data can be found at https://osf.io/y7pr6/ . Journal publication can
be found on Nature Machine Intelligence at
https://www.nature.com/articles/s42256-024-00976-7 .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 10 figures For the journal publication on Nature Machine
  Intelligence see https://www.nature.com/articles/s42256-024-00976-7 For the
  data and code see https://osf.io/y7pr6/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hallucination is Inevitable: An Innate Limitation of Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Xu, Sanjay Jain, Mohan Kankanhalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination has been widely recognized to be a significant drawback for
large language models (LLMs). There have been many works that attempt to reduce
the extent of hallucination. These efforts have mostly been empirical so far,
which cannot answer the fundamental question whether it can be completely
eliminated. In this paper, we formalize the problem and show that it is
impossible to eliminate hallucination in LLMs. Specifically, we define a formal
world where hallucination is defined as inconsistencies between a computable
LLM and a computable ground truth function. By employing results from learning
theory, we show that LLMs cannot learn all the computable functions and will
therefore inevitably hallucinate if used as general problem solvers. Since the
formal world is a part of the real world which is much more complicated,
hallucinations are also inevitable for real world LLMs. Furthermore, for real
world LLMs constrained by provable time complexity, we describe the
hallucination-prone tasks and empirically validate our claims. Finally, using
the formal world framework, we discuss the possible mechanisms and efficacies
of existing hallucination mitigators as well as the practical implications on
the safe deployment of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive NAD: Online and Self-adaptive Unsupervised Network Anomaly
  Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yachao Yuan, Yu Huang, Yali Yuan, Jin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread usage of the Internet of Things (IoT) has raised the risks of
cyber threats, thus developing Anomaly Detection Systems (ADSs) that can adapt
to evolving or new attacks is critical. Previous studies primarily focused on
offline unsupervised learning methods to safeguard ADSs, which is not
applicable in practical real-world applications. Besides, most of them strongly
rely on assumptions of known legitimates and fail to satisfy the interpretable
requirements in security applications, creating barriers to the adoption in
practice. In this paper, we design Adaptive NAD, a general framework to improve
and interpret online unsupervised anomaly detection in security domains. An
interpretable two-layer anomaly detection strategy is proposed to generate
reliable high-confidence pseudo-labels. Then, an online learning scheme is
introduced to update Adaptive NAD by a novel threshold calculation technique to
adapt to new threats. Experimental results demonstrate that Adaptive NAD
achieves more than 5.4%, 23.0%, and 3.2% improvements in SPAUC compared with
state-of-the-art solutions on the CIC-Darknet2020, CIC-DoHBrw-2020, and
Edge-IIoTset datasets, respectively. The code is released at
https://github.com/MyLearnCodeSpace/Adaptive-NAD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Internal Probability and Self-Consistency for Effective and
  Efficient LLM Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Zhou, Tan Yuhao, Zenan Li, Yuan Yao, Lan-Zhe Guo, Xiaoxing Ma, Yu-Feng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have demonstrated
remarkable reasoning capabilities. However, single-shot inference often yields
unreliable results for complex reasoning tasks, leading researchers to explore
multiple reasoning paths through methods such as perplexity and
self-consistency. In this paper, we present the first theoretical error
decomposition analysis of these techniques, breaking down their error into
estimation error and model error. Our analysis reveals a fundamental trade-off:
perplexity methods suffer from substantial model error due to the absence of a
proper consistency function, while self-consistency exhibits high estimation
error due to a slow error convergence rate. To overcome these limitations, we
propose Reasoning-Pruning Perplexity Consistency (RPC). This approach combines
Perplexity Consistency, which seamlessly integrates LLM perplexity with
self-consistency, and Reasoning Pruning, which eliminates low-probability
reasoning paths to effectively prevent the degeneration of estimation error
reduction. Theoretical analysis demonstrates that RPC not only accelerates the
convergence rate of estimation error to an exponential level but also holds
strong potential for further reducing model error. Extensive empirical
evaluations on seven benchmark datasets confirm that RPC can significantly
improve reasoning performance, sample efficiency, and confidence reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Loss Landscape Degeneracy Drives Stagewise Development in Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll, Susan Wei, Daniel Murfet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning involves navigating a high-dimensional loss landscape over the
neural network parameter space. Over the course of training, complex
computational structures form and re-form inside the neural network, leading to
shifts in input/output behavior. It is a priority for the science of deep
learning to uncover principles governing the development of neural network
structure and behavior. Drawing on the framework of singular learning theory,
we propose that model development is deeply linked to degeneracy in the local
geometry of the loss landscape. We investigate this link by monitoring loss
landscape degeneracy throughout training, as quantified by the local learning
coefficient, for a transformer language model and an in-context linear
regression transformer. We show that training can be divided into distinct
periods of change in loss landscape degeneracy, and that these changes in
degeneracy coincide with significant changes in the internal computational
structure and the input/output behavior of the transformers. This finding
underscores the potential of a degeneracy-based perspective for understanding
modern deep learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Material on essential dynamics from v1 of this preprint has been
  removed from v2 and developed in arXiv:2501.17745</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Inverse Linear Optimization: Improved Regret Bound, Robustness to
  Suboptimality, and Toward Tight Regret Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14349v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14349v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinsaku Sakaue, Taira Tsuchiya, Han Bao, Taihei Oki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study an online learning problem where, over $T$ rounds, a learner
observes both time-varying sets of feasible actions and an agent's optimal
actions, selected by solving linear optimization over the feasible actions. The
learner sequentially makes predictions of the agent's underlying linear
objective function, and their quality is measured by the regret, the cumulative
gap between optimal objective values and those achieved by following the
learner's predictions. A seminal work by B\"armann et al. (ICML 2017) showed
that online learning methods can be applied to this problem to achieve regret
bounds of $O(\sqrt{T})$. Recently, Besbes et al. (COLT 2021, Oper. Res. 2023)
significantly improved the result by achieving an $O(n^4\ln T)$ regret bound,
where $n$ is the dimension of the ambient space of objective vectors. Their
method, based on the ellipsoid method, runs in polynomial time but is
inefficient for large $n$ and $T$. In this paper, we obtain an $O(n\ln T)$
regret bound, improving upon the previous bound of $O(n^4\ln T)$ by a factor of
$n^3$. Our method is simple and efficient: we apply the online Newton step
(ONS) to appropriate exp-concave loss functions. Moreover, for the case where
the agent's actions are possibly suboptimal, we establish an $O(n\ln
T+\sqrt{\Delta_Tn\ln T})$ regret bound, where $\Delta_T$ is the cumulative
suboptimality of the agent's actions. This bound is achieved by using MetaGrad,
which runs ONS with $\Theta(\ln T)$ different learning rates in parallel. We
also provide a simple instance that implies an $\Omega(n)$ lower bound, showing
that our $O(n\ln T)$ bound is tight up to an $O(\ln T)$ factor. This gives rise
to a natural question: can the $O(\ln T)$ factor in the upper bound be removed?
For the special case of $n=2$, we show that an $O(1)$ regret bound is possible,
while we delineate challenges in extending this result to higher dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Physical Dynamics under Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Zhou, Xiaoxue Wang, Tianshu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating physically feasible dynamics in a data-driven context is
challenging, especially when adhering to physical priors expressed in specific
equations or formulas. Existing methodologies often overlook the integration of
physical priors, resulting in violation of basic physical laws and suboptimal
performance. In this paper, we introduce a novel framework that seamlessly
incorporates physical priors into diffusion-based generative models to address
this limitation. Our approach leverages two categories of priors: 1)
distributional priors, such as roto-translational invariance, and 2) physical
feasibility priors, including energy and momentum conservation laws and PDE
constraints. By embedding these priors into the generative process, our method
can efficiently generate physically realistic dynamics, encompassing
trajectories and flows. Empirical evaluations demonstrate that our method
produces high-quality dynamics across a diverse array of physical phenomena
with remarkable robustness, underscoring its potential to advance data-driven
studies in AI4Physics. Our contributions signify a substantial advancement in
the field of generative modeling, offering a robust solution to generate
accurate and physically consistent dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A fast neural hybrid Newton solver adapted to implicit methods for
  nonlinear dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03945v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03945v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Jin, Georg Maierhofer, Katharina Schratz, Yang Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of implicit time-stepping schemes for the numerical approximation of
solutions to stiff nonlinear time-evolution equations brings well-known
advantages including, typically, better stability behaviour and corresponding
support of larger time steps, and better structure preservation properties.
However, this comes at the price of having to solve a nonlinear equation at
every time step of the numerical scheme. In this work, we propose a novel deep
learning based hybrid Newton's method to accelerate this solution of the
nonlinear time step system for stiff time-evolution nonlinear equations. We
propose a targeted learning strategy which facilitates robust unsupervised
learning in an offline phase and provides a highly efficient initialisation for
the Newton iteration leading to consistent acceleration of Newton's method. A
quantifiable rate of improvement in Newton's method achieved by improved
initialisation is provided and we analyse the upper bound of the generalisation
error of our unsupervised learning strategy. These theoretical results are
supported by extensive numerical results, demonstrating the efficiency of our
proposed neural hybrid solver both in one- and two-dimensional cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Confidence: Adaptive Abstention in Dual-Threshold Conformal
  Prediction for Autonomous System Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divake Kumar, Nastaran Darabi, Sina Tayebati, Amit Ranjan Trivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety-critical perception systems require both reliable uncertainty
quantification and principled abstention mechanisms to maintain safety under
diverse operational conditions. We present a novel dual-threshold
conformalization framework that provides statistically-guaranteed uncertainty
estimates while enabling selective prediction in high-risk scenarios. Our
approach uniquely combines a conformal threshold ensuring valid prediction sets
with an abstention threshold optimized through ROC analysis, providing
distribution-free coverage guarantees (>= 1 - alpha) while identifying
unreliable predictions. Through comprehensive evaluation on CIFAR-100,
ImageNet1K, and ModelNet40 datasets, we demonstrate superior robustness across
camera and LiDAR modalities under varying environmental perturbations. The
framework achieves exceptional detection performance (AUC: 0.993 to 0.995)
under severe conditions while maintaining high coverage (>90.0%) and enabling
adaptive abstention (13.5% to 63.4% +/- 0.5) as environmental severity
increases. For LiDAR-based perception, our approach demonstrates particularly
strong performance, maintaining robust coverage (>84.5%) while appropriately
abstaining from unreliable predictions. Notably, the framework shows remarkable
stability under heavy perturbations, with detection performance (AUC: 0.995 +/-
0.001) significantly outperforming existing methods across all modalities. Our
unified approach bridges the gap between theoretical guarantees and practical
deployment needs, offering a robust solution for safety-critical autonomous
systems operating in challenging real-world conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and
  Harmlessness of Large Language Model via Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06876v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06876v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinluan Yang, Dingnan Jin, Anke Tang, Li Shen, Didi Zhu, Zhengyu Chen, Daixin Wang, Qing Cui, Zhiqiang Zhang, Jun Zhou, Fei Wu, Kun Kuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving balanced alignment of large language models (LLMs) in terms of
Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a
cornerstone of responsible AI, with existing methods like data mixture
strategies facing limitations including reliance on expert knowledge and
conflicting optimization signals. While model merging offers a promising
alternative by integrating specialized models, its potential for 3H
optimization remains underexplored. This paper establishes the first
comprehensive benchmark for model merging in 3H-aligned LLMs, systematically
evaluating 15 methods (12 training-free merging and 3 data mixture techniques)
across 10 datasets associated with 5 annotation dimensions, 2 LLM families, and
2 training paradigms. Our analysis reveals three pivotal insights: (i)
previously overlooked collaborative/conflicting relationships among 3H
dimensions, (ii) the consistent superiority of model merging over data mixture
approaches in balancing alignment trade-offs, and (iii) the critical role of
parameter-level conflict resolution through redundant component pruning and
outlier mitigation. Building on these findings, we propose R-TSVM, a
Reweighting-enhanced Task Singular Vector Merging method that incorporates
outlier-aware parameter weighting and sparsity-adaptive rank selection
strategies adapted to the heavy-tailed parameter distribution and sparsity for
LLMs, further improving LLM alignment across multiple evaluations. We release
our trained models for further exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Flow at the Network Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12469v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12469v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Shao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) and their multimodal
variants have led to remarkable progress across various domains, demonstrating
impressive capabilities and unprecedented potential. In the era of ubiquitous
connectivity, leveraging communication networks to distribute intelligence is a
transformative concept, envisioning AI-powered services accessible at the
network edge. However, pushing large models from the cloud to
resource-constrained environments faces critical challenges. Model inference on
low-end devices leads to excessive latency and performance bottlenecks, while
raw data transmission over limited bandwidth networks causes high communication
overhead. This article presents AI Flow, a framework that streamlines the
inference process by jointly leveraging the heterogeneous resources available
across devices, edge nodes, and cloud servers, making intelligence flow across
networks. To facilitate cooperation among multiple computational nodes, the
proposed framework explores a paradigm shift in the design of communication
network systems from transmitting information flow to intelligence flow, where
the goal of communications is task-oriented and folded into the inference
process. Experimental results demonstrate the effectiveness of the proposed
framework through an image captioning use case, showcasing the ability to
reduce response latency while maintaining high-quality captions. This article
serves as a position paper for identifying the motivation, challenges, and
principles of AI Flow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to IEEE Network Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Improving Transformers Overcome Easy-to-Hard and Length
  Generalization Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nayoung Lee, Ziyang Cai, Avi Schwarzschild, Kangwook Lee, Dimitris Papailiopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models often struggle with length generalization and solving
complex problem instances beyond their training distribution. We present a
self-improvement approach where models iteratively generate and learn from
their own solutions, progressively tackling harder problems while maintaining a
standard transformer architecture. Across diverse tasks including arithmetic,
string manipulation, and maze solving, self-improving enables models to solve
problems far beyond their initial training distribution-for instance,
generalizing from 10-digit to 100-digit addition without apparent saturation.
We observe that in some cases filtering for correct self-generated examples
leads to exponential improvements in out-of-distribution performance across
training rounds. Additionally, starting from pretrained models significantly
accelerates this self-improvement process for several tasks. Our results
demonstrate how controlled weak-to-strong curricula can systematically teach a
model logical extrapolation without any changes to the positional embeddings,
or the model architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Similarity Metrics for Data Selection for Language Model
  Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Sam, Ayan Chakrabarti, Afshin Rostamizadeh, Srikumar Ramalingam, Gui Citovsky, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Similarity between training examples is used to curate pretraining datasets
for language models by many methods -- for diversification and to select
examples similar to high-quality data. However, similarity is typically
measured with off-the-shelf embedding models that are generic or trained for
tasks such as retrieval. This paper introduces a framework to analyze the
suitability of embedding models specifically for data curation in the language
model pretraining setting. We quantify the correlation between similarity in
the embedding space to similarity in pretraining loss between different
training examples, and how diversifying in the embedding space affects
pretraining quality. We analyze a variety of embedding models in our framework,
with experiments using the Pile dataset for pretraining a 1.7B parameter
decoder-only language model. We find that the embedding models we consider are
all useful for pretraining data curation. Moreover, a simple approach of
averaging per-token embeddings proves to be surprisingly competitive with more
sophisticated embedding models -- likely because the latter are not designed
specifically for pretraining data curation. Indeed, we believe our analysis and
evaluation framework can serve as a foundation for the design of embedding
models that specifically reason about similarity in pretraining datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Invariant Graph Learning Meets Information Bottleneck for
  Out-of-Distribution Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01697v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01697v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyu Mao, Jiancan Wu, Haoyang Liu, Yongduo Sui, Xiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph out-of-distribution (OOD) generalization remains a major challenge in
graph learning since graph neural networks (GNNs) often suffer from severe
performance degradation under distribution shifts. Invariant learning, aiming
to extract invariant features across varied distributions, has recently emerged
as a promising approach for OOD generation. Despite the great success of
invariant learning in OOD problems for Euclidean data (i.e., images), the
exploration within graph data remains constrained by the complex nature of
graphs. Existing studies, such as data augmentation or causal intervention,
either suffer from disruptions to invariance during the graph manipulation
process or face reliability issues due to a lack of supervised signals for
causal parts. In this work, we propose a novel framework, called Invariant
Graph Learning based on Information bottleneck theory (InfoIGL), to extract the
invariant features of graphs and enhance models' generalization ability to
unseen distributions. Specifically, InfoIGL introduces a redundancy filter to
compress task-irrelevant information related to environmental factors.
Cooperating with our designed multi-level contrastive learning, we maximize the
mutual information among graphs of the same class in the downstream
classification tasks, preserving invariant features for prediction to a great
extent. An appealing feature of InfoIGL is its strong generalization ability
without depending on supervised signal of invariance. Experiments on both
synthetic and real-world datasets demonstrate that our method achieves
state-of-the-art performance under OOD generalization for graph classification
tasks. The source code is available at https://github.com/maowenyu-11/InfoIGL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has been accepted by Frontiers of Computer Science (FCS),
  with the DOI: {10.1007/s11704-025-40798-3}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE
  Solvers <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07856v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07856v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Li, Wei Fang, Hongbo Zhao, Le Lu, Ge Yang, Minfeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In applications of diffusion models, controllable generation is of practical
significance, but is also challenging. Current methods for controllable
generation primarily focus on modifying the score function of diffusion models,
while Mean Reverting (MR) Diffusion directly modifies the structure of the
stochastic differential equation (SDE), making the incorporation of image
conditions simpler and more natural. However, current training-free fast
samplers are not directly applicable to MR Diffusion. And thus MR Diffusion
requires hundreds of NFEs (number of function evaluations) to obtain
high-quality samples. In this paper, we propose a new algorithm named MRS (MR
Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time
SDE and the probability flow ordinary differential equation (PF-ODE) associated
with MR Diffusion, and derive semi-analytical solutions. The solutions consist
of an analytical function and an integral parameterized by a neural network.
Based on this solution, we can generate high-quality samples in fewer steps.
Our approach does not require training and supports all mainstream
parameterizations, including noise prediction, data prediction and velocity
prediction. Extensive experiments demonstrate that MR Sampler maintains high
sampling quality with a speedup of 10 to 20 times across ten different image
restoration tasks. Our algorithm accelerates the sampling procedure of MR
Diffusion, making it more practical in controllable generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tackling Data Corruption in Offline Reinforcement Learning via Sequence
  Modeling <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04285v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04285v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Xu, Rui Yang, Shuang Qiu, Feng Luo, Meng Fang, Baoxiang Wang, Lei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning policy from offline datasets through offline reinforcement learning
(RL) holds promise for scaling data-driven decision-making while avoiding
unsafe and costly online interactions. However, real-world data collected from
sensors or humans often contains noise and errors, posing a significant
challenge for existing offline RL methods, particularly when the real-world
data is limited. Our study reveals that prior research focusing on adapting
predominant offline RL methods based on temporal difference learning still
falls short under data corruption when the dataset is limited. In contrast, we
discover that vanilla sequence modeling methods, such as Decision Transformer,
exhibit robustness against data corruption, even without specialized
modifications. To unlock the full potential of sequence modeling, we propose
Robust Decision Rransformer (RDT) by incorporating three simple yet effective
robust techniques: embedding dropout to improve the model's robustness against
erroneous inputs, Gaussian weighted learning to mitigate the effects of
corrupted labels, and iterative data correction to eliminate corrupted data
from the source. Extensive experiments on MuJoCo, Kitchen, and Adroit tasks
demonstrate RDT's superior performance under various data corruption scenarios
compared to prior methods. Furthermore, RDT exhibits remarkable robustness in a
more challenging setting that combines training-time data corruption with
test-time observation perturbations. These results highlight the potential of
sequence modeling for learning from noisy or corrupted offline datasets,
thereby promoting the reliable application of offline RL in real-world
scenarios. Our code is available at
https://github.com/jiawei415/RobustDecisionTransformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZETA: Leveraging Z-order Curves for Efficient Top-k Attention <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhao Zeng, Jerry Huang, Peng Lu, Gezheng Xu, Boxing Chen, Charles Ling, Boyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over recent years, the Transformer has become a fundamental building block
for sequence modeling architectures. Yet at its core is the use of
self-attention, whose memory and computational cost grow quadratically with the
sequence length $N$, rendering it prohibitively expensive for long sequences. A
promising approach is top-$k$ attention, which selects only the $k$ most
relevant tokens and achieves performance comparable to vanilla self-attention
while significantly reducing space and computational demands. However, causal
masks require the current query token to only attend to past tokens, preventing
the existing top-$k$ attention method from efficiently searching for the most
relevant tokens in parallel, thereby limiting training efficiency. In this
work, we propose ZETA, leveraging \textbf{Z}-Order Curves for
\textbf{E}fficient \textbf{T}op-$k$ \textbf{A}ttention, to enable parallel
querying of past tokens for entire sequences. % in both space and time
complexity of $\mathcal{O}(N \log N)$. We first theoretically show that the
choice of key and query dimensions involves a trade-off between the curse of
dimensionality and the preservation of relative distances after projection. In
light of this insight, we propose reducing the dimensionality of keys and
queries in contrast to values and further leverage $Z$-order curves to map
low-dimensional keys and queries into \emph{one}-dimensional space, which
permits parallel sorting, thereby largely improving the efficiency for top-$k$
token selection. Experimental results demonstrate that ZETA matches the
performance of standard attention on the synthetic \textsc{Multi-Query
Associative Recall} task and outperforms attention and its variants on
\textsc{Long Range Arena} and \textsc{WikiText-103} language modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 4 figures, accepted in International Conference on Learning
  Representations (ICLR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA <span class="chip">ICML24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangyi Chen, Yuanxin Guo, Yue Ju, Harik Dalal, Ashish Khisti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation
(LoRA) optimize federated training by reducing computational and communication
costs. We propose RoLoRA, a federated framework using alternating optimization
to fine-tune LoRA adapters. Our approach emphasizes the importance of learning
up and down projection matrices to enhance expressiveness and robustness. We
use both theoretical analysis and extensive experiments to demonstrate the
advantages of RoLoRA over prior approaches that either generate imperfect model
updates or limit expressiveness of the model. We present theoretical analysis
on a simplified linear model to demonstrate the importance of learning both
down-projection and up-projection matrices in LoRA. We provide extensive
experimental evaluations on a toy neural network on MNIST as well as large
language models including RoBERTa-Large, Llama-2-7B on diverse tasks to
demonstrate the advantages of RoLoRA over other methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A preliminary version was in ICML24 workshop, arXiv:2409.02346</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiating Student Feedbacks for Knowledge Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Cui, Hong Qian, Chanjin Zheng, Lu Wang, Mo Yu, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge tracing (KT) is a crucial task in computer-aided education and
intelligent tutoring systems, predicting students' performance on new questions
from their responses to prior ones. An accurate KT model can capture a
student's mastery level of different knowledge topics, as reflected in their
predicted performance on different questions. This helps improve the learning
efficiency by suggesting appropriate new questions that complement students'
knowledge states. However, current KT models have significant drawbacks that
they neglect the imbalanced discrimination of historical responses. A
significant proportion of question responses provide limited information for
discerning students' knowledge mastery, such as those that demonstrate uniform
performance across different students. Optimizing the prediction of these cases
may increase overall KT accuracy, but also negatively impact the model's
ability to trace personalized knowledge states, especially causing a deceptive
surge of performance. Towards this end, we propose a framework to reweight the
contribution of different responses based on their discrimination in training.
Additionally, we introduce an adaptive predictive score fusion technique to
maintain accuracy on less discriminative responses, achieving proper balance
between student knowledge mastery and question difficulty. Experimental results
demonstrate that our framework enhances the performance of three mainstream KT
methods on three widely-used datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM TOIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unifying View of Linear Function Approximation in Off-Policy RL
  Through Matrix Splitting and Preconditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechen Wu, Amy Greenwald, Ronald Parr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, TD and FQI are viewed as differing in the number of updates
toward the target value function: TD makes one update, FQI makes an infinite
number, and Partial Fitted Q-Iteration (PFQI) performs a finite number, such as
the use of a target network in Deep Q-Networks (DQN) in the OPE setting. This
perspective, however, fails to capture the convergence connections between
these algorithms and may lead to incorrect conclusions, for example, that the
convergence of TD implies the convergence of FQI. In this paper, we focus on
linear value function approximation and offer a new perspective, unifying TD,
FQI, and PFQI as the same iterative method for solving the Least Squares
Temporal Difference (LSTD) system, but using different preconditioners and
matrix splitting schemes. TD uses a constant preconditioner, FQI employs a
data-feature adaptive preconditioner, and PFQI transitions between the two.
Then, we reveal that in the context of linear function approximation,
increasing the number of updates under the same target value function
essentially represents a transition from using a constant preconditioner to
data-feature adaptive preconditioner. This unifying perspective also simplifies
the analyses of the convergence conditions for these algorithms and clarifies
many issues. Consequently, we fully characterize the convergence of each
algorithm without assuming specific properties of the chosen features (e.g.,
linear independence). We also examine how common assumptions about feature
representations affect convergence, and discover new conditions on features
that are important for convergence. These convergence conditions allow us to
establish the convergence connections between these algorithms and to address
important questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00626v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00626v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko, Bryan A. Plummer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Typographic attacks, adding misleading text to images, can deceive
vision-language models (LVLMs). The susceptibility of recent large LVLMs like
GPT4-V to such attacks is understudied, raising concerns about amplified
misinformation in personal assistant applications. Previous attacks use simple
strategies, such as random misleading words, which don't fully exploit LVLMs'
language reasoning abilities. We introduce an experimental setup for testing
typographic attacks on LVLMs and propose two novel self-generated attacks: (1)
Class-based attacks, where the model identifies a similar class to deceive
itself, and (2) Reasoned attacks, where an advanced LVLM suggests an attack
combining a deceiving class and description. Our experiments show these attacks
significantly reduce classification performance by up to 60\% and are effective
across different models, including InstructBLIP and MiniGPT4. Code:
https://github.com/mqraitem/Self-Gen-Typo-Attack
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ For Better or For Worse? Learning Minimum Variance Features With Label
  Augmentation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06855v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06855v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muthu Chidambaram, Rong Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation has been pivotal in successfully training deep learning
models on classification tasks over the past decade. An important subclass of
data augmentation techniques - which includes both label smoothing and Mixup -
involves modifying not only the input data but also the input label during
model training. In this work, we analyze the role played by the label
augmentation aspect of such methods. We first prove that linear models on
binary classification data trained with label augmentation learn only the
minimum variance features in the data, while standard training (which includes
weight decay) can learn higher variance features. We then use our techniques to
show that even for nonlinear models and general data distributions, the label
smoothing and Mixup losses are lower bounded by a function of the model output
variance. Lastly, we demonstrate empirically that this aspect of label
smoothing and Mixup can be a positive and a negative. On the one hand, we show
that the strong performance of label smoothing and Mixup on image
classification benchmarks is correlated with learning low variance hidden
representations. On the other hand, we show that Mixup and label smoothing can
be more susceptible to low variance spurious correlations in the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025, 25 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for
  Entity Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijie Ding, Jie Yin, Daokun Zhang, Junbin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) aims at identifying equivalent entity pairs across
different knowledge graphs (KGs) that refer to the same real-world identity. To
systematically combat confirmation bias for pseudo-labeling-based entity
alignment, we propose a Unified Pseudo-Labeling framework for Entity Alignment
(UPL-EA) that explicitly eliminates pseudo-labeling errors to boost the
accuracy of entity alignment. UPL-EA consists of two complementary components:
(1) The Optimal Transport (OT)-based pseudo-labeling uses discrete OT modeling
as an effective means to enable more accurate determination of entity
correspondences across two KGs and to mitigate the adverse impact of erroneous
matches. A simple but highly effective criterion is further devised to derive
pseudo-labeled entity pairs that satisfy one-to-one correspondences at each
iteration. (2) The cross-iteration pseudo-label calibration operates across
multiple consecutive iterations to further improve the pseudo-labeling
precision rate by reducing the local pseudo-label selection variability with a
theoretical guarantee. The two components are respectively designed to
eliminate Type I and Type II pseudo-labeling errors identified through our
analyse. The calibrated pseudo-labels are thereafter used to augment prior
alignment seeds to reinforce subsequent model training for alignment inference.
The effectiveness of UPL-EA in eliminating pseudo-labeling errors is both
theoretically supported and experimentally validated. The experimental results
show that our approach achieves competitive performance with limited prior
alignment seeds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Technology Landscape through Topic Modeling, Expert
  Involvement, and Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Nazari, Michael Weiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's rapidly evolving technological landscape, organizations face the
challenge of integrating external insights into their decision-making processes
to stay competitive. To address this issue, this study proposes a method that
combines topic modeling, expert knowledge inputs, and reinforcement learning
(RL) to enhance the detection of technological changes. The method has four
main steps: (1) Build a relevant topic model, starting with textual data like
documents and reports to find key themes. (2) Create aspect-based topic models.
Experts use curated keywords to build models that showcase key domain-specific
aspects. (3) Iterative analysis and RL driven refinement: We examine metrics
such as topic magnitude, similarity, entropy shifts, and how models change over
time. We optimize topic selection with RL. Our reward function balances the
diversity and similarity of the topics. (4) Synthesis and operational
integration: Each iteration provides insights. In the final phase, the experts
check these insights and reach new conclusions. These conclusions are designed
for use in the firm's operational processes. The application is tested by
forecasting trends in quantum communication. Results demonstrate the method's
effectiveness in identifying, ranking, and tracking trends that align with
expert input, providing a robust tool for exploring evolving technological
landscapes. This research offers a scalable and adaptive solution for
organizations to make informed strategic decisions in dynamic environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VaiBot: Shuttle Between the Instructions and Parameters of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangtao Sun, Haotian Xu, Huanxuan Liao, Xuanqing Yu, Zhongtao Jiang, Shizhu He, Jun Zhao, Kang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to interact with LLMs through \emph{instructions} has been widely studied
by researchers. However, previous studies have treated the emergence of
instructions and the training of LLMs on task data as separate processes,
overlooking the inherent unity between the two. This paper proposes a neural
network framework, VaiBot, that integrates VAE and VIB, designed to uniformly
model, learn, and infer both deduction and induction tasks under LLMs. Through
experiments, we demonstrate that VaiBot performs on par with existing baseline
methods in terms of deductive capabilities while significantly surpassing them
in inductive capabilities. We also find that VaiBot can scale up using general
instruction-following data and exhibits excellent one-shot induction abilities.
We finally synergistically integrate the deductive and inductive processes of
VaiBot. Through T-SNE dimensionality reduction, we observe that its
inductive-deductive process significantly improves the distribution of training
parameters, enabling it to outperform baseline methods in inductive reasoning
tasks. The code and data for this paper can be found at
https://anonymous.4open.science/r/VaiBot-021F.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XAMBA: Enabling Efficient State Space Models on Resource-Constrained
  Neural Processing Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arghadip Das, Arnab Raha, Shamik Kundu, Soumendu Kumar Ghosh, Deepak Mathaikutty, Vijay Raghunathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-Space Models (SSMs) have emerged as efficient alternatives to
transformers for sequential data tasks, offering linear or near-linear
scalability with sequence length, making them ideal for long-sequence
applications in NLP, vision, and edge AI, including real-time transcription,
translation, and contextual search. These applications require lightweight,
high-performance models for deployment on resource-constrained devices like
laptops and PCs. Designing specialized accelerators for every emerging neural
network is costly and impractical; instead, optimizing models for existing NPUs
in AI PCs provides a scalable solution. To this end, we propose XAMBA, the
first framework to enable and optimize SSMs on commercial off-the-shelf (COTS)
state-of-the-art (SOTA) NPUs. XAMBA follows a three-step methodology: (1)
enabling SSMs on NPUs, (2) optimizing performance to meet KPI requirements, and
(3) trading accuracy for additional performance gains. After enabling SSMs on
NPUs, XAMBA mitigates key bottlenecks using CumBA and ReduBA, replacing
sequential CumSum and ReduceSum operations with matrix-based computations,
significantly improving execution speed and memory efficiency. Additionally,
ActiBA enhances performance by approximating expensive activation functions
(e.g., Swish, Softplus) using piecewise linear mappings, reducing latency with
minimal accuracy loss. Evaluations on an Intel Core Ultra Series 2 AI PC show
that XAMBA achieves up to 2.6X speed-up over the baseline. Our implementation
is available at https://github.com/arghadippurdue/XAMBA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraNNite: Enabling High-Performance Execution of Graph Neural Networks
  on Resource-Constrained Neural Processing Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arghadip Das, Shamik Kundu, Arnab Raha, Soumendu Ghosh, Deepak Mathaikutty, Vijay Raghunathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are vital for learning from graph-structured
data, enabling applications in network analysis, recommendation systems, and
speech analytics. Deploying them on edge devices like client PCs and laptops
enhances real-time processing, privacy, and cloud independence. GNNs aid
Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and
enable event-based vision tasks. However, irregular memory access, sparsity,
and dynamic structures cause high latency and energy overhead on
resource-constrained devices. While modern edge processors integrate CPUs,
GPUs, and NPUs, NPUs designed for data-parallel tasks struggle with irregular
GNN computations. We introduce GraNNite, the first hardware-aware framework
optimizing GNN execution on commercial-off-the-shelf (COTS) SOTA DNN
accelerators via a structured three-step methodology: (1) enabling NPU
execution, (2) optimizing performance, and (3) trading accuracy for efficiency
gains. Step 1 employs GraphSplit for workload distribution and StaGr for static
aggregation, while GrAd and NodePad handle dynamic graphs. Step 2 boosts
performance using EffOp for control-heavy tasks and GraSp for sparsity
exploitation. Graph Convolution optimizations PreG, SymG, and CacheG reduce
redundancy and memory transfers. Step 3 balances quality versus efficiency,
where QuantGr applies INT8 quantization, and GrAx1, GrAx2, and GrAx3 accelerate
attention, broadcast-add, and SAGE-max aggregation. On Intel Core Ultra AI PCs,
GraNNite achieves 2.6X to 7.6X speedups over default NPU mappings and up to
8.6X energy gains over CPUs and GPUs, delivering 10.8X and 6.7X higher
performance than CPUs and GPUs, respectively, across GNN models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Haoyang Li, Fei Teng, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classification is a fundamental task in data mining, pivotal to various
applications such as tabular understanding and recommendation. Although neural
network-based models, such as CNN and BERT, have demonstrated remarkable
performance in text classification, their effectiveness heavily relies on
abundant labeled training data. This dependency makes these models less
effective in dynamic few-shot text classification, where labeled data is
scarce, and new target labels frequently appear based on application needs.
Recently, large language models (LLMs) have shown promise due to their
extensive pretraining and contextual understanding ability. Current approaches
provide LLMs with text inputs, candidate labels, and additional side
information (e.g., descriptions) to classify texts. However, their
effectiveness is hindered by the increased input size and the noise introduced
through side information processing. To address these limitations, we propose a
graph-based online retrieval-augmented generation framework, namely GORAG, for
dynamic few-shot text classification. Rather than treating each input
independently, GORAG constructs and maintains a weighted graph by extracting
side information across all target texts. In this graph, text keywords and
labels are represented as nodes, with edges indicating the correlations between
them. To model these correlations, GORAG employs an edge weighting mechanism to
prioritize the importance and reliability of extracted information and
dynamically retrieves relevant context using a minimum-cost spanning tree
tailored for each text input. Empirical evaluations demonstrate that GORAG
outperforms existing approaches by providing more comprehensive and precise
contextual information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Partial Gromov-Wasserstein Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03664v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03664v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikun Bai, Rocio Diaz Martin, Abihith Kothapalli, Hengrong Du, Xinran Liu, Soheil Kolouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Gromov-Wasserstein (GW) distance has gained increasing interest in the
machine learning community in recent years, as it allows for the comparison of
measures in different metric spaces. To overcome the limitations imposed by the
equal mass requirements of the classical GW problem, researchers have begun
exploring its application in unbalanced settings. However, Unbalanced GW (UGW)
can only be regarded as a discrepancy rather than a rigorous metric/distance
between two metric measure spaces (mm-spaces). In this paper, we propose a
particular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW). We
establish that PGW is a well-defined metric between mm-spaces and discuss its
theoretical properties, including the existence of a minimizer for the PGW
problem and the relationship between PGW and GW, among others. We then propose
two variants of the Frank-Wolfe algorithm for solving the PGW problem and show
that they are mathematically and computationally equivalent. Moreover, based on
our PGW metric, we introduce the analogous concept of barycenters for
mm-spaces. Finally, we validate the effectiveness of our PGW metric and related
solvers in applications such as shape matching, shape retrieval, and shape
interpolation, comparing them against existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Neural Operators with Diffusion Models Improves Spectral
  Representation in Turbulence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Oommen, Aniruddha Bora, Zhen Zhang, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We integrate neural operators with diffusion models to address the spectral
limitations of neural operators in surrogate modeling of turbulent flows. While
neural operators offer computational efficiency, they exhibit deficiencies in
capturing high-frequency flow dynamics, resulting in overly smooth
approximations. To overcome this, we condition diffusion models on neural
operators to enhance the resolution of turbulent structures. Our approach is
validated for different neural operators on diverse datasets, including a high
Reynolds number jet flow simulation and experimental Schlieren velocimetry. The
proposed method significantly improves the alignment of predicted energy
spectra with true distributions compared to neural operators alone. This
enables the diffusion models to stabilize longer forecasts through
diffusion-corrected autoregressive rollouts, as we demonstrate in this work.
Additionally, proper orthogonal decomposition analysis demonstrates enhanced
spectral fidelity in space-time. This work establishes a new paradigm for
combining generative models with neural operators to advance surrogate modeling
of turbulent systems, and it can be used in other scientific applications that
involve microstructure and high-frequency content. See our project page:
vivekoommen.github.io/NO_DM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "It Might be Technically Impressive, But It's Practically Useless to
  us": Motivations, Practices, Challenges, and Opportunities for
  Cross-Functional Collaboration around AI within the News Industry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12000v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12000v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qing Xiao, Xianzhe Fan, Felix M. Simon, Bingbing Zhang, Motahhare Eslami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, an increasing number of news organizations have integrated
artificial intelligence (AI) into their workflows, leading to a further influx
of AI technologists and data workers into the news industry. This has initiated
cross-functional collaborations between these professionals and journalists.
Although prior research has explored the impact of AI-related roles entering
the news industry, there is a lack of studies on how internal cross-functional
collaboration around AI unfolds between AI professionals and journalists within
the news industry. Through interviews with 17 journalists, six AI
technologists, and three AI workers with cross-functional experience from
leading Chinese news organizations, we investigate the practices, challenges,
and opportunities for internal cross-functional collaboration around AI in news
industry. We first study how these journalists and AI professionals perceive
existing internal cross-collaboration strategies. We explore the challenges of
cross-functional collaboration and provide recommendations for enhancing future
cross-functional collaboration around AI in the news industry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, Accepted by CHI '25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Context-Enhanced Relational Joins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Sanca, Manos Chatzakis, Anastasia Ailamaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting data, extracting value, and combining insights from relational and
context-rich multi-modal sources in data processing pipelines presents a
challenge for traditional relational DBMS. While relational operators allow
declarative and optimizable query specification, they are limited to data
transformations unsuitable for capturing or analyzing context. On the other
hand, representation learning models can map context-rich data into embeddings,
allowing machine-automated context processing but requiring imperative data
transformation integration with the analytical query. To bridge this dichotomy,
we present a context-enhanced relational join and introduce an embedding
operator composable with relational operators. This enables hybrid relational
and context-rich vector data processing, with algebraic equivalences compatible
with relational algebra and corresponding logical and physical optimizations.
We investigate model-operator interaction with vector data processing and study
the characteristics of the E-join operator. Using an example of string
embeddings, we demonstrate enabling hybrid context-enhanced processing on
relational join operators with vector embeddings. The importance of holistic
optimization, from logical to physical, is demonstrated in an order of
magnitude execution time improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Non-Degeneracy: Revisiting Certainty Equivalent Heuristic for
  Online Linear Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01716v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01716v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Chen, Wenjia Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Certainty Equivalent heuristic (CE) is a widely-used algorithm for
various dynamic resource allocation problems in OR and OM. Despite its
popularity, existing theoretical guarantees of CE are limited to settings
satisfying restrictive fluid regularity conditions, particularly, the
non-degeneracy conditions, under the widely held belief that the violation of
such conditions leads to performance deterioration and necessitates algorithmic
innovation beyond CE.
  In this work, we conduct a refined performance analysis of CE within the
general framework of online linear programming. We show that CE achieves
uniformly near-optimal regret (up to a polylogarithmic factor in $T$) under
only mild assumptions on the underlying distribution, without relying on any
fluid regularity conditions. Our result implies that, contrary to prior belief,
CE effectively beats the curse of degeneracy for a wide range of problem
instances with continuous conditional reward distributions, highlighting the
distinction of the problem's structure between discrete and non-discrete
settings. Our explicit regret bound interpolates between the mild $(\log T)^2$
regime and the worst-case $\sqrt{T}$ regime with a parameter $\beta$
quantifying the minimal rate of probability accumulation of the conditional
reward distributions, generalizing prior findings in the multisecretary
setting.
  To achieve these results, we develop novel algorithmic analytical techniques.
Drawing tools from the empirical processes theory, we establish strong
concentration analysis of the solutions to random linear programs, leading to
improved regret analysis under significantly relaxed assumptions. These
techniques may find potential applications in broader online decision-making
contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Symbolic Regression through Offline Reinforcement Learning:
  A Co-Design Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Tian, Wenqi Zhou, Michele Viscione, Hao Dong, David Kammer, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic Regression (SR) holds great potential for uncovering underlying
mathematical and physical relationships from observed data. However, the vast
combinatorial space of possible expressions poses significant challenges for
both online search methods and pre-trained transformer models. Additionally,
current state-of-the-art approaches typically do not consider the integration
of domain experts' prior knowledge and do not support iterative interactions
with the model during the equation discovery process. To address these
challenges, we propose the Symbolic Q-network (Sym-Q), an advanced interactive
framework for large-scale symbolic regression. Unlike previous large-scale
transformer-based SR approaches, Sym-Q leverages reinforcement learning without
relying on a transformer-based decoder. This formulation allows the agent to
learn through offline reinforcement learning using any type of tree encoder,
enabling more efficient training and inference. Furthermore, we propose a
co-design mechanism, where the reinforcement learning-based Sym-Q facilitates
effective interaction with domain experts at any stage of the equation
discovery process. Users can dynamically modify generated nodes of the
expression, collaborating with the agent to tailor the mathematical expression
to best fit the problem and align with the assumed physical laws, particularly
when there is prior partial knowledge of the expected behavior. Our experiments
demonstrate that the pre-trained Sym-Q surpasses existing SR algorithms on the
challenging SSDNC benchmark. Moreover, we experimentally show on real-world
cases that its performance can be further enhanced by the interactive co-design
mechanism, with Sym-Q achieving greater performance gains than other
state-of-the-art models. Our reproducible code is available at
https://github.com/EPFL-IMOS/Sym-Q.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual-based spatial audio generation system for multi-speaker
  environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojing Liu, Ogulcan Gurelli, Yan Wang, Joshua Reiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multimedia applications such as films and video games, spatial audio
techniques are widely employed to enhance user experiences by simulating 3D
sound: transforming mono audio into binaural formats. However, this process is
often complex and labor-intensive for sound designers, requiring precise
synchronization of audio with the spatial positions of visual components. To
address these challenges, we propose a visual-based spatial audio generation
system - an automated system that integrates face detection YOLOv8 for object
detection, monocular depth estimation, and spatial audio techniques. Notably,
the system operates without requiring additional binaural dataset training. The
proposed system is evaluated against existing Spatial Audio generation system
using objective metrics. Experimental results demonstrate that our method
significantly improves spatial consistency between audio and video, enhances
speech quality, and performs robustly in multi-speaker scenarios. By
streamlining the audio-visual alignment process, the proposed system enables
sound engineers to achieve high-quality results efficiently, making it a
valuable tool for professionals in multimedia production.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework
  with Interactive Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Lin, Yingjing Xu, Xuanwen Bao, Zhou Zhao, Zhouyang Wang, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous advancement of vision language models (VLMs) technology,
remarkable research achievements have emerged in the dermatology field, the
fourth most prevalent human disease category. However, despite these
advancements, VLM still faces explainable problems to user in diagnosis due to
the inherent complexity of dermatological conditions, existing tools offer
relatively limited support for user comprehension. We propose SkinGEN, a
diagnosis-to-generation framework that leverages the stable diffusion(SD) model
to generate reference demonstrations from diagnosis results provided by VLM,
thereby enhancing the visual explainability for users. Through extensive
experiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for
skin condition image generation. We conduct a user study with 32 participants
evaluating both the system performance and explainability. Results demonstrate
that SkinGEN significantly improves users' comprehension of VLM predictions and
fosters increased trust in the diagnostic process. This work paves the way for
more transparent and user-centric VLM applications in dermatology and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-12T00:00:00Z">2025-02-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">115</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Survey</span> on Single-Image Reflection Removal using Deep Learning Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Yang, Huiming Sun, Jie Cai, Lan Fu, Jiaming Ding, Jinlong Li, Chiu Man Ho, Zibo Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The phenomenon of reflection is quite common in digital images, posing
significant challenges for various applications such as computer vision,
photography, and image processing. Traditional methods for reflection removal
often struggle to achieve clean results while maintaining high fidelity and
robustness, particularly in real-world scenarios. Over the past few decades,
numerous deep learning-based approaches for reflection removal have emerged,
yielding impressive results. In this survey, we conduct a comprehensive review
of the current literature by focusing on key venues such as ICCV, ECCV, CVPR,
NeurIPS, etc., as these conferences and journals have been central to advances
in the field. Our review follows a structured paper selection process, and we
critically assess both single-stage and two-stage deep learning methods for
reflection removal. The contribution of this survey is three-fold: first, we
provide a comprehensive summary of the most recent work on single-image
reflection removal; second, we outline task hypotheses, current deep learning
techniques, publicly available datasets, and relevant evaluation metrics; and
third, we identify key challenges and opportunities in deep learning-based
reflection removal, highlighting the potential of this rapidly evolving
research area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with
  Saliency Maps <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jocelyn Dzuong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent surge in advanced generative models, such as diffusion models and
generative adversarial networks (GANs), has led to an alarming rise in
AI-generated images across various domains on the web. While such technologies
offer benefits such as democratizing artistic creation, they also pose
challenges in misinformation, digital forgery, and authenticity verification.
Additionally, the uncredited use of AI-generated images in media and marketing
has sparked significant backlash from online communities. In response to this,
we introduce DejAIvu, a Chrome Web extension that combines real-time
AI-generated image detection with saliency-based explainability while users
browse the web. Using an ONNX-optimized deep learning model, DejAIvu
automatically analyzes images on websites such as Google Images, identifies
AI-generated content using model inference, and overlays a saliency heatmap to
highlight AI-related artifacts. Our approach integrates efficient in-browser
inference, gradient-based saliency analysis, and a seamless user experience,
ensuring that AI detection is both transparent and interpretable. We also
evaluate DejAIvu across multiple pretrained architectures and benchmark
datasets, demonstrating high accuracy and low latency, making it a practical
and deployable tool for enhancing AI image accountability. The code for this
system can be found at https://github.com/Noodulz/dejAIvu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, submitted to IJCAI 2025 demo track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\mathsf{CSMAE~}$:~Cataract Surgical Masked Autoencoder (MAE) based
  Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisarg A. Shah, Wele Gedara Chaminda Bandara, Shameema Skider, S. Swaroop Vedula, Vishal M. Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated analysis of surgical videos is crucial for improving surgical
training, workflow optimization, and postoperative assessment. We introduce a
CSMAE, Masked Autoencoder (MAE)-based pretraining approach, specifically
developed for Cataract Surgery video analysis, where instead of randomly
selecting tokens for masking, they are selected based on the spatiotemporal
importance of the token. We created a large dataset of cataract surgery videos
to improve the model's learning efficiency and expand its robustness in
low-data regimes. Our pre-trained model can be easily adapted for specific
downstream tasks via fine-tuning, serving as a robust backbone for further
analysis. Through rigorous testing on a downstream step-recognition task on two
Cataract Surgery video datasets, D99 and Cataract-101, our approach surpasses
current state-of-the-art self-supervised pretraining and adapter-based transfer
learning methods by a significant margin. This advancement not only
demonstrates the potential of our MAE-based pretraining in the field of
surgical video analysis but also sets a new benchmark for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, Accepted to IEEE International Symposium on Biomedical
  Imaging (ISBI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring Anxiety Levels with Head <span class="highlight-title">Motion</span> Patterns in Severe Depression
  Population 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fouad Boualeb, Emery Pierson, Nicolas Doudeau, Clémence Nineuil, Ali Amad, Mohamed Daoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depression and anxiety are prevalent mental health disorders that frequently
cooccur, with anxiety significantly influencing both the manifestation and
treatment of depression. An accurate assessment of anxiety levels in
individuals with depression is crucial to develop effective and personalized
treatment plans. This study proposes a new noninvasive method for quantifying
anxiety severity by analyzing head movements -specifically speed, acceleration,
and angular displacement - during video-recorded interviews with patients
suffering from severe depression. Using data from a new CALYPSO Depression
Dataset, we extracted head motion characteristics and applied regression
analysis to predict clinically evaluated anxiety levels. Our results
demonstrate a high level of precision, achieving a mean absolute error (MAE) of
0.35 in predicting the severity of psychological anxiety based on head movement
patterns. This indicates that our approach can enhance the understanding of
anxiety's role in depression and assist psychiatrists in refining treatment
strategies for individuals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19th IEEE International Conference on Automatic Face and Gesture
  Recognition (FG), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MRUCT: Mixed Reality Assistance for Acupuncture Guided by Ultrasonic
  Computed Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Yang, Xinkai Wang, Kehong Zhou, Xue Xie, Lifeng Zhu, Aiguo Song, Bruce Daniel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese acupuncture practitioners primarily depend on muscle memory and
tactile feedback to insert needles and accurately target acupuncture points, as
the current workflow lacks imaging modalities and visual aids. Consequently,
new practitioners often learn through trial and error, requiring years of
experience to become proficient and earn the trust of patients. Medical
students face similar challenges in mastering this skill. To address these
challenges, we developed an innovative system, MRUCT, that integrates
ultrasonic computed tomography (UCT) with mixed reality (MR) technology to
visualize acupuncture points in real-time. This system offers offline image
registration and real-time guidance during needle insertion, enabling them to
accurately position needles based on anatomical structures such as bones,
muscles, and auto-generated reference points, with the potential for clinical
implementation. In this paper, we outline the non-rigid registration methods
used to reconstruct anatomical structures from UCT data, as well as the key
design considerations of the MR system. We evaluated two different 3D user
interface (3DUI) designs and compared the performance of our system to
traditional workflows for both new practitioners and medical students. The
results highlight the potential of MR to enhance therapeutic medical practices
and demonstrate the effectiveness of the system we developed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SB-Bench: Stereotype Bias Benchmark for Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishal Narnaware, Ashmal Vayani, Rohit Gupta, Swetha Sirnam, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereotype biases in Large Multimodal Models (LMMs) perpetuate harmful
societal prejudices, undermining the fairness and equity of AI applications. As
LMMs grow increasingly influential, addressing and mitigating inherent biases
related to stereotypes, harmful generations, and ambiguous assumptions in
real-world scenarios has become essential. However, existing datasets
evaluating stereotype biases in LMMs often lack diversity and rely on synthetic
images, leaving a gap in bias evaluation for real-world visual contexts. To
address this, we introduce the Stereotype Bias Benchmark (SB-bench), the most
comprehensive framework to date for assessing stereotype biases across nine
diverse categories with non-synthetic images. SB-bench rigorously evaluates
LMMs through carefully curated, visually grounded scenarios, challenging them
to reason accurately about visual stereotypes. It offers a robust evaluation
framework featuring real-world visual samples, image variations, and
multiple-choice question formats. By introducing visually grounded queries that
isolate visual biases from textual ones, SB-bench enables a precise and nuanced
assessment of a model's reasoning capabilities across varying levels of
difficulty. Through rigorous testing of state-of-the-art open-source and
closed-source LMMs, SB-bench provides a systematic approach to assessing
stereotype biases in LMMs across key social dimensions. This benchmark
represents a significant step toward fostering fairness in AI systems and
reducing harmful biases, laying the groundwork for more equitable and socially
responsible LMMs. Our code and dataset are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal
  Brain in 3D Ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Omolegan, Pak Hei Yeung, Madeleine K. Wyburd, Linde Hesse, Monique Haak, Intergrowth-21st Consortium, Ana I. L. Namburete, Nicola K. Dinsdale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring the growth of subcortical regions of the fetal brain in ultrasound
(US) images can help identify the presence of abnormal development. Manually
segmenting these regions is a challenging task, but recent work has shown that
it can be automated using deep learning. However, applying pretrained models to
unseen freehand US volumes often leads to a degradation of performance due to
the vast differences in acquisition and alignment. In this work, we first
demonstrate that test time adaptation (TTA) can be used to improve model
performance in the presence of both real and simulated domain shifts. We
further propose a novel TTA method by incorporating a normative atlas as a
prior for anatomy. In the presence of various types of domain shifts, we
benchmark the performance of different TTA methods and demonstrate the
improvements brought by our proposed approach, which may further facilitate
automated monitoring of fetal brain development. Our code is available at
https://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cluster and Predict Latents Patches for Improved Masked Image Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothée Darcet, Federico Baldassarre, Maxime Oquab, Julien Mairal, Piotr Bojanowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Image Modeling (MIM) offers a promising approach to self-supervised
representation learning, however existing MIM models still lag behind the
state-of-the-art. In this paper, we systematically analyze target
representations, loss functions, and architectures, to introduce CAPI - a novel
pure-MIM framework that relies on the prediction of latent clusterings. Our
approach leverages a clustering-based loss, which is stable to train, and
exhibits promising scaling properties. Our ViT-L backbone, CAPI, achieves 83.8%
accuracy on ImageNet and 32.1% mIoU on ADE20K with simple linear probes,
substantially outperforming previous MIM methods and approaching the
performance of the current state-of-the-art, DINOv2. We release all our code
and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, submitted to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HistoSmith: Single-Stage Histology Image-Label Generation via
  Conditional Latent Diffusion for Enhanced Cell Segmentation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentina Vadori, Jean-Marie Graïc, Antonella Peruffo, Livio Finos, Ujwala Kiran Chaudhari, Enrico Grisan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise segmentation and classification of cell instances are vital for
analyzing the tissue microenvironment in histology images, supporting medical
diagnosis, prognosis, treatment planning, and studies of brain
cytoarchitecture. However, the creation of high-quality annotated datasets for
training remains a major challenge. This study introduces a novel single-stage
approach (HistoSmith) for generating image-label pairs to augment histology
datasets. Unlike state-of-the-art methods that utilize diffusion models with
separate components for label and image generation, our approach employs a
latent diffusion model to learn the joint distribution of cellular layouts,
classification masks, and histology images. This model enables tailored data
generation by conditioning on user-defined parameters such as cell types,
quantities, and tissue types. Trained on the Conic H&E histopathology dataset
and the Nissl-stained CytoDArk0 dataset, the model generates realistic and
diverse labeled samples. Experimental results demonstrate improvements in cell
instance segmentation and classification, particularly for underrepresented
cell types like neutrophils in the Conic dataset. These findings underscore the
potential of our approach to address data scarcity challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Poly-Autoregressive Prediction for Modeling Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neerja Thakkar, Tara Sadjadpour, Jathushan Rajasegaran, Shiry Ginosar, Jitendra Malik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a simple framework for predicting the behavior of an agent in
multi-agent settings. In contrast to autoregressive (AR) tasks, such as
language processing, our focus is on scenarios with multiple agents whose
interactions are shaped by physical constraints and internal motivations. To
this end, we propose Poly-Autoregressive (PAR) modeling, which forecasts an ego
agent's future behavior by reasoning about the ego agent's state history and
the past and current states of other interacting agents. At its core, PAR
represents the behavior of all agents as a sequence of tokens, each
representing an agent's state at a specific timestep. With minimal data
pre-processing changes, we show that PAR can be applied to three different
problems: human action forecasting in social situations, trajectory prediction
for autonomous vehicles, and object pose forecasting during hand-object
interaction. Using a small proof-of-concept transformer backbone, PAR
outperforms AR across these three scenarios. The project website can be found
at https://neerja.me/PAR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Real-to-Sim-to-Real Approach to Robotic Manipulation with
  VLM-Generated Iterative Keypoint Rewards <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivansh Patel, Xinchen Yin, Wenlong Huang, Shubham Garg, Hooshang Nayyeri, Li Fei-Fei, Svetlana Lazebnik, Yunzhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task specification for robotic manipulation in open-world environments is
challenging, requiring flexible and adaptive objectives that align with human
intentions and can evolve through iterative feedback. We introduce Iterative
Keypoint Reward (IKER), a visually grounded, Python-based reward function that
serves as a dynamic task specification. Our framework leverages VLMs to
generate and refine these reward functions for multi-step manipulation tasks.
Given RGB-D observations and free-form language instructions, we sample
keypoints in the scene and generate a reward function conditioned on these
keypoints. IKER operates on the spatial relationships between keypoints,
leveraging commonsense priors about the desired behaviors, and enabling precise
SE(3) control. We reconstruct real-world scenes in simulation and use the
generated rewards to train reinforcement learning (RL) policies, which are then
deployed into the real world-forming a real-to-sim-to-real loop. Our approach
demonstrates notable capabilities across diverse scenarios, including both
prehensile and non-prehensile tasks, showcasing multi-step task execution,
spontaneous error recovery, and on-the-fly strategy adjustments. The results
highlight IKER's effectiveness in enabling robots to perform multi-step tasks
in dynamic environments through iterative reward shaping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025, Project Page: https://iker-robot.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ellie Arar, Yarden Frenkel, Daniel Cohen-Or, Ariel Shamir, Yael Vinker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large vision-language models have enabled highly
expressive and diverse vector sketch generation. However, state-of-the-art
methods rely on a time-consuming optimization process involving repeated
feedback from a pretrained model to determine stroke placement. Consequently,
despite producing impressive sketches, these methods are limited in practical
applications. In this work, we introduce SwiftSketch, a diffusion model for
image-conditioned vector sketch generation that can produce high-quality
sketches in less than a second. SwiftSketch operates by progressively denoising
stroke control points sampled from a Gaussian distribution. Its
transformer-decoder architecture is designed to effectively handle the discrete
nature of vector representation and capture the inherent global dependencies
between strokes. To train SwiftSketch, we construct a synthetic dataset of
image-sketch pairs, addressing the limitations of existing sketch datasets,
which are often created by non-artists and lack professional quality. For
generating these synthetic sketches, we introduce ControlSketch, a method that
enhances SDS-based techniques by incorporating precise spatial control through
a depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across
diverse concepts, efficiently producing sketches that combine high fidelity
with a natural and visually appealing style.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://swiftsketch.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utility Engineering: Analyzing and <span class="highlight-title">Control</span>ling Emergent Value Systems in
  AIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AIs rapidly advance and become more agentic, the risk they pose is
governed not only by their capabilities but increasingly by their propensities,
including goals and values. Tracking the emergence of goals and values has
proven a longstanding problem, and despite much interest over the years it
remains unclear whether current AIs have meaningful values. We propose a
solution to this problem, leveraging the framework of utility functions to
study the internal coherence of AI preferences. Surprisingly, we find that
independently-sampled preferences in current LLMs exhibit high degrees of
structural coherence, and moreover that this emerges with scale. These findings
suggest that value systems emerge in LLMs in a meaningful sense, a finding with
broad implications. To study these emergent value systems, we propose utility
engineering as a research agenda, comprising both the analysis and control of
AI utilities. We uncover problematic and often shocking values in LLM
assistants despite existing control measures. These include cases where AIs
value themselves over humans and are anti-aligned with specific individuals. To
constrain these emergent value systems, we propose methods of utility control.
As a case study, we show how aligning utilities with a citizen assembly reduces
political biases and generalizes to new scenarios. Whether we like it or not,
value systems have already emerged in AIs, and much work remains to fully
understand and control these emergent representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CineMaster: A 3D-Aware and <span class="highlight-title">Control</span>lable Framework for Cinematic
  Text-to-<span class="highlight-title">Video</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present CineMaster, a novel framework for 3D-aware and
controllable text-to-video generation. Our goal is to empower users with
comparable controllability as professional film directors: precise placement of
objects within the scene, flexible manipulation of both objects and camera in
3D space, and intuitive layout control over the rendered frames. To achieve
this, CineMaster operates in two stages. In the first stage, we design an
interactive workflow that allows users to intuitively construct 3D-aware
conditional signals by positioning object bounding boxes and defining camera
movements within the 3D space. In the second stage, these control
signals--comprising rendered depth maps, camera trajectories and object class
labels--serve as the guidance for a text-to-video diffusion model, ensuring to
generate the user-intended video content. Furthermore, to overcome the scarcity
of in-the-wild datasets with 3D object motion and camera pose annotations, we
carefully establish an automated data annotation pipeline that extracts 3D
bounding boxes and camera trajectories from large-scale video data. Extensive
qualitative and quantitative experiments demonstrate that CineMaster
significantly outperforms existing methods and implements prominent 3D-aware
text-to-video generation. Project page: https://cinemaster-dev.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale
  Implicit Neural Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Lyu, Lipeng Ning, William Consagra, Qiang Liu, Richard J. Rushmore, Berkin Bilgic, Yogesh Rathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To develop and validate a novel image reconstruction technique using
implicit neural representations (INR) for multi-view thick-slice acquisitions
while reducing the scan time but maintaining high signal-to-noise ratio (SNR).
Methods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised
neural network-based algorithm designed to reconstruct MRI data from multi-view
thick slices, effectively reducing scan time by 2-fold while maintaining fine
anatomical details. We compare our method to both bicubic interpolation and the
current state-of-the-art regularized least-squares super-resolution
reconstruction (LS-SRR) technique. Validation is performed using ground-truth
ex-vivo monkey brain data, and we demonstrate superior reconstruction quality
across several in-vivo human datasets. Notably, we achieve the reconstruction
of a whole human brain in-vivo T2-weighted image with an unprecedented
180{\mu}m isotropic spatial resolution, accomplished in just 17 minutes of scan
time on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in
terms of reconstruction quality with 22.4% lower relative error (RE) and 7.5%
lower full-width half maximum (FWHM) indicating better preservation of fine
structural details in nearly half the scan time. Conclusion: ROVER-MRI offers
an efficient and robust approach for mesoscale MR imaging, enabling rapid,
high-resolution whole-brain scans. Its versatility holds great promise for
research applications requiring anatomical details and time-efficient imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Randomness of Low-Layer Parameters Determines Confusing Samples in Terms
  of Interaction Representations of a DNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Zhang, Lei Cheng, Qing Li, Liang Lin, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we find that the complexity of interactions encoded by a deep
neural network (DNN) can explain its generalization power. We also discover
that the confusing samples of a DNN, which are represented by non-generalizable
interactions, are determined by its low-layer parameters. In comparison, other
factors, such as high-layer parameters and network architecture, have much less
impact on the composition of confusing samples. Two DNNs with different
low-layer parameters usually have fully different sets of confusing samples,
even though they have similar performance. This finding extends the
understanding of the lottery ticket hypothesis, and well explains distinctive
representation power of different DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Light-A-<span class="highlight-title">Video</span>: Training-free <span class="highlight-title">Video</span> Relighting via Progressive Light
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Zhou, Jiazi Bu, Pengyang Ling, Pan Zhang, Tong Wu, Qidong Huang, Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Anyi Rao, Jiaqi Wang, Li Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in image relighting models, driven by large-scale
datasets and pre-trained diffusion models, have enabled the imposition of
consistent lighting. However, video relighting still lags, primarily due to the
excessive training costs and the scarcity of diverse, high-quality video
relighting datasets. A simple application of image relighting models on a
frame-by-frame basis leads to several issues: lighting source inconsistency and
relighted appearance inconsistency, resulting in flickers in the generated
videos. In this work, we propose Light-A-Video, a training-free approach to
achieve temporally smooth video relighting. Adapted from image relighting
models, Light-A-Video introduces two key techniques to enhance lighting
consistency. First, we design a Consistent Light Attention (CLA) module, which
enhances cross-frame interactions within the self-attention layers to stabilize
the generation of the background lighting source. Second, leveraging the
physical principle of light transport independence, we apply linear blending
between the source video's appearance and the relighted appearance, using a
Progressive Light Fusion (PLF) strategy to ensure smooth temporal transitions
in illumination. Experiments show that Light-A-Video improves the temporal
consistency of relighted video while maintaining the image quality, ensuring
coherent lighting transitions across frames. Project page:
https://bujiazi.github.io/light-a-video.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://bujiazi.github.io/light-a-video.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultrasound Image Generation using Latent Diffusion Models <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoit Freiche, Anthony El-Khoury, Ali Nasiri-Sarvi, Mahdi S. Hosseini, Damien Garcia, Adrian Basarab, Mathieu Boily, Hassan Rivaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models for image generation have been a subject of increasing
interest due to their ability to generate diverse, high-quality images. Image
generation has immense potential in medical imaging because open-source medical
images are difficult to obtain compared to natural images, especially for rare
conditions. The generated images can be used later to train classification and
segmentation models. In this paper, we propose simulating realistic ultrasound
(US) images by successive fine-tuning of large diffusion models on different
publicly available databases. To do so, we fine-tuned Stable Diffusion, a
state-of-the-art latent diffusion model, on BUSI (Breast US Images) an
ultrasound breast image dataset. We successfully generated high-quality US
images of the breast using simple prompts that specify the organ and pathology,
which appeared realistic to three experienced US scientists and a US
radiologist. Additionally, we provided user control by conditioning the model
with segmentations through ControlNet. We will release the source code at
http://code.sonography.ai/ to allow fast US image generation to the scientific
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages conference paper for SPIE medical imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Approach to for Multimodal E<span class="highlight-title">motion</span> Recognition : Multimodal
  semantic information fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Dai, Dequan Zheng, Feng Yu, Yanrong Zhang, Yaohui Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of artificial intelligence and computer vision
technologies, multimodal emotion recognition has become a prominent research
topic. However, existing methods face challenges such as heterogeneous data
fusion and the effective utilization of modality correlations. This paper
proposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on
the integration of contrastive learning and visual sequence compression. The
proposed method enhances cross-modal feature fusion through contrastive
learning and reduces redundancy in the visual modality by leveraging visual
sequence compression. Experimental results on two public datasets, IEMOCAP and
MELD, demonstrate that DeepMSI-MER significantly improves the accuracy and
robustness of emotion recognition, validating the effectiveness of multimodal
feature fusion and the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AR Glulam: Accurate Augmented Reality Using Multiple Fiducial Markers
  for Glulam Fabrication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Htet Kyaw, Arvin Xu, Sasa Zivkovic, Gwyllim Jahn, Cameron Newnham, Nick Van Den Berg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Augmented Reality (AR) have demonstrated applications
in architecture, design, and fabrication. Compared to conventional 2D
construction drawings, AR can be used to superimpose contextual instructions,
display 3D spatial information and enable on-site engagement. Despite the
potential of AR, the widespread adoption of the technology in the industry is
limited by its precision. Precision is important for projects requiring strict
construction tolerances, design fidelity, and fabrication feedback. For
example, the manufacturing of glulam beams requires tolerances of less than
2mm. The goal of this project is to explore the industrial application of using
multiple fiducial markers for high-precision AR fabrication. While the method
has been validated in lab settings with a precision of 0.97, this paper focuses
on fabricating glulam beams in a factory setting with an industry manufacturer,
Unalam Factory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Figures, Project Paper for Association for Computer Aided Design
  in Architecture</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Latent Progression: Individual-based Spatiotemporal Disease
  Progression on 3D Brain MRIs via Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lemuel Puglisi, Daniel C. Alexander, Daniele Ravì
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing availability of longitudinal Magnetic Resonance Imaging (MRI)
datasets has facilitated Artificial Intelligence (AI)-driven modeling of
disease progression, making it possible to predict future medical scans for
individual patients. However, despite significant advancements in AI, current
methods continue to face challenges including achieving patient-specific
individualization, ensuring spatiotemporal consistency, efficiently utilizing
longitudinal data, and managing the substantial memory demands of 3D scans. To
address these challenges, we propose Brain Latent Progression (BrLP), a novel
spatiotemporal model designed to predict individual-level disease progression
in 3D brain MRIs. The key contributions in BrLP are fourfold: (i) it operates
in a small latent space, mitigating the computational challenges posed by
high-dimensional imaging data; (ii) it explicitly integrates subject metadata
to enhance the individualization of predictions; (iii) it incorporates prior
knowledge of disease dynamics through an auxiliary model, facilitating the
integration of longitudinal data; and (iv) it introduces the Latent Average
Stabilization (LAS) algorithm, which (a) enforces spatiotemporal consistency in
the predicted progression at inference time and (b) allows us to derive a
measure of the uncertainty for the prediction. We train and evaluate BrLP on
11,730 T1-weighted (T1w) brain MRIs from 2,805 subjects and validate its
generalizability on an external test set comprising 2,257 MRIs from 962
subjects. Our experiments compare BrLP-generated MRI scans with real follow-up
MRIs, demonstrating state-of-the-art accuracy compared to existing methods. The
code is publicly available at: https://github.com/LemuelPuglisi/BrLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2405.03328</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Human</span>-Centric Foundation Models: Perception, Generation and Agentic
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixiang Tang, Yizhou Wang, Lu Chen, Yuan Wang, Sida Peng, Dan Xu, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human understanding and generation are critical for modeling digital humans
and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)
inspired by the success of generalist models, such as large language and vision
models, have emerged to unify diverse human-centric tasks into a single
framework, surpassing traditional task-specific approaches. In this survey, we
present a comprehensive overview of HcFMs by proposing a taxonomy that
categorizes current approaches into four groups: (1) Human-centric Perception
Foundation Models that capture fine-grained features for multi-modal 2D and 3D
understanding. (2) Human-centric AIGC Foundation Models that generate
high-fidelity, diverse human-related content. (3) Unified Perception and
Generation Models that integrate these capabilities to enhance both human
understanding and synthesis. (4) Human-centric Agentic Foundation Models that
extend beyond perception and generation to learn human-like intelligence and
interactive behaviors for humanoid embodied tasks. We review state-of-the-art
techniques, discuss emerging challenges and future research directions. This
survey aims to serve as a roadmap for researchers and practitioners working
towards more robust, versatile, and intelligent digital human and embodiments
modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Copula-based mixture model identification for subgroup clustering with
  imaging applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Zheng, Nicolas Duchateau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based clustering techniques have been widely applied to various
application areas, while most studies focus on canonical mixtures with unique
component distribution form. However, this strict assumption is often hard to
satisfy. In this paper, we consider the more flexible Copula-Based Mixture
Models (CBMMs) for clustering, which allow heterogeneous component
distributions composed by flexible choices of marginal and copula forms. More
specifically, we propose an adaptation of the Generalized Iterative Conditional
Estimation (GICE) algorithm to identify the CBMMs in an unsupervised manner,
where the marginal and copula forms and their parameters are estimated
iteratively. GICE is adapted from its original version developed for switching
Markov model identification with the choice of realization time. Our CBMM-GICE
clustering method is then tested on synthetic two-cluster data (N=2000 samples)
with discussion of the factors impacting its convergence. Finally, it is
compared to the Expectation Maximization identified mixture models with unique
component form on the entire MNIST database (N=70000), and on real cardiac
magnetic resonance data (N=276) to illustrate its value for imaging
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Image Quality Assessment: Insights, Analysis, and Future
  Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengqian Ma, Zhengyi Shi, Zhiqiang Lu, Shenghao Xie, Fei Chao, Yao Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality assessment (IQA) represents a pivotal challenge in
image-focused technologies, significantly influencing the advancement
trajectory of image processing and computer vision. Recently, IQA has witnessed
a notable surge in innovative research efforts, driven by the emergence of
novel architectural paradigms and sophisticated computational techniques. This
survey delivers an extensive analysis of contemporary IQA methodologies,
organized according to their application scenarios, serving as a beneficial
reference for both beginners and experienced researchers. We analyze the
advantages and limitations of current approaches and suggest potential future
research pathways. The survey encompasses both general and specific IQA
methodologies, including conventional statistical measures, machine learning
techniques, and cutting-edge deep learning models such as convolutional neural
networks (CNNs) and Transformer models. The analysis within this survey
highlights the necessity for distortion-specific IQA methods tailored to
various application scenarios, emphasizing the significance of practicality,
interpretability, and ease of implementation in future developments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao liu, Zelin Zhang, Songbai Chen, Cuihong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The properties of black holes and accretion flows can be inferred by fitting
Event Horizon Telescope (EHT) data to simulated images generated through
general relativistic ray tracing (GRRT). However, due to the computationally
intensive nature of GRRT, the efficiency of generating specific radiation flux
images needs to be improved. This paper introduces the Branch Correction
Denoising Diffusion Model (BCDDM), which uses a branch correction mechanism and
a weighted mixed loss function to improve the accuracy of generated black hole
images based on seven physical parameters of the radiatively inefficient
accretion flow (RIAF) model. Our experiments show a strong correlation between
the generated images and their physical parameters. By enhancing the GRRT
dataset with BCDDM-generated images and using ResNet50 for parameter
regression, we achieve significant improvements in parameter prediction
performance. This approach reduces computational costs and provides a faster,
more efficient method for dataset expansion, parameter estimation, and model
fitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Referring Remote Sensing Image Segmentation via Bidirectional Alignment
  Guided Joint Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiang Zhang, Zhaokun Wen, Bo Kong, Kecheng Liu, Yisi Zhang, Peixian Zhuang, Jiangyun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Remote Sensing Image Segmentation (RRSIS) is critical for
ecological monitoring, urban planning, and disaster management, requiring
precise segmentation of objects in remote sensing imagery guided by textual
descriptions. This task is uniquely challenging due to the considerable
vision-language gap, the high spatial resolution and broad coverage of remote
sensing imagery with diverse categories and small targets, and the presence of
clustered, unclear targets with blurred edges. To tackle these issues, we
propose \ours, a novel framework designed to bridge the vision-language gap,
enhance multi-scale feature interaction, and improve fine-grained object
differentiation. Specifically, \ours introduces: (1) the Bidirectional Spatial
Correlation (BSC) for improved vision-language feature alignment, (2) the
Target-Background TwinStream Decoder (T-BTD) for precise distinction between
targets and non-targets, and (3) the Dual-Modal Object Learning Strategy
(D-MOLS) for robust multimodal feature reconstruction. Extensive experiments on
the benchmark datasets RefSegRS and RRSIS-D demonstrate that \ours achieves
state-of-the-art performance. Specifically, \ours improves the overall IoU
(oIoU) by 3.76 percentage points (80.57) and 1.44 percentage points (79.23) on
the two datasets, respectively. Additionally, it outperforms previous methods
in the mean IoU (mIoU) by 5.37 percentage points (67.95) and 1.84 percentage
points (66.04), effectively addressing the core challenges of RRSIS with
enhanced precision and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mmE5: Improving Multimodal Multilingual Embeddings via High-quality
  Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal embedding models have gained significant attention for their
ability to map data from different modalities, such as text and images, into a
unified representation space. However, the limited labeled multimodal data
often hinders embedding performance. Recent approaches have leveraged data
synthesis to address this problem, yet the quality of synthetic data remains a
critical bottleneck. In this work, we identify three criteria for high-quality
synthetic multimodal data. First, broad scope ensures that the generated data
covers diverse tasks and modalities, making it applicable to various downstream
scenarios. Second, robust cross-modal alignment makes different modalities
semantically consistent. Third, high fidelity ensures that the synthetic data
maintains realistic details to enhance its reliability. Guided by these
principles, we synthesize datasets that: (1) cover a wide range of tasks,
modality combinations, and languages, (2) are generated via a deep thinking
process within a single pass of a multimodal large language model, and (3)
incorporate real-world images with accurate and relevant texts, ensuring
fidelity through self-evaluation and refinement. Leveraging these high-quality
synthetic and labeled datasets, we train a multimodal multilingual E5 model
mmE5. Extensive experiments demonstrate that mmE5 achieves state-of-the-art
performance on the MMEB Benchmark and superior multilingual performance on the
XTD benchmark. Our codes, datasets and models are released in
https://github.com/haon-chen/mmE5.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoigi Seo, Wongi Jeong, Jae-sun Seo, Se Young Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text encoders in text-to-image (T2I) diffusion models have
demonstrated exceptional performance in generating high-quality images from
textual prompts. Unlike denoising modules that rely on multiple iterative
steps, text encoders require only a single forward pass to produce text
embeddings. However, despite their minimal contribution to total inference time
and floating-point operations (FLOPs), text encoders demand significantly
higher memory usage, up to eight times more than denoising modules. To address
this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet
effective pruning strategy specifically designed for text encoders in T2I
diffusion models. Skrr exploits the inherent redundancy in transformer blocks
by selectively skipping or reusing certain layers in a manner tailored for T2I
tasks, thereby reducing memory consumption without compromising performance.
Extensive experiments demonstrate that Skrr maintains image quality comparable
to the original model even under high sparsity levels, outperforming existing
blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory
efficiency while preserving performance across multiple evaluation metrics,
including the FID, CLIP, DreamSim, and GenEval scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composite Sketch+Text Queries for Retrieving Objects with Elusive Names
  and Complex Interactions <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajwal Gatti, Kshitij Parikh, Dhriti Prasanna Paul, Manish Gupta, Anand Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-native speakers with limited vocabulary often struggle to name specific
objects despite being able to visualize them, e.g., people outside Australia
searching for numbats. Further, users may want to search for such elusive
objects with difficult-to-sketch interactions, e.g., numbat digging in the
ground. In such common but complex situations, users desire a search interface
that accepts composite multimodal queries comprising hand-drawn sketches of
difficult-to-name but easy-to-draw objects and text describing
difficult-to-sketch but easy-to-verbalize object attributes or interaction with
the scene. This novel problem statement distinctly differs from the previously
well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image
retrieval) problems. To study this under-explored task, we curate a dataset,
CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M
queries and 108K natural scene images. Further, as a solution to this problem,
we propose a pretrained multimodal transformer-based baseline, STNET
(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant
objects in the natural scene image, and encodes the text and image to perform
image retrieval. In addition to contrastive learning, we propose multiple
training objectives that improve the performance of our model. Extensive
experiments show that our proposed method outperforms several state-of-the-art
retrieval methods for text-only, sketch-only, and composite query modalities.
We make the dataset and code available at our project website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2024, 9 pages. Project Website:
  https://vl2g.github.io/projects/cstbir</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handwritten Text Recognition: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Garrido-Munoz, Antonio Rios-Vila, Jorge Calvo-Zaragoza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handwritten Text Recognition (HTR) has become an essential field within
pattern recognition and machine learning, with applications spanning historical
document preservation to modern data entry and accessibility solutions. The
complexity of HTR lies in the high variability of handwriting, which makes it
challenging to develop robust recognition systems. This survey examines the
evolution of HTR models, tracing their progression from early heuristic-based
approaches to contemporary state-of-the-art neural models, which leverage deep
learning techniques. The scope of the field has also expanded, with models
initially capable of recognizing only word-level content progressing to recent
end-to-end document-level approaches. Our paper categorizes existing work into
two primary levels of recognition: (1) \emph{up to line-level}, encompassing
word and line recognition, and (2) \emph{beyond line-level}, addressing
paragraph- and document-level challenges. We provide a unified framework that
examines research methodologies, recent advances in benchmarking, key datasets
in the field, and a discussion of the results reported in the literature.
Finally, we identify pressing research challenges and outline promising future
directions, aiming to equip researchers and practitioners with a roadmap for
advancing the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for
  Whole Slide Image Classification <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangbo Shi, Chen Li, Tieliang Gong, Yefeng Zheng, Huazhu Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple instance learning (MIL)-based framework has become the mainstream
for processing the whole slide image (WSI) with giga-pixel size and
hierarchical image context in digital pathology. However, these methods heavily
depend on a substantial number of bag-level labels and solely learn from the
original slides, which are easily affected by variations in data distribution.
Recently, vision language model (VLM)-based methods introduced the language
prior by pre-training on large-scale pathological image-text pairs. However,
the previous text prompt lacks the consideration of pathological prior
knowledge, therefore does not substantially boost the model's performance.
Moreover, the collection of such pairs and the pre-training process are very
time-consuming and source-intensive.To solve the above problems, we propose a
dual-scale vision-language multiple instance learning (ViLa-MIL) framework for
whole slide image classification. Specifically, we propose a dual-scale visual
descriptive text prompt based on the frozen large language model (LLM) to boost
the performance of VLM effectively. To transfer the VLM to process WSI
efficiently, for the image branch, we propose a prototype-guided patch decoder
to aggregate the patch features progressively by grouping similar patches into
the same prototype; for the text branch, we introduce a context-guided text
decoder to enhance the text features by incorporating the multi-granular image
contexts. Extensive studies on three multi-cancer and multi-center subtyping
datasets demonstrate the superiority of ViLa-MIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 (Updated version with corrections for typos and errors.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Frame Features Are Equal: <span class="highlight-title">Video</span>-to-4D Generation via Decoupling
  Dynamic-Static Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liying Yang, Chen Liu, Zhenwei Zhu, Ajian Liu, Hui Ma, Jian Nong, Yanyan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the generation of dynamic 3D objects from a video has shown
impressive results. Existing methods directly optimize Gaussians using whole
information in frames. However, when dynamic regions are interwoven with static
regions within frames, particularly if the static regions account for a large
proportion, existing methods often overlook information in dynamic regions and
are prone to overfitting on static regions. This leads to producing results
with blurry textures. We consider that decoupling dynamic-static features to
enhance dynamic representations can alleviate this issue. Thus, we propose a
dynamic-static feature decoupling module (DSFD). Along temporal axes, it
regards the portions of current frame features that possess significant
differences relative to reference frame features as dynamic features.
Conversely, the remaining parts are the static features. Then, we acquire
decoupled features driven by dynamic features and current frame features.
Moreover, to further enhance the dynamic representation of decoupled features
from different viewpoints and ensure accurate motion prediction, we design a
temporal-spatial similarity fusion module (TSSF). Along spatial axes, it
adaptively selects a similar information of dynamic regions. Hinging on the
above, we construct a novel approach, DS4D. Experimental results verify our
method achieves state-of-the-art (SOTA) results in video-to-4D. In addition,
the experiments on a real-world scenario dataset demonstrate its effectiveness
on the 4D scene. Our code will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdvSwap: Covert Adversarial Perturbation with High Frequency
  Info-swapping for Autonomous Driving Perception <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhao Huang, Qinfan Zhang, Jiandong Xing, Mengyue Cheng, Haiyang Yu, Yilong Ren, Xiao Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception module of Autonomous vehicles (AVs) are increasingly susceptible
to be attacked, which exploit vulnerabilities in neural networks through
adversarial inputs, thereby compromising the AI safety. Some researches focus
on creating covert adversarial samples, but existing global noise techniques
are detectable and difficult to deceive the human visual system. This paper
introduces a novel adversarial attack method, AdvSwap, which creatively
utilizes wavelet-based high-frequency information swapping to generate covert
adversarial samples and fool the camera. AdvSwap employs invertible neural
network for selective high-frequency information swapping, preserving both
forward propagation and data integrity. The scheme effectively removes the
original label data and incorporates the guidance image data, producing
concealed and robust adversarial samples. Experimental evaluations and
comparisons on the GTSRB and nuScenes datasets demonstrate that AdvSwap can
make concealed attacks on common traffic targets. The generates adversarial
samples are also difficult to perceive by humans and algorithms. Meanwhile, the
method has strong attacking robustness and attacking transferability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27th IEEE International Conference on Intelligent Transportation
  Systems (ITSC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Aware <span class="highlight-title">Human</span>-machine Collaboration in Camouflaged Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Yang, Kehan Wang, Yuhang Ming, Yong Peng, Han Yang, Qiong Chen, Wanzeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camouflaged Object Detection (COD), the task of identifying objects concealed
within their environments, has seen rapid growth due to its wide range of
practical applications. A key step toward developing trustworthy COD systems is
the estimation and effective utilization of uncertainty. In this work, we
propose a human-machine collaboration framework for classifying the presence of
camouflaged objects, leveraging the complementary strengths of computer vision
(CV) models and noninvasive brain-computer interfaces (BCIs). Our approach
introduces a multiview backbone to estimate uncertainty in CV model
predictions, utilizes this uncertainty during training to improve efficiency,
and defers low-confidence cases to human evaluation via RSVP-based BCIs during
testing for more reliable decision-making. We evaluated the framework in the
CAMO dataset, achieving state-of-the-art results with an average improvement of
4.56\% in balanced accuracy (BA) and 3.66\% in the F1 score compared to
existing methods. For the best-performing participants, the improvements
reached 7.6\% in BA and 6.66\% in the F1 score. Analysis of the training
process revealed a strong correlation between our confidence measures and
precision, while an ablation study confirmed the effectiveness of the proposed
training policy and the human-machine collaboration strategy. In general, this
work reduces human cognitive load, improves system reliability, and provides a
strong foundation for advancements in real-world COD applications and
human-computer interaction. Our code and data are available at:
https://github.com/ziyuey/Uncertainty-aware-human-machine-collaboration-in-camouflaged-object-identification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images
  with Depth and Normal Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Liu, Shuangming Zhao, Wanshou Jiang, Bingxuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With advancements in satellite imaging technology, acquiring high-resolution
multi-view satellite imagery has become increasingly accessible, enabling rapid
and location-independent ground model reconstruction. However, traditional
stereo matching methods struggle to capture fine details, and while neural
radiance fields (NeRFs) achieve high-quality reconstructions, their training
time is prohibitively long. Moreover, challenges such as low visibility of
building facades, illumination and style differences between pixels, and weakly
textured regions in satellite imagery further make it hard to reconstruct
reasonable terrain geometry and detailed building facades. To address these
issues, we propose Sat-DN, a novel framework leveraging a progressively trained
multi-resolution hash grid reconstruction architecture with explicit depth
guidance and surface normal consistency constraints to enhance reconstruction
quality. The multi-resolution hash grid accelerates training, while the
progressive strategy incrementally increases the learning frequency, using
coarse low-frequency geometry to guide the reconstruction of fine
high-frequency details. The depth and normal constraints ensure a clear
building outline and correct planar distribution. Extensive experiments on the
DFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving
state-of-the-art results in both qualitative and quantitative evaluations. The
code is available at https://github.com/costune/SatDN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hi-End-MAE: Hierarchical encoder-driven masked autoencoders are stronger
  vision learners for medical image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fenghe Tang, Qingsong Yao, Wenxin Ma, Chenxu Wu, Zihang Jiang, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation remains a formidable challenge due to the label
scarcity. Pre-training Vision Transformer (ViT) through masked image modeling
(MIM) on large-scale unlabeled medical datasets presents a promising solution,
providing both computational efficiency and model generalization for various
downstream tasks. However, current ViT-based MIM pre-training frameworks
predominantly emphasize local aggregation representations in output layers and
fail to exploit the rich representations across different ViT layers that
better capture fine-grained semantic information needed for more precise
medical downstream tasks. To fill the above gap, we hereby present Hierarchical
Encoder-driven MAE (Hi-End-MAE), a simple yet effective ViT-based pre-training
solution, which centers on two key innovations: (1) Encoder-driven
reconstruction, which encourages the encoder to learn more informative features
to guide the reconstruction of masked patches; and (2) Hierarchical dense
decoding, which implements a hierarchical decoding structure to capture rich
representations across different layers. We pre-train Hi-End-MAE on a
large-scale dataset of 10K CT scans and evaluated its performance across seven
public medical image segmentation benchmarks. Extensive experiments demonstrate
that Hi-End-MAE achieves superior transfer learning capabilities across various
downstream tasks, revealing the potential of ViT in medical imaging
applications. The code is available at:
https://github.com/FengheTan9/Hi-End-MAE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, Code: https://github.com/FengheTan9/Hi-End-MAE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foundation Models in Computational Pathology: A <span class="highlight-title">Review</span> of Challenges,
  Opportunities, and Impact 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsin Bilal,  Aadam, Manahil Raza, Youssef Altherwy, Anas Alsuhaibani, Abdulrahman Abduljabbar, Fahdah Almarshad, Paul Golding, Nasir Rajpoot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From self-supervised, vision-only models to contrastive visual-language
frameworks, computational pathology has rapidly evolved in recent years.
Generative AI "co-pilots" now demonstrate the ability to mine subtle,
sub-visual tissue cues across the cellular-to-pathology spectrum, generate
comprehensive reports, and respond to complex user queries. The scale of data
has surged dramatically, growing from tens to millions of multi-gigapixel
tissue images, while the number of trainable parameters in these models has
risen to several billion. The critical question remains: how will this new wave
of generative and multi-purpose AI transform clinical diagnostics? In this
article, we explore the true potential of these innovations and their
integration into clinical practice. We review the rapid progress of foundation
models in pathology, clarify their applications and significance. More
precisely, we examine the very definition of foundational models, identifying
what makes them foundational, general, or multipurpose, and assess their impact
on computational pathology. Additionally, we address the unique challenges
associated with their development and evaluation. These models have
demonstrated exceptional predictive and generative capabilities, but
establishing global benchmarks is crucial to enhancing evaluation standards and
fostering their widespread clinical adoption. In computational pathology, the
broader impact of frontier AI ultimately depends on widespread adoption and
societal acceptance. While direct public exposure is not strictly necessary, it
remains a powerful tool for dispelling misconceptions, building trust, and
securing regulatory support.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Screener: Self-supervised Pathology Segmentation Model for 3D Medical
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Goncharov, Eugenia Soboleva, Mariia Donskova, Ivan Oseledets, Marina Munkhoeva, Maxim Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of all pathological findings in 3D medical images
remains a significant challenge, as supervised models are limited to detecting
only the few pathology classes annotated in existing datasets. To address this,
we frame pathology segmentation as an unsupervised visual anomaly segmentation
(UVAS) problem, leveraging the inherent rarity of pathological patterns
compared to healthy ones. We enhance the existing density-based UVAS framework
with two key innovations: (1) dense self-supervised learning (SSL) for feature
extraction, eliminating the need for supervised pre-training, and (2) learned,
masking-invariant dense features as conditioning variables, replacing
hand-crafted positional encodings. Trained on over 30,000 unlabeled 3D CT
volumes, our model, Screener, outperforms existing UVAS methods on four
large-scale test datasets comprising 1,820 scans with diverse pathologies. Code
and pre-trained models will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Multimodal Spatial Relations through
  Constraint-Aware Prompting <span class="chip">NAACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Wu, Zhuo Liu, Hangfeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial relation hallucinations pose a persistent challenge in large
vision-language models (LVLMs), leading to generate incorrect predictions about
object positions and spatial configurations within an image. To address this
issue, we propose a constraint-aware prompting framework designed to reduce
spatial relation hallucinations. Specifically, we introduce two types of
constraints: (1) bidirectional constraint, which ensures consistency in
pairwise object relations, and (2) transitivity constraint, which enforces
relational dependence across multiple objects. By incorporating these
constraints, LVLMs can produce more spatially coherent and consistent outputs.
We evaluate our method on three widely-used spatial relation datasets,
demonstrating performance improvements over existing approaches. Additionally,
a systematic analysis of various bidirectional relation analysis choices and
transitivity reference selections highlights greater possibilities of our
methods in incorporating constraints to mitigate spatial relation
hallucinations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, accepted to NAACL Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BEAM: Bridging Physically-based Rendering and Gaussian Modeling for
  Relightable Volumetric <span class="highlight-title">Video</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Hong, Yize Wu, Zhehao Shen, Chengcheng Guo, Yuheng Jiang, Yingliang Zhang, Jingyi Yu, Lan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video enables immersive experiences by capturing dynamic 3D
scenes, enabling diverse applications for virtual reality, education, and
telepresence. However, traditional methods struggle with fixed lighting
conditions, while neural approaches face trade-offs in efficiency, quality, or
adaptability for relightable scenarios. To address these limitations, we
present BEAM, a novel pipeline that bridges 4D Gaussian representations with
physically-based rendering (PBR) to produce high-quality, relightable
volumetric videos from multi-view RGB footage. BEAM recovers detailed geometry
and PBR properties via a series of available Gaussian-based techniques. It
first combines Gaussian-based performance tracking with geometry-aware
rasterization in a coarse-to-fine optimization framework to recover spatially
and temporally consistent geometries. We further enhance Gaussian attributes by
incorporating PBR properties step by step. We generate roughness via a
multi-view-conditioned diffusion model, and then derive AO and base color using
a 2D-to-3D strategy, incorporating a tailored Gaussian-based ray tracer for
efficient visibility computation. Once recovered, these dynamic, relightable
assets integrate seamlessly into traditional CG pipelines, supporting real-time
rendering with deferred shading and offline rendering with ray tracing. By
offering realistic, lifelike visualizations under diverse lighting conditions,
BEAM opens new possibilities for interactive entertainment, storytelling, and
creative visualization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRISP: A Framework for Cryo-EM Image Segmentation and Processing with
  Conditional Random Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Szu-Chi Chung, Po-Cheng Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiating signals from the background in micrographs is a critical
initial step for cryogenic electron microscopy (cryo-EM), yet it remains
laborious due to low signal-to-noise ratio (SNR), the presence of contaminants
and densely packed particles of varying sizes. Although image segmentation has
recently been introduced to distinguish particles at the pixel level, the low
SNR complicates the automated generation of accurate annotations for training
supervised models. Moreover, platforms for systematically comparing different
design choices in pipeline construction are lacking. Thus, a modular framework
is essential to understand the advantages and limitations of this approach and
drive further development. To address these challenges, we present a pipeline
that automatically generates high-quality segmentation maps from cryo-EM data
to serve as ground truth labels. Our modular framework enables the selection of
various segmentation models and loss functions. We also integrate Conditional
Random Fields (CRFs) with different solvers and feature sets to refine coarse
predictions, thereby producing fine-grained segmentation. This flexibility
facilitates optimal configurations tailored to cryo-EM datasets. When trained
on a limited set of micrographs, our approach achieves over 90% accuracy,
recall, precision, Intersection over Union (IoU), and F1-score on synthetic
data. Furthermore, to demonstrate our framework's efficacy in downstream
analyses, we show that the particles extracted by our pipeline produce 3D
density maps with higher resolution than those generated by existing particle
pickers on real experimental datasets, while achieving performance comparable
to that of manually curated datasets from experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 28 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully-Geometric Cross-Attention for Point Cloud Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Wang, Guofeng Mei, Jian Zhang, Nicu Sebe, Bruno Lepri, Fabio Poiesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud registration approaches often fail when the overlap between point
clouds is low due to noisy point correspondences. This work introduces a novel
cross-attention mechanism tailored for Transformer-based architectures that
tackles this problem, by fusing information from coordinates and features at
the super-point level between point clouds. This formulation has remained
unexplored primarily because it must guarantee rotation and translation
invariance since point clouds reside in different and independent reference
frames. We integrate the Gromov-Wasserstein distance into the cross-attention
formulation to jointly compute distances between points across different point
clouds and account for their geometric structure. By doing so, points from two
distinct point clouds can attend to each other under arbitrary rigid
transformations. At the point level, we also devise a self-attention mechanism
that aggregates the local geometric structure information into point features
for fine matching. Our formulation boosts the number of inlier correspondences,
thereby yielding more precise registration results compared to state-of-the-art
approaches. We have conducted an extensive evaluation on 3DMatch, 3DLoMatch,
KITTI, and 3DCSR datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Is That Talk About? A <span class="highlight-title">Video</span>-to-Text Summarization <span class="highlight-title">Dataset</span> for
  Scientific Presentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transforming recorded videos into concise and accurate textual summaries is a
growing challenge in multimodal learning. This paper introduces VISTA, a
dataset specifically designed for video-to-text summarization in scientific
domains. VISTA contains 18,599 recorded AI conference presentations paired with
their corresponding paper abstracts. We benchmark the performance of
state-of-the-art large models and apply a plan-based framework to better
capture the structured nature of abstracts. Both human and automated
evaluations confirm that explicit planning enhances summary quality and factual
consistency. However, a considerable gap remains between models and human
performance, highlighting the challenges of scientific video summarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2306.02873 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniCoRN: Unified Commented Retrieval Network with LMMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Jaritz, Matthieu Guillaumin, Sabine Sternig, Loris Bazzani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal retrieval methods have limitations in handling complex,
compositional queries that require reasoning about the visual content of both
the query and the retrieved entities. On the other hand, Large Multimodal
Models (LMMs) can answer with language to more complex visual questions, but
without the inherent ability to retrieve relevant entities to support their
answers. We aim to address these limitations with UniCoRN, a Unified Commented
Retrieval Network that combines the strengths of composed multimodal retrieval
methods and generative language approaches, going beyond Retrieval-Augmented
Generation (RAG). We introduce an entity adapter module to inject the retrieved
multimodal entities back into the LMM, so it can attend to them while
generating answers and comments. By keeping the base LMM frozen, UniCoRN
preserves its original capabilities while being able to perform both retrieval
and text generation tasks under a single integrated framework. To assess these
new abilities, we introduce the Commented Retrieval task (CoR) and a
corresponding dataset, with the goal of retrieving an image that accurately
answers a given question and generate an additional textual response that
provides further clarification and details about the visual information. We
demonstrate the effectiveness of UniCoRN on several datasets showing
improvements of +4.5% recall over the state of the art for composed multimodal
retrieval and of +14.9% METEOR / +18.4% BEM over RAG for commenting in CoR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FloVD: Optical Flow Meets <span class="highlight-title">Video</span> Diffusion Model for Enhanced
  Camera-<span class="highlight-title">Control</span>led <span class="highlight-title">Video</span> Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjoon Jin, Qi Dai, Chong Luo, Seung-Hwan Baek, Sunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents FloVD, a novel optical-flow-based video diffusion model
for camera-controllable video generation. FloVD leverages optical flow maps to
represent motions of the camera and moving objects. This approach offers two
key benefits. Since optical flow can be directly estimated from videos, our
approach allows for the use of arbitrary training videos without ground-truth
camera parameters. Moreover, as background optical flow encodes 3D correlation
across different viewpoints, our method enables detailed camera control by
leveraging the background motion. To synthesize natural object motion while
supporting detailed camera control, our framework adopts a two-stage video
synthesis pipeline consisting of optical flow generation and flow-conditioned
video synthesis. Extensive experiments demonstrate the superiority of our
method over previous approaches in terms of accurate camera control and natural
object motion synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://jinwonjoon.github.io/flovd_site/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning <span class="highlight-title">Human</span> Skill Generators at Key-Step Levels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilu Wu, Chenhui Zhu, Shuai Wang, Hanlin Wang, Jing Wang, Zhaoxiang Zhang, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are committed to learning human skill generators at key-step levels. The
generation of skills is a challenging endeavor, but its successful
implementation could greatly facilitate human skill learning and provide more
experience for embodied intelligence. Although current video generation models
can synthesis simple and atomic human operations, they struggle with human
skills due to their complex procedure process. Human skills involve multi-step,
long-duration actions and complex scene transitions, so the existing naive
auto-regressive methods for synthesizing long videos cannot generate human
skills. To address this, we propose a novel task, the Key-step Skill Generation
(KS-Gen), aimed at reducing the complexity of generating human skill videos.
Given the initial state and a skill description, the task is to generate video
clips of key steps to complete the skill, rather than a full-length video. To
support this task, we introduce a carefully curated dataset and define multiple
evaluation metrics to assess performance. Considering the complexity of KS-Gen,
we propose a new framework for this task. First, a multimodal large language
model (MLLM) generates descriptions for key steps using retrieval argument.
Subsequently, we use a Key-step Image Generator (KIG) to address the
discontinuity between key steps in skill videos. Finally, a video generation
model uses these descriptions and key-step images to generate video clips of
the key steps with high temporal consistency. We offer a detailed analysis of
the results, hoping to provide more insights on human skill generation. All
models and data are available at https://github.com/MCG-NJU/KS-Gen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plantation Monitoring Using Drone Images: A <span class="highlight-title">Dataset</span> and Performance
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yashwanth Karumanchi, Gudala Laxmi Prasanna, Snehasis Mukherjee, Nagesh Kolagani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic monitoring of tree plantations plays a crucial role in agriculture.
Flawless monitoring of tree health helps farmers make informed decisions
regarding their management by taking appropriate action. Use of drone images
for automatic plantation monitoring can enhance the accuracy of the monitoring
process, while still being affordable to small farmers in developing countries
such as India. Small, low cost drones equipped with an RGB camera can capture
high-resolution images of agricultural fields, allowing for detailed analysis
of the well-being of the plantations. Existing methods of automated plantation
monitoring are mostly based on satellite images, which are difficult to get for
the farmers. We propose an automated system for plantation health monitoring
using drone images, which are becoming easier to get for the farmers. We
propose a dataset of images of trees with three categories: ``Good health",
``Stunted", and ``Dead". We annotate the dataset using CVAT annotation tool,
for use in research purposes. We experiment with different well-known CNN
models to observe their performance on the proposed dataset. The initial low
accuracy levels show the complexity of the proposed dataset. Further, our study
revealed that, depth-wise convolution operation embedded in a deep CNN model,
can enhance the performance of the model on drone dataset. Further, we apply
state-of-the-art object detection models to identify individual trees to better
monitor them automatically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRISHUL: Towards Region Identification and Screen Hierarchy
  Understanding for Large VLM based GUI Agents <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Singh, Shreyas Singh, Mukund Khanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Vision Language Models (LVLMs) have enabled the
development of LVLM-based Graphical User Interface (GUI) agents under various
paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle
with cross-dataset and cross-platform generalization due to their reliance on
dataset-specific training. Generalist LVLMs, such as GPT-4V, employ
Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires
metadata like HTML source, which is not consistently available across
platforms. Moreover, existing methods often specialize in singular GUI tasks
rather than achieving comprehensive GUI understanding. To address these
limitations, we introduce TRISHUL, a novel, training-free agentic framework
that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior
works that focus on either action grounding (mapping instructions to GUI
elements) or GUI referring (describing GUI elements given a location), TRISHUL
seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen
Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module,
which work synergistically to provide multi-granular, spatially, and
semantically enriched representations of GUI elements. Our results demonstrate
TRISHUL's superior performance in action grounding across the ScreenSpot,
VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring,
TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new
standard for robust and adaptable GUI comprehension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ICML 2025, 8 pages 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Take What You Need: Flexible Multi-Task Semantic Communications with
  Channel Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Chen, Shuying Gan, Chenyuan Feng, Xijun Wang, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing demand for efficient semantic communication systems capable of
managing diverse tasks and adapting to fluctuating channel conditions has
driven the development of robust, resource-efficient frameworks. This article
introduces a novel channel-adaptive and multi-task-aware semantic communication
framework based on a masked auto-encoder architecture. Our framework optimizes
the transmission of meaningful information by incorporating a multi-task-aware
scoring mechanism that identifies and prioritizes semantically significant data
across multiple concurrent tasks. A channel-aware extractor is employed to
dynamically select relevant information in response to real-time channel
conditions. By jointly optimizing semantic relevance and transmission
efficiency, the framework ensures minimal performance degradation under
resource constraints. Experimental results demonstrate the superior performance
of our framework compared to conventional methods in tasks such as image
reconstruction and object detection. These results underscore the framework's
adaptability to heterogeneous channel environments and its scalability for
multi-task applications, positioning it as a promising solution for
next-generation semantic communication networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deepfake Detection with Spatio-Temporal Consistency and Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhuo Chen, Naveed Akhtar, Nur Al Hasan Haldar, Ajmal Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfake videos are causing growing concerns among communities due to their
ever-increasing realism. Naturally, automated detection of forged Deepfake
videos is attracting a proportional amount of interest of researchers. Current
methods for detecting forged videos mainly rely on global frame features and
under-utilize the spatio-temporal inconsistencies found in the manipulated
videos. Moreover, they fail to attend to manipulation-specific subtle and
well-localized pattern variations along both spatial and temporal dimensions.
Addressing these gaps, we propose a neural Deepfake detector that focuses on
the localized manipulative signatures of the forged videos at individual frame
level as well as frame sequence level. Using a ResNet backbone, it strengthens
the shallow frame-level feature learning with a spatial attention mechanism.
The spatial stream of the model is further helped by fusing texture enhanced
shallow features with the deeper features. Simultaneously, the model processes
frame sequences with a distance attention mechanism that further allows fusion
of temporal attention maps with the learned features at the deeper layers. The
overall model is trained to detect forged content as a classifier. We evaluate
our method on two popular large data sets and achieve significant performance
over the state-of-the-art methods.Moreover, our technique also provides memory
and computational advantages over the competitive techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ActiveSSF: An Active-Learning-Guided Self-Supervised Framework for
  Long-Tailed Megakaryocyte Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linghao Zhuang, Ying Zhang, Gege Yuan, Xingyue Zhao, Zhiping Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise classification of megakaryocytes is crucial for diagnosing
myelodysplastic syndromes. Although self-supervised learning has shown promise
in medical image analysis, its application to classifying megakaryocytes in
stained slides faces three main challenges: (1) pervasive background noise that
obscures cellular details, (2) a long-tailed distribution that limits data for
rare subtypes, and (3) complex morphological variations leading to high
intra-class variability. To address these issues, we propose the ActiveSSF
framework, which integrates active learning with self-supervised pretraining.
Specifically, our approach employs Gaussian filtering combined with K-means
clustering and HSV analysis (augmented by clinical prior knowledge) for
accurate region-of-interest extraction; an adaptive sample selection mechanism
that dynamically adjusts similarity thresholds to mitigate class imbalance; and
prototype clustering on labeled samples to overcome morphological complexity.
Experimental results on clinical megakaryocyte datasets demonstrate that
ActiveSSF not only achieves state-of-the-art performance but also significantly
improves recognition accuracy for rare subtypes. Moreover, the integration of
these advanced techniques further underscores the practical potential of
ActiveSSF in clinical settings. To foster further research, the code and
datasets will be publicly released in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, submitted to EMBC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnyCharV: Bootstrap <span class="highlight-title">Control</span>lable Character <span class="highlight-title">Video</span> Generation with
  Fine-to-Coarse Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Wang, Hao Wen, Lingting Zhu, Chenming Shang, Yujiu Yang, Qi Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Character video generation is a significant real-world application focused on
producing high-quality videos featuring specific characters. Recent
advancements have introduced various control signals to animate static
characters, successfully enhancing control over the generation process.
However, these methods often lack flexibility, limiting their applicability and
making it challenging for users to synthesize a source character into a desired
target scene. To address this issue, we propose a novel framework, AnyCharV,
that flexibly generates character videos using arbitrary source characters and
target scenes, guided by pose information. Our approach involves a two-stage
training process. In the first stage, we develop a base model capable of
integrating the source character with the target scene using pose guidance. The
second stage further bootstraps controllable generation through a self-boosting
mechanism, where we use the generated video in the first stage and replace the
fine mask with the coarse one, enabling training outcomes with better
preservation of character details. Experimental results demonstrate the
effectiveness and robustness of our proposed method. Our project page is
https://anycharv.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latest Advancements Towards Catastrophic Forgetting under Data Scarcity:
  A Comprehensive <span class="highlight-title">Survey</span> on Few-Shot Class Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Anwar Ma'sum, Mahardhika Pratama, Igor Skrjanc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity significantly complicates the continual learning problem, i.e.,
how a deep neural network learns in dynamic environments with very few samples.
However, the latest progress of few-shot class incremental learning (FSCIL)
methods and related studies show insightful knowledge on how to tackle the
problem. This paper presents a comprehensive survey on FSCIL that highlights
several important aspects i.e. comprehensive and formal objectives of FSCIL
approaches, the importance of prototype rectifications, the new learning
paradigms based on pre-trained model and language-guided mechanism, the deeper
analysis of FSCIL performance metrics and evaluation, and the practical
contexts of FSCIL in various areas. Our extensive discussion presents the open
challenges, potential solutions, and future directions of FSCIL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoDynTrust: Robust Asynchronous Collaborative Perception via Dynamic
  Feature Trust Modulus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjiang Xu, Lingzhi Li, Jin Wang, Benyuan Yang, Zhiwen Wu, Xinhong Chen, Jianping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative perception, fusing information from multiple agents, can extend
perception range so as to improve perception performance. However, temporal
asynchrony in real-world environments, caused by communication delays, clock
misalignment, or sampling configuration differences, can lead to information
mismatches. If this is not well handled, then the collaborative performance is
patchy, and what's worse safety accidents may occur. To tackle this challenge,
we propose CoDynTrust, an uncertainty-encoded asynchronous fusion perception
framework that is robust to the information mismatches caused by temporal
asynchrony. CoDynTrust generates dynamic feature trust modulus (DFTM) for each
region of interest by modeling aleatoric and epistemic uncertainty as well as
selectively suppressing or retaining single-vehicle features, thereby
mitigating information mismatches. We then design a multi-scale fusion module
to handle multi-scale feature maps processed by DFTM. Compared to existing
works that also consider asynchronous collaborative perception, CoDynTrust
combats various low-quality information in temporally asynchronous scenarios
and allows uncertainty to be propagated to downstream tasks such as planning
and control. Experimental results demonstrate that CoDynTrust significantly
reduces performance degradation caused by temporal asynchrony across multiple
datasets, achieving state-of-the-art detection performance even with temporal
asynchrony. The code is available at https://github.com/CrazyShout/CoDynTrust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNNs May Determine Major Properties of Their Outputs Early, with Timing
  Possibly Driven by Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Park, Sanghyuk Chun, Byeongho Heo, Dongyoon Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper argues that deep neural networks (DNNs) mostly determine their
outputs during the early stages of inference, where biases inherent in the
model play a crucial role in shaping this process. We draw a parallel between
this phenomenon and human decision-making, which often relies on fast,
intuitive heuristics. Using diffusion models (DMs) as a case study, we
demonstrate that DNNs often make early-stage decision-making influenced by the
type and extent of bias in their design and training. Our findings offer a new
perspective on bias mitigation, efficient inference, and the interpretation of
machine learning systems. By identifying the temporal dynamics of
decision-making in DNNs, this paper aims to inspire further discussion and
research within the machine learning community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multispectral Remote Sensing for Weed Detection in West Australian
  Agricultural Lands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitian Wang, Muhammad Ibrahim, Yumeng Miao, D ustin Severtson, Atif Mansoor, Ajmal S. Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Kondinin region in Western Australia faces significant agricultural
challenges due to pervasive weed infestations, causing economic losses and
ecological impacts. This study constructs a tailored multispectral remote
sensing dataset and an end-to-end framework for weed detection to advance
precision agriculture practices. Unmanned aerial vehicles were used to collect
raw multispectral data from two experimental areas (E2 and E8) over four years,
covering 0.6046 km^{2} and ground truth annotations were created with
GPS-enabled vehicles to manually label weeds and crops. The dataset is
specifically designed for agricultural applications in Western Australia. We
propose an end-to-end framework for weed detection that includes extensive
preprocessing steps, such as denoising, radiometric calibration, image
alignment, orthorectification, and stitching. The proposed method combines
vegetation indices (NDVI, GNDVI, EVI, SAVI, MSAVI) with multispectral channels
to form classification features, and employs several deep learning models to
identify weeds based on the input features. Among these models, ResNet achieves
the highest performance, with a weed detection accuracy of 0.9213, an F1-Score
of 0.8735, an mIOU of 0.7888, and an mDC of 0.8865, validating the efficacy of
the dataset and the proposed weed detection method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, 1 table, Accepted for oral presentation at IEEE
  25th International Conference on Digital Image Computing: Techniques and
  Applications (DICTA 2024). Conference Proceeding:
  979-8-3503-7903-7/24/\$31.00 (C) 2024 IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Force Matching with Relativistic Constraints: A Physics-Inspired
  Approach to Stable and Efficient Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Cao, Bo Chen, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, Mingda Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Force Matching (ForM), a novel framework for generative
modeling that represents an initial exploration into leveraging special
relativistic mechanics to enhance the stability of the sampling process. By
incorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring
that sample velocities remain bounded within a constant limit. This constraint
serves as a fundamental mechanism for stabilizing the generative dynamics,
leading to a more robust and controlled sampling process. We provide a rigorous
theoretical analysis demonstrating that the velocity constraint is preserved
throughout the sampling procedure within the ForM framework. To validate the
effectiveness of our approach, we conduct extensive empirical evaluations. On
the \textit{half-moons} dataset, ForM significantly outperforms baseline
methods, achieving the lowest Euclidean distance loss of \textbf{0.714}, in
contrast to vanilla first-order flow matching (5.853) and first- and
second-order flow matching (5.793). Additionally, we perform an ablation study
to further investigate the impact of our velocity constraint, reaffirming the
superiority of ForM in stabilizing the generative process. The theoretical
guarantees and empirical results underscore the potential of integrating
special relativity principles into generative modeling. Our findings suggest
that ForM provides a promising pathway toward achieving stable, efficient, and
flexible generative processes. This work lays the foundation for future
advancements in high-dimensional generative modeling, opening new avenues for
the application of physical principles in machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Class Discovery in Instance Segmentation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cuong Manh Hoang, Yeejin Lee, Byeongkeun Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the task of generalized class discovery (GCD) in instance
segmentation. The goal is to discover novel classes and obtain a model capable
of segmenting instances of both known and novel categories, given labeled and
unlabeled data. Since the real world contains numerous objects with long-tailed
distributions, the instance distribution for each class is inherently
imbalanced. To address the imbalanced distributions, we propose an
instance-wise temperature assignment (ITA) method for contrastive learning and
class-wise reliability criteria for pseudo-labels. The ITA method relaxes
instance discrimination for samples belonging to head classes to enhance GCD.
The reliability criteria are to avoid excluding most pseudo-labels for tail
classes when training an instance segmentation network using pseudo-labels from
GCD. Additionally, we propose dynamically adjusting the criteria to leverage
diverse samples in the early stages while relying only on reliable
pseudo-labels in the later stages. We also introduce an efficient soft
attention module to encode object-specific representations for GCD. Finally, we
evaluate our proposed method by conducting experiments on two settings:
COCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results
demonstrate that the proposed method outperforms previous state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Riemannian Complex Hermit Positive Definite Convolution Network for
  Polarimetric SAR Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfei Shi, Mengmeng Nie, Yuke Li, Haiyan Jin, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning can learn high-level semantic features in Euclidean space
effectively for PolSAR images, while they need to covert the complex covariance
matrix into a feature vector or complex-valued vector as the network input.
However, the complex covariance matrices are essentially a complex Hermit
positive definite (HPD) matrix endowed in Riemannian manifold rather than
Euclidean space. The matrix's real and imagery parts are with the same
significance, as the imagery part represents the phase information. The matrix
vectorization will destroy the geometric structure and manifold characteristics
of complex covariance matrices. To learn complex HPD matrices directly, we
propose a Riemannian complex HPD convolution network(HPD\_CNN) for PolSAR
images. This method consists of a complex HPD unfolding network(HPDnet) and a
CV-3DCNN enhanced network. The proposed complex HPDnet defines the HPD mapping,
rectifying and the logEig layers to learn geometric features of complex
matrices. In addition, a fast eigenvalue decomposition method is designed to
reduce computation burden. Finally, a Riemannian-to-Euclidean enhanced network
is defined to enhance contextual information for classification. Experimental
results on two real PolSSAR datasets demonstrate the proposed method can
achieve superior performance than the state-of-the-art methods especially in
heterogeneous regions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Data Curation for Visual Contrastive Learning: Why Crafting
  Effective Positive and Negative Pairs Matters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shasvat Desai, Debasmita Ghose, Deep Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual contrastive learning aims to learn representations by contrasting
similar (positive) and dissimilar (negative) pairs of data samples. The design
of these pairs significantly impacts representation quality, training
efficiency, and computational cost. A well-curated set of pairs leads to
stronger representations and faster convergence. As contrastive pre-training
sees wider adoption for solving downstream tasks, data curation becomes
essential for optimizing its effectiveness. In this survey, we attempt to
create a taxonomy of existing techniques for positive and negative pair
curation in contrastive learning, and describe them in detail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LIR-LIVO: A Lightweight,Robust LiDAR/Vision/Inertial Odometry with
  Illumination-Resilient Deep Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shujie Zhou, Zihao Wang, Xinye Dai, Weiwei Song, Shengfeng Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose LIR-LIVO, a lightweight and robust
LiDAR-inertial-visual odometry system designed for challenging illumination and
degraded environments. The proposed method leverages deep learning-based
illumination-resilient features and LiDAR-Inertial-Visual Odometry (LIVO). By
incorporating advanced techniques such as uniform depth distribution of
features enabled by depth association with LiDAR point clouds and adaptive
feature matching utilizing Superpoint and LightGlue, LIR-LIVO achieves
state-of-the-art (SOTA) accuracy and robustness with low computational cost.
Experiments are conducted on benchmark datasets, including NTU-VIRAL, Hilti'22,
and R3LIVE-Dataset. The corresponding results demonstrate that our proposed
method outperforms other SOTA methods on both standard and challenging
datasets. Particularly, the proposed method demonstrates robust pose estimation
under poor ambient lighting conditions in the Hilti'22 dataset. The code of
this work is publicly accessible on GitHub to facilitate advancements in the
robotics community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Stage Segmentation of Cervical Tumors using PocketNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11456v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11456v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Awj Twam, Adrian E. Celaya, Megan C. Jacobsen, Rachel Glenn, Peng Wei, Jia Sun, Ann Klopp, Aradhana M. Venkatesan, David Fuentes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cervical cancer remains the fourth most common malignancy amongst women
worldwide.1 Concurrent chemoradiotherapy (CRT) serves as the mainstay
definitive treatment regimen for locally advanced cervical cancers and includes
external beam radiation followed by brachytherapy.2 Integral to radiotherapy
treatment planning is the routine contouring of both the target tumor at the
level of the cervix, associated gynecologic anatomy and the adjacent organs at
risk (OARs). However, manual contouring of these structures is both time and
labor intensive and associated with known interobserver variability that can
impact treatment outcomes. While multiple tools have been developed to
automatically segment OARs and the high-risk clinical tumor volume (HR-CTV)
using computed tomography (CT) images,3,4,5,6 the development of deep
learning-based tumor segmentation tools using routine T2-weighted (T2w)
magnetic resonance imaging (MRI) addresses an unmet clinical need to improve
the routine contouring of both anatomical structures and cervical cancers,
thereby increasing quality and consistency of radiotherapy planning. This work
applied a novel deep-learning model (PocketNet) to segment the cervix, vagina,
uterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecture
was evaluated, when trained on data via five-fold cross validation. PocketNet
achieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% for
tumor segmentation and 80% for organ segmentation. Validation on a publicly
available dataset from The Cancer Imaging Archive (TCIA) demonstrated the
models robustness, achieving DSC scores of 67.3% for tumor segmentation and
80.8% for organ segmentation. These results suggest that PocketNet is robust to
variations in contrast protocols, providing reliable segmentation of the
regions of interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Traveling Waves Integrate Spatial Information Into Spectral
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mozes Jacobs, Roberto C. Budzinski, Lyle Muller, Demba Ba, T. Anderson Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traveling waves are widely observed in the brain, but their precise
computational function remains unclear. One prominent hypothesis is that they
enable the transfer and integration of spatial information across neural
populations. However, few computational models have explored how traveling
waves might be harnessed to perform such integrative processing. Drawing
inspiration from the famous ``Can one hear the shape of a drum?'' problem --
which highlights how spectral modes encode geometric information -- we
introduce a set of convolutional recurrent neural networks that learn to
produce traveling waves in their hidden states in response to visual stimuli.
By applying a spectral decomposition to these wave-like activations, we obtain
a powerful new representational space that outperforms equivalently local
feed-forward networks on tasks requiring global spatial context. In particular,
we observe that traveling waves effectively expand the receptive field of
locally connected neurons, supporting long-range encoding and communication of
information. We demonstrate that models equipped with this mechanism and
spectral readouts solve visual semantic segmentation tasks demanding global
integration, where local feed-forward models fail. As a first step toward
traveling-wave-based representations in artificial networks, our findings
suggest potential efficiency benefits and offer a new framework for connecting
to biological recordings of neural activity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Learned Image Compression via Cross Window-based Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21144v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21144v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Mudgal, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, learned image compression methods have demonstrated superior
rate-distortion performance compared to traditional image compression methods.
Recent methods utilize convolutional neural networks (CNN), variational
autoencoders (VAE), invertible neural networks (INN), and transformers. Despite
their significant contributions, a main drawback of these models is their poor
performance in capturing local redundancy. Therefore, to leverage global
features along with local redundancy, we propose a CNN-based solution
integrated with a feature encoding module. The feature encoding module encodes
important features before feeding them to the CNN and then utilizes cross-scale
window-based attention, which further captures local redundancy. Cross-scale
window-based attention is inspired by the attention mechanism in transformers
and effectively enlarges the receptive field. Both the feature encoding module
and the cross-scale window-based attention module in our architecture are
flexible and can be incorporated into any other network architecture. We
evaluate our method on the Kodak and CLIC datasets and demonstrate that our
approach is effective and on par with state-of-the-art methods. Our code is
publicly available at https://github.com/prmudgal/CWAM_IC_ISVC. .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted and presented in ISVC'24. Copyrights stay with ISVC
  Our code is available at: https://github.com/prmudgal/CWAM_IC_ISVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting the Second-Order Effects of Neurons in CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04341v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04341v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We interpret the function of individual neurons in CLIP by automatically
describing them using text. Analyzing the direct effects (i.e. the flow from a
neuron through the residual stream to the output) or the indirect effects
(overall contribution) fails to capture the neurons' function in CLIP.
Therefore, we present the "second-order lens", analyzing the effect flowing
from a neuron through the later attention heads, directly to the output. We
find that these effects are highly selective: for each neuron, the effect is
significant for <2% of the images. Moreover, each effect can be approximated by
a single direction in the text-image space of CLIP. We describe neurons by
decomposing these directions into sparse sets of text representations. The sets
reveal polysemantic behavior - each neuron corresponds to multiple, often
unrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, we
mass-produce "semantic" adversarial examples by generating images with concepts
spuriously correlated to the incorrect class. Additionally, we use the
second-order effects for zero-shot segmentation, outperforming previous
methods. Our results indicate that an automated interpretation of neurons can
be used for model deception and for introducing new model capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page:
  https://yossigandelsman.github.io/clip_neurons/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive
  Modality Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models, particularly following GPT-4o, have
sparked increasing interest in developing omni-modal models capable of
understanding more modalities. While some open-source alternatives have
emerged, there is still a notable lag behind specialized single-modality models
in performance. In this paper, we present Ola, an Omni-modal language model
that achieves competitive performance across image, video, and audio
understanding compared to specialized counterparts. The core design of Ola lies
in its progressive modality alignment strategy that extends the supporting
modality of the language model progressively. Our training pipeline begins with
the most distinct modalities: image and text, then gradually expands the skill
sets of the model using speech data that connects language and audio knowledge,
and video data that connects all modalities. The progressive learning pipeline
also enables us to maintain a relatively small size of the cross-modal
alignment data, making developing omni-modal from existing vision-language
models easy and less costly. Moreover, to unlock an advanced interactive
experience like GPT-4o, we further design a sentence-wise decoding solution for
streaming speech generation. Extensive experiments demonstrate that Ola
surpasses existing open omni-modal LLMs across all modalities while achieving
highly competitive performance compared to state-of-the-art specialized models
of similar sizes. We aim to make Ola a fully open omni-modal understanding
solution to advance future research in this emerging field. Model weights,
code, and data are open-sourced at https://github.com/Ola-Omni/Ola.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Spatiotemporal Clutter Filtering of Transthoracic Echocardiographic
  Images: Leveraging Contextual Attention and Residual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13147v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13147v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Tabassian, Somayeh Akbari, Sandro Queirós, Jan D'hooge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a deep convolutional autoencoder network for filtering
reverberation clutter from transthoracic echocardiographic (TTE) image
sequences. Given the spatiotemporal nature of this type of clutter, the
filtering network employs 3D convolutional layers to suppress it throughout the
cardiac cycle. The design of the network incorporates two key features that
contribute to the effectiveness of the filter: 1) an attention mechanism for
focusing on cluttered regions and leveraging contextual information, and 2)
residual learning for preserving fine image structures. To train the network, a
diverse set of artifact patterns was simulated and superimposed onto
ultra-realistic synthetic TTE sequences from six ultrasound vendors, generating
input for the filtering network. The artifact-free sequences served as
ground-truth. Performance of the filtering network was evaluated using unseen
synthetic and in vivo artifactual sequences. Results from the in vivo dataset
confirmed the network's strong generalization capabilities, despite being
trained solely on synthetic data and simulated artifacts. The suitability of
the filtered sequences for downstream processing was assessed by computing
segmental strain curves. A significant reduction in the discrepancy between
strain profiles computed from cluttered and clutter-free segments was observed
after filtering the cluttered sequences with the proposed network. The trained
network processes a TTE sequence in a fraction of a second, enabling real-time
clutter filtering and potentially improving the precision of clinically
relevant indices derived from TTE sequences. The source code of the proposed
method and example video files of the filtering results are available at:
\href{https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main}{https://github.com/MahdiTabassian/Deep-Clutter-Filtering/tree/main}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sketched Equivariant Imaging Regularization and Deep Internal Learning
  for Inverse Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05771v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05771v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guixian Xu, Jinglai Li, Junqi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivariant Imaging (EI) regularization has become the de-facto technique for
unsupervised training of deep imaging networks, without any need of
ground-truth data. Observing that the EI-based unsupervised training paradigm
currently has significant computational redundancy leading to inefficiency in
high-dimensional applications, we propose a sketched EI regularization which
leverages the randomized sketching techniques for acceleration. We then extend
our sketched EI regularization to develop an accelerated deep internal learning
framework, Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be
efficiently applied for single-image and task-adapted reconstruction.
Additionally, for network adaptation tasks, we propose a parameter-efficient
approach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the
normalization layers. Our numerical study on X-ray CT and multi-coil MRI image
reconstruction tasks demonstrate that our approach can achieve significant
computational acceleration over standard EI-based counterpart in single-input
setting and network adaptation at test time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeSuite: Improving MLLMs for Long <span class="highlight-title">Video</span> Understanding via Grounded
  Tuning <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated impressive
performance in short video understanding. However, understanding long-form
videos still remains challenging for MLLMs. This paper proposes TimeSuite, a
collection of new designs to adapt the existing short-form video MLLMs for long
video understanding, including a simple yet efficient framework to process long
video sequence, a high-quality video dataset for grounded tuning of MLLMs, and
a carefully-designed instruction tuning task to explicitly incorporate the
grounding supervision in the traditional QA format. Specifically, based on
VideoChat, we propose our long-video MLLM, coined as VideoChat-T, by
implementing a token shuffling to compress long video tokens and introducing
Temporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of
visual representation. Meanwhile, we introduce the TimePro, a comprehensive
grounding-centric instruction tuning dataset composed of 9 tasks and 349k
high-quality grounded annotations. Notably, we design a new instruction tuning
task type, called Temporal Grounded Caption, to peform detailed video
descriptions with the corresponding time stamps prediction. This explicit
temporal location prediction will guide MLLM to correctly attend on the visual
content when generating description, and thus reduce the hallucination risk
caused by the LLMs. Experimental results demonstrate that our TimeSuite
provides a successful solution to enhance the long video understanding
capability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the
benchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T
exhibits robust zero-shot temporal grounding capabilities, significantly
outperforming the existing state-of-the-art MLLMs. After fine-tuning, it
performs on par with the traditional supervised expert models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annealed Winner-Takes-All for <span class="highlight-title">Motion</span> Forecasting <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11172v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11172v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Xu, Victor Letzelter, Mickaël Chen, Éloi Zablocki, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, motion prediction aims at forecasting the future
trajectories of nearby agents, helping the ego vehicle to anticipate behaviors
and drive safely. A key challenge is generating a diverse set of future
predictions, commonly addressed using data-driven models with Multiple Choice
Learning (MCL) architectures and Winner-Takes-All (WTA) training objectives.
However, these methods face initialization sensitivity and training
instabilities. Additionally, to compensate for limited performance, some
approaches rely on training with a large set of hypotheses, requiring a
post-selection step during inference to significantly reduce the number of
predictions. To tackle these issues, we take inspiration from annealed MCL, a
recently introduced technique that improves the convergence properties of MCL
methods through an annealed Winner-Takes-All loss (aWTA). In this paper, we
demonstrate how the aWTA loss can be integrated with state-of-the-art motion
forecasting models to enhance their performance using only a minimal set of
hypotheses, eliminating the need for the cumbersome post-selection step. Our
approach can be easily incorporated into any trajectory prediction model
normally trained using WTA and yields significant improvements. To facilitate
the application of our approach to future motion forecasting models, the code
is made publicly available: https://github.com/valeoai/MF_aWTA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, Accepted to ICRA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Next Block Prediction: <span class="highlight-title">Video</span> Generation via Semi-Autoregressive Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhuai Ren, Shuming Ma, Xu Sun, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next-Token Prediction (NTP) is a de facto approach for autoregressive (AR)
video generation, but it suffers from suboptimal unidirectional dependencies
and slow inference speed. In this work, we propose a semi-autoregressive
(semi-AR) framework, called Next-Block Prediction (NBP), for video generation.
By uniformly decomposing video content into equal-sized blocks (e.g., rows or
frames), we shift the generation unit from individual tokens to blocks,
allowing each token in the current block to simultaneously predict the
corresponding token in the next block. Unlike traditional AR modeling, our
framework employs bidirectional attention within each block, enabling tokens to
capture more robust spatial dependencies. By predicting multiple tokens in
parallel, NBP models significantly reduce the number of generation steps,
leading to faster and more efficient inference. Our model achieves FVD scores
of 103.3 on UCF101 and 25.5 on K600, outperforming the vanilla NTP model by an
average of 4.4. Furthermore, thanks to the reduced number of inference steps,
the NBP model generates 8.89 frames (128x128 resolution) per second, achieving
an 11x speedup. We also explored model scales ranging from 700M to 3B
parameters, observing significant improvements in generation quality, with FVD
scores dropping from 103.3 to 55.3 on UCF101 and from 25.5 to 19.5 on K600,
demonstrating the scalability of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://renshuhuai-andy.github.io/NBP-project/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span> on AI-Generated Media Detection: From Non-MLLM to MLLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueying Zou, Peipei Li, Zekun Li, Huaibo Huang, Xing Cui, Xuannan Liu, Chenghanyu Zhang, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of AI-generated media poses significant challenges to
information authenticity and social trust, making reliable detection methods
highly demanded. Methods for detecting AI-generated media have evolved rapidly,
paralleling the advancement of Multimodal Large Language Models (MLLMs).
Current detection approaches can be categorized into two main groups:
Non-MLLM-based and MLLM-based methods. The former employs high-precision,
domain-specific detectors powered by deep learning techniques, while the latter
utilizes general-purpose detectors based on MLLMs that integrate authenticity
verification, explainability, and localization capabilities. Despite
significant progress in this field, there remains a gap in literature regarding
a comprehensive survey that examines the transition from domain-specific to
general-purpose detection methods. This paper addresses this gap by providing a
systematic review of both approaches, analyzing them from single-modal and
multi-modal perspectives. We present a detailed comparative analysis of these
categories, examining their methodological similarities and differences.
Through this analysis, we explore potential hybrid approaches and identify key
challenges in forgery detection, providing direction for future research.
Additionally, as MLLMs become increasingly prevalent in detection tasks,
ethical and security considerations have emerged as critical global concerns.
We examine the regulatory landscape surrounding Generative AI (GenAI) across
various jurisdictions, offering valuable insights for researchers and
practitioners in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Transformer for Classification of Breast Ultrasound Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14731v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14731v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behnaz Gheflati, Hassan Rivaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical ultrasound (US) imaging has become a prominent modality for breast
cancer imaging due to its ease-of-use, low-cost and safety. In the past decade,
convolutional neural networks (CNNs) have emerged as the method of choice in
vision applications and have shown excellent potential in automatic
classification of US images. Despite their success, their restricted local
receptive field limits their ability to learn global context information.
Recently, Vision Transformer (ViT) designs that are based on self-attention
between image patches have shown great potential to be an alternative to CNNs.
In this study, for the first time, we utilize ViT to classify breast US images
using different augmentation strategies. The results are provided as
classification accuracy and Area Under the Curve (AUC) metrics, and the
performance is compared with the state-of-the-art CNNs. The results indicate
that the ViT models have comparable efficiency with or even better than the
CNNs in classification of US breast images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, Published in EMBC 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image
  Using Cross-Sectional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19604v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19604v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuelle Bourigault, Abdullah Hamdi, Amir Jamaludin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but
high-resolution scans are often slow and expensive due to extensive data
acquisition requirements. Traditional MRI reconstruction methods aim to
expedite this process by filling in missing frequency components in the
K-space, performing 3D-to-3D reconstructions that demand full 3D scans. In
contrast, we introduce X-Diffusion, a novel cross-sectional diffusion model
that reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain
inputs, achieving 2D-to-3D reconstruction from as little as a single 2D MRI
slice or few slices. A key aspect of X-Diffusion is that it models MRI data as
holistic 3D volumes during the cross-sectional training and inference, unlike
previous learning approaches that treat MRI scans as collections of 2D slices
in standard planes (coronal, axial, sagittal). We evaluated X-Diffusion on
brain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank
dataset. Our results demonstrate that X-Diffusion not only surpasses
state-of-the-art methods in quantitative accuracy (PSNR) on unseen data but
also preserves critical anatomical features such as tumor profiles, spine
curvature, and brain volume. Remarkably, the model generalizes beyond the
training domain, successfully reconstructing knee MRIs despite being trained
exclusively on brain data. Medical expert evaluations further confirm the
clinical relevance and fidelity of the generated images.To our knowledge,
X-Diffusion is the first method capable of producing detailed 3D MRIs from
highly limited 2D input data, potentially accelerating MRI acquisition and
reducing associated costs. The code is available on the project website
https://emmanuelleb985.github.io/XDiffusion/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, project website:
  https://emmanuelleb985.github.io/XDiffusion/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalViT: Analyzing Locality in Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.05707v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.05707v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, Michele Magno, Luca Benini, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to study the influence of locality mechanisms in
vision transformers. Transformers originated from machine translation and are
particularly good at modelling long-range dependencies within a long sequence.
Although the global interaction between the token embeddings could be well
modelled by the self-attention mechanism of transformers, what is lacking is a
locality mechanism for information exchange within a local region. In this
paper, locality mechanism is systematically investigated by carefully designed
controlled experiments. We add locality to vision transformers into the
feed-forward network. This seemingly simple solution is inspired by the
comparison between feed-forward networks and inverted residual blocks. The
importance of locality mechanisms is validated in two ways: 1) A wide range of
design choices (activation function, layer placement, expansion ratio) are
available for incorporating locality mechanisms and proper choices can lead to
a performance gain over the baseline, and 2) The same locality mechanism is
successfully applied to vision transformers with different architecture
designs, which shows the generalization of the locality concept. For
ImageNet2012 classification, the locality-enhanced transformers outperform the
baselines Swin-T, DeiT-T, and PVT-T by 1.0%, 2.6% and 3.1% with a negligible
increase in the number of parameters and computational effort. Code is
available at https://github.com/ofsoundof/LocalViT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on <span class="highlight-title">Video</span> Analytics in Cloud-Edge-Terminal Collaborative Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linxiao Gong, Hao Yang, Gaoyun Fang, Bobo Ju, Juncen Guo, Xiaoguang Zhu, Yan Wang, Xiping Hu, Peng Sun, Azzedine Boukerche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive growth of video data has driven the development of distributed
video analytics in cloud-edge-terminal collaborative (CETC) systems, enabling
efficient video processing, real-time inference, and privacy-preserving
analysis. Among multiple advantages, CETC systems can distribute video
processing tasks and enable adaptive analytics across cloud, edge, and terminal
devices, leading to breakthroughs in video surveillance, autonomous driving,
and smart cities. In this survey, we first analyze fundamental architectural
components, including hierarchical, distributed, and hybrid frameworks,
alongside edge computing platforms and resource management mechanisms. Building
upon these foundations, edge-centric approaches emphasize on-device processing,
edge-assisted offloading, and edge intelligence, while cloud-centric methods
leverage powerful computational capabilities for complex video understanding
and model training. Our investigation also covers hybrid video analytics
incorporating adaptive task offloading and resource-aware scheduling techniques
that optimize performance across the entire system. Beyond conventional
approaches, recent advances in large language models and multimodal integration
reveal both opportunities and challenges in platform scalability, data
protection, and system reliability. Future directions also encompass
explainable systems, efficient processing mechanisms, and advanced video
analytics, offering valuable insights for researchers and practitioners in this
dynamic field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gramian Multimodal Representation Learning and Alignment <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giordano Cicchetti, Eleonora Grassucci, Luigi Sigillo, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human perception integrates multiple modalities, such as vision, hearing, and
language, into a unified understanding of the surrounding reality. While recent
multimodal models have achieved significant progress by aligning pairs of
modalities via contrastive learning, their solutions are unsuitable when
scaling to multiple modalities. These models typically align each modality to a
designated anchor without ensuring the alignment of all modalities with each
other, leading to suboptimal performance in tasks requiring a joint
understanding of multiple modalities. In this paper, we structurally rethink
the pairwise conventional approach to multimodal learning and we present the
novel Gramian Representation Alignment Measure (GRAM), which overcomes the
above-mentioned limitations. GRAM learns and then aligns $n$ modalities
directly in the higher-dimensional space in which modality embeddings lie by
minimizing the Gramian volume of the $k$-dimensional parallelotope spanned by
the modality vectors, ensuring the geometric alignment of all modalities
simultaneously. GRAM can replace cosine similarity in any downstream method,
holding for 2 to $n$ modalities and providing more meaningful alignment with
respect to previous similarity measures. The novel GRAM-based contrastive loss
function enhances the alignment of multimodal models in the higher-dimensional
embedding space, leading to new state-of-the-art performance in downstream
tasks such as video-audio-text retrieval and audio-video classification. The
project page, the code, and the pretrained models are available at
https://ispamm.github.io/GRAM/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Visual Representation Learning with Multi-modal Prior Knowledge
  for Image Classification Under Distribution Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongkuan Zhou, Lavdim Halilaj, Sebastian Monka, Stefan Schmid, Yuqicheng Zhu, Bo Xiong, Steffen Staab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success of deep neural networks (DNNs) in computer
vision, they fail to remain high-performing when facing distribution shifts
between training and testing data. In this paper, we propose Knowledge-Guided
Visual representation learning (KGV) - a distribution-based learning approach
leveraging multi-modal prior knowledge - to improve generalization under
distribution shift. It integrates knowledge from two distinct modalities: 1) a
knowledge graph (KG) with hierarchical and association relationships; and 2)
generated synthetic images of visual elements semantically represented in the
KG. The respective embeddings are generated from the given modalities in a
common latent space, i.e., visual embeddings from original and synthetic images
as well as knowledge graph embeddings (KGEs). These embeddings are aligned via
a novel variant of translation-based KGE methods, where the node and relation
embeddings of the KG are modeled as Gaussian distributions and translations,
respectively. We claim that incorporating multi-model prior knowledge enables
more regularized learning of image representations. Thus, the models are able
to better generalize across different data distributions. We evaluate KGV on
different image classification tasks with major or minor distribution shifts,
namely road sign classification across datasets from Germany, China, and
Russia, image classification with the mini-ImageNet dataset and its variants,
as well as the DVM-CAR dataset. The results demonstrate that KGV consistently
exhibits higher accuracy and data efficiency across all experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Cross-Domain Few-Shot Visual Recognition: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08557v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08557v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huali Xu, Shuaifeng Zhi, Shuzhou Sun, Vishal M. Patel, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning excels in computer vision tasks with abundant labeled
data, its performance diminishes significantly in scenarios with limited
labeled samples. To address this, Few-shot learning (FSL) enables models to
perform the target tasks with very few labeled examples by leveraging prior
knowledge from related tasks. However, traditional FSL assumes that both the
related and target tasks come from the same domain, which is a restrictive
assumption in many real-world scenarios where domain differences are common. To
overcome this limitation, Cross-domain few-shot learning (CDFSL) has gained
attention, as it allows source and target data to come from different domains
and label spaces. This paper presents the first comprehensive review of
Cross-domain Few-shot Learning (CDFSL), a field that has received less
attention compared to traditional FSL due to its unique challenges. We aim to
provide both a position paper and a tutorial for researchers, covering key
problems, existing methods, and future research directions. The review begins
with a formal definition of CDFSL, outlining its core challenges, followed by a
systematic analysis of current approaches, organized under a clear taxonomy.
Finally, we discuss promising future directions in terms of problem setups,
applications, and theoretical advancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM Computing Surveys; 35 pages, 12 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Similarity and Quality Metrics for MR Image-To-Image Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08431v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08431v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melanie Dohmen, Mark A. Klemens, Ivo M. Baltruschat, Tuan Truong, Matthias Lenga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-image translation can create large impact in medical imaging, as
images can be synthetically transformed to other modalities, sequence types,
higher resolutions or lower noise levels. To ensure patient safety, these
methods should be validated by human readers, which requires a considerable
amount of time and costs. Quantitative metrics can effectively complement such
studies and provide reproducible and objective assessment of synthetic images.
If a reference is available, the similarity of MR images is frequently
evaluated by SSIM and PSNR metrics, even though these metrics are not or too
sensitive regarding specific distortions. When reference images to compare with
are not available, non-reference quality metrics can reliably detect specific
distortions, such as blurriness. To provide an overview on distortion
sensitivity, we quantitatively analyze 11 similarity (reference) and 12 quality
(non-reference) metrics for assessing synthetic images. We additionally include
a metric on a downstream segmentation task. We investigate the sensitivity
regarding 11 kinds of distortions and typical MR artifacts, and analyze the
influence of different normalization methods on each metric and distortion.
Finally, we derive recommendations for effective usage of the analyzed
similarity and quality metrics for evaluation of image-to-image translation
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages (main: 22 pages, 3 figures, supplement: 22 pages, 15
  figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIPeR: Visual Incremental Place Recognition with Adaptive Mining and
  Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21416v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21416v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Ming, Minyang Xu, Xingrui Yang, Weicai Ye, Weihan Wang, Yong Peng, Weichen Dai, Wanzeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual place recognition (VPR) is an essential component of many autonomous
and augmented/virtual reality systems. It enables the systems to robustly
localize themselves in large-scale environments. Existing VPR methods
demonstrate attractive performance at the cost of heavy pre-training and
limited generalizability. When deployed in unseen environments, these methods
exhibit significant performance drops. Targeting this issue, we present VIPeR,
a novel approach for visual incremental place recognition with the ability to
adapt to new environments while retaining the performance of previous
environments. We first introduce an adaptive mining strategy that balances the
performance within a single environment and the generalizability across
multiple environments. Then, to prevent catastrophic forgetting in lifelong
learning, we draw inspiration from human memory systems and design a novel
memory bank for our VIPeR. Our memory bank contains a sensory memory, a working
memory and a long-term memory, with the first two focusing on the current
environment and the last one for all previously visited environments.
Additionally, we propose a probabilistic knowledge distillation to explicitly
safeguard the previously learned knowledge. We evaluate our proposed VIPeR on
three large-scale datasets, namely Oxford Robotcar, Nordland, and TartanAir.
For comparison, we first set a baseline performance with naive finetuning.
Then, several more recent lifelong learning methods are compared. Our VIPeR
achieves better performance in almost all aspects with the biggest improvement
of 13.65% in average performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures. In IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All You Need in Knowledge Distillation Is a Tailored Coordinate System <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Zhou, Ke Zhu, Jianxin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Distillation (KD) is essential in transferring dark knowledge from
a large teacher to a small student network, such that the student can be much
more efficient than the teacher but with comparable accuracy. Existing KD
methods, however, rely on a large teacher trained specifically for the target
task, which is both very inflexible and inefficient. In this paper, we argue
that a SSL-pretrained model can effectively act as the teacher and its dark
knowledge can be captured by the coordinate system or linear subspace where the
features lie in. We then need only one forward pass of the teacher, and then
tailor the coordinate system (TCS) for the student network. Our TCS method is
teacher-free and applies to diverse architectures, works well for KD and
practical few-shot learning, and allows cross-architecture distillation with
large capacity gap. Experiments show that TCS achieves significantly higher
accuracy than state-of-the-art KD methods, while only requiring roughly half of
their training time and GPU memory costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyogon Ryu, NaHyeon Park, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread use of text-to-image diffusion models across various
tasks, their computational and memory demands limit practical applications. To
mitigate this issue, quantization of diffusion models has been explored. It
reduces memory usage and computational costs by compressing weights and
activations into lower-bit formats. However, existing methods often struggle to
preserve both image quality and text-image alignment, particularly in
lower-bit($<$ 8bits) quantization. In this paper, we analyze the challenges
associated with quantizing text-to-image diffusion models from a distributional
perspective. Our analysis reveals that activation outliers play a crucial role
in determining image quality. Additionally, we identify distinctive patterns in
cross-attention scores, which significantly affects text-image alignment. To
address these challenges, we propose Distribution-aware Group Quantization
(DGQ), a method that identifies and adaptively handles pixel-wise and
channel-wise outliers to preserve image quality. Furthermore, DGQ applies
prompt-specific logarithmic quantization scales to maintain text-image
alignment. Our method demonstrates remarkable performance on datasets such as
MS-COCO and PartiPrompts. We are the first to successfully achieve low-bit
quantization of text-to-image diffusion models without requiring additional
fine-tuning of weight quantization parameters. Code is available at
https://github.com/ugonfor/DGQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted ICLR 2025. Project page: https://ugonfor.kr/DGQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning without Forgetting for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da-Wei Zhou, Yuanhan Zhang, Yan Wang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-Incremental Learning (CIL) or continual learning is a desired
capability in the real world, which requires a learning system to adapt to new
tasks without forgetting former ones. While traditional CIL methods focus on
visual information to grasp core features, recent advances in Vision-Language
Models (VLM) have shown promising capabilities in learning generalizable
representations with the aid of textual information. However, when continually
trained with new classes, VLMs often suffer from catastrophic forgetting of
former knowledge. Applying VLMs to CIL poses two major challenges: 1) how to
adapt the model without forgetting; and 2) how to make full use of the
multi-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that
enables VLMs to learn without forgetting. To handle the first challenge, we
propose training task-specific projections based on the frozen image/text
encoders. When facing new tasks, new projections are expanded and former
projections are fixed, alleviating the forgetting of old concepts. For the
second challenge, we propose the fusion module to better utilize the
cross-modality information. By jointly adjusting visual and textual features,
the model can capture semantic information with stronger representation
ability. Extensive experiments on nine benchmark datasets validate PROOF
achieves state-of-the-art performance. Code is available at
https://github.com/zhoudw-zdw/PROOF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TPAMI. Code is available at
  https://github.com/zhoudw-zdw/PROOF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robotic Grasping of Harvested Tomato Trusses Using Vision and Online
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luuk van den Bent, Tomás Coleman, Robert Babuška
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, truss tomato weighing and packaging require significant manual
work. The main obstacle to automation lies in the difficulty of developing a
reliable robotic grasping system for already harvested trusses. We propose a
method to grasp trusses that are stacked in a crate with considerable clutter,
which is how they are commonly stored and transported after harvest. The method
consists of a deep learning-based vision system to first identify the
individual trusses in the crate and then determine a suitable grasping location
on the stem. To this end, we have introduced a grasp pose ranking algorithm
with online learning capabilities. After selecting the most promising grasp
pose, the robot executes a pinch grasp without needing touch sensors or
geometric models. Lab experiments with a robotic manipulator equipped with an
eye-in-hand RGB-D camera showed a 100% clearance rate when tasked to pick all
trusses from a pile. 93% of the trusses were successfully grasped on the first
try, while the remaining 7% required more attempts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lumina-<span class="highlight-title">Video</span>: Efficient and Flexible <span class="highlight-title">Video</span> Generation with Multi-scale
  Next-DiT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyang Liu, Shicheng Li, Yutong Liu, Zhen Li, Kai Wang, Xinyue Li, Qi Qin, Yufei Liu, Yi Xin, Zhongyu Li, Bin Fu, Chenyang Si, Yuewen Cao, Conghui He, Ziwei Liu, Yu Qiao, Qibin Hou, Hongsheng Li, Peng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have established Diffusion Transformers (DiTs) as a
dominant framework in generative modeling. Building on this success,
Lumina-Next achieves exceptional performance in the generation of
photorealistic images with Next-DiT. However, its potential for video
generation remains largely untapped, with significant challenges in modeling
the spatiotemporal complexity inherent to video data. To address this, we
introduce Lumina-Video, a framework that leverages the strengths of Next-DiT
while introducing tailored solutions for video synthesis. Lumina-Video
incorporates a Multi-scale Next-DiT architecture, which jointly learns multiple
patchifications to enhance both efficiency and flexibility. By incorporating
the motion score as an explicit condition, Lumina-Video also enables direct
control of generated videos' dynamic degree. Combined with a progressive
training scheme with increasingly higher resolution and FPS, and a multi-source
training scheme with mixed natural and synthetic data, Lumina-Video achieves
remarkable aesthetic quality and motion smoothness at high training and
inference efficiency. We additionally propose Lumina-V2A, a video-to-audio
model based on Next-DiT, to create synchronized sounds for generated videos.
Codes are released at https://www.github.com/Alpha-VLLM/Lumina-Video.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARIO: A Mixed Annotation Framework For Polyp Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Li, Yiwen Hu, Jun Wei, Zhen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing polyp segmentation models are limited by high labeling costs and the
small size of datasets. Additionally, vast polyp datasets remain underutilized
because these models typically rely on a single type of annotation. To address
this dilemma, we introduce MARIO, a mixed supervision model designed to
accommodate various annotation types, significantly expanding the range of
usable data. MARIO learns from underutilized datasets by incorporating five
forms of supervision: pixel-level, box-level, polygon-level, scribblelevel, and
point-level. Each form of supervision is associated with a tailored loss that
effectively leverages the supervision labels while minimizing the noise. This
allows MARIO to move beyond the constraints of relying on a single annotation
type. Furthermore, MARIO primarily utilizes dataset with weak and cheap
annotations, reducing the dependence on large-scale, fully annotated ones.
Experimental results across five benchmark datasets demonstrate that MARIO
consistently outperforms existing methods, highlighting its efficacy in
balancing trade-offs between different forms of supervision and maximizing
polyp segmentation performance
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ISBI 2025 4-page paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toddlers' Active Gaze Behavior Supports Self-Supervised Object Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyang Yu, Arthur Aubret, Marcel C. Raabe, Jane Yang, Chen Yu, Jochen Triesch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Toddlers learn to recognize objects from different viewpoints with almost no
supervision. Recent works argue that toddlers develop this ability by mapping
close-in-time visual inputs to similar representations while interacting with
objects. High acuity vision is only available in the central visual field,
which May explain why toddlers (much like adults) constantly move around their
gaze during such interactions. It is unclear whether/how much toddlers curate
their visual experience through these eye movements to support their learning
of object representations. In this work, we explore whether a bio-inspired
visual learning model can harness toddlers' gaze behavior during a play session
to develop view-invariant object recognition. Exploiting head-mounted eye
tracking during dyadic play, we simulate toddlers' central visual field
experience by cropping image regions centered on the gaze location. This visual
stream feeds time-based self-supervised learning algorithms. Our experiments
demonstrate that toddlers' gaze strategy supports the learning of invariant
object representations. Our analysis also reveals that the limited size of the
central visual field where acuity is high is crucial for this. We further find
that toddlers' visual experience elicits more robust representations compared
to adults', mostly because toddlers look at objects they hold themselves for
longer bouts. Overall, our work reveals how toddlers' gaze behavior supports
self-supervised learning of view-invariant object recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Unconstrained Head <span class="highlight-title">Pose</span> Estimation in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02544v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02544v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayi Zhou, Fei Jiang, Jin Yuan, Yong Rui, Hongtao Lu, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research on unconstrained in-the-wild head pose estimation suffers
from the flaws of its datasets, which consist of either numerous samples by
non-realistic synthesis or constrained collection, or small-scale natural
images yet with plausible manual annotations. This makes fully-supervised
solutions compromised due to the reliance on generous labels. To alleviate it,
we propose the first semi-supervised unconstrained head pose estimation method
SemiUHPE, which can leverage abundant easily available unlabeled head images.
Technically, we choose semi-supervised rotation regression and adapt it to the
error-sensitive and label-scarce problem of unconstrained head pose. Our method
is based on the observation that the aspect-ratio invariant cropping of wild
heads is superior to previous landmark-based affine alignment given that
landmarks of unconstrained human heads are usually unavailable, especially for
underexplored non-frontal heads. Instead of using a pre-fixed threshold to
filter out pseudo labeled heads, we propose dynamic entropy based filtering to
adaptively remove unlabeled outliers as training progresses by updating the
threshold in multiple stages. We then revisit the design of weak-strong
augmentations and improve it by devising two novel head-oriented strong
augmentations, termed pose-irrelevant cut-occlusion and pose-altering rotation
consistency respectively. Extensive experiments and ablation studies show that
SemiUHPE outperforms its counterparts greatly on public benchmarks under both
the front-range and full-range settings. Furthermore, our proposed method is
also beneficial for solving other closely related problems, including generic
object rotation regression and 3D head reconstruction, demonstrating good
versatility and extensibility. Code is in https://github.com/hnuzhy/SemiUHPE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review. Semi-Supervised Unconstrained Head Pose Estimation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TASAR: Transfer-based Attack on Skeletal Action Recognition <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02483v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02483v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfeng Diao, Baiqi Wu, Ruixuan Zhang, Ajian Liu, Xiaoshuai Hao, Xingxing Wei, Meng Wang, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeletal sequence data, as a widely employed representation of human actions,
are crucial in Human Activity Recognition (HAR). Recently, adversarial attacks
have been proposed in this area, which exposes potential security concerns, and
more importantly provides a good tool for model robustness test. Within this
research, transfer-based attack is an important tool as it mimics the
real-world scenario where an attacker has no knowledge of the target model, but
is under-explored in Skeleton-based HAR (S-HAR). Consequently, existing S-HAR
attacks exhibit weak adversarial transferability and the reason remains largely
unknown. In this paper, we investigate this phenomenon via the characterization
of the loss function. We find that one prominent indicator of poor
transferability is the low smoothness of the loss function. Led by this
observation, we improve the transferability by properly smoothening the loss
when computing the adversarial examples. This leads to the first Transfer-based
Attack on Skeletal Action Recognition, TASAR. TASAR explores the smoothened
model posterior of pre-trained surrogates, which is achieved by a new
post-train Dual Bayesian optimization strategy. Furthermore, unlike existing
transfer-based methods which overlook the temporal coherence within sequences,
TASAR incorporates motion dynamics into the Bayesian attack, effectively
disrupting the spatial-temporal coherence of S-HARs. For exhaustive evaluation,
we build the first large-scale robust S-HAR benchmark, comprising 7 S-HAR
models, 10 attack methods, 3 S-HAR datasets and 2 defense models. Extensive
results demonstrate the superiority of TASAR. Our benchmark enables easy
comparisons for future studies, with the code available in the
https://github.com/yunfengdiao/Skeleton-Robustness-Benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Diffusion Step to Real-World Super-Resolution via Flow Trajectory
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianze Li, Jiezhang Cao, Yong Guo, Wenbo Li, Yulun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have significantly advanced the development of
real-world image super-resolution (Real-ISR), but the computational cost of
multi-step diffusion models limits their application. One-step diffusion models
generate high-quality images in a one sampling step, greatly reducing
computational overhead and inference latency. However, most existing one-step
diffusion methods are constrained by the performance of the teacher model,
where poor teacher performance results in image artifacts. To address this
limitation, we propose FluxSR, a novel one-step diffusion Real-ISR technique
based on flow matching models. We use the state-of-the-art diffusion model
FLUX.1-dev as both the teacher model and the base model. First, we introduce
Flow Trajectory Distillation (FTD) to distill a multi-step flow matching model
into a one-step Real-ISR. Second, to improve image realism and address
high-frequency artifact issues in generated images, we propose TV-LPIPS as a
perceptual loss and introduce Attention Diversification Loss (ADL) as a
regularization term to reduce token similarity in transformer, thereby
eliminating high-frequency artifacts. Comprehensive experiments demonstrate
that our method outperforms existing one-step diffusion-based Real-ISR methods.
The code and model will be released at https://github.com/JianzeLi-114/FluxSR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Space-Aware Instruction Tuning: <span class="highlight-title">Dataset</span> and Benchmark for Guide Dog
  Robots Assisting the Visually Impaired <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ByungOk Han, Woo-han Yun, Beom-Su Seo, Jaehong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Guide dog robots offer promising solutions to enhance mobility and safety for
visually impaired individuals, addressing the limitations of traditional guide
dogs, particularly in perceptual intelligence and communication. With the
emergence of Vision-Language Models (VLMs), robots are now capable of
generating natural language descriptions of their surroundings, aiding in safer
decision-making. However, existing VLMs often struggle to accurately interpret
and convey spatial relationships, which is crucial for navigation in complex
environments such as street crossings. We introduce the Space-Aware Instruction
Tuning (SAIT) dataset and the Space-Aware Benchmark (SA-Bench) to address the
limitations of current VLMs in understanding physical environments. Our
automated data generation pipeline focuses on the virtual path to the
destination in 3D space and the surroundings, enhancing environmental
comprehension and enabling VLMs to provide more accurate guidance to visually
impaired individuals. We also propose an evaluation protocol to assess VLM
effectiveness in delivering walking guidance. Comparative experiments
demonstrate that our space-aware instruction-tuned model outperforms
state-of-the-art algorithms. We have fully open-sourced the SAIT dataset and
SA-Bench, along with the related code, at
https://github.com/byungokhan/Space-awareVLM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaterialFusion: High-Quality, Zero-Shot, and <span class="highlight-title">Control</span>lable Material
  Transfer with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamil Garifullin, Maxim Nikolaev, Andrey Kuznetsov, Aibek Alanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulating the material appearance of objects in images is critical for
applications like augmented reality, virtual prototyping, and digital content
creation. We present MaterialFusion, a novel framework for high-quality
material transfer that allows users to adjust the degree of material
application, achieving an optimal balance between new material properties and
the object's original features. MaterialFusion seamlessly integrates the
modified object into the scene by maintaining background consistency and
mitigating boundary artifacts. To thoroughly evaluate our approach, we have
compiled a dataset of real-world material transfer examples and conducted
complex comparative analyses. Through comprehensive quantitative evaluations
and user studies, we demonstrate that MaterialFusion significantly outperforms
existing methods in terms of quality, user control, and background
preservation. Code is available at
https://github.com/ControlGenAI/MaterialFusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing General Multimodal Capability of Vision-language Models with
  Pyramid-descent Visual Position Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanpeng Chen, Mingxiao Li, Ziyang Chen, Nan Du, Xiaolong Li, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language Models (VLMs) have shown remarkable capabilities in advancing
general artificial intelligence, yet the irrational encoding of visual
positions persists in inhibiting the models' comprehensive perception
performance across different levels of granularity. In this work, we propose
Pyramid-descent Visual Position Encoding (PyPE), a novel approach designed to
enhance the perception of visual tokens within VLMs. By assigning visual
position indexes from the periphery to the center and expanding the central
receptive field incrementally, PyPE addresses the limitations of traditional
raster-scan methods and mitigates the long-term decay effects induced by Rotary
Position Embedding (RoPE). Our method reduces the relative distance between
interrelated visual elements and instruction tokens, promoting a more rational
allocation of attention weights and allowing for a multi-granularity perception
of visual elements and countering the over-reliance on anchor tokens. Extensive
experimental evaluations demonstrate that PyPE consistently improves the
general capabilities of VLMs across various sizes. Code is available at
https://github.com/SakuraTroyChen/PyPE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding Medical Vision-Language Models with Explicit Visual Prompts:
  Framework Design and Comprehensive Exploration of Prompt Variations <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyu Zhu, Ziyuan Qin, Huahui Yi, Zekun Jiang, Qicheng Lao, Shaoting Zhang, Kang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While mainstream vision-language models (VLMs) have advanced rapidly in
understanding image level information, they still lack the ability to focus on
specific areas designated by humans. Rather, they typically rely on large
volumes of high-quality image-text paired data to learn and generate posterior
attention maps. To address this critical issue, we propose leveraging visual
prompts:simple visual markers in various forms to guide and enhance the
formation of region-specific attention. Thus, we introduce MedVP, a pioneering
framework that integrates medical entity extraction, visual prompt generation,
and dataset adaptation for visual prompt guided fine-tuning. We successfully
outperform recent state-of-the-art large models across multiple medical VQA
datasets. Extensive experiments and Human evaluation are conducted to analyze
the impact of different visual prompt forms and how they contribute to
performance improvement. The results demonstrate both the effectiveness and
clinical significance of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial Degradation-Aware and Temporal Consistent Diffusion Model for
  Compressed <span class="highlight-title">Video</span> Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu An, Xinfeng Zhang, Shijie Zhao, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to limitations of storage and bandwidth, videos stored and transmitted on
the Internet are usually low-quality with low-resolution and compression noise.
Although video super-resolution (VSR) is an efficient technique to enhance
video resolution, relatively VSR methods focus on compressed videos. Directly
applying general VSR approaches leads to the failure of improving practical
videos, especially when frames are highly compressed at a low bit rate.
Recently, diffusion models have achieved superior performance in low-level
visual tasks, and their high-realism generation capability enables them to be
applied in VSR. To synthesize more compression-lost details and refine temporal
consistency, we propose a novel Spatial Degradation-Aware and Temporal
Consistent (SDATC) diffusion model for compressed VSR. Specifically, we
introduce a distortion Control module (DCM) to modulate diffusion model inputs
and guide the generation. Next, the diffusion model executes the denoising
process for texture generation with fine-tuned spatial prompt-based
compression-aware module (PCAM) and spatio-temporal attention module (STAM).
PCAM extracts features to encode specific compression information dynamically.
STAM extends the spatial attention mechanism to a spatio-temporal dimension for
capturing temporal correlation. Extensive experimental results on benchmark
datasets demonstrate the effectiveness of the proposed modules in enhancing
compressed videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VidCRAFT3: Camera, Object, and Lighting <span class="highlight-title">Control</span> for Image-to-<span class="highlight-title">Video</span>
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent image-to-video generation methods have demonstrated success in
enabling control over one or two visual elements, such as camera trajectory or
object motion. However, these methods are unable to offer control over multiple
visual elements due to limitations in data and network efficacy. In this paper,
we introduce VidCRAFT3, a novel framework for precise image-to-video generation
that enables control over camera motion, object motion, and lighting direction
simultaneously. To better decouple control over each visual element, we propose
the Spatial Triple-Attention Transformer, which integrates lighting direction,
text, and image in a symmetric way. Since most real-world video datasets lack
lighting annotations, we construct a high-quality synthetic video dataset, the
VideoLightingDirection (VLD) dataset. This dataset includes lighting direction
annotations and objects of diverse appearance, enabling VidCRAFT3 to
effectively handle strong light transmission and reflection effects.
Additionally, we propose a three-stage training strategy that eliminates the
need for training data annotated with multiple visual elements (camera motion,
object motion, and lighting direction) simultaneously. Extensive experiments on
benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing
high-quality video content, surpassing existing state-of-the-art methods in
terms of control granularity and visual coherence. All code and data will be
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiraGe: Editable 2D Images using Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Waczyńska, Tomasz Szczepanik, Piotr Borycki, Sławomir Tadeja, Thomas Bohné, Przemysław Spurek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) approximate discrete data through
continuous functions and are commonly used for encoding 2D images. Traditional
image-based INRs employ neural networks to map pixel coordinates to RGB values,
capturing shapes, colors, and textures within the network's weights. Recently,
GaussianImage has been proposed as an alternative, using Gaussian functions
instead of neural networks to achieve comparable quality and compression. Such
a solution obtains a quality and compression ratio similar to classical INR
models but does not allow image modification. In contrast, our work introduces
a novel method, MiraGe, which uses mirror reflections to perceive 2D images in
3D space and employs flat-controlled Gaussians for precise 2D image editing.
Our approach improves the rendering quality and allows realistic image
modifications, including human-inspired perception of photos in the 3D world.
Thanks to modeling images in 3D space, we obtain the illusion of 3D-based
modification in 2D images. We also show that our Gaussian representation can be
easily combined with a physics engine to produce physics-based modification of
2D images. Consequently, MiraGe allows for better quality than the standard
approach and natural modification of 2D images
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning
  in Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyush Tiwary, Atri Guha, Subhodip Panda, Prathosh A. P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the growing concerns about privacy and regulatory compliance, it is
desirable to regulate the output of generative models. To that end, the
objective of this work is to prevent the generation of outputs containing
undesired features from a pre-trained Generative Adversarial Network (GAN)
where the underlying training data set is inaccessible. Our approach is
inspired by the observation that the parameter space of GANs exhibits
meaningful directions that can be leveraged to suppress specific undesired
features. However, such directions usually result in the degradation of the
quality of generated samples. Our proposed two-stage method, known as
'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also
maintaining the quality of generated samples. In the initial stage, we adapt a
pre-trained GAN on a set of negative samples (containing undesired features)
provided by the user. Subsequently, we train the original pre-trained GAN using
positive samples, along with a repulsion regularizer. This regularizer
encourages the learned model parameters to move away from the parameters of the
adapted model (first stage) while not degrading the generation quality. We
provide theoretical insights into the proposed method. To the best of our
knowledge, our approach stands as the first method addressing unlearning within
the realm of high-fidelity GANs (such as StyleGAN). We validate the
effectiveness of our method through comprehensive experiments, encompassing
both class-level unlearning on the MNIST and AFHQ dataset and feature-level
unlearning tasks on the CelebA-HQ dataset. Our code and implementation is
available at: https://github.com/atriguha/Adapt_Unlearn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Gaze Pattern Differences Between ASD and TD Children Using
  Internal Cluster Validity Indices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyan Shi, Haihong Zhang, Ruiqing Ding, YongWei Zhu, Wei Wang, Kenny Tsu Wei Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autism Spectrum Disorder (ASD) affects children's social and communication
abilities, with eye-tracking widely used to identify atypical gaze patterns.
While unsupervised clustering can automate the creation of areas of interest
for gaze feature extraction, the use of internal cluster validity indices, like
Silhouette Coefficient, to distinguish gaze pattern differences between ASD and
typically developing (TD) children remains underexplored. We explore whether
internal cluster validity indices can distinguish ASD from TD children.
Specifically, we apply seven clustering algorithms to gaze points and extract
63 internal cluster validity indices to reveal correlations with ASD diagnosis.
Using these indices, we train predictive models for ASD diagnosis. Experiments
on three datasets demonstrate high predictive accuracy (81\% AUC), validating
the effectiveness of these indices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAID: A Framework of Product-Centric Advertising Image Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Chen, Min Zhou, Jing Jiang, Jiale Chen, Yang Lu, Bo Xiao, Tiezheng Ge, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating visually appealing advertising images is often a labor-intensive and
time-consuming process. Is it possible to automatically generate such images
using only basic product information--specifically, a product foreground image,
taglines, and a target size? Existing methods mainly focus on parts of the
problem and fail to provide a comprehensive solution. To address this gap, we
propose a novel multistage framework called Product-Centric Advertising Image
Design (PAID). It consists of four sequential stages to highlight product
foregrounds and taglines while achieving overall image aesthetics: prompt
generation, layout generation, background image generation, and graphics
rendering. Different expert models are designed and trained for the first three
stages: First, we use a visual language model (VLM) to generate background
prompts that match the products. Next, a VLM-based layout generation model
arranges the placement of product foregrounds, graphic elements (taglines and
decorative underlays), and various nongraphic elements (objects from the
background prompt). Following this, we train an SDXL-based image generation
model that can simultaneously accept prompts, layouts, and foreground controls.
To support the PAID framework, we create corresponding datasets with over
50,000 labeled images. Extensive experimental results and online A/B tests
demonstrate that PAID can produce more visually appealing advertising images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAM-DiffSR: Structure-Modulated Diffusion Model for Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengcheng Wang, Zhiwei Hao, Yehui Tang, Jianyuan Guo, Yujie Yang, Kai Han, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based super-resolution (SR) models have recently garnered
significant attention due to their potent restoration capabilities. But
conventional diffusion models perform noise sampling from a single
distribution, constraining their ability to handle real-world scenes and
complex textures across semantic regions. With the success of segment anything
model (SAM), generating sufficiently fine-grained region masks can enhance the
detail recovery of diffusion-based SR model. However, directly integrating SAM
into SR models will result in much higher computational cost. In this paper, we
propose the SAM-DiffSR model, which can utilize the fine-grained structure
information from SAM in the process of sampling noise to improve the image
quality without additional computational cost during inference. In the process
of training, we encode structural position information into the segmentation
mask from SAM. Then the encoded mask is integrated into the forward diffusion
process by modulating it to the sampled noise. This adjustment allows us to
independently adapt the noise mean within each corresponding segmentation area.
The diffusion model is trained to estimate this modulated noise. Crucially, our
proposed framework does NOT change the reverse diffusion process and does NOT
require SAM at inference. Experimental results demonstrate the effectiveness of
our proposed method, showcasing superior performance in suppressing artifacts,
and surpassing existing diffusion-based methods by 0.74 dB at the maximum in
terms of PSNR on DIV2K dataset. The code and dataset are available at
https://github.com/lose4578/SAM-DiffSR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Experience Replay Facilitates Safety Red-Teaming of
  Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Yi Chin, Mario Fritz, Pin-Yu Chen, Wei-Chen Chiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models have shown remarkable progress, but their
potential to generate harmful content remains a critical concern in the ML
community. While various safety mechanisms have been developed, the field lacks
systematic tools for evaluating their effectiveness against real-world misuse
scenarios. In this work, we propose ICER, a novel red-teaming framework that
leverages Large Language Models (LLMs) and a bandit optimization-based
algorithm to generate interpretable and semantic meaningful problematic prompts
by learning from past successful red-teaming attempts. Our ICER efficiently
probes safety mechanisms across different T2I models without requiring internal
access or additional training, making it broadly applicable to deployed
systems. Through extensive experiments, we demonstrate that ICER significantly
outperforms existing prompt attack methods in identifying model vulnerabilities
while maintaining high semantic similarity with intended content. By uncovering
that successful jailbreaking instances can systematically facilitate the
discovery of new vulnerabilities, our work provides crucial insights for
developing more robust safety mechanisms in T2I systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion
  Models and Large Language Models <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02252v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02252v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Ma, Yonglin Deng, Chen Chen, Nanyang Du, Haonan Lu, Zhenyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Posters play a crucial role in marketing and advertising by enhancing visual
communication and brand visibility, making significant contributions to
industrial design. With the latest advancements in controllable T2I diffusion
models, increasing research has focused on rendering text within synthesized
images. Despite improvements in text rendering accuracy, the field of automatic
poster generation remains underexplored. In this paper, we propose an automatic
poster generation framework with text rendering capabilities leveraging LLMs,
utilizing a triple-cross attention mechanism based on alignment learning. This
framework aims to create precise poster text within a detailed contextual
background. Additionally, the framework supports controllable fonts, adjustable
image resolution, and the rendering of posters with descriptions and text in
both English and Chinese.Furthermore, we introduce a high-resolution font
dataset and a poster dataset with resolutions exceeding 1024 pixels. Our
approach leverages the SDXL architecture. Extensive experiments validate our
method's capability in generating poster images with complex and contextually
rich backgrounds.Codes is available at
https://github.com/OPPO-Mente-Lab/GlyphDraw2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Inbetweening: Adapting Image-to-<span class="highlight-title">Video</span> Models for Keyframe
  Interpolation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, Steven M. Seitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for generating video sequences with coherent motion
between a pair of input key frames. We adapt a pretrained large-scale
image-to-video diffusion model (originally trained to generate videos moving
forward in time from a single input image) for key frame interpolation, i.e.,
to produce a video in between two input frames. We accomplish this adaptation
through a lightweight fine-tuning technique that produces a version of the
model that instead predicts videos moving backwards in time from a single input
image. This model (along with the original forward-moving model) is
subsequently used in a dual-directional diffusion sampling process that
combines the overlapping model estimates starting from each of the two
keyframes. Our experiments show that our method outperforms both existing
diffusion-based methods and traditional frame interpolation techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025; Project page:
  https://svd-keyframe-interpolation.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safety at Scale: A Comprehensive <span class="highlight-title">Survey</span> of Large Model Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjun Ma, Yifeng Gao, Yixu Wang, Ruofan Wang, Xin Wang, Ye Sun, Yifan Ding, Hengyuan Xu, Yunhao Chen, Yunhan Zhao, Hanxun Huang, Yige Li, Jiaming Zhang, Xiang Zheng, Yang Bai, Zuxuan Wu, Xipeng Qiu, Jingfeng Zhang, Yiming Li, Jun Sun, Cong Wang, Jindong Gu, Baoyuan Wu, Siheng Chen, Tianwei Zhang, Yang Liu, Mingming Gong, Tongliang Liu, Shirui Pan, Cihang Xie, Tianyu Pang, Yinpeng Dong, Ruoxi Jia, Yang Zhang, Shiqing Ma, Xiangyu Zhang, Neil Gong, Chaowei Xiao, Sarah Erfani, Bo Li, Masashi Sugiyama, Dacheng Tao, James Bailey, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large models, driven by their exceptional abilities
in learning and generalization through large-scale pre-training, has reshaped
the landscape of Artificial Intelligence (AI). These models are now
foundational to a wide range of applications, including conversational AI,
recommendation systems, autonomous driving, content generation, medical
diagnostics, and scientific discovery. However, their widespread deployment
also exposes them to significant safety risks, raising concerns about
robustness, reliability, and ethical implications. This survey provides a
systematic review of current safety research on large models, covering Vision
Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language
Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models
(DMs), and large-model-based Agents. Our contributions are summarized as
follows: (1) We present a comprehensive taxonomy of safety threats to these
models, including adversarial attacks, data poisoning, backdoor attacks,
jailbreak and prompt injection attacks, energy-latency attacks, data and model
extraction attacks, and emerging agent-specific threats. (2) We review defense
strategies proposed for each type of attacks if available and summarize the
commonly used datasets and benchmarks for safety research. (3) Building on
this, we identify and discuss the open challenges in large model safety,
emphasizing the need for comprehensive safety evaluations, scalable and
effective defense mechanisms, and sustainable data practices. More importantly,
we highlight the necessity of collective efforts from the research community
and international collaboration. Our work can serve as a useful reference for
researchers and practitioners, fostering the ongoing development of
comprehensive defense systems and platforms to safeguard AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 3 figures, 11 tables GitHub:
  https://github.com/xingjunm/Awesome-Large-Model-Safety</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against
  Jailbreak Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhan Zhao, Xiang Zheng, Lin Luo, Yige Li, Xingjun Ma, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on black-box defense for VLMs against jailbreak
attacks. Existing black-box defense methods are either unimodal or bimodal.
Unimodal methods enhance either the vision or language module of the VLM, while
bimodal methods robustify the model through text-image representation
realignment. However, these methods suffer from two limitations: 1) they fail
to fully exploit the cross-modal information, or 2) they degrade the model
performance on benign inputs. To address these limitations, we propose a novel
blue-team method BlueSuffix that defends target VLMs against jailbreak attacks
without compromising its performance under black-box setting. BlueSuffix
includes three key components: 1) a visual purifier against jailbreak images,
2) a textual purifier against jailbreak texts, and 3) a blue-team suffix
generator using reinforcement fine-tuning for enhancing cross-modal robustness.
We empirically show on four VLMs (LLaVA, MiniGPT-4, InstructionBLIP, and
Gemini) and four safety benchmarks (Harmful Instruction, AdvBench,
MM-SafetyBench, and RedTeam-2K) that BlueSuffix outperforms the baseline
defenses by a significant margin. Our BlueSuffix opens up a promising direction
for defending VLMs against jailbreak attacks. Code is available at
https://github.com/Vinsonzyh/BlueSuffix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DriveGPT: Scaling Autoregressive Behavior Models for Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Huang, Eric M. Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David S. Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, Xiaobai Chen, Chingiz Tairbekov, Pratik Agarwal, Tianshi Gao, Yuning Chai, Siddhartha Srinivasa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DriveGPT, a scalable behavior model for autonomous driving. We
model driving as a sequential decision-making task, and learn a transformer
model to predict future agent states as tokens in an autoregressive fashion. We
scale up our model parameters and training data by multiple orders of
magnitude, enabling us to explore the scaling properties in terms of dataset
size, model parameters, and compute. We evaluate DriveGPT across different
scales in a planning task, through both quantitative metrics and qualitative
examples, including closed-loop driving in complex real-world scenarios. In a
separate prediction task, DriveGPT outperforms state-of-the-art baselines and
exhibits improved performance by pretraining on a large-scale dataset, further
validating the benefits of data scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 16 figures, 8 tables, and 1 video link</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Observe Then Act: Asynchronous Active Vision-Action Model for Robotic
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14891v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14891v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guokang Wang, Hang Li, Shuyuan Zhang, Di Guo, Yanhong Liu, Huaping Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, many robotic manipulation tasks are hindered by
occlusions and limited fields of view, posing significant challenges for
passive observation-based models that rely on fixed or wrist-mounted cameras.
In this paper, we investigate the problem of robotic manipulation under limited
visual observation and propose a task-driven asynchronous active vision-action
model.Our model serially connects a camera Next-Best-View (NBV) policy with a
gripper Next-Best Pose (NBP) policy, and trains them in a sensor-motor
coordination framework using few-shot reinforcement learning. This approach
allows the agent to adjust a third-person camera to actively observe the
environment based on the task goal, and subsequently infer the appropriate
manipulation actions.We trained and evaluated our model on 8
viewpoint-constrained tasks in RLBench. The results demonstrate that our model
consistently outperforms baseline algorithms, showcasing its effectiveness in
handling visual constraints in manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Gaussian Splatting as Markov Chain Monte Carlo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09591v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09591v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While 3D Gaussian Splatting has recently become popular for neural rendering,
current methods rely on carefully engineered cloning and splitting strategies
for placing Gaussians, which can lead to poor-quality renderings, and reliance
on a good initialization. In this work, we rethink the set of 3D Gaussians as a
random sample drawn from an underlying probability distribution describing the
physical representation of the scene-in other words, Markov Chain Monte Carlo
(MCMC) samples. Under this view, we show that the 3D Gaussian updates can be
converted as Stochastic Gradient Langevin Dynamics (SGLD) updates by simply
introducing noise. We then rewrite the densification and pruning strategies in
3D Gaussian Splatting as simply a deterministic state transition of MCMC
samples, removing these heuristics from the framework. To do so, we revise the
'cloning' of Gaussians into a relocalization scheme that approximately
preserves sample probability. To encourage efficient use of Gaussians, we
introduce a regularizer that promotes the removal of unused Gaussians. On
various standard evaluation scenes, we show that our method provides improved
rendering quality, easy control over the number of Gaussians, and robustness to
initialization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmbodiedSAM: Online Segment Any 3D Thing in Real Time <span class="chip">ICLR25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11811v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11811v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied tasks require the agent to fully understand 3D scenes simultaneously
with its exploration, so an online, real-time, fine-grained and
highly-generalized 3D perception model is desperately needed. Since
high-quality 3D data is limited, directly training such a model in 3D is almost
infeasible. Meanwhile, vision foundation models (VFM) has revolutionized the
field of 2D computer vision with superior performance, which makes the use of
VFM to assist embodied 3D perception a promising direction. However, most
existing VFM-assisted 3D perception methods are either offline or too slow that
cannot be applied in practical embodied tasks. In this paper, we aim to
leverage Segment Anything Model (SAM) for real-time 3D instance segmentation in
an online setting. This is a challenging problem since future frames are not
available in the input streaming RGB-D video, and an instance may be observed
in several frames so object matching between frames is required. To address
these challenges, we first propose a geometric-aware query lifting module to
represent the 2D masks generated by SAM by 3D-aware queries, which is then
iteratively refined by a dual-level query decoder. In this way, the 2D masks
are transferred to fine-grained shapes on 3D point clouds. Benefit from the
query representation for 3D masks, we can compute the similarity matrix between
the 3D masks from different views by efficient matrix operation, which enables
real-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan
show our method achieves leading performance even compared with offline
methods. Our method also demonstrates great generalization ability in several
zero-shot dataset transferring experiments and show great potential in
open-vocabulary and data-efficient setting. Code and demo are available at
https://xuxw98.github.io/ESAM/, with only one RTX 3090 GPU required for
training and evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR25 Oral. Project page: https://xuxw98.github.io/ESAM/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Calibration by Gaining Aware of Prediction Correctness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13016v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13016v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchi Liu, Lei Wang, Yuli Zou, James Zou, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model calibration aims to align confidence with prediction correctness. The
Cross-Entropy (CE) loss is widely used for calibrator training, which enforces
the model to increase confidence on the ground truth class. However, we find
the CE loss has intrinsic limitations. For example, for a narrow
misclassification (e.g., a test sample is wrongly classified and its softmax
score on the ground truth class is 0.4), a calibrator trained by the CE loss
often produces high confidence on the wrongly predicted class, which is
undesirable. In this paper, we propose a new post-hoc calibration objective
derived from the aim of calibration. Intuitively, the proposed objective
function asks that the calibrator decrease model confidence on wrongly
predicted samples and increase confidence on correctly predicted samples.
Because a sample itself has insufficient ability to indicate correctness, we
use its transformed versions (e.g., rotated, greyscaled, and color-jittered)
during calibrator training. Trained on an in-distribution validation set and
tested with isolated, individual test samples, our method achieves competitive
calibration performance on both in-distribution and out-of-distribution test
sets compared with the state of the art. Further, our analysis points out the
difference between our method and commonly used objectives such as CE loss and
Mean Square Error (MSE) loss, where the latters sometimes deviates from the
calibration aim.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparsity Meets Similarity: Leveraging Long-Tail Distribution for Dynamic
  Optimized Token Representation in Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaotong Yu, Yi Chen, Jian Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multimodal large language models (MM-LLMs) have achieved
significant success in various tasks, but their high computational costs limit
widespread application. The main computational burden arises from processing
concatenated text and visual tokens in the LLM layer, where input token length
directly affects efficiency. Our analysis of visual tokens reveals that their
similarity to the CLS token follows a long-tail distribution, with only a few
showing high similarity. To address this, we propose a dynamic pruning
algorithm that identifies the inflection point in the visual CLS token
similarity curve, enabling effective trimming of visual markers to accelerate
model performance. Additionally, we perform a second round of pruning in the
LLM layer, filtering out low-correlation tokens through the interaction between
visual and textual features. Experimental results demonstrate that our method
achieves performance comparable to the original while utilizing only 22% of the
original token quantity. Our source code will be made publicly available upon
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Multi-Teacher Knowledge Distillation for Real-Time Object
  Detection using 4D Radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seung-Hyun Song, Dong-Hee Paek, Minh-Quan Dao, Ezio Malis, Seung-Hyun Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D object detection is crucial for safe autonomous navigation,
requiring reliable performance across diverse weather conditions. While LiDAR
performance deteriorates in challenging weather, Radar systems maintain their
reliability. Traditional Radars have limitations due to their lack of elevation
data, but the recent 4D Radars overcome this by measuring elevation alongside
range, azimuth, and Doppler velocity, making them invaluable for autonomous
vehicles. The primary challenge in utilizing 4D Radars is the sparsity of their
point clouds. Previous works address this by developing architectures that
better capture semantics and context in sparse point cloud, largely drawing
from LiDAR-based approaches. However, these methods often overlook a unique
advantage of 4D Radars: the dense Radar tensor, which encapsulates power
measurements across three spatial dimensions and the Doppler dimension. Our
paper leverages this tensor to tackle the sparsity issue. We introduce a novel
knowledge distillation framework that enables a student model to densify its
sparse input in the latent space by emulating an ensemble of teacher models.
Our experiments demonstrate a 25% performance improvement over the
state-of-the-art RTNH model on the K-Radar dataset. Notably, this improvement
is achieved while still maintaining a real-time inference speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arxiv preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Appearance Particle Neural Radiance Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ancheng Lin, Yusheng Xiang, Jun Li, Mukesh Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRFs) have shown great potential in modeling 3D
scenes. Dynamic NeRFs extend this model by capturing time-varying elements,
typically using deformation fields. The existing dynamic NeRFs employ a similar
Eulerian representation for both light radiance and deformation fields. This
leads to a close coupling of appearance and motion and lacks a physical
interpretation. In this work, we propose Dynamic Appearance Particle Neural
Radiance Field (DAP-NeRF), which introduces particle-based representation to
model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists
of the superposition of a static field and a dynamic field. The dynamic field
is quantized as a collection of appearance particles, which carries the visual
information of a small dynamic element in the scene and is equipped with a
motion model. All components, including the static field, the visual features
and the motion models of particles, are learned from monocular videos without
any prior geometric knowledge of the scene. We develop an efficient
computational framework for the particle-based model. We also construct a new
dataset to evaluate motion modeling. Experimental results show that DAP-NeRF is
an effective technique to capture not only the appearance but also the
physically meaningful motions in a 3D dynamic scene. Code is available at:
https://github.com/Cenbylin/DAP-NeRF.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">263</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Reversible Solver for Diffusion SDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zander W. Blasingame, Chen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have quickly become the state-of-the-art for generation
tasks across many different data modalities. An important ability of diffusion
models is the ability to encode samples from the data distribution back into
the sampling prior distribution. This is useful for performing alterations to
real data samples along with guided generation via the continuous adjoint
equations. We propose an algebraically reversible solver for diffusion SDEs
that can exactly invert real data samples into the prior distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLayer-FL: A Principled Approach to Personalized Layer-wise Cross-Silo
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Elhussein, Gamze Gürsoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-identically distributed data is a major challenge in Federated Learning
(FL). Personalized FL tackles this by balancing local model adaptation with
global model consistency. One variant, partial FL, leverages the observation
that early layers learn more transferable features by federating only early
layers. However, current partial FL approaches use predetermined,
architecture-specific rules to select layers, limiting their applicability. We
introduce Principled Layer-wise-FL (PLayer-FL), which uses a novel federation
sensitivity metric to identify layers that benefit from federation. This
metric, inspired by model pruning, quantifies each layer's contribution to
cross-client generalization after the first training epoch, identifying a
transition point in the network where the benefits of federation diminish. We
first demonstrate that our federation sensitivity metric shows strong
correlation with established generalization measures across diverse
architectures. Next, we show that PLayer-FL outperforms existing FL algorithms
on a range of tasks, also achieving more uniform performance improvements
across clients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Data-Centric AI: Tabular Learning from Reinforcement
  Learning and Generative AI Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangyang Ying, Cong Wei, Nanxu Gong, Xinyuan Wang, Haoyue Bai, Arun Vignesh Malarkkan, Sixun Dong, Dongjie Wang, Denghui Zhang, Yanjie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data is one of the most widely used data formats across various
domains such as bioinformatics, healthcare, and marketing. As artificial
intelligence moves towards a data-centric perspective, improving data quality
is essential for enhancing model performance in tabular data-driven
applications. This survey focuses on data-driven tabular data optimization,
specifically exploring reinforcement learning (RL) and generative approaches
for feature selection and feature generation as fundamental techniques for
refining data spaces. Feature selection aims to identify and retain the most
informative attributes, while feature generation constructs new features to
better capture complex data patterns. We systematically review existing
generative methods for tabular data engineering, analyzing their latest
advancements, real-world applications, and respective strengths and
limitations. This survey emphasizes how RL-based and generative techniques
contribute to the automation and intelligence of feature engineering. Finally,
we summarize the existing challenges and discuss future research directions,
aiming to provide insights that drive continued innovation in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DejAIvu: Identifying and Explaining AI Art on the Web in Real-Time with
  Saliency Maps <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jocelyn Dzuong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent surge in advanced generative models, such as diffusion models and
generative adversarial networks (GANs), has led to an alarming rise in
AI-generated images across various domains on the web. While such technologies
offer benefits such as democratizing artistic creation, they also pose
challenges in misinformation, digital forgery, and authenticity verification.
Additionally, the uncredited use of AI-generated images in media and marketing
has sparked significant backlash from online communities. In response to this,
we introduce DejAIvu, a Chrome Web extension that combines real-time
AI-generated image detection with saliency-based explainability while users
browse the web. Using an ONNX-optimized deep learning model, DejAIvu
automatically analyzes images on websites such as Google Images, identifies
AI-generated content using model inference, and overlays a saliency heatmap to
highlight AI-related artifacts. Our approach integrates efficient in-browser
inference, gradient-based saliency analysis, and a seamless user experience,
ensuring that AI detection is both transparent and interpretable. We also
evaluate DejAIvu across multiple pretrained architectures and benchmark
datasets, demonstrating high accuracy and low latency, making it a practical
and deployable tool for enhancing AI image accountability. The code for this
system can be found at https://github.com/Noodulz/dejAIvu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, submitted to IJCAI 2025 demo track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A First-order Generative Bilevel Optimization Framework for Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Xiao, Hui Yuan, A F M Saif, Gaowen Liu, Ramana Kompella, Mengdi Wang, Tianyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models, which iteratively denoise data samples to synthesize
high-quality outputs, have achieved empirical success across domains. However,
optimizing these models for downstream tasks often involves nested bilevel
structures, such as tuning hyperparameters for fine-tuning tasks or noise
schedules in training dynamics, where traditional bilevel methods fail due to
the infinite-dimensional probability space and prohibitive sampling costs. We
formalize this challenge as a generative bilevel optimization problem and
address two key scenarios: (1) fine-tuning pre-trained models via an
inference-only lower-level solver paired with a sample-efficient gradient
estimator for the upper level, and (2) training diffusion models from scratch
with noise schedule optimization by reparameterizing the lower-level problem
and designing a computationally tractable gradient estimator. Our first-order
bilevel framework overcomes the incompatibility of conventional bilevel methods
with diffusion processes, offering theoretical grounding and computational
practicality. Experiments demonstrate that our method outperforms existing
fine-tuning and hyperparameter search baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data
  Volume Variation in DNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan He, Anderson Truong, Yingqi Cao, Jason Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of deep neural networks (DNNs) has driven a boom in AI services,
which results in an increased demand for computing power and memory. In modern
DNNs, the data sizes produced and consumed are highly varied across operations
(high data volume variation, HDV). Because existing design paradigms use fixed
execution patterns that lead to either low computational efficiency due to
pipeline stalls or frequent off-chip memory accesses to manage large
intermediate data, HDV applications are challenging to accelerate on FPGAs. To
address these challenges, we introduce the Inter-Task Auto-Reconfigurable
Accelerator (InTAR), a novel accelerator design for HDV applications on FPGAs.
InTAR combines the high computational efficiency of sequential execution with
the reduced off-chip memory overhead of dataflow execution. It switches
execution patterns automatically with a static schedule determined before
circuit design based on resource constraints and model parameters. Unlike
previous reconfigurable accelerators, InTAR encodes reconfiguration schedules
during circuit design, allowing model-specific optimizations that allocate only
the necessary logic and interconnects. Thus, InTAR achieves a high clock
frequency with fewer resources and low reconfiguration time. Furthermore, InTAR
supports high-level tools such as HLS for fast design generation. We implement
a set of multi-task kernels in various HDV DNNs using InTAR. Compared with
dataflow and sequential accelerators, InTAR exhibits $1.8\times$ and $7.1
\times$ speedups correspondingly. We also implement InTAR for GPT-2 medium as a
more complex example, which achieves a speedup of $\mathbf{3.65 \sim
39.14\times}$ and a $\mathbf{1.72 \sim 10.44\times}$ boost in DSP efficiency
compared to the corresponding SoTA accelerators on FPGAs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLOVER: A Test Case Generation Benchmark with Coverage, Long-Context,
  and Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Xu, Bo Pang, Jin Qu, Hiroaki Hayashi, Caiming Xiong, Yingbo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software testing is a critical aspect of software development, yet generating
test cases remains a routine task for engineers. This paper presents a
benchmark, CLOVER, to evaluate models' capabilities in generating and
completing test cases under specific conditions. Spanning from simple assertion
completions to writing test cases that cover specific code blocks across
multiple files, these tasks are based on 12 python repositories, analyzing 845
problems with context lengths ranging from 4k to 128k tokens. Utilizing code
testing frameworks, we propose a method to construct retrieval contexts using
coverage information. While models exhibit comparable performance with short
contexts, notable differences emerge with 16k contexts. Notably, models like
GPT-4o and Claude 3.5 can effectively leverage relevant snippets; however, all
models score below 35\% on the complex Task III, even with the oracle context
provided, underscoring the benchmark's significance and the potential for model
improvement. The benchmark is containerized for code execution across tasks,
and we will release the code, data, and construction methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep EEG Super-Resolution: Upsampling EEG Spatial Resolution with
  Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Corley, Yufei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalography (EEG) activity contains a wealth of information about
what is happening within the human brain. Recording more of this data has the
potential to unlock endless future applications. However, the cost of EEG
hardware is increasingly expensive based upon the number of EEG channels being
recorded simultaneously. We combat this problem in this paper by proposing a
novel deep EEG super-resolution (SR) approach based on Generative Adversarial
Networks (GANs). This approach can produce high spatial resolution EEG data
from low resolution samples, by generating channel-wise upsampled data to
effectively interpolate numerous missing channels, thus reducing the need for
expensive EEG equipment. We tested the performance using an EEG dataset from a
mental imagery task. Our proposed GAN model provided 10^4 fold and 10^2 fold
reduction in mean-squared error (MSE) and mean-absolute error (MAE),
respectively, over the baseline bicubic interpolation method. We further
validate our method by training a classifier on the original classification
task, which displayed minimal loss in accuracy while using the super-resolved
data. The proposed SR EEG by GAN is a promising approach to improve the spatial
resolution of low density EEG headsets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Resolution Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo Lobo Lustosa Cabral, Larissa Driemeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expanding scale of large neural network models introduces significant
challenges, driving efforts to reduce memory usage and enhance computational
efficiency. Such measures are crucial to ensure the practical implementation
and effective application of these sophisticated models across a wide array of
use cases. This study examines the impact of parameter bit precision on model
performance compared to standard 32-bit models, with a focus on multiclass
object classification in images. The models analyzed include those with fully
connected layers, convolutional layers, and transformer blocks, with model
weight resolution ranging from 1 bit to 4.08 bits. The findings indicate that
models with lower parameter bit precision achieve results comparable to 32-bit
models, showing promise for use in memory-constrained devices. While
low-resolution models with a small number of parameters require more training
epochs to achieve accuracy comparable to 32-bit models, those with a large
number of parameters achieve similar performance within the same number of
epochs. Additionally, data augmentation can destabilize training in
low-resolution models, but including zero as a potential value in the weight
parameters helps maintain stability and prevents performance degradation.
Overall, 2.32-bit weights offer the optimal balance of memory reduction,
performance, and efficiency. However, further research should explore other
dataset types and more complex and larger models. These findings suggest a
potential new era for optimized neural network models with reduced memory
requirements and improved computational efficiency, though advancements in
dedicated hardware are necessary to fully realize this potential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Journey: How Transformers Predict the Shortest Path 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Cohen, Andrey Gromov, Kaiyu Yang, Yuandong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoder-only transformers lead to a step-change in capability of large
language models. However, opinions are mixed as to whether they are really
planning or reasoning. A path to making progress in this direction is to study
the model's behavior in a setting with carefully controlled data. Then
interpret the learned representations and reverse-engineer the computation
performed internally. We study decoder-only transformer language models trained
from scratch to predict shortest paths on simple, connected and undirected
graphs. In this setting, the representations and the dynamics learned by the
model are interpretable. We present three major results: (1) Two-layer
decoder-only language models can learn to predict shortest paths on simple,
connected graphs containing up to 10 nodes. (2) Models learn a graph embedding
that is correlated with the spectral decomposition of the line graph. (3)
Following the insights, we discover a novel approximate path-finding algorithm
Spectral Line Navigator (SLN) that finds shortest path by greedily selecting
nodes in the space of spectral embedding of the line graph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ If Multi-Agent Debate is the Answer, What is the Question? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangfan Zhang, Zhiyao Cui, Xinrun Wang, Qiaosheng Zhang, Zhen Wang, Dinghao Wu, Shuyue Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent debate (MAD) has emerged as a promising approach to enhance the
factual accuracy and reasoning quality of large language models (LLMs) by
engaging multiple agents in iterative discussions during inference. Despite its
potential, we argue that current MAD research suffers from critical
shortcomings in evaluation practices, including limited dataset overlap and
inconsistent baselines, raising significant concerns about generalizability.
Correspondingly, this paper presents a systematic evaluation of five
representative MAD methods across nine benchmarks using four foundational
models. Surprisingly, our findings reveal that MAD methods fail to reliably
outperform simple single-agent baselines such as Chain-of-Thought and
Self-Consistency, even when consuming additional inference-time computation.
From our analysis, we found that model heterogeneity can significantly improve
MAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the
output from heterogeneous foundation models, which boosts the performance of
current MAD frameworks. Finally, we outline potential directions for advancing
MAD, aiming to spark a broader conversation and inspire future work in this
area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This position paper takes a critical view of the status quo of MAD
  research, and outline multiple potential directions to improve MAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decision Tree Based Wrappers for Hearing Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miguel Rabuge, Nuno Lourenço
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audiology entities are using Machine Learning (ML) models to guide their
screening towards people at risk. Feature Engineering (FE) focuses on
optimizing data for ML models, with evolutionary methods being effective in
feature selection and construction tasks. This work aims to benchmark an
evolutionary FE wrapper, using models based on decision trees as proxies. The
FEDORA framework is applied to a Hearing Loss (HL) dataset, being able to
reduce data dimensionality and statistically maintain baseline performance.
Compared to traditional methods, FEDORA demonstrates superior performance, with
a maximum balanced accuracy of 76.2%, using 57 features. The framework also
generated an individual that achieved 72.8% balanced accuracy using a single
feature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Discontinuous Galerkin Solutions to Elliptic Problems via Small
  Linear Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Celaya, Yimo Wang, David Fuentes, Beatrice Riviere
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been an increasing interest in using deep learning
and neural networks to tackle scientific problems, particularly in solving
partial differential equations (PDEs). However, many neural network-based
methods, such as physics-informed neural networks, depend on automatic
differentiation and the sampling of collocation points, which can result in a
lack of interpretability and lower accuracy compared to traditional numerical
methods. To address this issue, we propose two approaches for learning
discontinuous Galerkin solutions to PDEs using small linear convolutional
neural networks. Our first approach is supervised and depends on labeled data,
while our second approach is unsupervised and does not rely on any training
data. In both cases, our methods use substantially fewer parameters than
similar numerics-based neural networks while also demonstrating comparable
accuracy to the true and DG solutions for elliptic problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Treatment response as a latent variable 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Tosh, Boyuan Zhang, Wesley Tansey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientists often need to analyze the samples in a study that responded to
treatment in order to refine their hypotheses and find potential causal drivers
of response. Natural variation in outcomes makes teasing apart responders from
non-responders a statistical inference problem. To handle latent responses, we
introduce the causal two-groups (C2G) model, a causal extension of the
classical two-groups model. The C2G model posits that treated samples may or
may not experience an effect, according to some prior probability. We propose
two empirical Bayes procedures for the causal two-groups model, one under
semi-parametric conditions and another under fully nonparametric conditions.
The semi-parametric model assumes additive treatment effects and is
identifiable from observed data. The nonparametric model is unidentifiable, but
we show it can still be used to test for response in each treated sample. We
show empirically and theoretically that both methods for selecting responders
control the false discovery rate at the target level with near-optimal power.
We also propose two novel estimands of interest and provide a strategy for
deriving estimand intervals in the unidentifiable nonparametric model. On a
cancer immunotherapy dataset, the nonparametric C2G model recovers
clinically-validated predictive biomarkers of both positive and negative
outcomes. Code is available at https://github.com/tansey-lab/causal2groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal
  Brain in 3D Ultrasound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Omolegan, Pak Hei Yeung, Madeleine K. Wyburd, Linde Hesse, Monique Haak, Intergrowth-21st Consortium, Ana I. L. Namburete, Nicola K. Dinsdale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring the growth of subcortical regions of the fetal brain in ultrasound
(US) images can help identify the presence of abnormal development. Manually
segmenting these regions is a challenging task, but recent work has shown that
it can be automated using deep learning. However, applying pretrained models to
unseen freehand US volumes often leads to a degradation of performance due to
the vast differences in acquisition and alignment. In this work, we first
demonstrate that test time adaptation (TTA) can be used to improve model
performance in the presence of both real and simulated domain shifts. We
further propose a novel TTA method by incorporating a normative atlas as a
prior for anatomy. In the presence of various types of domain shifts, we
benchmark the performance of different TTA methods and demonstrate the
improvements brought by our proposed approach, which may further facilitate
automated monitoring of fetal brain development. Our code is available at
https://github.com/joshuaomolegan/TTA-for-3D-Fetal-Subcortical-Segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Model Routing for Efficient LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wittawat Jitkrittum, Harikrishna Narasimhan, Ankit Singh Rawat, Jeevesh Juneja, Zifeng Wang, Chen-Yu Lee, Pradeep Shenoy, Rina Panigrahy, Aditya Krishna Menon, Sanjiv Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models' significant advances in capabilities are accompanied
by significant increases in inference costs. Model routing is a simple
technique for reducing inference cost, wherein one maintains a pool of
candidate LLMs, and learns to route each prompt to the smallest feasible LLM.
Existing works focus on learning a router for a fixed pool of LLMs. In this
paper, we consider the problem of dynamic routing, where new, previously
unobserved LLMs are available at test time. We propose a new approach to this
problem that relies on representing each LLM as a feature vector, derived based
on predictions on a set of representative prompts. Based on this, we detail two
effective strategies, relying on cluster-based routing and a learned cluster
map respectively. We prove that these strategies are estimates of a
theoretically optimal routing rule, and provide an excess risk bound to
quantify their errors. Experiments on a range of public benchmarks show the
effectiveness of the proposed strategies in routing amongst more than 30 unseen
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Mental Health: Exploring College Students' Well-being through
  Smartphone Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xuan, Meghna Roy Chowdhury, Yi Ding, Yixue Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global mental health crisis is a pressing concern, with college students
particularly vulnerable to rising mental health disorders. The widespread use
of smartphones among young adults, while offering numerous benefits, has also
been linked to negative outcomes such as addiction and regret, significantly
impacting well-being. Leveraging the longest longitudinal dataset collected
over four college years through passive mobile sensing, this study is the first
to examine the relationship between students' smartphone unlocking behaviors
and their mental health at scale in real-world settings. We provide the first
evidence demonstrating the predictability of phone unlocking behaviors for
mental health outcomes based on a large dataset, highlighting the potential of
these novel features for future predictive models. Our findings reveal
important variations in smartphone usage across genders and locations, offering
a deeper understanding of the interplay between digital behaviors and mental
health. We highlight future research directions aimed at mitigating adverse
effects and promoting digital well-being in this population.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Conference on Mobile Software Engineering
  and Systems (MOBILESoft 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demand Response Optimization MILP Framework for Microgrids with DERs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K. Victor Sam Moses Babu, Pratyush Chakraborty, Mayukha Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of renewable energy sources in microgrids introduces
significant operational challenges due to their intermittent nature and the
mismatch between generation and demand patterns. Effective demand response (DR)
strategies are crucial for maintaining system stability and economic
efficiency, particularly in microgrids with high renewable penetration. This
paper presents a comprehensive mixed-integer linear programming (MILP)
framework for optimizing DR operations in a microgrid with solar generation and
battery storage systems. The framework incorporates load classification,
dynamic price thresholding, and multi-period coordination for optimal DR event
scheduling. Analysis across seven distinct operational scenarios demonstrates
consistent peak load reduction of 10\% while achieving energy cost savings
ranging from 13.1\% to 38.0\%. The highest performance was observed in
scenarios with high solar generation, where the framework achieved 38.0\%
energy cost reduction through optimal coordination of renewable resources and
DR actions. The results validate the framework's effectiveness in managing
diverse operational challenges while maintaining system stability and economic
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compression of Site-Specific Deep Neural Networks for Massive MIMO
  Precoding <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghazal Kasalaee, Ali Hasanzadeh Karkan, Jean-François Frigon, François Leduc-Primeau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of deep learning (DL) models for precoding in massive
multiple-input multiple-output (mMIMO) systems is often constrained by high
computational demands and energy consumption. In this paper, we investigate the
compute energy efficiency of mMIMO precoders using DL-based approaches,
comparing them to conventional methods such as zero forcing and weighted
minimum mean square error (WMMSE). Our energy consumption model accounts for
both memory access and calculation energy within DL accelerators. We propose a
framework that incorporates mixed-precision quantization-aware training and
neural architecture search to reduce energy usage without compromising
accuracy. Using a ray-tracing dataset covering various base station sites, we
analyze how site-specific conditions affect the energy efficiency of compressed
models. Our results show that deep neural network compression generates
precoders with up to 35 times higher energy efficiency than WMMSE at equal
performance, depending on the scenario and the desired rate. These results
establish a foundation and a benchmark for the development of energy-efficient
DL-based mMIMO precoders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint comprises 6 pages and features 3 figures. It has been
  accepted to the IEEE International Conference on Machine Learning and
  Computer Networking (ICMLCN) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Low-Complexity Plug-and-Play Deep Learning Model for Massive MIMO
  Precoding Across Sites <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hasanzadeh Karkan, Ahmed Ibrahim, Jean-François Frigon, François Leduc-Primeau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massive multiple-input multiple-output (mMIMO) technology has transformed
wireless communication by enhancing spectral efficiency and network capacity.
This paper proposes a novel deep learning-based mMIMO precoder to tackle the
complexity challenges of existing approaches, such as weighted minimum mean
square error (WMMSE), while leveraging meta-learning domain generalization and
a teacher-student architecture to improve generalization across diverse
communication environments. When deployed to a previously unseen site, the
proposed model achieves excellent sum-rate performance while maintaining low
computational complexity by avoiding matrix inversions and by using a simpler
neural network structure. The model is trained and tested on a custom
ray-tracing dataset composed of several base station locations. The
experimental results indicate that our method effectively balances
computational efficiency with high sum-rate performance while showcasing strong
generalization performance in unseen environments. Furthermore, with
fine-tuning, the proposed model outperforms WMMSE across all tested sites and
SNR conditions while reducing complexity by at least 73$\times$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint comprises 6 pages and features 2 figures. It has been
  accepted to the IEEE International Conference on Machine Learning and
  Computer Networking (ICMLCN) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recurrent Memory for Online Interdomain Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Chen, Naoki Kiyohara, Harrison Bo Hua Zhu, Yingzhen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel online Gaussian process (GP) model that is capable of
capturing long-term memory in sequential data in an online regression setting.
Our model, Online HiPPO Sparse Variational Gaussian Process Regression
(OHSGPR), leverages the HiPPO (High-order Polynomial Projection Operators)
framework, which is popularized in the RNN domain due to its long-range memory
modeling capabilities. We interpret the HiPPO time-varying orthogonal
projections as inducing variables with time-dependent orthogonal polynomial
basis functions, which allows the SGPR inducing points to memorize the process
history. We show that the HiPPO framework fits naturally into the interdomain
GP framework and demonstrate that the kernel matrices can also be updated
online in a recurrence form based on the ODE evolution of HiPPO. We evaluate
our method on time series regression tasks, showing that it outperforms the
existing online GP method in terms of predictive performance and computational
efficiency
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Bounds for Sparse Variational Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michalis K. Titsias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse variational Gaussian processes (GPs) construct tractable posterior
approximations to GP models. At the core of these methods is the assumption
that the true posterior distribution over training function values ${\bf f}$
and inducing variables ${\bf u}$ is approximated by a variational distribution
that incorporates the conditional GP prior $p({\bf f} | {\bf u})$ in its
factorization. While this assumption is considered as fundamental, we show that
for model training we can relax it through the use of a more general
variational distribution $q({\bf f} | {\bf u})$ that depends on $N$ extra
parameters, where $N$ is the number of training examples. In GP regression, we
can analytically optimize the evidence lower bound over the extra parameters
and express a tractable collapsed bound that is tighter than the previous
bound. The new bound is also amenable to stochastic optimization and its
implementation requires minor modifications to existing sparse GP code.
Further, we also describe extensions to non-Gaussian likelihoods. On several
datasets we demonstrate that our method can reduce bias when learning the
hyperpaparameters and can lead to better predictive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Machine Learning Algorithms for Stock Price
  Prediction Using Insider Trading Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amitabh Chakravorty, Nelly Elsayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research paper empirically investigates several machine learning
algorithms to forecast stock prices depending on insider trading information.
Insider trading offers special insights into market sentiment, pointing to
upcoming changes in stock prices. This study examines the effectiveness of
algorithms like decision trees, random forests, support vector machines (SVM)
with different kernels, and K-Means Clustering using a dataset of Tesla stock
transactions. Examining past data from April 2020 to March 2023, this study
focuses on how well these algorithms identify trends and forecast stock price
fluctuations. The paper uses Recursive Feature Elimination (RFE) and feature
importance analysis to optimize the feature set and, hence, increase prediction
accuracy. While it requires substantially greater processing time than other
models, SVM with the Radial Basis Function (RBF) kernel displays the best
accuracy. This paper highlights the trade-offs between accuracy and efficiency
in machine learning models and proposes the possibility of pooling multiple
data sources to raise prediction performance. The results of this paper aim to
help financial analysts and investors in choosing strong algorithms to optimize
investment strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, accepted to publish</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Discrete Diffusion Samplers: Combinatorial Optimization and
  Statistical Physics <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Sanokowski, Wilhelm Berghammer, Martin Ennemoser, Haoyu Peter Wang, Sepp Hochreiter, Sebastian Lehner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to sample from complex unnormalized distributions over discrete
domains emerged as a promising research direction with applications in
statistical physics, variational inference, and combinatorial optimization.
Recent work has demonstrated the potential of diffusion models in this domain.
However, existing methods face limitations in memory scaling and thus the
number of attainable diffusion steps since they require backpropagation through
the entire generative process. To overcome these limitations we introduce two
novel training methods for discrete diffusion samplers, one grounded in the
policy gradient theorem and the other one leveraging Self-Normalized Neural
Importance Sampling (SN-NIS). These methods yield memory-efficient training and
achieve state-of-the-art results in unsupervised combinatorial optimization.
Numerous scientific applications additionally require the ability of unbiased
sampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte
Carlo that enable for the first time the application of discrete diffusion
models to this problem. We validate our methods on Ising model benchmarks and
find that they outperform popular autoregressive approaches. Our work opens new
avenues for applying diffusion models to a wide range of scientific
applications in discrete domains that were hitherto restricted to exact
likelihood models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utility Engineering: Analyzing and <span class="highlight-title">Control</span>ling Emergent Value Systems in
  AIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AIs rapidly advance and become more agentic, the risk they pose is
governed not only by their capabilities but increasingly by their propensities,
including goals and values. Tracking the emergence of goals and values has
proven a longstanding problem, and despite much interest over the years it
remains unclear whether current AIs have meaningful values. We propose a
solution to this problem, leveraging the framework of utility functions to
study the internal coherence of AI preferences. Surprisingly, we find that
independently-sampled preferences in current LLMs exhibit high degrees of
structural coherence, and moreover that this emerges with scale. These findings
suggest that value systems emerge in LLMs in a meaningful sense, a finding with
broad implications. To study these emergent value systems, we propose utility
engineering as a research agenda, comprising both the analysis and control of
AI utilities. We uncover problematic and often shocking values in LLM
assistants despite existing control measures. These include cases where AIs
value themselves over humans and are anti-aligned with specific individuals. To
constrain these emergent value systems, we propose methods of utility control.
As a case study, we show how aligning utilities with a citizen assembly reduces
political biases and generalizes to new scenarios. Whether we like it or not,
value systems have already emerged in AIs, and much work remains to fully
understand and control these emergent representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Transmit and Pinching Beamforming for PASS: Optimization-Based or
  Learning-Based? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxia Xu, Xidong Mu, Yuanwei Liu, Arumugam Nallanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel pinching antenna system (PASS)-enabled downlink multi-user
multiple-input single-output (MISO) framework is proposed. PASS consists of
multiple waveguides spanning over thousands of wavelength, which equip numerous
low-cost dielectric particles, named pinching antennas (PAs), to radiate
signals into free space. The positions of PAs can be reconfigured to change
both the large-scale path losses and phases of signals, thus facilitating the
novel pinching beamforming design. A sum rate maximization problem is
formulated, which jointly optimizes the transmit and pinching beamforming to
adaptively achieve constructive signal enhancement and destructive interference
mitigation. To solve this highly coupled and nonconvex problem, both
optimization-based and learning-based methods are proposed. 1) For the
optimization-based method, a majorization-minimization and penalty dual
decomposition (MM-PDD) algorithm is developed, which handles the nonconvex
complex exponential component using a Lipschitz surrogate function and then
invokes PDD for problem decoupling. 2) For the learning-based method, a novel
Karush-Kuhn-Tucker (KKT)-guided dual learning (KDL) approach is proposed, which
enables KKT solutions to be reconstructed in a data-driven manner by learning
dual variables. Following this idea, a KDL-Tranformer algorithm is developed,
which captures both inter-PA/inter-user dependencies and
channel-state-information (CSI)-beamforming dependencies by attention
mechanisms. Simulation results demonstrate that: i) The proposed PASS framework
significantly outperforms conventional massive multiple input multiple output
(MIMO) system even with a few PAs. ii) The proposed KDL-Transformer can improve
over 30% system performance than MM-PDD algorithm, while achieving a
millisecond-level response on modern GPUs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rapid Whole Brain Mesoscale In-vivo MR Imaging using Multi-scale
  Implicit Neural Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Lyu, Lipeng Ning, William Consagra, Qiang Liu, Richard J. Rushmore, Berkin Bilgic, Yogesh Rathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To develop and validate a novel image reconstruction technique using
implicit neural representations (INR) for multi-view thick-slice acquisitions
while reducing the scan time but maintaining high signal-to-noise ratio (SNR).
Methods: We propose Rotating-view super-resolution (ROVER)-MRI, an unsupervised
neural network-based algorithm designed to reconstruct MRI data from multi-view
thick slices, effectively reducing scan time by 2-fold while maintaining fine
anatomical details. We compare our method to both bicubic interpolation and the
current state-of-the-art regularized least-squares super-resolution
reconstruction (LS-SRR) technique. Validation is performed using ground-truth
ex-vivo monkey brain data, and we demonstrate superior reconstruction quality
across several in-vivo human datasets. Notably, we achieve the reconstruction
of a whole human brain in-vivo T2-weighted image with an unprecedented
180{\mu}m isotropic spatial resolution, accomplished in just 17 minutes of scan
time on a 7T MRI scanner. Results: ROVER-MRI outperformed LS-SRR method in
terms of reconstruction quality with 22.4% lower relative error (RE) and 7.5%
lower full-width half maximum (FWHM) indicating better preservation of fine
structural details in nearly half the scan time. Conclusion: ROVER-MRI offers
an efficient and robust approach for mesoscale MR imaging, enabling rapid,
high-resolution whole-brain scans. Its versatility holds great promise for
research applications requiring anatomical details and time-efficient imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Necessary and Sufficient Oracles: Toward a Computational Taxonomy For
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Rohatgi, Dylan J. Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithms for reinforcement learning (RL) in large state spaces crucially
rely on supervised learning subroutines to estimate objects such as value
functions or transition probabilities. Since only the simplest supervised
learning problems can be solved provably and efficiently, practical performance
of an RL algorithm depends on which of these supervised learning "oracles" it
assumes access to (and how they are implemented). But which oracles are better
or worse? Is there a minimal oracle?
  In this work, we clarify the impact of the choice of supervised learning
oracle on the computational complexity of RL, as quantified by the oracle
strength. First, for the task of reward-free exploration in Block MDPs in the
standard episodic access model -- a ubiquitous setting for RL with function
approximation -- we identify two-context regression as a minimal oracle, i.e.
an oracle that is both necessary and sufficient (under a mild regularity
assumption). Second, we identify one-context regression as a near-minimal
oracle in the stronger reset access model, establishing a provable
computational benefit of resets in the process. Third, we broaden our focus to
Low-Rank MDPs, where we give cryptographic evidence that the analogous oracle
from the Block MDP setting is insufficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>84 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bayesian Nonparametric Perspective on Mahalanobis Distance for Out of
  Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Randolph W. Linderman, Yiran Chen, Scott W. Linderman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian nonparametric methods are naturally suited to the problem of
out-of-distribution (OOD) detection. However, these techniques have largely
been eschewed in favor of simpler methods based on distances between
pre-trained or learned embeddings of data points. Here we show a formal
relationship between Bayesian nonparametric models and the relative Mahalanobis
distance score (RMDS), a commonly used method for OOD detection. Building on
this connection, we propose Bayesian nonparametric mixture models with
hierarchical priors that generalize the RMDS. We evaluate these models on the
OpenOOD detection benchmark and show that Bayesian nonparametric methods can
improve upon existing OOD methods, especially in regimes where training classes
differ in their covariance structure and where there are relatively few data
points per class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 5 figures, code is available at
  https://github.com/rwl93/bnp4ood</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concentration Inequalities for the Stochastic Optimization of Unbounded
  Objectives with Application to Denoising Score Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremiah Birrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We derive novel concentration inequalities that bound the statistical error
for a large class of stochastic optimization problems, focusing on the case of
unbounded objective functions. Our derivations utilize the following tools: 1)
A new form of McDiarmid's inequality that is based on sample dependent one
component difference bounds and which leads to a novel uniform law of large
numbers result for unbounded functions. 2) A Rademacher complexity bound for
families of functions that satisfy an appropriate local Lipschitz property. As
an application of these results, we derive statistical error bounds for
denoising score matching (DSM), an application that inherently requires one to
consider unbounded objective functions, even when the data distribution has
bounded support. In addition, our results establish the benefit of sample reuse
in algorithms that employ easily sampled auxiliary random variables in addition
to the training data, e.g., as in DSM, which uses auxiliary Gaussian random
variables.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Randomness of Low-Layer Parameters Determines Confusing Samples in Terms
  of Interaction Representations of a DNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Zhang, Lei Cheng, Qing Li, Liang Lin, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we find that the complexity of interactions encoded by a deep
neural network (DNN) can explain its generalization power. We also discover
that the confusing samples of a DNN, which are represented by non-generalizable
interactions, are determined by its low-layer parameters. In comparison, other
factors, such as high-layer parameters and network architecture, have much less
impact on the composition of confusing samples. Two DNNs with different
low-layer parameters usually have fully different sets of confusing samples,
even though they have similar performance. This finding extends the
understanding of the lottery ticket hypothesis, and well explains distinctive
representation power of different DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting Drought Using Machine Learning in California 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan K. Li, Angela Chang, David Sherman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drought is a frequent and costly natural disaster in California, with major
negative impacts on agricultural production and water resource availability,
particularly groundwater. This study investigated the performance of applying
different machine learning approaches to predicting the U.S. Drought Monitor
classification in California. Four approaches were used: a convolutional neural
network (CNN), random forest, XGBoost, and long short term memory (LSTM)
recurrent neural network, and compared to a baseline persistence model. We
evaluated the models' performance in predicting severe drought (USDM drought
category D2 or higher) using a macro F1 binary classification metric. The LSTM
model emerged as the top performer, followed by XGBoost, CNN, and random
forest. Further evaluation of our results at the county level suggested that
the LSTM model would perform best in counties with more consistent drought
patterns and where severe drought was more common, and the LSTM model would
perform worse where drought scores increased rapidly. Utilizing 30 weeks of
historical data, the LSTM model successfully forecasted drought scores for a
12-week period with a Mean Absolute Error (MAE) of 0.33, equivalent to less
than half a drought category on a scale of 0 to 5. Additionally, the LSTM
achieved a macro F1 score of 0.9, indicating high accuracy in binary
classification for severe drought conditions. Evaluation of different window
and future horizon sizes in weeks suggested that at least 24 weeks of data
would result in the best performance, with best performance for shorter horizon
sizes, particularly less than eight weeks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mathematical Data Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael R. Douglas, Kyu-Hwan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can machine learning help discover new mathematical structures? In this
article we discuss an approach to doing this which one can call "mathematical
data science". In this paradigm, one studies mathematical objects collectively
rather than individually, by creating datasets and doing machine learning
experiments and interpretations. After an overview, we present two case
studies: murmurations in number theory and loadings of partitions related to
Kronecker coefficients in representation theory and combinatorics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Cardiac Arrest Prediction in ICU using PPG Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Kataria, Ran Xiao, Timothy Ruchti, Matthew Clark, Jiaying Lu, Randall J. Lee, Jocelyn Grunwell, Xiao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-invasive patient monitoring for tracking and predicting adverse acute
health events is an emerging area of research. We pursue in-hospital cardiac
arrest (IHCA) prediction using only single-channel finger photoplethysmography
(PPG) signals. Our proposed two-stage model Feature Extractor-Aggregator
Network (FEAN) leverages powerful representations from pre-trained PPG
foundation models (PPG-GPT of size up to 1 Billion) stacked with sequential
classification models. We propose two FEAN variants ("1H", "FH") which use the
latest one-hour and (max) 24-hour history to make decisions respectively. Our
study is the first to present IHCA prediction results in ICU patients using
only unimodal (continuous PPG signal) waveform deep representations. With our
best model, we obtain an average of 0.79 AUROC over 24~h prediction window
before CA event onset with our model peaking performance at 0.82 one hour
before CA. We also provide a comprehensive analysis of our model through
architectural tuning and PaCMAP visualization of patient health trajectory in
latent space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustly Learning Monotone Generalized Linear Models via Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, Jelena Diakonikolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the task of learning Generalized Linear models (GLMs) in the
agnostic model under the Gaussian distribution. We give the first
polynomial-time algorithm that achieves a constant-factor approximation for
\textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners
succeed for a substantially smaller class of activations. Our work resolves a
well-known open problem, by developing a robust counterpart to the classical
GLMtron algorithm (Kakade et al., 2011). Our robust learner applies more
generally, encompassing all monotone activations with bounded
$(2+\zeta)$-moments, for any fixed $\zeta>0$ -- a condition that is essentially
necessary. To obtain our results, we leverage a novel data augmentation
technique with decreasing Gaussian noise injection and prove a number of
structural results that may be useful in other settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distillation Scaling Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Busbridge, Amitis Shidani, Floris Weers, Jason Ramapuram, Etai Littwin, Russ Webb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a distillation scaling law that estimates distilled model
performance based on a compute budget and its allocation between the student
and teacher. Our findings reduce the risks associated with using distillation
at scale; compute allocation for both the teacher and student models can now be
done to maximize student performance. We provide compute optimal distillation
recipes for when 1) a teacher exists, or 2) a teacher needs training. If many
students are to be distilled, or a teacher already exists, distillation
outperforms supervised pretraining until a compute level which grows
predictably with student size. If one student is to be distilled and a teacher
also needs training, supervised learning should be done instead. Additionally,
we provide insights across our large scale study of distillation, which
increase our understanding of distillation and inform experimental design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>67 pages, 54 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CurvGAD: Leveraging Curvature for Enhanced Graph Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karish Grover, Geoffrey J. Gordon, Christos Faloutsos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does the intrinsic curvature of complex networks hold the key to unveiling
graph anomalies that conventional approaches overlook? Reconstruction-based
graph anomaly detection (GAD) methods overlook such geometric outliers,
focusing only on structural and attribute-level anomalies. To this end, we
propose CurvGAD - a mixed-curvature graph autoencoder that introduces the
notion of curvature-based geometric anomalies. CurvGAD introduces two parallel
pipelines for enhanced anomaly interpretability: (1) Curvature-equivariant
geometry reconstruction, which focuses exclusively on reconstructing the edge
curvatures using a mixed-curvature, Riemannian encoder and Gaussian
kernel-based decoder; and (2) Curvature-invariant structure and attribute
reconstruction, which decouples structural and attribute anomalies from
geometric irregularities by regularizing graph curvature under discrete
Ollivier-Ricci flow, thereby isolating the non-geometric anomalies. By
leveraging curvature, CurvGAD refines the existing anomaly classifications and
identifies new curvature-driven anomalies. Extensive experimentation over 10
real-world datasets (both homophilic and heterophilic) demonstrates an
improvement of up to 6.5% over state-of-the-art GAD methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Thermodynamic Second-order Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaelan Donatella, Samuel Duffield, Denis Melanson, Maxwell Aifer, Phoebe Klett, Rajath Salegame, Zach Belateche, Gavin Crooks, Antonio J. Martinez, Patrick J. Coles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many hardware proposals have aimed to accelerate inference in AI workloads.
Less attention has been paid to hardware acceleration of training, despite the
enormous societal impact of rapid training of AI models. Physics-based
computers, such as thermodynamic computers, offer an efficient means to solve
key primitives in AI training algorithms. Optimizers that normally would be
computationally out-of-reach (e.g., due to expensive matrix inversions) on
digital hardware could be unlocked with physics-based hardware. In this work,
we propose a scalable algorithm for employing thermodynamic computers to
accelerate a popular second-order optimizer called Kronecker-factored
approximate curvature (K-FAC). Our asymptotic complexity analysis predicts
increasing advantage with our algorithm as $n$, the number of neurons per
layer, increases. Numerical experiments show that even under significant
quantization noise, the benefits of second-order optimization can be preserved.
Finally, we predict substantial speedups for large-scale vision and graph
problems based on realistic hardware characteristics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-stage hybrid models for enhancing forecasting accuracy on
  heterogeneous time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junru Ren, Shaomin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to local models built in a series-by-series manner, global models
leverage relevant information across time series, resulting in improved
forecasting performance and generalization capacity. Constructing global models
on a set of time series is becoming mainstream in the field of time series
forecasting. However, the advantages of global models may not always be
realized when dealing with heterogeneous data. While they can adapt to
heterogeneous datasets by increasing the model complexity, the model cannot be
infinitely complex due to the finite sample size, which poses challenges for
the application of global models. Additionally, determining whether the time
series data is homogeneous or heterogeneous can be ambiguous in practice. To
address these research gaps, this paper argues that the heterogeneity of the
data should be defined by the global model used, and for each series, the
portion not modelled by the global model represents heterogeneity. It further
proposes two-stage hybrid models, which include a second stage to identify and
model heterogeneous patterns. In this second stage, we can estimate either all
local models or sub-global models across different domains divided based on
heterogeneity. Experiments on four open datasets reveal that the proposed
methods significantly outperform five existing models, indicating they
contribute to fully unleash the potential of global models on heterogeneous
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Diffusion Models Efficiency by Disentangling Total-Variance
  and Signal-to-Noise Ratio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Kahouli, Winfried Ripken, Stefan Gugler, Oliver T. Unke, Klaus-Robert Müller, Shinichi Nakajima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The long sampling time of diffusion models remains a significant bottleneck,
which can be mitigated by reducing the number of diffusion time steps. However,
the quality of samples with fewer steps is highly dependent on the noise
schedule, i.e., the specific manner in which noise is introduced and the signal
is reduced at each step. Although prior work has improved upon the original
variance-preserving and variance-exploding schedules, these approaches
$\textit{passively}$ adjust the total variance, without direct control over it.
In this work, we propose a novel total-variance/signal-to-noise-ratio
disentangled (TV/SNR) framework, where TV and SNR can be controlled
independently. Our approach reveals that different existing schedules, where
the TV explodes exponentially, can be $\textit{improved}$ by setting a constant
TV schedule while preserving the same SNR schedule. Furthermore, generalizing
the SNR schedule of the optimal transport flow matching significantly improves
the performance in molecular structure generation, achieving few step
generation of stable molecules. A similar tendency is observed in image
generation, where our approach with a uniform diffusion time grid performs
comparably to the highly tailored EDM sampler.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Li, Yin Zhou, Vethavikashini Chithrra Raghuram, Tom Goldstein, Micah Goldblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A high volume of recent ML security literature focuses on attacks against
aligned large language models (LLMs). These attacks may extract private
information or coerce the model into producing harmful outputs. In real-world
deployments, LLMs are often part of a larger agentic pipeline including memory
systems, retrieval, web access, and API calling. Such additional components
introduce vulnerabilities that make these LLM-powered agents much easier to
attack than isolated LLMs, yet relatively little work focuses on the security
of LLM agents. In this paper, we analyze security and privacy vulnerabilities
that are unique to LLM agents. We first provide a taxonomy of attacks
categorized by threat actors, objectives, entry points, attacker observability,
attack strategies, and inherent vulnerabilities of agent pipelines. We then
conduct a series of illustrative attacks on popular open-source and commercial
agents, demonstrating the immediate practical implications of their
vulnerabilities. Notably, our attacks are trivial to implement and require no
understanding of machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Bilevel Loss Balancing for Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyao Xiao, Chaosheng Dong, Shaofeng Zou, Kaiyi Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) has been widely adopted for its ability to
simultaneously learn multiple tasks. While existing gradient manipulation
methods often yield more balanced solutions than simple scalarization-based
approaches, they typically incur a significant computational overhead of
$\mathcal{O}(K)$ in both time and memory, where $K$ is the number of tasks. In
this paper, we propose BiLB4MTL, a simple and scalable loss balancing approach
for MTL, formulated from a novel bilevel optimization perspective. Our method
incorporates three key components: (i) an initial loss normalization, (ii) a
bilevel loss-balancing formulation, and (iii) a scalable first-order algorithm
that requires only $\mathcal{O}(1)$ time and memory. Theoretically, we prove
that BiLB4MTL guarantees convergence not only to a stationary point of the
bilevel loss balancing problem but also to an $\epsilon$-accurate Pareto
stationary point for all $K$ loss functions under mild conditions. Extensive
experiments on diverse multi-task datasets demonstrate that BiLB4MTL achieves
state-of-the-art performance in both accuracy and efficiency. Code is available
at https://github.com/OptMN-Lab/-BiLB4MTL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A method for classification of data with uncertainty using hypothesis
  testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoma Yokura, Akihisa Ichiki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary classification is a task that involves the classification of data into
one of two distinct classes. It is widely utilized in various fields. However,
conventional classifiers tend to make overconfident predictions for data that
belong to overlapping regions of the two class distributions or for data
outside the distributions (out-of-distribution data). Therefore, conventional
classifiers should not be applied in high-risk fields where classification
results can have significant consequences. In order to address this issue, it
is necessary to quantify uncertainty and adopt decision-making approaches that
take it into account. Many methods have been proposed for this purpose;
however, implementing these methods often requires performing resampling,
improving the structure or performance of models, and optimizing the thresholds
of classifiers. We propose a new decision-making approach using two types of
hypothesis testing. This method is capable of detecting ambiguous data that
belong to the overlapping regions of two class distributions, as well as
out-of-distribution data that are not included in the training data
distribution. In addition, we quantify uncertainty using the empirical
distribution of feature values derived from the training data obtained through
the trained model. The classification threshold is determined by the
$\alpha$-quantile and ($1-\alpha$)-quantile, where the significance level
$\alpha$ is set according to each specific situation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FBFL: A Field-Based Coordination Approach for Data Heterogeneity in
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Domini, Gianluca Aguzzi, Lukas Esterle, Mirko Viroli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last years, Federated learning (FL) has become a popular solution to
train machine learning models in domains with high privacy concerns. However,
FL scalability and performance face significant challenges in real-world
deployments where data across devices are non-independently and identically
distributed (non-IID). The heterogeneity in data distribution frequently arises
from spatial distribution of devices, leading to degraded model performance in
the absence of proper handling. Additionally, FL typical reliance on
centralized architectures introduces bottlenecks and single-point-of-failure
risks, particularly problematic at scale or in dynamic environments. To close
this gap, we propose Field-Based Federated Learning (FBFL), a novel approach
leveraging macroprogramming and field coordination to address these limitations
through: (i) distributed spatial-based leader election for personalization to
mitigate non-IID data challenges; and (ii) construction of a self-organizing,
hierarchical architecture using advanced macroprogramming patterns. Moreover,
FBFL not only overcomes the aforementioned limitations, but also enables the
development of more specialized models tailored to the specific data
distribution in each subregion. This paper formalizes FBFL and evaluates it
extensively using MNIST, FashionMNIST, and Extended MNIST datasets. We
demonstrate that, when operating under IID data conditions, FBFL performs
comparably to the widely-used FedAvg algorithm. Furthermore, in challenging
non-IID scenarios, FBFL not only outperforms FedAvg but also surpasses other
state-of-the-art methods, namely FedProx and Scaffold, which have been
specifically designed to address non-IID data distributions. Additionally, we
showcase the resilience of FBFL's self-organizing hierarchical architecture
against server failures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping the Landscape of Generative AI in Network Monitoring and
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giampaolo Bovenzi, Francesco Cerasuolo, Domenico Ciuonzo, Davide Di Monda, Idio Guarino, Antonio Montieri, Valerio Persico, Antonio Pescapè
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and
Diffusion Models have recently gained widespread attention from both the
research and the industrial communities. This survey explores their application
in network monitoring and management, focusing on prominent use cases, as well
as challenges and opportunities. We discuss how network traffic generation and
classification, network intrusion detection, networked system log analysis, and
network digital assistance can benefit from the use of GenAI models.
Additionally, we provide an overview of the available GenAI models, datasets
for large-scale training phases, and platforms for the development of such
models. Finally, we discuss research directions that potentially mitigate the
roadblocks to the adoption of GenAI for network monitoring and management. Our
investigation aims to map the current landscape and pave the way for future
research in leveraging GenAI for network monitoring and management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 9 figure, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COAST: Intelligent Time-Adaptive Neural Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikai Wu, Shiyang Zhang, Sizhuang He, Sifan Wang, Min Zhu, Anran Jiao, Lu Lu, David van Dijk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Causal Operator with Adaptive Solver Transformer (COAST), a
novel neural operator learning method that leverages a causal language model
(CLM) framework to dynamically adapt time steps. Our method predicts both the
evolution of a system and its optimal time step, intelligently balancing
computational efficiency and accuracy. We find that COAST generates variable
step sizes that correlate with the underlying system intrinsicities, both
within and across dynamical systems. Within a single trajectory, smaller steps
are taken in regions of high complexity, while larger steps are employed in
simpler regions. Across different systems, more complex dynamics receive more
granular time steps. Benchmarked on diverse systems with varied dynamics, COAST
consistently outperforms state-of-the-art methods, achieving superior
performance in both efficiency and accuracy. This work underscores the
potential of CLM-based intelligent adaptive solvers for scalable operator
learning of dynamical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QA-Expand: Multi-Question Answer Generation for Enhanced Query Expansion
  in Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonduk Seo, Seunghyun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query expansion is widely used in Information Retrieval (IR) to improve
search outcomes by enriching queries with additional contextual information.
Although recent Large Language Model (LLM) based methods generate
pseudo-relevant content and expanded terms via multiple prompts, they often
yield repetitive, narrow expansions that lack the diverse context needed to
retrieve all relevant information. In this paper, we introduce QA-Expand, a
novel and effective framework for query expansion. It first generates multiple
relevant questions from the initial query and subsequently produces
corresponding pseudo-answers as surrogate documents. A feedback model further
rewrites and filters these answers to ensure only the most informative
augmentations are incorporated. Extensive experiments on benchmarks such as
BEIR and TREC demonstrate that QA-Expand enhances retrieval performance by up
to 13% over state-of-the-art methods, offering a robust solution for modern
retrieval challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Human</span>-Centric Foundation Models: Perception, Generation and Agentic
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixiang Tang, Yizhou Wang, Lu Chen, Yuan Wang, Sida Peng, Dan Xu, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human understanding and generation are critical for modeling digital humans
and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)
inspired by the success of generalist models, such as large language and vision
models, have emerged to unify diverse human-centric tasks into a single
framework, surpassing traditional task-specific approaches. In this survey, we
present a comprehensive overview of HcFMs by proposing a taxonomy that
categorizes current approaches into four groups: (1) Human-centric Perception
Foundation Models that capture fine-grained features for multi-modal 2D and 3D
understanding. (2) Human-centric AIGC Foundation Models that generate
high-fidelity, diverse human-related content. (3) Unified Perception and
Generation Models that integrate these capabilities to enhance both human
understanding and synthesis. (4) Human-centric Agentic Foundation Models that
extend beyond perception and generation to learn human-like intelligence and
interactive behaviors for humanoid embodied tasks. We review state-of-the-art
techniques, discuss emerging challenges and future research directions. This
survey aims to serve as a roadmap for researchers and practitioners working
towards more robust, versatile, and intelligent digital human and embodiments
modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Machine Learning-Ready Data Processing Tool for Near Real-Time
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maher A Dayeh, Michael J Starkey, Subhamoy Chatterjee, Heather Elliott, Samuel Hart, Kimberly Moreland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space weather forecasting is critical for mitigating radiation risks in space
exploration and protecting Earth-based technologies from geomagnetic
disturbances. This paper presents the development of a Machine Learning (ML)-
ready data processing tool for Near Real-Time (NRT) space weather forecasting.
By merging data from diverse NRT sources such as solar imagery, magnetic field
measurements, and energetic particle fluxes, the tool addresses key gaps in
current space weather prediction capabilities. The tool processes and
structures the data for machine learning models, focusing on time-series
forecasting and event detection for extreme solar events. It provides users
with a framework to download, process, and label data for ML applications,
streamlining the workflow for improved NRT space weather forecasting and
scientific research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Copula-based mixture model identification for subgroup clustering with
  imaging applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Zheng, Nicolas Duchateau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based clustering techniques have been widely applied to various
application areas, while most studies focus on canonical mixtures with unique
component distribution form. However, this strict assumption is often hard to
satisfy. In this paper, we consider the more flexible Copula-Based Mixture
Models (CBMMs) for clustering, which allow heterogeneous component
distributions composed by flexible choices of marginal and copula forms. More
specifically, we propose an adaptation of the Generalized Iterative Conditional
Estimation (GICE) algorithm to identify the CBMMs in an unsupervised manner,
where the marginal and copula forms and their parameters are estimated
iteratively. GICE is adapted from its original version developed for switching
Markov model identification with the choice of realization time. Our CBMM-GICE
clustering method is then tested on synthetic two-cluster data (N=2000 samples)
with discussion of the factors impacting its convergence. Finally, it is
compared to the Expectation Maximization identified mixture models with unique
component form on the entire MNIST database (N=70000), and on real cardiac
magnetic resonance data (N=276) to illustrate its value for imaging
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Predictions: A Participatory Framework for Multi-Stakeholder
  Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vittoria Vineis, Giuseppe Perelli, Gabriele Tolomei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional decision-support systems, primarily based on supervised
learning, focus on outcome prediction models to recommend actions. However,
they often fail to account for the complexities of multi-actor environments,
where diverse and potentially conflicting stakeholder preferences must be
balanced. In this paper, we propose a novel participatory framework that
redefines decision-making as a multi-stakeholder optimization problem,
capturing each actor's preferences through context-dependent reward functions.
Our framework leverages $k$-fold cross-validation to fine-tune user-provided
outcome prediction models and evaluate decision strategies, including
compromise functions mediating stakeholder trade-offs. We introduce a synthetic
scoring mechanism that exploits user-defined preferences across multiple
metrics to rank decision-making strategies and identify the optimal
decision-maker. The selected decision-maker can then be used to generate
actionable recommendations for new data. We validate our framework using two
real-world use cases, demonstrating its ability to deliver recommendations that
effectively balance multiple metrics, achieving results that are often beyond
the scope of purely prediction-based methods. Ablation studies demonstrate that
our framework, with its modular, model-agnostic, and inherently transparent
design, integrates seamlessly with various predictive models, reward
structures, evaluation metrics, and sample sizes, making it particularly suited
for complex, high-stakes decision-making contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matrix Completion with Graph Information: A Provable Nonconvex
  Optimization Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Wang, Yiyang Yang, Kaidong Wang, Shanxing Gao, Xiuwu Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of matrix completion with graphs as side information
depicting the interrelations between variables. The key challenge lies in
leveraging the similarity structure of the graph to enhance matrix recovery.
Existing approaches, primarily based on graph Laplacian regularization, suffer
from several limitations: (1) they focus only on the similarity between
neighboring variables, while overlooking long-range correlations; (2) they are
highly sensitive to false edges in the graphs and (3) they lack theoretical
guarantees regarding statistical and computational complexities. To address
these issues, we propose in this paper a novel graph regularized matrix
completion algorithm called GSGD, based on preconditioned projected gradient
descent approach. We demonstrate that GSGD effectively captures the
higher-order correlation information behind the graphs, and achieves superior
robustness and stability against the false edges. Theoretically, we prove that
GSGD achieves linear convergence to the global optimum with near-optimal sample
complexity, providing the first theoretical guarantees for both recovery
accuracy and efficacy in the perspective of nonconvex optimization. Our
numerical experiments on both synthetic and real-world data further validate
that GSGD achieves superior recovery accuracy and scalability compared with
several popular alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Different Notions of Redundancy in Conditional-Independence-Based
  Discovery of Graphical Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp M. Faller, Dominik Janzing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of conditional-independence-based discovery of graphical models is
to find a graph that represents the independence structure of variables in a
given dataset. To learn such a representation, conditional-independence-based
approaches conduct a set of statistical tests that suffices to identify the
graphical representation under some assumptions on the underlying distribution
of the data. In this work, we highlight that due to the conciseness of the
graphical representation, there are often many tests that are not used in the
construction of the graph. These redundant tests have the potential to detect
or sometimes correct errors in the learned model. We show that not all tests
contain this additional information and that such redundant tests have to be
applied with care. Precisely, we argue that particularly those conditional
(in)dependence statements are interesting that follow only from graphical
assumptions but do not hold for every probability distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Pretraining with Continuous Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, Xian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next token prediction has been the standard training objective used in large
language model pretraining. Representations are learned as a result of
optimizing for token-level perplexity. We propose Continuous Concept Mixing
(CoCoMix), a novel pretraining framework that combines discrete next token
prediction with continuous concepts. Specifically, CoCoMix predicts continuous
concepts learned from a pretrained sparse autoencoder and mixes them into the
model's hidden state by interleaving with token hidden representations. Through
experiments on multiple benchmarks, including language modeling and downstream
reasoning tasks, we show that CoCoMix is more sample efficient and consistently
outperforms standard next token prediction, knowledge distillation and
inserting pause tokens. We find that combining both concept learning and
interleaving in an end-to-end framework is critical to performance gains.
Furthermore, CoCoMix enhances interpretability and steerability by allowing
direct inspection and modification of the predicted concept, offering a
transparent way to guide the model's internal reasoning process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedMHO: Heterogeneous One-Shot Federated Learning Towards
  Resource-Constrained Edge Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dezhong Yao, Yuexin Shi, Tongtong Liu, Zhiqiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is increasingly adopted in edge computing scenarios,
where a large number of heterogeneous clients operate under constrained or
sufficient resources. The iterative training process in conventional FL
introduces significant computation and communication overhead, which is
unfriendly for resource-constrained edge devices. One-shot FL has emerged as a
promising approach to mitigate communication overhead, and model-heterogeneous
FL solves the problem of diverse computing resources across clients. However,
existing methods face challenges in effectively managing model-heterogeneous
one-shot FL, often leading to unsatisfactory global model performance or
reliance on auxiliary datasets. To address these challenges, we propose a novel
FL framework named FedMHO, which leverages deep classification models on
resource-sufficient clients and lightweight generative models on
resource-constrained devices. On the server side, FedMHO involves a two-stage
process that includes data generation and knowledge fusion. Furthermore, we
introduce FedMHO-MD and FedMHO-SD to mitigate the knowledge-forgetting problem
during the knowledge fusion stage, and an unsupervised data optimization
solution to improve the quality of synthetic samples. Comprehensive experiments
demonstrate the effectiveness of our methods, as they outperform
state-of-the-art baselines in various experimental setups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Split Learning LSTM Models for FPGA-based Edge IoT Devices <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romina Soledad Molina, Vukan Ninkovic, Dejan Vukobratovic, Maria Liz Crespo, Marco Zennaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Split Learning (SL) recently emerged as an efficient paradigm for distributed
Machine Learning (ML) suitable for the Internet Of Things (IoT)-Cloud systems.
However, deploying SL on resource-constrained edge IoT platforms poses a
significant challenge in terms of balancing the model performance against the
processing, memory, and energy resources. In this work, we present a practical
study of deploying SL framework on a real-world Field-Programmable Gate Array
(FPGA)-based edge IoT platform. We address the SL framework applied to a
time-series processing model based on Recurrent Neural Networks (RNNs). Set in
the context of river water quality monitoring and using real-world data, we
train, optimize, and deploy a Long Short-Term Memory (LSTM) model on a given
edge IoT FPGA platform in different SL configurations. Our results demonstrate
the importance of aligning design choices with specific application
requirements, whether it is maximizing speed, minimizing power, or optimizing
for resource constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at IEEE ICMLCN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Paradox of Stochasticity: Limited Creativity and Computational
  Decoupling in Temperature-Varied LLM Outputs of Structured Fictional Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgenii Evstafev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines how temperature settings and model architectures affect
the generation of structured fictional data (names, birthdates) across three
large language models (LLMs): llama3.1:8b, deepseek-r1:8b, and mistral:latest.
By systematically testing temperature values from 0.0 to 1.0 in increments of
0.1, we conducted 330 trials yielding 889 structured entities, validated for
syntactic consistency. Key findings reveal that model architecture
significantly influences computational efficiency, with mistral:latest and
llama3.1:8b processing data 8x faster than deepseek-r1:8b. Contrary to
expectations, temperature showed no correlation with processing time,
challenging assumptions about stochastic sampling costs. Output diversity
remained limited, as models consistently defaulted to common name archetypes
(e.g., 'John Doe' and 'Jane Smith') across all temperatures, though rare names
clustered at intermediate values (0.3-0.7). These results demonstrate that
architectural optimizations, rather than temperature adjustments, dominate
performance in structured generation tasks. The findings emphasize prioritizing
model selection over hyperparameter tuning for efficiency and suggest explicit
diversity constraints are necessary to mitigate default output biases in
synthetic data pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Domain Adaptation and Graph Neural Networks: A Tensor-Based
  Framework for Effective Label Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wen, Elynn Chen, Yuzhou Chen, Qi Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have recently become the predominant tools for
studying graph data. Despite state-of-the-art performance on graph
classification tasks, GNNs are overwhelmingly trained in a single domain under
supervision, thus necessitating a prohibitively high demand for labels and
resulting in poorly transferable representations. To address this challenge, we
propose the Label-Propagation Tensor Graph Neural Network (LP-TGNN) framework
to bridge the gap between graph data and traditional domain adaptation methods.
It extracts graph topological information holistically with a tensor
architecture and then reduces domain discrepancy through label propagation. It
is readily compatible with general GNNs and domain adaptation techniques with
minimal adjustment through pseudo-labeling. Experiments on various real-world
benchmarks show that our LP-TGNN outperforms baselines by a notable margin. We
also validate and analyze each component of the proposed framework in the
ablation study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning Topics through Weighting Aspect Keywords 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Nazari, Michael Weiss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling often requires examining topics from multiple perspectives to
uncover hidden patterns, especially in less explored areas. This paper presents
an approach to address this need, utilizing weighted keywords from various
aspects derived from a domain knowledge. The research method starts with
standard topic modeling. Then, it adds a process consisting of four key steps.
First, it defines keywords for each aspect. Second, it gives weights to these
keywords based on their relevance. Third, it calculates relevance scores for
aspect-weighted keywords and topic keywords to create aspect-topic models.
Fourth, it uses these scores to tune relevant new documents. Finally, the
generated topic models are interpreted and validated. The findings show that
top-scoring documents are more likely to be about the same aspect of a topic.
This highlights the model's effectiveness in finding the related documents to
the aspects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Federated Learning with Classifier-Free Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Obaidullah Zaland, Shutong Jin, Florian T. Pokorny, Monowar Bhuyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables collaborative learning without data
centralization but introduces significant communication costs due to multiple
communication rounds between clients and the server. One-shot federated
learning (OSFL) addresses this by forming a global model with a single
communication round, often relying on the server's model distillation or
auxiliary dataset generation - often through pre-trained diffusion models
(DMs). Existing DM-assisted OSFL methods, however, typically employ
classifier-guided DMs, which require training auxiliary classifier models at
each client, introducing additional computation overhead. This work introduces
OSCAR (One-Shot Federated Learning with Classifier-Free Diffusion Models), a
novel OSFL approach that eliminates the need for auxiliary models. OSCAR uses
foundation models to devise category-specific data representations at each
client, seamlessly integrated into a classifier-free diffusion model pipeline
for server-side data generation. OSCAR is a simple yet cost-effective OSFL
approach that outperforms the state-of-the-art on four benchmarking datasets
while reducing the communication load by at least 99%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Auto-regressive Chain-of-Thought through Loop-Aligned
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifan Yu, Zhenyu He, Sijie Li, Xun Zhou, Jun Zhang, Jingjing Xu, Di He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) prompting has emerged as a powerful technique for
enhancing language model's reasoning capabilities. However, generating long and
correct CoT trajectories is challenging. Recent studies have demonstrated that
Looped Transformers possess remarkable length generalization capabilities, but
their limited generality and adaptability prevent them from serving as an
alternative to auto-regressive solutions. To better leverage the strengths of
Looped Transformers, we propose RELAY (REasoning through Loop Alignment
iterativelY). Specifically, we align the steps of Chain-of-Thought (CoT)
reasoning with loop iterations and apply intermediate supervision during the
training of Looped Transformers. This additional iteration-wise supervision not
only preserves the Looped Transformer's ability for length generalization but
also enables it to predict CoT reasoning steps for unseen data. Therefore, we
leverage this Looped Transformer to generate accurate reasoning chains for
complex problems that exceed the training length, which will then be used to
fine-tune an auto-regressive model. We conduct extensive experiments, and the
results demonstrate the effectiveness of our approach, with significant
improvements in the performance of the auto-regressive model. Code will be
released at https://github.com/qifanyu/RELAY.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Numerical Schemes for Signature Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Cass, Francesco Piatti, Jeffrey Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Signature kernels have emerged as a powerful tool within kernel methods for
sequential data. In the paper "The Signature Kernel is the solution of a
Goursat PDE", the authors identify a kernel trick that demonstrates that, for
continuously differentiable paths, the signature kernel satisfies a Goursat
problem for a hyperbolic partial differential equation (PDE) in two independent
time variables. While finite difference methods have been explored for this
PDE, they face limitations in accuracy and stability when handling highly
oscillatory inputs. In this work, we introduce two advanced numerical schemes
that leverage polynomial representations of boundary conditions through either
approximation or interpolation techniques, and rigorously establish the
theoretical convergence of the polynomial approximation scheme. Experimental
evaluations reveal that our approaches yield improvements of several orders of
magnitude in mean absolute percentage error (MAPE) compared to traditional
finite difference schemes, without increasing computational complexity.
Furthermore, like finite difference methods, our algorithms can be
GPU-parallelized to reduce computational complexity from quadratic to linear in
the length of the input sequences, thereby improving scalability for
high-frequency data. We have implemented these algorithms in a dedicated Python
library, which is publicly available at:
https://github.com/FrancescoPiatti/polysigkernel.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skrr: Skip and Re-use Text Encoder Layers for Memory Efficient
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoigi Seo, Wongi Jeong, Jae-sun Seo, Se Young Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text encoders in text-to-image (T2I) diffusion models have
demonstrated exceptional performance in generating high-quality images from
textual prompts. Unlike denoising modules that rely on multiple iterative
steps, text encoders require only a single forward pass to produce text
embeddings. However, despite their minimal contribution to total inference time
and floating-point operations (FLOPs), text encoders demand significantly
higher memory usage, up to eight times more than denoising modules. To address
this inefficiency, we propose Skip and Re-use layers (Skrr), a simple yet
effective pruning strategy specifically designed for text encoders in T2I
diffusion models. Skrr exploits the inherent redundancy in transformer blocks
by selectively skipping or reusing certain layers in a manner tailored for T2I
tasks, thereby reducing memory consumption without compromising performance.
Extensive experiments demonstrate that Skrr maintains image quality comparable
to the original model even under high sparsity levels, outperforming existing
blockwise pruning methods. Furthermore, Skrr achieves state-of-the-art memory
efficiency while preserving performance across multiple evaluation metrics,
including the FID, CLIP, DreamSim, and GenEval scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Theory for Kernel Bilevel Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fares El Khoury, Edouard Pauwels, Samuel Vaiter, Michael Arbel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization has emerged as a technique for addressing a wide range
of machine learning problems that involve an outer objective implicitly
determined by the minimizer of an inner problem. In this paper, we investigate
the generalization properties for kernel bilevel optimization problems where
the inner objective is optimized over a Reproducing Kernel Hilbert Space. This
setting enables rich function approximation while providing a foundation for
rigorous theoretical analysis. In this context, we establish novel
generalization error bounds for the bilevel problem under finite-sample
approximation. Our approach adopts a functional perspective, inspired by
(Petrulionyte et al., 2024), and leverages tools from empirical process theory
and maximal inequalities for degenerate $U$-processes to derive uniform error
bounds. These generalization error estimates allow to characterize the
statistical accuracy of gradient-based methods applied to the empirical
discretization of the bilevel problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monge SAM: Robust Reparameterization-Invariant Sharpness-Aware
  Minimization Based on Loss Geometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Kjøller Jacobsen, Georgios Arvanitidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies on deep neural networks show that flat minima of the loss
landscape correlate with improved generalization. Sharpness-aware minimization
(SAM) efficiently finds flat regions by updating the parameters according to
the gradient at an adversarial perturbation. The perturbation depends on the
Euclidean metric, making SAM non-invariant under reparametrizations, which
blurs sharpness and generalization. We propose Monge SAM (M-SAM), a
reparametrization invariant version of SAM by considering a Riemannian metric
in the parameter space induced naturally by the loss surface. Compared to
previous approaches, M-SAM works under any modeling choice, relies only on mild
assumptions while being as computationally efficient as SAM. We theoretically
argue that M-SAM varies between SAM and gradient descent (GD), which increases
robustness to hyperparameter selection and reduces attraction to suboptimal
equilibria like saddle points. We demonstrate this behavior both theoretically
and empirically on a multi-modal representation alignment task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\texttt{LucidAtlas}$: Learning Uncertainty-Aware,
  Covariate-Disentangled, Individualized Atlas Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Jiao, Sreekalyani Bhamidi, Huaizhi Qu, Carlton Zdanski, Julia Kimbell, Andrew Prince, Cameron Worden, Samuel Kirse, Christopher Rutter, Benjamin Shields, William Dunn, Jisan Mahmud, Tianlong Chen, Marc Niethammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this work is to develop principled techniques to extract
information from high dimensional data sets with complex dependencies in areas
such as medicine that can provide insight into individual as well as population
level variation. We develop $\texttt{LucidAtlas}$, an approach that can
represent spatially varying information, and can capture the influence of
covariates as well as population uncertainty. As a versatile atlas
representation, $\texttt{LucidAtlas}$ offers robust capabilities for covariate
interpretation, individualized prediction, population trend analysis, and
uncertainty estimation, with the flexibility to incorporate prior knowledge.
Additionally, we discuss the trustworthiness and potential risks of neural
additive models for analyzing dependent covariates and then introduce a
marginalization approach to explain the dependence of an individual predictor
on the models' response (the atlas). To validate our method, we demonstrate its
generalizability on two medical datasets. Our findings underscore the critical
role of by-construction interpretable models in advancing scientific discovery.
Our code will be publicly available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Haystack to Needle: Label Space Reduction for Zero-shot
  Classification <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Vandemoortele, Bram Steenwinckel, Femke Ongenae, Sofie Van Hoecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Label Space Reduction (LSR), a novel method for improving
zero-shot classification performance of Large Language Models (LLMs). LSR
iteratively refines the classification label space by systematically ranking
and reducing candidate classes, enabling the model to concentrate on the most
relevant options. By leveraging unlabeled data with the statistical learning
capabilities of data-driven models, LSR dynamically optimizes the label space
representation at test time. Our experiments across seven benchmarks
demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to
14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet
compared to standard zero-shot classification baselines. To reduce the
computational overhead of LSR, which requires an additional LLM call at each
iteration, we propose distilling the model into a probabilistic classifier,
allowing for efficient inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Closer through commonality: Enhancing hypergraph contrastive learning
  with shared groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daeyoung Roh, Donghee Han, Daehee Kim, Keejun Han, Mun Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypergraphs provide a superior modeling framework for representing complex
multidimensional relationships in the context of real-world interactions that
often occur in groups, overcoming the limitations of traditional homogeneous
graphs. However, there have been few studies on hypergraphbased contrastive
learning, and existing graph-based contrastive learning methods have not been
able to fully exploit the highorder correlation information in hypergraphs.
Here, we propose a Hypergraph Fine-grained contrastive learning (HyFi) method
designed to exploit the complex high-dimensional information inherent in
hypergraphs. While avoiding traditional graph augmentation methods that corrupt
the hypergraph topology, the proposed method provides a simple and efficient
learning augmentation function by adding noise to node features. Furthermore,
we expands beyond the traditional dichotomous relationship between positive and
negative samples in contrastive learning by introducing a new relationship of
weak positives. It demonstrates the importance of fine-graining positive
samples in contrastive learning. Therefore, HyFi is able to produce highquality
embeddings, and outperforms both supervised and unsupervised baselines in
average rank on node classification across 10 datasets. Our approach
effectively exploits high-dimensional hypergraph information, shows significant
improvement over existing graph-based contrastive learning methods, and is
efficient in terms of training speed and GPU memory cost. The source code is
available at https://github.com/Noverse0/HyFi.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11page, 5 figures, 6 tables, 2024 IEEE International Conference on
  Big Data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Learning for Molecular Communication in Internet of Bio-Nano
  Things 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Cai, Ozgur B. Akan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular communication (MC) provides a foundational framework for
information transmission in the Internet of Bio-Nano Things (IoBNT), where
efficiency and reliability are crucial. However, the inherent limitations of
molecular channels, such as low transmission rates, noise, and inter-symbol
interference (ISI), limit their ability to support complex data transmission.
This paper proposes an end-to-end semantic learning framework designed to
optimize task-oriented molecular communication, with a focus on biomedical
diagnostic tasks under resource-constrained conditions. The proposed framework
employs a deep encoder-decoder architecture to efficiently extract, quantize,
and decode semantic features, prioritizing task-relevant semantic information
to enhance diagnostic classification performance. Additionally, a probabilistic
channel network is introduced to approximate molecular propagation dynamics,
enabling gradient-based optimization for end-to-end learning. Experimental
results demonstrate that the proposed semantic framework improves diagnostic
accuracy by at least 25% compared to conventional JPEG compression with LDPC
coding methods under resource-constrained communication scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multifidelity Simulation-based Inference for Computationally Expensive
  Simulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia N. Krouglova, Hayden R. Johnson, Basile Confavreux, Michael Deistler, Pedro J. Gonçalves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Across many domains of science, stochastic models are an essential tool to
understand the mechanisms underlying empirically observed data. Models can be
of different levels of detail and accuracy, with models of high-fidelity (i.e.,
high accuracy) to the phenomena under study being often preferable. However,
inferring parameters of high-fidelity models via simulation-based inference is
challenging, especially when the simulator is computationally expensive. We
introduce MF-NPE, a multifidelity approach to neural posterior estimation that
leverages inexpensive low-fidelity simulations to infer parameters of
high-fidelity simulators within a limited simulation budget. MF-NPE performs
neural posterior estimation with limited high-fidelity resources by virtue of
transfer learning, with the ability to prioritize individual observations using
active learning. On one statistical task with analytical ground-truth and two
real-world tasks, MF-NPE shows comparable performance to current approaches
while requiring up to two orders of magnitude fewer high-fidelity simulations.
Overall, MF-NPE opens new opportunities to perform efficient Bayesian inference
on computationally expensive simulators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Estimation of Inverse Covariance and Partial Correlation Matrices
  via Joint Partial Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Erickson, Tobias Rydén
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new method for estimating high-dimensional sparse partial
correlation and inverse covariance matrices, which exploits the connection
between the inverse covariance matrix and linear regression. The method is a
two-stage estimation method wherein each individual feature is regressed on all
other features while positive semi-definiteness is enforced simultaneously. We
provide statistical rates of convergence for the proposed method which match,
and improve upon, the state-of-the-art for inverse covariance and partial
correlation matrix estimation, respectively. We also propose an efficient
proximal splitting algorithm for numerically computing the estimate. The
effectiveness of the proposed method is demonstrated on both synthetic and
real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strong bounds for large-scale Minimum Sum-of-Squares Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Livia Croella, Veronica Piccialli, Antonio M. Sudoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clustering is a fundamental technique in data analysis and machine learning,
used to group similar data points together. Among various clustering methods,
the Minimum Sum-of-Squares Clustering (MSSC) is one of the most widely used.
MSSC aims to minimize the total squared Euclidean distance between data points
and their corresponding cluster centroids. Due to the unsupervised nature of
clustering, achieving global optimality is crucial, yet computationally
challenging. The complexity of finding the global solution increases
exponentially with the number of data points, making exact methods impractical
for large-scale datasets. Even obtaining strong lower bounds on the optimal
MSSC objective value is computationally prohibitive, making it difficult to
assess the quality of heuristic solutions. We address this challenge by
introducing a novel method to validate heuristic MSSC solutions through
optimality gaps. Our approach employs a divide-and-conquer strategy,
decomposing the problem into smaller instances that can be handled by an exact
solver. The decomposition is guided by an auxiliary optimization problem, the
"anticlustering problem", for which we design an efficient heuristic.
Computational experiments demonstrate the effectiveness of the method for
large-scale instances, achieving optimality gaps below 3% in most cases while
maintaining reasonable computational times. These results highlight the
practicality of our approach in assessing feasible clustering solutions for
large datasets, bridging a critical gap in MSSC evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning <span class="highlight-title">Human</span>oid Standing-up <span class="highlight-title">Control</span> across Diverse Postures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Huang, Junli Ren, Huayi Wang, Zirui Wang, Qingwei Ben, Muning Wen, Xiao Chen, Jianan Li, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standing-up control is crucial for humanoid robots, with the potential for
integration into current locomotion and loco-manipulation systems, such as fall
recovery. Existing approaches are either limited to simulations that overlook
hardware constraints or rely on predefined ground-specific motion trajectories,
failing to enable standing up across postures in real-world scenes. To bridge
this gap, we present HoST (Humanoid Standing-up Control), a reinforcement
learning framework that learns standing-up control from scratch, enabling
robust sim-to-real transfer across diverse postures. HoST effectively learns
posture-adaptive motions by leveraging a multi-critic architecture and
curriculum-based training on diverse simulated terrains. To ensure successful
real-world deployment, we constrain the motion with smoothness regularization
and implicit motion speed bound to alleviate oscillatory and violent motions on
physical hardware, respectively. After simulation-based training, the learned
control policies are directly deployed on the Unitree G1 humanoid robot. Our
experimental results demonstrate that the controllers achieve smooth, stable,
and robust standing-up motions across a wide range of laboratory and outdoor
environments. Videos are available at
https://taohuang13.github.io/humanoid-standingup.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Humanoid Standing-up Control, 12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Load Forecasting with GAT-LSTM: Leveraging Grid and Temporal
  Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ugochukwu Orji, Çiçek Güven, Dan Stowell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate power load forecasting is essential for the efficient operation and
planning of electrical grids, particularly given the increased variability and
complexity introduced by renewable energy sources. This paper introduces
GAT-LSTM, a hybrid model that combines Graph Attention Networks (GAT) and Long
Short-Term Memory (LSTM) networks. A key innovation of the model is the
incorporation of edge attributes, such as line capacities and efficiencies,
into the attention mechanism, enabling it to dynamically capture spatial
relationships grounded in grid-specific physical and operational constraints.
Additionally, by employing an early fusion of spatial graph embeddings and
temporal sequence features, the model effectively learns and predicts complex
interactions between spatial dependencies and temporal patterns, providing a
realistic representation of the dynamics of power grids. Experimental
evaluations on the Brazilian Electricity System dataset demonstrate that the
GAT-LSTM model significantly outperforms state-of-the-art models, achieving
reductions of 21. 8% in MAE, 15. 9% in RMSE and 20. 2% in MAPE. These results
underscore the robustness and adaptability of the GAT-LSTM model, establishing
it as a powerful tool for applications in grid management and energy planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Principled Multi-Agent Task Agnostic Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Zamboni, Mirco Mutti, Marcello Restelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning, we typically refer to task-agnostic exploration
when we aim to explore the environment without access to the task specification
a priori. In a single-agent setting the problem has been extensively studied
and mostly understood. A popular approach cast the task-agnostic objective as
maximizing the entropy of the state distribution induced by the agent's policy,
from which principles and methods follows. In contrast, little is known about
task-agnostic exploration in multi-agent settings, which are ubiquitous in the
real world. How should different agents explore in the presence of others? In
this paper, we address this question through a generalization to multiple
agents of the problem of maximizing the state distribution entropy. First, we
investigate alternative formulations, highlighting respective positives and
negatives. Then, we present a scalable, decentralized, trust-region policy
search algorithm to address the problem in practical settings. Finally, we
provide proof of concept experiments to both corroborate the theoretical
findings and pave the way for task-agnostic exploration in challenging
multi-agent settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Pre-Trained Diffusion Model Distillations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhui Fan, Zhangkai Wu, Hongyu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models~(DMs) have emerged as the dominant approach in Generative
Artificial Intelligence (GenAI), owing to their remarkable performance in tasks
such as text-to-image synthesis. However, practical DMs, such as stable
diffusion, are typically trained on massive datasets and thus usually require
large storage. At the same time, many steps may be required, i.e., recursively
evaluating the trained neural network, to generate a high-quality image, which
results in significant computational costs during sample generation. As a
result, distillation methods on pre-trained DM have become widely adopted
practices to develop smaller, more efficient models capable of rapid, few-step
generation in low-resource environment. When these distillation methods are
developed from different perspectives, there is an urgent need for a systematic
survey, particularly from a methodological perspective. In this survey, we
review distillation methods through three aspects: output loss distillation,
trajectory distillation and adversarial distillation. We also discuss current
challenges and outline future research directions in the conclusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing machine fault diagnosis: A detailed examination of
  convolutional neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Govind Vashishtha, Sumika Chauhan, Mert Sehri, Justyna Hebda-Sobkowicz, Radoslaw Zimroz, Patrick Dumond, Rajesh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing complexity of machinery and the increasing demand for operational
efficiency and safety have driven the development of advanced fault diagnosis
techniques. Among these, convolutional neural networks (CNNs) have emerged as a
powerful tool, offering robust and accurate fault detection and classification
capabilities. This comprehensive review delves into the application of CNNs in
machine fault diagnosis, covering its theoretical foundation, architectural
variations, and practical implementations. The strengths and limitations of
CNNs are analyzed in this domain, discussing their effectiveness in handling
various fault types, data complexities, and operational environments.
Furthermore, we explore the evolving landscape of CNN-based fault diagnosis,
examining recent advancements in data augmentation, transfer learning, and
hybrid architectures. Finally, we highlight future research directions and
potential challenges to further enhance the application of CNNs for reliable
and proactive machine fault diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Loss Landscape Analysis for Reliable Quantized ML Models for Scientific
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Baldi, Javier Campos, Olivia Weng, Caleb Geniesse, Nhan Tran, Ryan Kastner, Alessandro Biondi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a method to perform empirical analysis of the loss
landscape of machine learning (ML) models. The method is applied to two ML
models for scientific sensing, which necessitates quantization to be deployed
and are subject to noise and perturbations due to experimental conditions. Our
method allows assessing the robustness of ML models to such effects as a
function of quantization precision and under different regularization
techniques -- two crucial concerns that remained underexplored so far. By
investigating the interplay between performance, efficiency, and robustness by
means of loss landscape analysis, we both established a strong correlation
between gently-shaped landscapes and robustness to input and weight
perturbations and observed other intriguing and non-obvious phenomena. Our
method allows a systematic exploration of such trade-offs a priori, i.e.,
without training and testing multiple models, leading to more efficient
development workflows. This work also highlights the importance of
incorporating robustness into the Pareto optimization of ML models, enabling
more reliable and adaptive scientific sensing systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustworthy GNNs with LLMs: A Systematic <span class="highlight-title">Review</span> and Taxonomy <span class="chip">IJCAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhan Xue, Huimin Deng, Fang He, Maojun Wang, Zeyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the extensive application of Graph Neural Networks (GNNs) across various
domains, their trustworthiness has emerged as a focal point of research. Some
existing studies have shown that the integration of large language models
(LLMs) can improve the semantic understanding and generation capabilities of
GNNs, which in turn improves the trustworthiness of GNNs from various aspects.
Our review introduces a taxonomy that offers researchers a clear framework for
comprehending the principles and applications of different methods and helps
clarify the connections and differences among various approaches. Then we
systematically survey representative approaches along the four categories of
our taxonomy. Through our taxonomy, researchers can understand the applicable
scenarios, potential advantages, and limitations of each approach for the the
trusted integration of GNNs with LLMs. Finally, we present some promising
directions of work and future trends for the integration of LLMs and GNNs to
improve model trustworthiness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IJCAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Foundation Models for Recommendation: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wu, Yihang Wang, Yuanhao Zeng, Jiawei Liu, Jiashu Zhao, Cheng Yang, Yawen Li, Long Xia, Dawei Yin, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems (RS) serve as a fundamental tool for navigating the vast
expanse of online information, with deep learning advancements playing an
increasingly important role in improving ranking accuracy. Among these, graph
neural networks (GNNs) excel at extracting higher-order structural information,
while large language models (LLMs) are designed to process and comprehend
natural language, making both approaches highly effective and widely adopted.
Recent research has focused on graph foundation models (GFMs), which integrate
the strengths of GNNs and LLMs to model complex RS problems more efficiently by
leveraging the graph-based structure of user-item relationships alongside
textual understanding. In this survey, we provide a comprehensive overview of
GFM-based RS technologies by introducing a clear taxonomy of current
approaches, diving into methodological details, and highlighting key challenges
and future directions. By synthesizing recent advancements, we aim to offer
valuable insights into the evolving landscape of GFM-based recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Learning-based Graph Partition for Large-scale Vehicle
  Routing Problems <span class="chip">AAMAS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Pan, Ruohong Liu, Yize Chen, Zhiguang Cao, Fangzhen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural solvers based on the divide-and-conquer approach for Vehicle Routing
Problems (VRPs) in general, and capacitated VRP (CVRP) in particular,
integrates the global partition of an instance with local constructions for
each subproblem to enhance generalization. However, during the global partition
phase, misclusterings within subgraphs have a tendency to progressively
compound throughout the multi-step decoding process of the learning-based
partition policy. This suboptimal behavior in the global partition phase, in
turn, may lead to a dramatic deterioration in the performance of the overall
decomposition-based system, despite using optimal local constructions. To
address these challenges, we propose a versatile Hierarchical Learning-based
Graph Partition (HLGP) framework, which is tailored to benefit the partition of
CVRP instances by synergistically integrating global and local partition
policies. Specifically, the global partition policy is tasked with creating the
coarse multi-way partition to generate the sequence of simpler two-way
partition subtasks. These subtasks mark the initiation of the subsequent K
local partition levels. At each local partition level, subtasks exclusive for
this level are assigned to the local partition policy which benefits from the
insensitive local topological features to incrementally alleviate the
compounded errors. This framework is versatile in the sense that it optimizes
the involved partition policies towards a unified objective harmoniously
compatible with both reinforcement learning (RL) and supervised learning (SL).
(*Due to the notification of arXiv "The Abstract field cannot be longer than
1,920 characters", the appeared Abstract is shortened. For the full Abstract,
please download the Article.)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Full Paper at AAMAS 2025 (24th International Conference
  on Autonomous Agents and Multiagent Systems)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EEG Artifact Detection and Correction with Deep Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Aquilué-Llorens, Aureli Soria-Frisch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  EEG signals convey important information about brain activity both in healthy
and pathological conditions. However, they are inherently noisy, which poses
significant challenges for accurate analysis and interpretation. Traditional
EEG artifact removal methods, while effective, often require extensive expert
intervention. This study presents LSTEEG, a novel LSTM-based autoencoder
designed for the detection and correction of artifacts in EEG signals.
Leveraging deep learning, particularly LSTM layers, LSTEEG captures non-linear
dependencies in sequential EEG data. LSTEEG demonstrates superior performance
in both artifact detection and correction tasks compared to other
state-of-the-art convolutional autoencoders. Our methodology enhances the
interpretability and utility of the autoencoder's latent space, enabling
data-driven automated artefact removal in EEG its application in downstream
tasks. This research advances the field of efficient and accurate multi-channel
EEG preprocessing, and promotes the implementation and usage of automated EEG
analysis pipelines for brain health applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Models! Explainable Data Valuation and Metric Adaption for
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renqi Jia, Xiaokun Zhang, Bowei He, Qiannan Zhu, Weitao Xu, Jiehao Chen, Chen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User behavior records serve as the foundation for recommender systems. While
the behavior data exhibits ease of acquisition, it often suffers from varying
quality. Current methods employ data valuation to discern high-quality data
from low-quality data. However, they tend to employ black-box design, lacking
transparency and interpretability. Besides, they are typically tailored to
specific evaluation metrics, leading to limited generality across various
tasks. To overcome these issues, we propose an explainable and versatile
framework DVR which can enhance the efficiency of data utilization tailored to
any requirements of the model architectures and evaluation metrics. For
explainable data valuation, a data valuator is presented to evaluate the data
quality via calculating its Shapley value from the game-theoretic perspective,
ensuring robust mathematical properties and reliability. In order to
accommodate various evaluation metrics, including differentiable and
non-differentiable ones, a metric adapter is devised based on reinforcement
learning, where a metric is treated as the reinforcement reward that guides
model optimization. Extensive experiments conducted on various benchmarks
verify that our framework can improve the performance of current recommendation
algorithms on various metrics including ranking accuracy, diversity, and
fairness. Specifically, our framework achieves up to 34.7\% improvements over
existing methods in terms of representative NDCG metric. The code is available
at https://github.com/renqii/DVR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Multi-Agent Framework for Carbon-Efficient Liquid-Cooled
  Data Center Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyendu Sarkar, Avisek Naug, Antonio Guillen, Vineet Gundecha, Ricardo Luna Gutierrez, Sahand Ghorbanpour, Sajad Mousavi, Ashwin Ramesh Babu, Desik Rengarajan, Cullen Bash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing the environmental impact of cloud computing requires efficient
workload distribution across geographically dispersed Data Center Clusters
(DCCs) and simultaneously optimizing liquid and air (HVAC) cooling with time
shift of workloads within individual data centers (DC). This paper introduces
Green-DCC, which proposes a Reinforcement Learning (RL) based hierarchical
controller to optimize both workload and liquid cooling dynamically in a DCC.
By incorporating factors such as weather, carbon intensity, and resource
availability, Green-DCC addresses realistic constraints and interdependencies.
We demonstrate how the system optimizes multiple data centers synchronously,
enabling the scope of digital twins, and compare the performance of various RL
approaches based on carbon emissions and sustainability metrics while also
offering a framework and benchmark simulation for broader ML research in
sustainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Free Counterfactual Subset Selection at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Hieu Nguyen, Viet Hung Doan, Anh Tuan Nguyen, Jun Jo, Quoc Viet Hung Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring transparency in AI decision-making requires interpretable
explanations, particularly at the instance level. Counterfactual explanations
are a powerful tool for this purpose, but existing techniques frequently depend
on synthetic examples, introducing biases from unrealistic assumptions, flawed
models, or skewed data. Many methods also assume full dataset availability, an
impractical constraint in real-time environments where data flows continuously.
In contrast, streaming explanations offer adaptive, real-time insights without
requiring persistent storage of the entire dataset. This work introduces a
scalable, model-free approach to selecting diverse and relevant counterfactual
examples directly from observed data. Our algorithm operates efficiently in
streaming settings, maintaining $O(\log k)$ update complexity per item while
ensuring high-quality counterfactual selection. Empirical evaluations on both
real-world and synthetic datasets demonstrate superior performance over
baseline methods, with robust behavior even under adversarial conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Evaluation for Job-Shop Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imanol Echeverria, Maialen Murua, Roberto Santana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial optimization problems, such as scheduling and route planning,
are crucial in various industries but are computationally intractable due to
their NP-hard nature. Neural Combinatorial Optimization methods leverage
machine learning to address these challenges but often depend on sequential
decision-making, which is prone to error accumulation as small mistakes
propagate throughout the process. Inspired by self-evaluation techniques in
Large Language Models, we propose a novel framework that generates and
evaluates subsets of assignments, moving beyond traditional stepwise
approaches. Applied to the Job-Shop Scheduling Problem, our method integrates a
heterogeneous graph neural network with a Transformer to build a policy model
and a self-evaluation function. Experimental validation on challenging,
well-known benchmarks demonstrates the effectiveness of our approach,
surpassing state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning approach for parametrized and time dependent Partial
  Differential Equations using Dimensionality Reduction and Neural ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Longhi, Danny Lathouwers, Zoltán Perkó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial Differential Equations (PDEs) are central to science and engineering.
Since solving them is computationally expensive, a lot of effort has been put
into approximating their solution operator via both traditional and recently
increasingly Deep Learning (DL) techniques. A conclusive methodology capable of
accounting both for (continuous) time and parameter dependency in such DL
models however is still lacking. In this paper, we propose an autoregressive
and data-driven method using the analogy with classical numerical solvers for
time-dependent, parametric and (typically) nonlinear PDEs. We present how
Dimensionality Reduction (DR) can be coupled with Neural Ordinary Differential
Equations (NODEs) in order to learn the solution operator of arbitrary PDEs.
The idea of our work is that it is possible to map the high-fidelity (i.e.,
high-dimensional) PDE solution space into a reduced (low-dimensional) space,
which subsequently exhibits dynamics governed by a (latent) Ordinary
Differential Equation (ODE). Solving this (easier) ODE in the reduced space
allows avoiding solving the PDE in the high-dimensional solution space, thus
decreasing the computational burden for repeated calculations for e.g.,
uncertainty quantification or design optimization purposes. The main outcome of
this work is the importance of exploiting DR as opposed to the recent trend of
building large and complex architectures: we show that by leveraging DR we can
deliver not only more accurate predictions, but also a considerably lighter and
faster DL model compared to existing methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HDT: Hierarchical Discrete Transformer for Multivariate Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibo Feng, Peilin Zhao, Liu Liu, Pengcheng Wu, Zhiqi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have gained significant attention in multivariate time
series forecasting (MTS), particularly due to their ability to generate
high-fidelity samples. Forecasting the probability distribution of multivariate
time series is a challenging yet practical task. Although some recent attempts
have been made to handle this task, two major challenges persist: 1) some
existing generative methods underperform in high-dimensional multivariate time
series forecasting, which is hard to scale to higher dimensions; 2) the
inherent high-dimensional multivariate attributes constrain the forecasting
lengths of existing generative models. In this paper, we point out that
discrete token representations can model high-dimensional MTS with faster
inference time, and forecasting the target with long-term trends of itself can
extend the forecasting length with high accuracy. Motivated by this, we propose
a vector quantized framework called Hierarchical Discrete Transformer (HDT)
that models time series into discrete token representations with l2
normalization enhanced vector quantized strategy, in which we transform the MTS
forecasting into discrete tokens generation. To address the limitations of
generative models in long-term forecasting, we propose a hierarchical discrete
Transformer. This model captures the discrete long-term trend of the target at
the low level and leverages this trend as a condition to generate the discrete
representation of the target at the high level that introduces the features of
the target itself to extend the forecasting length in high-dimensional MTS.
Extensive experiments on five popular MTS datasets verify the effectiveness of
our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Existing Optimization Algorithms with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camilo Chacón Sartori, Christian Blum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Large Language Models (LLMs) into optimization has created
a powerful synergy, opening exciting research opportunities. This paper
investigates how LLMs can enhance existing optimization algorithms. Using their
pre-trained knowledge, we demonstrate their ability to propose innovative
heuristic variations and implementation strategies. To evaluate this, we
applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt
(CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that
incorporates a heuristic in the solution construction phase. Our results show
that an alternative heuristic proposed by GPT-4o outperforms the
expert-designed heuristic of CMSA, with the performance gap widening on larger
and denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Pre-trained Embeddings in Binary Code Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alwin Maier, Felix Weissberg, Konrad Rieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has enabled remarkable progress in binary code analysis. In
particular, pre-trained embeddings of assembly code have become a gold standard
for solving analysis tasks, such as measuring code similarity or recognizing
functions. These embeddings are capable of learning a vector representation
from unlabeled code. In contrast to natural language processing, however, label
information is not scarce for many tasks in binary code analysis. For example,
labeled training data for function boundaries, optimization levels, and
argument types can be easily derived from debug information provided by a
compiler. Consequently, the main motivation of embeddings does not transfer
directly to binary code analysis.
  In this paper, we explore the role of pre-trained embeddings from a critical
perspective. To this end, we systematically evaluate recent embeddings for
assembly code on five downstream tasks using a corpus of 1.2 million functions
from the Debian distribution. We observe that several embeddings perform
similarly when sufficient labeled data is available, and that differences
reported in prior work are hardly noticeable. Surprisingly, we find that
end-to-end learning without pre-training performs best on average, which calls
into question the need for specialized embeddings. By varying the amount of
labeled data, we eventually derive guidelines for when embeddings offer
advantages and when end-to-end learning is preferable for binary code analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Pricing for Graph Neural Networks without Pre-purchased Inspection <span class="chip">AAMAS-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Liu, Mengxiao Zhang, Jiamou Liu, Song Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) models have become essential tools in various
scenarios. Their effectiveness, however, hinges on a substantial volume of data
for satisfactory performance. Model marketplaces have thus emerged as crucial
platforms bridging model consumers seeking ML solutions and data owners
possessing valuable data. These marketplaces leverage model trading mechanisms
to properly incentive data owners to contribute their data, and return a well
performing ML model to the model consumers. However, existing model trading
mechanisms often assume the data owners are willing to share their data before
being paid, which is not reasonable in real world. Given that, we propose a
novel mechanism, named Structural Importance based Model Trading (SIMT)
mechanism, that assesses the data importance and compensates data owners
accordingly without disclosing the data. Specifically, SIMT procures feature
and label data from data owners according to their structural importance, and
then trains a graph neural network for model consumers. Theoretically, SIMT
ensures incentive compatible, individual rational and budget feasible. The
experiments on five popular datasets validate that SIMT consistently
outperforms vanilla baselines by up to $40\%$ in both MacroF1 and MicroF1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAMAS-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Individualised Treatment Effects Estimation with Composite Treatments
  and Composite Outcomes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinod Kumar Chauhan, Lei Clifton, Gaurav Nigam, David A. Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating individualised treatment effect (ITE) -- that is the causal effect
of a set of variables (also called exposures, treatments, actions, policies, or
interventions), referred to as \textit{composite treatments}, on a set of
outcome variables of interest, referred to as \textit{composite outcomes}, for
a unit from observational data -- remains a fundamental problem in causal
inference with applications across disciplines, such as healthcare, economics,
education, social science, marketing, and computer science. Previous work in
causal machine learning for ITE estimation is limited to simple settings, like
single treatments and single outcomes. This hinders their use in complex
real-world scenarios; for example, consider studying the effect of different
ICU interventions, such as beta-blockers and statins for a patient admitted for
heart surgery, on different outcomes of interest such as atrial fibrillation
and in-hospital mortality. The limited research into composite treatments and
outcomes is primarily due to data scarcity for all treatments and outcomes. To
address the above challenges, we propose a novel and innovative
hypernetwork-based approach, called \emph{H-Learner}, to solve ITE estimation
under composite treatments and composite outcomes, which tackles the data
scarcity issue by dynamically sharing information across treatments and
outcomes. Our empirical analysis with binary and arbitrary composite treatments
and outcomes demonstrates the effectiveness of the proposed approach compared
to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages (double column), 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dealing with Annotator Disagreement in Hate Speech Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Somaiyeh Dehghan, Mehmet Umut Sen, Berrin Yanikoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech detection is a crucial task, especially on social media, where
harmful content can spread quickly. Implementing machine learning models to
automatically identify and address hate speech is essential for mitigating its
impact and preventing its proliferation. The first step in developing an
effective hate speech detection model is to acquire a high-quality dataset for
training. Labeled data is foundational for most natural language processing
tasks, but categorizing hate speech is difficult due to the diverse and often
subjective nature of hate speech, which can lead to varying interpretations and
disagreements among annotators. This paper examines strategies for addressing
annotator disagreement, an issue that has been largely overlooked. In
particular, we evaluate different approaches to deal with annotator
disagreement regarding hate speech classification in Turkish tweets, based on a
fine-tuned BERT model. Our work highlights the importance of the problem and
provides state-of-art benchmark results for detection and understanding of hate
speech in online discourse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centrally Coordinated Multi-Agent Reinforcement Learning for Power Grid
  Topology <span class="highlight-title">Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barbera de Mol, Davide Barbieri, Jan Viebahn, Davide Grossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Power grid operation is becoming more complex due to the increase in
generation of renewable energy. The recent series of Learning To Run a Power
Network (L2RPN) competitions have encouraged the use of artificial agents to
assist human dispatchers in operating power grids. However, the combinatorial
nature of the action space poses a challenge to both conventional optimizers
and learned controllers. Action space factorization, which breaks down
decision-making into smaller sub-tasks, is one approach to tackle the curse of
dimensionality. In this study, we propose a centrally coordinated multi-agent
(CCMA) architecture for action space factorization. In this approach, regional
agents propose actions and subsequently a coordinating agent selects the final
action. We investigate several implementations of the CCMA architecture, and
benchmark in different experimental settings against various L2RPN baseline
approaches. The CCMA architecture exhibits higher sample efficiency and
superior final performance than the baseline approaches. The results suggest
high potential of the CCMA approach for further application in
higher-dimensional L2RPN as well as real-world power grid settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenIAS: Generator for Instantiating Anomalies in time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Zamanzadeh Darban, Qizhou Wang, Geoffrey I. Webb, Shirui Pan, Charu C. Aggarwal, Mahsa Salehi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent and promising approach for building time series anomaly detection
(TSAD) models is to inject synthetic samples of anomalies within real data
sets. The existing injection mechanisms have significant limitations - most of
them rely on ad hoc, hand-crafted strategies which fail to capture the natural
diversity of anomalous patterns, or are restricted to univariate time series
settings. To address these challenges, we design a generative model for TSAD
using a variational autoencoder, which is referred to as a Generator for
Instantiating Anomalies in Time Series (GenIAS). GenIAS is designed to produce
diverse and realistic synthetic anomalies for TSAD tasks. By employing a novel
learned perturbation mechanism in the latent space and injecting the perturbed
patterns in different segments of time series, GenIAS can generate anomalies
with greater diversity and varying scales. Further, guided by a new triplet
loss function, which uses a min-max margin and a new variance-scaling approach
to further enforce the learning of compact normal patterns, GenIAS ensures that
anomalies are distinct from normal samples while remaining realistic. The
approach is effective for both univariate and multivariate time series. We
demonstrate the diversity and realism of the generated anomalies. Our extensive
experiments demonstrate that GenIAS - when integrated into a TSAD task -
consistently outperforms seventeen traditional and deep anomaly detection
models, thereby highlighting the potential of generative models for time series
anomaly generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing optimism and pessimism in offline-to-online learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sentenac Flore, Lee Albin, Szepesvari Csaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider what we call the offline-to-online learning setting, focusing on
stochastic finite-armed bandit problems. In offline-to-online learning, a
learner starts with offline data collected from interactions with an unknown
environment in a way that is not under the learner's control. Given this data,
the learner begins interacting with the environment, gradually improving its
initial strategy as it collects more data to maximize its total reward. The
learner in this setting faces a fundamental dilemma: if the policy is deployed
for only a short period, a suitable strategy (in a number of senses) is the
Lower Confidence Bound (LCB) algorithm, which is based on pessimism. LCB can
effectively compete with any policy that is sufficiently "covered" by the
offline data. However, for longer time horizons, a preferred strategy is the
Upper Confidence Bound (UCB) algorithm, which is based on optimism. Over time,
UCB converges to the performance of the optimal policy at a rate that is nearly
the best possible among all online algorithms. In offline-to-online learning,
however, UCB initially explores excessively, leading to worse short-term
performance compared to LCB. This suggests that a learner not in control of how
long its policy will be in use should start with LCB for short horizons and
gradually transition to a UCB-like strategy as more rounds are played. This
article explores how and why this transition should occur. Our main result
shows that our new algorithm performs nearly as well as the better of LCB and
UCB at any point in time. The core idea behind our algorithm is broadly
applicable, and we anticipate that our results will extend beyond the
multi-armed bandit setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mathematical Reasoning in Large Language Models: Assessing Logical and
  Arithmetic Errors across Wide Numerical Ranges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safal Shrestha, Minwu Kim, Keith Ross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning in Large Language Models (LLMs) is often evaluated
using benchmarks with limited numerical ranges, failing to reflect real-world
problem-solving across diverse scales. Furthermore, most existing evaluation
methods only compare model outputs to ground-truth answers, obscuring insights
into reasoning processes. To address these limitations, we introduce
GSM-Ranges, a dataset generator derived from GSM8K that systematically perturbs
numerical values in math problems to assess model robustness across varying
numerical scales. Additionally, we propose a novel grading methodology that
distinguishes between logical and non-logical errors, offering a more precise
evaluation of reasoning processes beyond computational accuracy. Our
experiments with various models reveal a significant increase in logical error
rates-up to 14 percentage points-as numerical complexity rises, demonstrating a
general weakness in reasoning with out-of-distribution numerical values.
Moreover, while models demonstrate high accuracy on standalone arithmetic
tasks, their performance deteriorates substantially when computations are
embedded within word problems. These findings provide a comprehensive
evaluation of LLMs' mathematical reasoning capabilities and inform future
research directions for improving numerical generalization in language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-View Oriented GPLVM: Expressiveness and Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Yang, Ying Li, Zhidi Lin, Michael Minyi Zhang, Pablo M. Olmos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multi-view Gaussian process latent variable model (MV-GPLVM) aims to
learn a unified representation from multi-view data but is hindered by
challenges such as limited kernel expressiveness and low computational
efficiency. To overcome these issues, we first introduce a new duality between
the spectral density and the kernel function. By modeling the spectral density
with a bivariate Gaussian mixture, we then derive a generic and expressive
kernel termed Next-Gen Spectral Mixture (NG-SM) for MV-GPLVMs. To address the
inherent computational inefficiency of the NG-SM kernel, we propose a random
Fourier feature approximation. Combined with a tailored reparameterization
trick, this approximation enables scalable variational inference for both the
model and the unified latent representations. Numerical evaluations across a
diverse range of multi-view datasets demonstrate that our proposed method
consistently outperforms state-of-the-art models in learning meaningful latent
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keep your distance: learning dispersed embeddings on $\mathbb{S}_d$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgeniia Tokarchuk, Hua Chang Bakker, Vlad Niculae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning well-separated features in high-dimensional spaces, such as text or
image embeddings, is crucial for many machine learning applications. Achieving
such separation can be effectively accomplished through the dispersion of
embeddings, where unrelated vectors are pushed apart as much as possible. By
constraining features to be on a hypersphere, we can connect dispersion to
well-studied problems in mathematics and physics, where optimal solutions are
known for limited low-dimensional cases. However, in representation learning we
typically deal with a large number of features in high-dimensional space, and
moreover, dispersion is usually traded off with some other task-oriented
training objective, making existing theoretical and numerical solutions
inapplicable. Therefore, it is common to rely on gradient-based methods to
encourage dispersion, usually by minimizing some function of the pairwise
distances. In this work, we first give an overview of existing methods from
disconnected literature, making new connections and highlighting similarities.
Next, we introduce some new angles. We propose to reinterpret pairwise
dispersion using a maximum mean discrepancy (MMD) motivation. We then propose
an online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an
effective alternative regularizer for dispersion on generic domains. Finally,
we derive a novel dispersion method that directly exploits properties of the
hypersphere. Our experiments show the importance of dispersion in image
classification and natural language processing tasks, and how algorithms
exhibit different trade-offs in different regimes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Sample Selection by Cutting Mislabeled Easy Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suqin Yuan, Lei Feng, Bo Han, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample selection is a prevalent approach in learning with noisy labels,
aiming to identify confident samples for training. Although existing sample
selection methods have achieved decent results by reducing the noise rate of
the selected subset, they often overlook that not all mislabeled examples harm
the model's performance equally. In this paper, we demonstrate that mislabeled
examples correctly predicted by the model early in the training process are
particularly harmful to model performance. We refer to these examples as
Mislabeled Easy Examples (MEEs). To address this, we propose Early Cutting,
which introduces a recalibration step that employs the model's later training
state to re-select the confident subset identified early in training, thereby
avoiding misleading confidence from early learning and effectively filtering
out MEEs. Experiments on the CIFAR, WebVision, and full ImageNet-1k datasets
demonstrate that our method effectively improves sample selection and model
performance by reducing MEEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRISHUL: Towards Region Identification and Screen Hierarchy
  Understanding for Large VLM based GUI Agents <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Singh, Shreyas Singh, Mukund Khanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Vision Language Models (LVLMs) have enabled the
development of LVLM-based Graphical User Interface (GUI) agents under various
paradigms. Training-based approaches, such as CogAgent and SeeClick, struggle
with cross-dataset and cross-platform generalization due to their reliance on
dataset-specific training. Generalist LVLMs, such as GPT-4V, employ
Set-of-Marks (SoM) for action grounding, but obtaining SoM labels requires
metadata like HTML source, which is not consistently available across
platforms. Moreover, existing methods often specialize in singular GUI tasks
rather than achieving comprehensive GUI understanding. To address these
limitations, we introduce TRISHUL, a novel, training-free agentic framework
that enhances generalist LVLMs for holistic GUI comprehension. Unlike prior
works that focus on either action grounding (mapping instructions to GUI
elements) or GUI referring (describing GUI elements given a location), TRISHUL
seamlessly integrates both. At its core, TRISHUL employs Hierarchical Screen
Parsing (HSP) and the Spatially Enhanced Element Description (SEED) module,
which work synergistically to provide multi-granular, spatially, and
semantically enriched representations of GUI elements. Our results demonstrate
TRISHUL's superior performance in action grounding across the ScreenSpot,
VisualWebBench, AITW, and Mind2Web datasets. Additionally, for GUI referring,
TRISHUL surpasses the ToL agent on the ScreenPR benchmark, setting a new
standard for robust and adaptable GUI comprehension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at ICML 2025, 8 pages 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Driven Malware Classification with API Call Sequence
  Analysis and Concept Drift Handling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishwajit Prasad Gond, Durga Prasad Mohapatra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malware classification in dynamic environments presents a significant
challenge due to concept drift, where the statistical properties of malware
data evolve over time, complicating detection efforts. To address this issue,
we propose a deep learning framework enhanced with a genetic algorithm to
improve malware classification accuracy and adaptability. Our approach
incorporates mutation operations and fitness score evaluations within genetic
algorithms to continuously refine the deep learning model, ensuring robustness
against evolving malware threats. Experimental results demonstrate that this
hybrid method significantly enhances classification performance and
adaptability, outperforming traditional static models. Our proposed approach
offers a promising solution for real-time malware classification in
ever-changing cybersecurity landscapes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Modules: Knowledge Transfer from a Large to a Small Model using
  Enhanced Cross-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Kolomeitsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose an architecture of LLM Modules that enables the
transfer of knowledge from a large pre-trained model to a smaller model using
an Enhanced Cross-Attention mechanism. In the proposed scheme, the Qwen2-1.5B
model is frozen and its representations are passed through specially designed
attention layers to the GPT-Neo-125M model, which is trained on limited
computational resources. Experimental results on the Bespoke-Stratos-17k
dataset demonstrate that after 15 epochs of training, the combined model
generates responses comparable in quality to those obtained by distillation. We
discuss the advantages of the modular approach, provide examples of input
queries and comparative analysis, and outline prospects for further extension
of the method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and pre-trained weights available at
  https://huggingface.co/kkolomeitsev/llm-modules</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quality over Quantity: Boosting Data Efficiency Through Ensembled
  Multimodal Data Curation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinda Xu, Yuhao Song, Daming Wang, Weiwei Zhao, Minghua Chen, Kangliang Chen, Qinya Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era overwhelmed by vast amounts of data, the effective curation of
web-crawl datasets is essential for optimizing model performance. This paper
tackles the challenges associated with the unstructured and heterogeneous
nature of such datasets. Traditional heuristic curation methods often
inadequately capture complex features, resulting in biases and the exclusion of
relevant data. We introduce an advanced, learning-driven approach, Ensemble
Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel
quality-guided deduplication method to ensure balanced feature distributions.
EcoDatum strategically integrates various unimodal and multimodal data curation
operators within a weak supervision ensemble framework, utilizing automated
optimization to score each data point effectively. EcoDatum, which
significantly improves the data curation quality and efficiency, outperforms
existing state-of-the-art (SOTA) techniques, ranked 1st on the DataComp
leaderboard, with an average performance score of 0.182 across 38 diverse
evaluation datasets. This represents a 28% improvement over the DataComp
baseline method, demonstrating its effectiveness in improving dataset curation
and model training efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equivariant Masked Position Prediction for Efficient Molecular
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi An, Chao Qu, Yun-Fei Shi, XinHao Liu, Qianwei Tang, Fenglei Cao, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have shown considerable promise in computational
chemistry. However, the limited availability of molecular data raises concerns
regarding GNNs' ability to effectively capture the fundamental principles of
physics and chemistry, which constrains their generalization capabilities. To
address this challenge, we introduce a novel self-supervised approach termed
Equivariant Masked Position Prediction (EMPP), grounded in intramolecular
potential and force theory. Unlike conventional attribute masking techniques,
EMPP formulates a nuanced position prediction task that is more well-defined
and enhances the learning of quantum mechanical features. EMPP also bypasses
the approximation of the Gaussian mixture distribution commonly used in
denoising methods, allowing for more accurate acquisition of physical
properties. Experimental results indicate that EMPP significantly enhances
performance of advanced molecular architectures, surpassing state-of-the-art
self-supervised approaches. Our code is released in
https://github.com/ajy112/EMPP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Exploration in Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Papenmeier, Nuojin Cheng, Stephen Becker, Luigi Nardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A well-balanced exploration-exploitation trade-off is crucial for successful
acquisition functions in Bayesian optimization. However, there is a lack of
quantitative measures for exploration, making it difficult to analyze and
compare different acquisition functions. This work introduces two novel
approaches - observation traveling salesman distance and observation entropy -
to quantify the exploration characteristics of acquisition functions based on
their selected observations. Using these measures, we examine the explorative
nature of several well-known acquisition functions across a diverse set of
black-box problems, uncover links between exploration and empirical
performance, and reveal new relationships among existing acquisition functions.
Beyond enabling a deeper understanding of acquisition functions, these measures
also provide a foundation for guiding their design in a more principled and
systematic manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 34 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Asynchronous Federated Learning: A Delicate Trade-Off Between
  Model-Parameter Staleness and Update Frequency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelkrim Alahyane, Céline Comte, Matthieu Jonckheere, Éric Moulines
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synchronous federated learning (FL) scales poorly with the number of clients
due to the straggler effect. Algorithms like FedAsync and GeneralizedFedAsync
address this limitation by enabling asynchronous communication between clients
and the central server. In this work, we rely on stochastic modeling to better
understand the impact of design choices in asynchronous FL algorithms, such as
the concurrency level and routing probabilities, and we leverage this knowledge
to optimize loss. We characterize in particular a fundamental trade-off for
optimizing asynchronous FL: minimizing gradient estimation errors by avoiding
model parameter staleness, while also speeding up the system by increasing the
throughput of model updates. Our two main contributions can be summarized as
follows. First, we prove a discrete variant of Little's law to derive a
closed-form expression for relative delay, a metric that quantifies staleness.
This allows us to efficiently minimize the average loss per model update, which
has been the gold standard in literature to date. Second, we observe that
naively optimizing this metric leads us to slow down the system drastically by
overemphazing staleness at the detriment of throughput. This motivates us to
introduce an alternative metric that also takes system speed into account, for
which we derive a tractable upper-bound that can be minimized numerically.
Extensive numerical results show that these optimizations enhance accuracy by
10% to 30%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wisdom of the Crowds in Forecasting: Forecast Summarization for
  Supporting Future Event Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anisha Saha, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future Event Prediction (FEP) is an essential activity whose demand and
application range across multiple domains. While traditional methods like
simulations, predictive and time-series forecasting have demonstrated promising
outcomes, their application in forecasting complex events is not entirely
reliable due to the inability of numerical data to accurately capture the
semantic information related to events. One forecasting way is to gather and
aggregate collective opinions on the future to make predictions as cumulative
perspectives carry the potential to help estimating the likelihood of upcoming
events. In this work, we organize the existing research and frameworks that aim
to support future event prediction based on crowd wisdom through aggregating
individual forecasts. We discuss the challenges involved, available datasets,
as well as the scope of improvement and future research directions for this
task. We also introduce a novel data model to represent individual forecast
statements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy amplification by random allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitaly Feldman, Moshe Shenfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the privacy guarantees of an algorithm in which a user's data is
used in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$
differentially private steps. We demonstrate that the privacy guarantees of
this sampling scheme can be upper bound by the privacy guarantees of the
well-studied independent (or Poisson) subsampling in which each step uses the
user's data with probability $(1+ o(1))k/t $. Further, we provide two
additional analysis techniques that lead to numerical improvements in some
parameter regimes. The case of $k=1$ has been previously studied in the context
of DP-SGD in Balle et al. (2020) and very recently in Chua et al. (2024).
Privacy analysis of Balle et al. (2020) relies on privacy amplification by
shuffling which leads to overly conservative bounds. Privacy analysis of Chua
et al. (2024a) relies on Monte Carlo simulations that are computationally
prohibitive in many practical scenarios and have additional inherent
limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latest Advancements Towards Catastrophic Forgetting under Data Scarcity:
  A Comprehensive <span class="highlight-title">Survey</span> on Few-Shot Class Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Anwar Ma'sum, Mahardhika Pratama, Igor Skrjanc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity significantly complicates the continual learning problem, i.e.,
how a deep neural network learns in dynamic environments with very few samples.
However, the latest progress of few-shot class incremental learning (FSCIL)
methods and related studies show insightful knowledge on how to tackle the
problem. This paper presents a comprehensive survey on FSCIL that highlights
several important aspects i.e. comprehensive and formal objectives of FSCIL
approaches, the importance of prototype rectifications, the new learning
paradigms based on pre-trained model and language-guided mechanism, the deeper
analysis of FSCIL performance metrics and evaluation, and the practical
contexts of FSCIL in various areas. Our extensive discussion presents the open
challenges, potential solutions, and future directions of FSCIL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNNs May Determine Major Properties of Their Outputs Early, with Timing
  Possibly Driven by Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Park, Sanghyuk Chun, Byeongho Heo, Dongyoon Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper argues that deep neural networks (DNNs) mostly determine their
outputs during the early stages of inference, where biases inherent in the
model play a crucial role in shaping this process. We draw a parallel between
this phenomenon and human decision-making, which often relies on fast,
intuitive heuristics. Using diffusion models (DMs) as a case study, we
demonstrate that DNNs often make early-stage decision-making influenced by the
type and extent of bias in their design and training. Our findings offer a new
perspective on bias mitigation, efficient inference, and the interpretation of
machine learning systems. By identifying the temporal dynamics of
decision-making in DNNs, this paper aims to inspire further discussion and
research within the machine learning community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Individual Experience to Collective Evidence: A Reporting-Based
  Framework for Identifying Systemic Harms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Dai, Paula Gradu, Inioluwa Deborah Raji, Benjamin Recht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When an individual reports a negative interaction with some system, how can
their personal experience be contextualized within broader patterns of system
behavior? We study the incident database problem, where individual reports of
adverse events arrive sequentially, and are aggregated over time. In this work,
our goal is to identify whether there are subgroups--defined by any combination
of relevant features--that are disproportionately likely to experience harmful
interactions with the system. We formalize this problem as a sequential
hypothesis test, and identify conditions on reporting behavior that are
sufficient for making inferences about disparities in true rates of harm across
subgroups. We show that algorithms for sequential hypothesis tests can be
applied to this problem with a standard multiple testing correction. We then
demonstrate our method on real-world datasets, including mortgage decisions and
vaccine side effects; on each, our method (re-)identifies subgroups known to
experience disproportionate harm using only a fraction of the data that was
initially used to discover them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vertical Federated Learning in Practice: The Good, the Bad, and the Ugly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaomin Wu, Zhen Qin, Junyi Hou, Haodong Zhao, Qinbin Li, Bingsheng He, Lixin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertical Federated Learning (VFL) is a privacy-preserving collaborative
learning paradigm that enables multiple parties with distinct feature sets to
jointly train machine learning models without sharing their raw data. Despite
its potential to facilitate cross-organizational collaborations, the deployment
of VFL systems in real-world applications remains limited. To investigate the
gap between existing VFL research and practical deployment, this survey
analyzes the real-world data distributions in potential VFL applications and
identifies four key findings that highlight this gap. We propose a novel
data-oriented taxonomy of VFL algorithms based on real VFL data distributions.
Our comprehensive review of existing VFL algorithms reveals that some common
practical VFL scenarios have few or no viable solutions. Based on these
observations, we outline key research directions aimed at bridging the gap
between current VFL research and real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DGSense: A Domain Generalization Framework for Wireless Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhou, Yu Cheng, Songlin Li, Hongwang Zhang, Chenxu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wireless sensing is of great benefits to our daily lives. However, wireless
signals are sensitive to the surroundings. Various factors, e.g. environments,
locations, and individuals, may induce extra impact on wireless propagation.
Such a change can be regarded as a domain, in which the data distribution
shifts. A vast majority of the sensing schemes are learning-based. They are
dependent on the training domains, resulting in performance degradation in
unseen domains. Researchers have proposed various solutions to address this
issue. But these solutions leverage either semi-supervised or unsupervised
domain adaptation techniques. They still require some data in the target
domains and do not perform well in unseen domains. In this paper, we propose a
domain generalization framework DGSense, to eliminate the domain dependence
problem in wireless sensing. The framework is a general solution working across
diverse sensing tasks and wireless technologies. Once the sensing model is
built, it can generalize to unseen domains without any data from the target
domain. To achieve the goal, we first increase the diversity of the training
set by a virtual data generator, and then extract the domain independent
features via episodic training between the main feature extractor and the
domain feature extractors. The feature extractors employ a pre-trained Residual
Network (ResNet) with an attention mechanism for spatial features, and a 1D
Convolutional Neural Network (1DCNN) for temporal features. To demonstrate the
effectiveness and generality of DGSense, we evaluated on WiFi gesture
recognition, Millimeter Wave (mmWave) activity recognition, and acoustic fall
detection. All the systems exhibited high generalization capability to unseen
domains, including new users, locations, and environments, free of new data and
retraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Differential Privacy is Not Enough: A Sample Reconstruction Attack
  against Federated Learning with Local Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao You, Xuewen Dong, Shujun Li, Ximeng Liu, Siqi Ma, Yulong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstruction attacks against federated learning (FL) aim to reconstruct
users' samples through users' uploaded gradients. Local differential privacy
(LDP) is regarded as an effective defense against various attacks, including
sample reconstruction in FL, where gradients are clipped and perturbed.
Existing attacks are ineffective in FL with LDP since clipped and perturbed
gradients obliterate most sample information for reconstruction. Besides,
existing attacks embed additional sample information into gradients to improve
the attack effect and cause gradient expansion, leading to a more severe
gradient clipping in FL with LDP. In this paper, we propose a sample
reconstruction attack against LDP-based FL with any target models to
reconstruct victims' sensitive samples to illustrate that FL with LDP is not
flawless. Considering gradient expansion in reconstruction attacks and noise in
LDP, the core of the proposed attack is gradient compression and reconstructed
sample denoising. For gradient compression, an inference structure based on
sample characteristics is presented to reduce redundant gradients against LDP.
For reconstructed sample denoising, we artificially introduce zero gradients to
observe noise distribution and scale confidence interval to filter the noise.
Theoretical proof guarantees the effectiveness of the proposed attack.
Evaluations show that the proposed attack is the only attack that reconstructs
victims' training samples in LDP-based FL and has little impact on the target
model's accuracy. We conclude that LDP-based FL needs further improvements to
defend against sample reconstruction attacks effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Force Matching with Relativistic Constraints: A Physics-Inspired
  Approach to Stable and Efficient Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Cao, Bo Chen, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song, Mingda Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Force Matching (ForM), a novel framework for generative
modeling that represents an initial exploration into leveraging special
relativistic mechanics to enhance the stability of the sampling process. By
incorporating the Lorentz factor, ForM imposes a velocity constraint, ensuring
that sample velocities remain bounded within a constant limit. This constraint
serves as a fundamental mechanism for stabilizing the generative dynamics,
leading to a more robust and controlled sampling process. We provide a rigorous
theoretical analysis demonstrating that the velocity constraint is preserved
throughout the sampling procedure within the ForM framework. To validate the
effectiveness of our approach, we conduct extensive empirical evaluations. On
the \textit{half-moons} dataset, ForM significantly outperforms baseline
methods, achieving the lowest Euclidean distance loss of \textbf{0.714}, in
contrast to vanilla first-order flow matching (5.853) and first- and
second-order flow matching (5.793). Additionally, we perform an ablation study
to further investigate the impact of our velocity constraint, reaffirming the
superiority of ForM in stabilizing the generative process. The theoretical
guarantees and empirical results underscore the potential of integrating
special relativity principles into generative modeling. Our findings suggest
that ForM provides a promising pathway toward achieving stable, efficient, and
flexible generative processes. This work lays the foundation for future
advancements in high-dimensional generative modeling, opening new avenues for
the application of physical principles in machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Guided Wasserstein Distributionally Robust Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitao Wang, Ziyuan Wang, Molei Liu, Nian Si
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning is a popular strategy to leverage external knowledge and
improve statistical efficiency, particularly with a limited target sample. We
propose a novel knowledge-guided Wasserstein Distributionally Robust
Optimization (KG-WDRO) framework that adaptively incorporates multiple sources
of external knowledge to overcome the conservativeness of vanilla WDRO, which
often results in overly pessimistic shrinkage toward zero. Our method
constructs smaller Wasserstein ambiguity sets by controlling the transportation
along directions informed by the source knowledge. This strategy can alleviate
perturbations on the predictive projection of the covariates and protect
against information loss. Theoretically, we establish the equivalence between
our WDRO formulation and the knowledge-guided shrinkage estimation based on
collinear similarity, ensuring tractability and geometrizing the feasible set.
This also reveals a novel and general interpretation for recent shrinkage-based
transfer learning approaches from the perspective of distributional robustness.
In addition, our framework can adjust for scaling differences in the regression
models between the source and target and accommodates general types of
regularization such as lasso and ridge. Extensive simulations demonstrate the
superior performance and adaptivity of KG-WDRO in enhancing small-sample
transfer learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Democratizing AI: Open-source Scalable LLM Training on GPU-based
  Supercomputers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Prajwal Singhania, Aditya Ranjan, John Kirchenbauer, Jonas Geiping, Yuxin Wen, Neel Jain, Abhimanyu Hans, Manli Shu, Aditya Tomar, Tom Goldstein, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training and fine-tuning large language models (LLMs) with hundreds of
billions to trillions of parameters requires tens of thousands of GPUs, and a
highly scalable software stack. In this work, we present a novel
four-dimensional hybrid parallel algorithm implemented in a highly scalable,
portable, open-source framework called AxoNN. We describe several performance
optimizations in AxoNN to improve matrix multiply kernel performance, overlap
non-blocking collectives with computation, and performance modeling to choose
performance optimal configurations. These have resulted in unprecedented
scaling and peak flop/s (bf16) for training of GPT-style transformer models on
Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423
Exaflop/s).
  While the abilities of LLMs improve with the number of trainable parameters,
so do privacy and copyright risks caused by memorization of training data,
which can cause disclosure of sensitive or private information at inference
time. We highlight this side effect of scale through experiments that explore
"catastrophic memorization", where models are sufficiently large to memorize
training data in a single pass, and present an approach to prevent it. As part
of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using
AxoNN on Frontier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-dependent Bounds with $T$-Optimal Best-of-Both-Worlds Guarantees in
  Multi-Armed Bandits using Stability-Penalty Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Nguyen, Shinji Ito, Junpei Komiyama, Nishant A. Mehta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing data-dependent and best-of-both-worlds regret bounds for multi-armed
bandits problems have limited adaptivity as they are either data-dependent but
not best-of-both-worlds (BOBW), BOBW but not data-dependent or have sub-optimal
$O(\sqrt{T\ln{T}})$ worst-case guarantee in the adversarial regime. To overcome
these limitations, we propose real-time stability-penalty matching (SPM), a new
method for obtaining regret bounds that are simultaneously data-dependent,
best-of-both-worlds and $T$-optimal for multi-armed bandits problems. In
particular, we show that real-time SPM obtains bounds with worst-case
guarantees of order $O(\sqrt{T})$ in the adversarial regime and $O(\ln{T})$ in
the stochastic regime while simultaneously being adaptive to data-dependent
quantities such as sparsity, variations, and small losses. Our results are
obtained by extending the SPM technique for tuning the learning rates in the
follow-the-regularized-leader (FTRL) framework, which further indicates that
the combination of SPM and FTRL is a promising approach for proving new
adaptive bounds in online learning problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Zhou, Qizheng Zhang, Hermann Kumbong, Kunle Olukotun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) is increasingly costly as models
scale to hundreds of billions of parameters, and even parameter-efficient
fine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce
LowRA, the first framework to enable LoRA fine-tuning below 2 bits per
parameter with minimal performance loss. LowRA optimizes fine-grained
quantization - mapping, threshold selection, and precision assignment - while
leveraging efficient CUDA kernels for scalable deployment. Extensive
evaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior
performance-precision trade-off above 2 bits and remains accurate down to 1.15
bits, reducing memory usage by up to 50%. Our results highlight the potential
of ultra-low-bit LoRA fine-tuning for resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Learning of Linear Dynamical Systems with Transformers: Error
  Bounds and Depth-Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Cole, Yulong Lu, Tianhao Zhang, Yuxuan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates approximation-theoretic aspects of the in-context
learning capability of the transformers in representing a family of noisy
linear dynamical systems. Our first theoretical result establishes an upper
bound on the approximation error of multi-layer transformers with respect to an
$L^2$-testing loss uniformly defined across tasks. This result demonstrates
that transformers with logarithmic depth can achieve error bounds comparable
with those of the least-squares estimator. In contrast, our second result
establishes a non-diminishing lower bound on the approximation error for a
class of single-layer linear transformers, which suggests a depth-separation
phenomenon for transformers in the in-context learning of dynamical systems.
Moreover, this second result uncovers a critical distinction in the
approximation power of single-layer linear transformers when learning from IID
versus non-IID data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SS4Rec: Continuous-Time Sequential Recommendation with State Space
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xiao, Huiying Wang, Qifeng Zhou, Qing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation is a key area in the field of recommendation
systems aiming to model user interest based on historical interaction sequences
with irregular intervals. While previous recurrent neural network-based and
attention-based approaches have achieved significant results, they have
limitations in capturing system continuity due to the discrete characteristics.
In the context of continuous-time modeling, state space model (SSM) offers a
potential solution, as it can effectively capture the dynamic evolution of user
interest over time. However, existing SSM-based approaches ignore the impact of
irregular time intervals within historical user interactions, making it
difficult to model complexed user-item transitions in sequences. To address
this issue, we propose a hybrid SSM-based model called SS4Rec for
continuous-time sequential recommendation. SS4Rec integrates a time-aware SSM
to handle irregular time intervals and a relation-aware SSM to model contextual
dependencies, enabling it to infer user interest from both temporal and
sequential perspectives. In the training process, the time-aware SSM and the
relation-aware SSM are discretized by variable stepsizes according to user
interaction time intervals and input data, respectively. This helps capture the
continuous dependency from irregular time intervals and provides time-specific
personalized recommendations. Experimental studies on five benchmark datasets
demonstrate the superiority and effectiveness of SS4Rec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incremental Approximate Single-Source Shortest Paths with Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel McCauley, Benjamin Moseley, Aidin Niaparast, Helia Niaparast, Shikha Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The algorithms-with-predictions framework has been used extensively to
develop online algorithms with improved beyond-worst-case competitive ratios.
Recently, there is growing interest in leveraging predictions for designing
data structures with improved beyond-worst-case running times. In this paper,
we study the fundamental data structure problem of maintaining approximate
shortest paths in incremental graphs in the algorithms-with-predictions model.
Given a sequence $\sigma$ of edges that are inserted one at a time, the goal is
to maintain approximate shortest paths from the source to each vertex in the
graph at each time step. Before any edges arrive, the data structure is given a
prediction of the online edge sequence $\hat{\sigma}$ which is used to ``warm
start'' its state.
  As our main result, we design a learned algorithm that maintains
$(1+\epsilon)$-approximate single-source shortest paths, which runs in
$\tilde{O}(m \eta \log W/\epsilon)$ time, where $W$ is the weight of the
heaviest edge and $\eta$ is the prediction error. We show these techniques
immediately extend to the all-pairs shortest-path setting as well. Our
algorithms are consistent (performing nearly as fast as the offline algorithm)
when predictions are nearly perfect, have a smooth degradation in performance
with respect to the prediction error and, in the worst case, match the best
offline algorithm up to logarithmic factors.
  As a building block, we study the offline incremental approximate
single-source shortest-paths problem. In this problem, the edge sequence
$\sigma$ is known a priori and the goal is to efficiently return the length of
the shortest paths in the intermediate graph $G_t$ consisting of the first $t$
edges, for all $t$. Note that the offline incremental problem is defined in the
worst-case setting (without predictions) and is of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Robust Federated Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Fang, Xilong Wang, Neil Zhenqiang Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated reinforcement learning (FRL) allows agents to jointly learn a
global decision-making policy under the guidance of a central server. While FRL
has advantages, its decentralized design makes it prone to poisoning attacks.
To mitigate this, Byzantine-robust aggregation techniques tailored for FRL have
been introduced. Yet, in our work, we reveal that these current
Byzantine-robust techniques are not immune to our newly introduced Normalized
attack. Distinct from previous attacks that targeted enlarging the distance of
policy updates before and after an attack, our Normalized attack emphasizes on
maximizing the angle of deviation between these updates. To counter these
threats, we develop an ensemble FRL approach that is provably secure against
both known and our newly proposed attacks. Our ensemble method involves
training multiple global policies, where each is learnt by a group of agents
using any foundational aggregation rule. These well-trained global policies
then individually predict the action for a specific test state. The ultimate
action is chosen based on a majority vote for discrete action systems or the
geometric median for continuous ones. Our experimental results across different
settings show that the Normalized attack can greatly disrupt non-ensemble
Byzantine-robust methods, and our ensemble approach offers substantial
resistance against poisoning attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in The Web Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hookpad Aria: A Copilot for Songwriters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Donahue, Shih-Lun Wu, Yewon Kim, Dave Carlton, Ryan Miyakawa, John Thickstun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Hookpad Aria, a generative AI system designed to assist musicians
in writing Western pop songs. Our system is seamlessly integrated into Hookpad,
a web-based editor designed for the composition of lead sheets: symbolic music
scores that describe melody and harmony. Hookpad Aria has numerous generation
capabilities designed to assist users in non-sequential composition workflows,
including: (1) generating left-to-right continuations of existing material, (2)
filling in missing spans in the middle of existing material, and (3) generating
harmony from melody and vice versa. Hookpad Aria is also a scalable data
flywheel for music co-creation -- since its release in March 2024, Aria has
generated 318k suggestions for 3k users who have accepted 74k into their songs.
  More information about Hookpad Aria is available at
https://www.hooktheory.com/hookpad/aria
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended abstract presented in the Late-Breaking Demo Session at
  ISMIR 2024 (ISMIR LBD 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Wang, Sizhe Wei, Xiaoming Huo, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have made significant advancements in recent years. However,
their performance often deteriorates when trained or fine-tuned on imbalanced
datasets. This degradation is largely due to the disproportionate
representation of majority and minority data in image-text pairs. In this
paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address
this challenge. Rather than directly minimizing the KL divergence between the
predicted and ground-truth distributions, PoGDiff replaces the ground-truth
distribution with a Product of Gaussians (PoG), which is constructed by
combining the original ground-truth targets with the predicted distribution
conditioned on a neighboring text embedding. Experiments on real-world datasets
demonstrate that our method effectively addresses the imbalance problem in
diffusion models, improving both generation accuracy and quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Out-of-Distribution Detection on Graphs: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyi Cai, Yunliang Jiang, Yixin Liu, Ming Li, Changqin Huang, Shirui Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph machine learning has witnessed rapid growth, driving advancements
across diverse domains. However, the in-distribution assumption, where training
and testing data share the same distribution, often breaks in real-world
scenarios, leading to degraded model performance under distribution shifts.
This challenge has catalyzed interest in graph out-of-distribution (GOOD)
detection, which focuses on identifying graph data that deviates from the
distribution seen during training, thereby enhancing model robustness. In this
paper, we provide a rigorous definition of GOOD detection and systematically
categorize existing methods into four types: enhancement-based,
reconstruction-based, information propagation-based, and classification-based
approaches. We analyze the principles and mechanisms of each approach and
clarify the distinctions between GOOD detection and related fields, such as
graph anomaly detection, outlier detection, and GOOD generalization. Beyond
methodology, we discuss practical applications and theoretical foundations,
highlighting the unique challenges posed by graph data. Finally, we discuss the
primary challenges and propose future directions to advance this emerging
field. The repository of this survey is available at
https://github.com/ca1man-2022/Awesome-GOOD-Detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Tokenized Graph Transformers for Node Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinsong Chen, Chenyang Li, GaiChao Li, John E. Hopcroft, Kun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Node tokenized graph Transformers (GTs) have shown promising performance in
node classification. The generation of token sequences is the key module in
existing tokenized GTs which transforms the input graph into token sequences,
facilitating the node representation learning via Transformer. In this paper,
we observe that the generations of token sequences in existing GTs only focus
on the first-order neighbors on the constructed similarity graphs, which leads
to the limited usage of nodes to generate diverse token sequences, further
restricting the potential of tokenized GTs for node classification. To this
end, we propose a new method termed SwapGT. SwapGT first introduces a novel
token swapping operation based on the characteristics of token sequences that
fully leverages the semantic relevance of nodes to generate more informative
token sequences. Then, SwapGT leverages a Transformer-based backbone to learn
node representations from the generated token sequences. Moreover, SwapGT
develops a center alignment loss to constrain the representation learning from
multiple token sequences, further enhancing the model performance. Extensive
empirical results on various datasets showcase the superiority of SwapGT for
node classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised categorization of similarity measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshiyuki Ohmura, Wataru Shimaya, Yasuo Kuniyoshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In general, objects can be distinguished on the basis of their features, such
as color or shape. In particular, it is assumed that similarity judgments about
such features can be processed independently in different metric spaces.
However, the unsupervised categorization mechanism of metric spaces
corresponding to object features remains unknown. Here, we show that the
artificial neural network system can autonomously categorize metric spaces
through representation learning to satisfy the algebraic independence between
neural networks, and project sensory information onto multiple high-dimensional
metric spaces to independently evaluate the differences and similarities
between features. Conventional methods often constrain the axes of the latent
space to be mutually independent or orthogonal. However, the independent axes
are not suitable for categorizing metric spaces. High-dimensional metric spaces
that are independent of each other are not uniquely determined by the mutually
independent axes, because any combination of independent axes can form mutually
independent spaces. In other words, the mutually independent axes cannot be
used to naturally categorize different feature spaces, such as color space and
shape space. Therefore, constraining the axes to be mutually independent makes
it difficult to categorize high-dimensional metric spaces. To overcome this
problem, we developed a method to constrain only the spaces to be mutually
independent and not the composed axes to be independent. Our theory provides
general conditions for the unsupervised categorization of independent metric
spaces, thus advancing the mathematical theory of functional differentiation of
neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2306.00239</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Throughput SAT Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arash Ardakani, Minwoo Kang, Kevin He, Qijing Huang, John Wawrzynek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a novel technique for GPU-accelerated Boolean
satisfiability (SAT) sampling. Unlike conventional sampling algorithms that
directly operate on conjunctive normal form (CNF), our method transforms the
logical constraints of SAT problems by factoring their CNF representations into
simplified multi-level, multi-output Boolean functions. It then leverages
gradient-based optimization to guide the search for a diverse set of valid
solutions. Our method operates directly on the circuit structure of refactored
SAT instances, reinterpreting the SAT problem as a supervised multi-output
regression task. This differentiable technique enables independent bit-wise
operations on each tensor element, allowing parallel execution of learning
processes. As a result, we achieve GPU-accelerated sampling with significant
runtime improvements ranging from $33.6\times$ to $523.6\times$ over
state-of-the-art heuristic samplers. We demonstrate the superior performance of
our sampling method through an extensive evaluation on $60$ instances from a
public domain benchmark suite utilized in previous studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture of Decoupled Message Passing Experts with Entropy Constraint for
  General Node Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanze Chen, Jiajun Zhou, Jinsong Chen, Shanqing Yu, Qi Xuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The varying degrees of homophily and heterophily in real-world graphs
persistently constrain the universality of graph neural networks (GNNs) for
node classification. Adopting a data-centric perspective, this work reveals an
inherent preference of different graphs towards distinct message encoding
schemes: homophilous graphs favor local propagation, while heterophilous graphs
exhibit preference for flexible combinations of propagation and transformation.
To address this, we propose GNNMoE, a universal node classification framework
based on the Mixture-of-Experts (MoE) mechanism. The framework first constructs
diverse message-passing experts through recombination of fine-grained encoding
operators, then designs soft and hard gating layers to allocate the most
suitable expert networks for each node's representation learning, thereby
enhancing both model expressiveness and adaptability to diverse graphs.
Furthermore, considering that soft gating might introduce encoding noise in
homophilous scenarios, we introduce an entropy constraint to guide sharpening
of soft gates, achieving organic integration of weighted combination and Top-K
selection. Extensive experiments demonstrate that GNNMoE significantly
outperforms mainstream GNNs, heterophilous GNNs, and graph transformers in both
node classification performance and universality across diverse graph datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2412.08193</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cascading Bandits Robust to Adversarial Corruptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jize Xie, Cheng Chen, Zhiyong Wang, Shuai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online learning to rank sequentially recommends a small list of items to
users from a large candidate set and receives the users' click feedback. In
many real-world scenarios, users browse the recommended list in order and click
the first attractive item without checking the rest. Such behaviors are usually
formulated as the cascade model. Many recent works study algorithms for
cascading bandits, an online learning to rank framework in the cascade model.
However, the performance of existing methods may drop significantly if part of
the user feedback is adversarially corrupted (e.g., click fraud). In this work,
we study how to resist adversarial corruptions in cascading bandits. We first
formulate the ``\textit{Cascading Bandits with Adversarial Corruptions}" (CBAC)
problem, which assumes that there is an adaptive adversary that may manipulate
the user feedback. Then we propose two robust algorithms for this problem,
which assume the corruption level is known and agnostic, respectively. We show
that both algorithms can achieve logarithmic regret when the algorithm is not
under attack, and the regret increases linearly with the corruption level. The
experimental results also verify the robustness of our methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Performative Prediction Beyond the Insensitivity Assumption:
  A Case Study for Mortgage Competition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghui Wang, Krishna Acharya, Lokranjan Lakshmikanthan, Vidya Muthukumar, Juba Ziani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performative prediction models account for feedback loops in decision-making
processes where predictions influence future data distributions. While existing
work largely assumes insensitivity of data distributions to small strategy
changes, this assumption usually fails in real-world competitive (i.e.
multi-agent) settings. For example, in Bertrand-type competitions, a small
reduction in one firm's price can lead that firm to capture the entire demand,
while all others sharply lose all of their customers.
  We study a representative setting of multi-agent performative prediction in
which insensitivity assumptions do not hold, and investigate the convergence of
natural dynamics. To do so, we focus on a specific game that we call the ''Bank
Game'', where two lenders compete over interest rates and credit score
thresholds. Consumers act similarly as to in a Bertrand Competition, with each
consumer selecting the firm with the lowest interest rate that they are
eligible for based on the firms' credit thresholds. Our analysis characterizes
the equilibria of this game and demonstrates that when both firms use a common
and natural no-regret learning dynamic -- exponential weights -- with proper
initialization, the dynamics always converge to stable outcomes despite the
general-sum structure. Notably, our setting admits multiple stable equilibria,
with convergence dependent on initial conditions. We also provide theoretical
convergence results in the stochastic case when the utility matrix is not fully
known, but each learner can observe sufficiently many samples of consumers at
each time step to estimate it, showing robustness to slight mis-specifications.
Finally, we provide experimental results that validate our theoretical
findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Mechanistic Circuits for Extractive Question-Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samyadeep Basu, Vlad Morariu, Zichao Wang, Ryan Rossi, Cherry Zhao, Soheil Feizi, Varun Manjunatha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are increasingly used to process documents and
facilitate question-answering on them. In our paper, we extract mechanistic
circuits for this real-world language modeling task: context-augmented language
modeling for extractive question-answering (QA) tasks and understand the
potential benefits of circuits towards downstream applications such as data
attribution to context information. We extract circuits as a function of
internal model components (e.g., attention heads, MLPs) using causal mediation
analysis techniques. Leveraging the extracted circuits, we first understand the
interplay between the model's usage of parametric memory and retrieved context
towards a better mechanistic understanding of context-augmented language
models. We then identify a small set of attention heads in our circuit which
performs reliable data attribution by default, thereby obtaining attribution
for free in just the model's forward pass. Using this insight, we then
introduce ATTNATTRIB, a fast data attribution algorithm which obtains
state-of-the-art attribution results across various extractive QA benchmarks.
Finally, we show the possibility to steer the language model towards answering
from the context, instead of the parametric memory by using the attribution
from ATTNATTRIB as an additional signal during the forward pass. Beyond
mechanistic understanding, our paper provides tangible applications of circuits
in the form of reliable data attribution and model steering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ General Coded Computing: Adversarial Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parsa Moradi, Hanzaleh Akbarinodehi, Mohammad Ali Maddah-Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional coded computing frameworks are predominantly tailored for
structured computations, such as matrix multiplication and polynomial
evaluation. Such tasks allow the reuse of tools and techniques from algebraic
coding theory to improve the reliability of distributed systems in the presence
of stragglers and adversarial servers.
  This paper lays the foundation for general coded computing, which extends the
applicability of coded computing to handle a wide class of computations. In
addition, it particularly addresses the challenging problem of managing
adversarial servers. We demonstrate that, in the proposed scheme, for a system
with $N$ servers, where $\mathcal{O}(N^a)$, $a \in [0,1)$, are adversarial, the
supremum of the average approximation error over all adversarial strategies
decays at a rate of $N^{\frac{6}{5}(a-1)}$, under minimal assumptions on the
computing tasks. Furthermore, we show that within a general framework, the
proposed scheme achieves optimal adversarial robustness, in terms of maximum
number of adversarial servers it can tolerate. This marks a significant step
toward practical and reliable general coded computing. Implementation results
further validate the effectiveness of the proposed method in handling various
computations, including inference in deep neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cognify: Supercharging Gen-AI Workflows With Hierarchical Autotuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian He, Reyna Abhyankar, Vikranth Srivatsa, Yiying Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's gen-AI workflows that involve multiple ML model calls, tool/API
calls, data retrieval, or generic code execution are often tuned manually in an
ad-hoc way that is both time-consuming and error-prone. In this paper, we
propose a systematic approach for automatically tuning gen-AI workflows. Our
key insight is that gen-AI workflows can benefit from structure, operator, and
prompt changes, but unique properties of gen-AI workflows require new
optimization techniques. We propose AdaSeek, an adaptive hierarchical search
algorithm for autotuning gen-AI workflows. AdaSeek organizes workflow tuning
methods into different layers based on the user-specified total search budget
and distributes the budget across different layers based on the complexity of
each layer. During its hierarchical search, AdaSeek redistributes the search
budget from less useful to more promising tuning configurations based on
workflow-level evaluation results. We implement AdaSeek in a workflow
autotuning framework called Cognify and evaluate Cognify using six types of
workflows such as RAG-based QA and text-to-SQL transformation. Overall, Cognify
improves these workflows' generation quality by up to 2.8x, reduces execution
monetary cost by up to 10x, and reduces end-to-end latency by 2.7x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLVR: Securely Leveraging Client Validation for Robust Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihye Choi, Sai Rahul Rachuri, Ke Wang, Somesh Jha, Yizhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative model training while keeping
client data private. However, exposing individual client updates makes FL
vulnerable to reconstruction attacks. Secure aggregation mitigates such privacy
risks but prevents the server from verifying the validity of each client
update, creating a privacy-robustness tradeoff. Recent efforts attempt to
address this tradeoff by enforcing checks on client updates using
zero-knowledge proofs, but they support limited predicates and often depend on
public validation data. We propose SLVR, a general framework that securely
leverages clients' private data through secure multi-party computation. By
utilizing clients' data, SLVR not only eliminates the need for public
validation data, but also enables a wider range of checks for robustness,
including cross-client accuracy validation. It also adapts naturally to
distribution shifts in client data as it can securely refresh its validation
data up-to-date. Our empirical evaluations show that SLVR improves robustness
against model poisoning attacks, particularly outperforming existing methods by
up to 50% under adaptive attacks. Additionally, SLVR demonstrates effective
adaptability and stable convergence under various distribution shift scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual
  Occluded Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yamada, Alexander L. Mitchell, Jack Collins, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of occluded robot grasping, i.e. grasping
in situations where the desired grasp poses are kinematically infeasible due to
environmental constraints such as surface collisions. Traditional robot
manipulation approaches struggle with the complexity of non-prehensile or
bimanual strategies commonly used by humans in these circumstances.
State-of-the-art reinforcement learning (RL) methods are unsuitable due to the
inherent complexity of the task. In contrast, learning from demonstration
requires collecting a significant number of expert demonstrations, which is
often infeasible. Instead, inspired by human bimanual manipulation strategies,
where two hands coordinate to stabilise and reorient objects, we focus on a
bimanual robotic setup to tackle this challenge. In particular, we introduce
Constraint-based Manipulation for Bimanual Occluded Grasping (COMBO-Grasp), a
learning-based approach which leverages two coordinated policies: a constraint
policy trained using self-supervised datasets to generate stabilising poses and
a grasping policy trained using RL that reorients and grasps the target object.
A key contribution lies in value function-guided policy coordination.
Specifically, during RL training for the grasping policy, the constraint
policy's output is refined through gradients from a jointly trained value
function, improving bimanual coordination and task performance. Lastly,
COMBO-Grasp employs teacher-student policy distillation to effectively deploy
point cloud-based policies in real-world environments. Empirical evaluations
demonstrate that COMBO-Grasp significantly improves task success rates compared
to competitive baseline approaches, with successful generalisation to unseen
objects in both simulated and real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Art of Misclassification: Too Many Classes, Not Enough Points 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Franco, Gerardo Febres, Nelson Fernández, Carlos Gershenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classification is a ubiquitous and fundamental problem in artificial
intelligence and machine learning, with extensive efforts dedicated to
developing more powerful classifiers and larger datasets. However, the
classification task is ultimately constrained by the intrinsic properties of
datasets, independently of computational power or model complexity. In this
work, we introduce a formal entropy-based measure of classificability, which
quantifies the inherent difficulty of a classification problem by assessing the
uncertainty in class assignments given feature representations. This measure
captures the degree of class overlap and aligns with human intuition, serving
as an upper bound on classification performance for classification problems.
Our results establish a theoretical limit beyond which no classifier can
improve the classification accuracy, regardless of the architecture or amount
of data, in a given problem. Our approach provides a principled framework for
understanding when classification is inherently fallible and fundamentally
ambiguous.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Predictive Planner for Autonomous Driving with Consistency
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjian Li, Sangjae Bae, David Isele, Ryne Beeson, Faizan M. Tariq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory prediction and planning are fundamental components for autonomous
vehicles to navigate safely and efficiently in dynamic environments.
Traditionally, these components have often been treated as separate modules,
limiting the ability to perform interactive planning and leading to
computational inefficiency in multi-agent scenarios. In this paper, we present
a novel unified and data-driven framework that integrates prediction and
planning with a single consistency model. Trained on real-world human driving
datasets, our consistency model generates samples from high-dimensional,
multimodal joint trajectory distributions of the ego and multiple surrounding
agents, enabling end-to-end predictive planning. It effectively produces
interactive behaviors, such as proactive nudging and yielding to ensure both
safe and efficient interactions with other road users. To incorporate
additional planning constraints on the ego vehicle, we propose an alternating
direction method for multi-objective guidance in online guided sampling.
Compared to diffusion models, our consistency model achieves better performance
with fewer sampling steps, making it more suitable for real-time deployment.
Experimental results on Waymo Open Motion Dataset (WOMD) demonstrate our
method's superiority in trajectory quality, constraint satisfaction, and
interactive behavior compared to various existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COAT: Compressing Optimizer states and Activation for Memory-Efficient
  FP8 Training <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19313v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19313v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haocheng Xi, Han Cai, Ligeng Zhu, Yao Lu, Kurt Keutzer, Jianfei Chen, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FP8 training has emerged as a promising method for improving training
efficiency. Existing frameworks accelerate training by applying FP8 computation
to linear layers while leaving optimizer states and activations in higher
precision, which fails to fully optimize memory usage. This paper introduces
COAT (Compressing Optimizer States and Activations for FP8 Training), a novel
FP8 training framework designed to significantly reduce memory footprint when
training large models. COAT addresses current limitations through two key
innovations: (1) Dynamic Range Expansion, which aligns optimizer state
distributions more closely with the FP8 representation range, thereby reducing
quantization error, and (2) Mixed-Granularity Activation Quantization, which
optimizes activation memory using a combination of per-tensor and per-group
quantization strategies. Experiments demonstrate that COAT effectively reduces
end-to-end training memory footprint by 1.54x compared to BF16 while achieving
nearly lossless performance across various tasks, such as Large Language Model
pretraining and fine-tuning and Vision Language Model training. COAT also
achieves a 1.43x end-to-end training speedup compared to BF16, performing on
par with or surpassing TransformerEngine's speedup. COAT enables efficient
full-parameter training of large models on fewer GPUs, and facilitates doubling
the batch size in distributed training settings, providing a practical solution
for scaling large-scale model training. The code is available at
https://github.com/NVlabs/COAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. 22 pages. 9 Figures. 13 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Oversight and <span class="highlight-title">Human</span> Mistakes: Evidence from Centre Court 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16754v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16754v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Almog, Romain Gauriot, Lionel Page, Daniel Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powered by the increasing predictive capabilities of machine learning
algorithms, artificial intelligence (AI) systems have the potential to overrule
human mistakes in many settings. We provide the first field evidence that the
use of AI oversight can impact human decision-making. We investigate one of the
highest visibility settings where AI oversight has occurred: Hawk-Eye review of
umpires in top tennis tournaments. We find that umpires lowered their overall
mistake rate after the introduction of Hawk-Eye review, but also that umpires
increased the rate at which they called balls in, producing a shift from making
Type II errors (calling a ball out when in) to Type I errors (calling a ball in
when out). We structurally estimate the psychological costs of being overruled
by AI using a model of attention-constrained umpires, and our results suggest
that because of these costs, umpires cared 37% more about Type II errors under
AI oversight.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixuan Huang, Jiacheng Guo, Zihao Li, Xiang Ji, Jiawei Ge, Wenzhe Li, Yingqing Guo, Tianle Cai, Hui Yuan, Runzhe Wang, Yue Wu, Ming Yin, Shange Tang, Yangsibo Huang, Chi Jin, Xinyun Chen, Chiyuan Zhang, Mengdi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated impressive performance on challenging
mathematical reasoning tasks, which has triggered the discussion of whether the
performance is achieved by true reasoning capability or memorization. To
investigate this question, prior work has constructed mathematical benchmarks
when questions undergo simple perturbations -- modifications that still
preserve the underlying reasoning patterns of the solutions. However, no work
has explored hard perturbations, which fundamentally change the nature of the
problem so that the original solution steps do not apply. To bridge the gap, we
construct MATH-P-Simple and MATH-P-Hard via simple perturbation and hard
perturbation, respectively. Each consists of 279 perturbed math problems
derived from level-5 (hardest) problems in the MATH dataset (Hendrycksmath et.
al., 2021). We observe significant performance drops on MATH-P-Hard across
various models, including o1-mini (-16.49%) and gemini-2.0-flash-thinking
(-12.9%). We also raise concerns about a novel form of memorization where
models blindly apply learned problem-solving skills without assessing their
applicability to modified contexts. This issue is amplified when using original
problems for in-context learning. We call for research efforts to address this
challenge, which is critical for developing more robust and reliable reasoning
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: fix bugs in Fig. 1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-Term Fairness Inquiries and Pursuits in Machine Learning: A <span class="highlight-title">Survey</span>
  of Notions, Methods, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usman Gohar, Zeyu Tang, Jialu Wang, Kun Zhang, Peter L. Spirtes, Yang Liu, Lu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread integration of Machine Learning systems in daily life,
particularly in high-stakes domains, has raised concerns about the fairness
implications. While prior works have investigated static fairness measures,
recent studies reveal that automated decision-making has long-term implications
and that off-the-shelf fairness approaches may not serve the purpose of
achieving long-term fairness. Additionally, the existence of feedback loops and
the interaction between models and the environment introduces additional
complexities that may deviate from the initial fairness goals. In this survey,
we review existing literature on long-term fairness from different perspectives
and present a taxonomy for long-term fairness studies. We highlight key
challenges and consider future research directions, analyzing both current
issues and potential further explorations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the relation between trainability and dequantization of variational
  quantum learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elies Gil-Fuster, Casper Gyurik, Adrián Pérez-Salinas, Vedran Dunjko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quest for successful variational quantum machine learning (QML) relies on
the design of suitable parametrized quantum circuits (PQCs), as analogues to
neural networks in classical machine learning. Successful QML models must
fulfill the properties of trainability and non-dequantization, among others.
Recent works have highlighted an intricate interplay between trainability and
dequantization of such models, which is still unresolved. In this work we
contribute to this debate from the perspective of machine learning, proving a
number of results identifying, among others when trainability and
non-dequantization are not mutually exclusive. We begin by providing a number
of new somewhat broader definitions of the relevant concepts, compared to what
is found in other literature, which are operationally motivated, and consistent
with prior art. With these precise definitions given and motivated, we then
study the relation between trainability and dequantization of variational QML.
Next, we also discuss the degrees of "variationalness" of QML models, where we
distinguish between models like the hardware efficient ansatz and quantum
kernel methods. Finally, we introduce recipes for building PQC-based QML models
which are both trainable and nondequantizable, and corresponding to different
degrees of variationalness. We do not address the practical utility for such
models. Our work however does point toward a way forward for finding more
general constructions, for which finding applications may become feasible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages (14+11), 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Medical Code Tokenizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaorui Su, Shvat Messica, Yepeng Huang, Ruth Johnson, Lukas Fesser, Shanghua Gao, Faryad Sahneh, Marinka Zitnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models trained on patient electronic health records (EHRs) require
tokenizing medical data into sequences of discrete vocabulary items. Existing
tokenizers treat medical codes from EHRs as isolated textual tokens. However,
each medical code is defined by its textual description, its position in
ontological hierarchies, and its relationships to other codes, such as disease
co-occurrences and drug-treatment associations. Medical vocabularies contain
more than 600,000 codes with critical information for clinical reasoning. We
introduce MedTok, a multimodal medical code tokenizer that uses the text
descriptions and relational context of codes. MedTok processes text using a
language model encoder and encodes the relational structure with a graph
encoder. It then quantizes both modalities into a unified token space,
preserving modality-specific and cross-modality information. We integrate
MedTok into five EHR models and evaluate it on operational and clinical tasks
across in-patient and out-patient datasets, including outcome prediction,
diagnosis classification, drug recommendation, and risk stratification.
Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR
models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.30% on EHRShot, with
the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate
using MedTok tokenizer with medical QA systems. Our results demonstrate the
potential of MedTok as a unified tokenizer for medical codes, improving
tokenization for medical foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoDiCast: Conditional Diffusion Model for Global Weather Prediction with
  Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05975v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05975v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimeng Shi, Bowen Jin, Jiawei Han, Sundararaman Gopalakrishnan, Giri Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate weather forecasting is critical for science and society. Yet,
existing methods have not managed to simultaneously have the properties of high
accuracy, low uncertainty, and high computational efficiency. On one hand, to
quantify the uncertainty in weather predictions, the strategy of ensemble
forecast (i.e., generating a set of diverse predictions) is often employed.
However, traditional ensemble numerical weather prediction (NWP) is
computationally intensive. On the other hand, most existing machine
learning-based weather prediction (MLWP) approaches are efficient and accurate.
Nevertheless, they are deterministic and cannot capture the uncertainty of
weather forecasting. In this work, we propose CoDiCast, a conditional diffusion
model to generate accurate global weather prediction, while achieving
uncertainty quantification with ensemble forecasts and modest computational
cost. The key idea is to simulate a conditional version of the reverse
denoising process in diffusion models, which starts from pure Gaussian noise to
generate realistic weather scenarios for a future time point. Each denoising
step is conditioned on observations from the recent past. Ensemble forecasts
are achieved by repeatedly sampling from stochastic Gaussian noise to represent
uncertainty quantification. CoDiCast is trained on a decade of ERA5 reanalysis
data from the European Centre for Medium-Range Weather Forecasts (ECMWF).
Experimental results demonstrate that our approach outperforms several existing
data-driven methods in accuracy. Our conditional diffusion model, CoDiCast, can
generate 6-day global weather forecasts, at 6-hour steps and $5.625^\circ$
latitude-longitude resolution, for over 5 variables, in about 12 minutes on a
commodity A100 GPU machine with 80GB memory. The open-souced code is provided
at https://github.com/JimengShi/CoDiCast.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-end Training for Recommendation with Language-based User Profiles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaolin Gao, Joyce Zhou, Yijia Dai, Thorsten Joachims
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in natural language-based user profiles for
recommender systems, which aims to enhance transparency and scrutability
compared with embedding-based methods. Existing studies primarily generate
these profiles using zero-shot inference from large language models (LLMs), but
their quality remains insufficient, leading to suboptimal recommendation
performance. In this paper, we introduce LangPTune, the first end-to-end
training framework to optimize LLM-generated user profiles. Our method
significantly outperforms zero-shot approaches by explicitly training the LLM
for the recommendation objective. Through extensive evaluations across diverse
training configurations and benchmarks, we demonstrate that LangPTune not only
surpasses zero-shot baselines but can also matches the performance of
state-of-the-art embedding-based methods. Finally, we investigate whether the
training procedure preserves the interpretability of these profiles compared to
zero-shot inference through both GPT-4 simulations and crowdworker user
studies. Implementation of LangPTune can be found at
https://github.com/ZhaolinGao/LangPTune.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Benefits of Attribute-Driven Graph Domain Adaptation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyi Fang, Bingheng Li, Zhao Kang, Qiuhao Zeng, Ruizhi Pu, Nima Hosseini Dashtbayaz, Boyu Wang, Charles Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network
learning, particularly pertinent due to the absence of labeled data in
real-world graph datasets. Recent studies attempted to learn domain invariant
representations by eliminating structural shifts between graphs. In this work,
we show that existing methodologies have overlooked the significance of the
graph node attribute, a pivotal factor for graph domain alignment.
Specifically, we first reveal the impact of node attributes for GDA by
theoretically proving that in addition to the graph structural divergence
between the domains, the node attribute discrepancy also plays a critical role
in GDA. Moreover, we also empirically show that the attribute shift is more
substantial than the topology shift, which further underscores the importance
of node attribute alignment in GDA. Inspired by this finding, a novel
cross-channel module is developed to fuse and align both views between the
source and target graphs for GDA. Experimental results on a variety of
benchmarks verify the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theoretically Grounded Framework for LLM Watermarking: A
  Distribution-Adaptive Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02890v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02890v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyun He, Yepeng Liu, Ziqiao Wang, Yongyi Mao, Yuheng Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking has emerged as a crucial method to distinguish AI-generated text
from human-created text. In this paper, we present a novel theoretical
framework for watermarking Large Language Models (LLMs) that jointly optimizes
both the watermarking scheme and the detection process. Our approach focuses on
maximizing detection performance while maintaining control over the worst-case
Type-I error and text distortion. We characterize \emph{the universally minimum
Type-II error}, showing a fundamental trade-off between watermark detectability
and text distortion. Importantly, we identify that the optimal watermarking
schemes are adaptive to the LLM generative distribution. Building on our
theoretical insights, we propose an efficient, model-agnostic,
distribution-adaptive watermarking algorithm, utilizing a surrogate model
alongside the Gumbel-max trick. Experiments conducted on Llama2-13B and
Mistral-8$\times$7B models confirm the effectiveness of our approach.
Additionally, we examine incorporating robustness into our framework, paving a
way to future watermarking systems that withstand adversarial attacks more
effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Large Language Models to Enhance Machine Learning
  Interpretability and Predictive Performance: A Case Study on Emergency
  Department Returns for Mental Health Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulaziz Ahmed, Mohammad Saleem, Mohammed Alzeen, Badari Birur, Rachel E Fargason, Bradley G Burk, Hannah Rose Harkins, Ahmed Alhassan, Mohammed Ali Al-Garadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To evaluate whether integrating large language models (LLMs) with
traditional machine learning approaches improves both the predictive accuracy
and clinical interpretability of ED mental health returns risk models. Methods:
This retrospective cohort study analyzed 42,464 ED visits for 27,904 unique
mental health patients at an Academic Medical Center in the deep South of the
United States between January 2018 and December 2022. Main Outcomes and
Measures: Two primary outcomes were evaluated: (1) 30 days ED return prediction
accuracy and (2) model interpretability through a novel retrieval-augmented
generation (RAG) framework integrating SHAP (SHapley Additive exPlanations)
values with contextual clinical knowledge. Results: The proposed machine
learning interpretability framework, leveraging LLM, achieved 99% accuracy in
translating complex model predictions into clinically relevant explanations.
Integration of LLM-extracted features enhanced predictive performance,
improving the XGBoost model area under the curve (AUC) from 0.73 to 0.76. The
LLM-based feature extraction using 10-shot learning significantly outperformed
traditional approaches, achieving an accuracy of 0.882 and an F1 score of 0.86
for chief complaint classification (compared to conventional methods with an
accuracy range of 0.59 to 0.63) and demonstrating accuracy values ranging from
0.65 to 0.93 across multiple SDoH categories, underscoring its robust
performance in extracting features from clinical notes. Conclusions and
Relevance: Integrating LLMs with traditional machine learning models yielded
modest but consistent improvements in ED return prediction accuracy while
substantially enhancing model interpretability through automated, clinically
relevant explanations. This approach offers a framework for translating complex
predictive analytics into actionable clinical insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Stage Segmentation of Cervical Tumors using PocketNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11456v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11456v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Awj Twam, Adrian E. Celaya, Megan C. Jacobsen, Rachel Glenn, Peng Wei, Jia Sun, Ann Klopp, Aradhana M. Venkatesan, David Fuentes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cervical cancer remains the fourth most common malignancy amongst women
worldwide.1 Concurrent chemoradiotherapy (CRT) serves as the mainstay
definitive treatment regimen for locally advanced cervical cancers and includes
external beam radiation followed by brachytherapy.2 Integral to radiotherapy
treatment planning is the routine contouring of both the target tumor at the
level of the cervix, associated gynecologic anatomy and the adjacent organs at
risk (OARs). However, manual contouring of these structures is both time and
labor intensive and associated with known interobserver variability that can
impact treatment outcomes. While multiple tools have been developed to
automatically segment OARs and the high-risk clinical tumor volume (HR-CTV)
using computed tomography (CT) images,3,4,5,6 the development of deep
learning-based tumor segmentation tools using routine T2-weighted (T2w)
magnetic resonance imaging (MRI) addresses an unmet clinical need to improve
the routine contouring of both anatomical structures and cervical cancers,
thereby increasing quality and consistency of radiotherapy planning. This work
applied a novel deep-learning model (PocketNet) to segment the cervix, vagina,
uterus, and tumor(s) on T2w MRI. The performance of the PocketNet architecture
was evaluated, when trained on data via five-fold cross validation. PocketNet
achieved a mean Dice-Sorensen similarity coefficient (DSC) exceeding 70% for
tumor segmentation and 80% for organ segmentation. Validation on a publicly
available dataset from The Cancer Imaging Archive (TCIA) demonstrated the
models robustness, achieving DSC scores of 67.3% for tumor segmentation and
80.8% for organ segmentation. These results suggest that PocketNet is robust to
variations in contrast protocols, providing reliable segmentation of the
regions of interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-Robust Federated Learning over Ring-All-Reduce Distributed
  Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Fang, Zhuqing Liu, Xuecen Zhao, Jia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has gained attention as a distributed learning
paradigm for its data privacy benefits and accelerated convergence through
parallel computation. Traditional FL relies on a server-client (SC)
architecture, where a central server coordinates multiple clients to train a
global model, but this approach faces scalability challenges due to server
communication bottlenecks. To overcome this, the ring-all-reduce (RAR)
architecture has been introduced, eliminating the central server and achieving
bandwidth optimality. However, the tightly coupled nature of RAR's ring
topology exposes it to unique Byzantine attack risks not present in SC-based
FL. Despite its potential, designing Byzantine-robust RAR-based FL algorithms
remains an open problem. To address this gap, we propose BRACE
(Byzantine-robust ring-all-reduce), the first RAR-based FL algorithm to achieve
both Byzantine robustness and communication efficiency. We provide theoretical
guarantees for the convergence of BRACE under Byzantine attacks, demonstrate
its bandwidth efficiency, and validate its practical effectiveness through
experiments. Our work offers a foundational understanding of Byzantine-robust
RAR-based FL design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in The Web Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What can Large Language Models Capture about Code Functional
  Equivalence? <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nickil Maveli, Antonio Vergari, Shay B. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress
in learning rich representations of the structure and syntax of code,
successfully using it to generate or classify code fragments. At the same time,
understanding if they are able to do so because they capture code semantics,
and how well, is still an open question. In this paper, we tackle this problem
by introducing SeqCoBench, a benchmark for systematically assessing how
Code-LLMs can capture code functional equivalence. SeqCoBench contains over 20
code transformations that either preserve or alter the semantics of Python
programs. We conduct extensive evaluations in different settings, including
zero-shot and parameter-efficient finetuning methods on state-of-the-art
(Code)-LLMs to see if they can discern semantically equivalent or different
pairs of programs in SeqCoBench. We find that the performance gap between these
LLMs and classical match-based retrieval scores is minimal, with both
approaches showing a concerning lack of depth in understanding code semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Interior Point Optimization Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02596v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02596v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Klamkin, Mathieu Tanneau, Pascal Van Hentenryck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many practical applications of constrained optimization, scale and solving
time limits make traditional optimization solvers prohibitively slow. Thus, the
research question of how to design optimization proxies -- machine learning
models that produce high-quality solutions -- has recently received significant
attention. Orthogonal to this research thread which focuses on learning primal
solutions, this paper studies how to learn dual feasible solutions that
complement primal approaches and provide quality guarantees. The paper makes
two distinct contributions. First, to train dual linear optimization proxies,
the paper proposes a smoothed self-supervised loss function that augments the
objective function with a dual penalty term. Second, the paper proposes a novel
dual completion strategy that guarantees dual feasibility by solving a convex
optimization problem. Moreover, the paper derives closed-form solutions to this
completion optimization for several classes of dual penalties, eliminating the
need for computationally-heavy implicit layers. Numerical results are presented
on large linear optimization problems and demonstrate the effectiveness of the
proposed approach. The proposed dual completion outperforms methods for
learning optimization proxies which do not exploit the structure of the dual
problem. Compared to commercial optimization solvers, the learned dual proxies
achieve optimality gaps below $1\%$ and several orders of magnitude speedups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARR: Question Answering with Large Language Models via Analyzing,
  Retrieving, and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Yin, Giuseppe Carenini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) achieve remarkable performance on challenging
benchmarks that are often structured as multiple-choice question-answering (QA)
tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs
but provides only vague and generic guidance ("think step by step"). This paper
introduces ARR, an intuitive and effective zero-shot prompting method that
explicitly incorporates three key steps in QA solving: analyzing the intent of
the question, retrieving relevant information, and reasoning step by step.
Comprehensive experiments across diverse and challenging QA tasks demonstrate
that ARR consistently improves the Baseline (without ARR prompting) and
outperforms CoT. Ablation and case studies further validate the positive
contributions of each component: analyzing, retrieving, and reasoning. Notably,
intent analysis plays a vital role in ARR. Additionally, extensive evaluations
across various model sizes, LLM series, and generation settings solidify the
effectiveness, robustness, and generalizability of ARR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages. Code: https://github.com/YuweiYin/ARR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transcoders Beat Sparse Autoencoders for Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonçalo Paulo, Stepan Shabalin, Nora Belrose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse autoencoders (SAEs) extract human-interpretable features from deep
neural networks by transforming their activations into a sparse, higher
dimensional latent space, and then reconstructing the activations from these
latents. Transcoders are similar to SAEs, but they are trained to reconstruct
the output of a component of a deep network given its input. In this work, we
compare the features found by transcoders and SAEs trained on the same model
and data, finding that transcoder features are significantly more
interpretable. We also propose skip transcoders, which add an affine skip
connection to the transcoder architecture, and show that these achieve lower
reconstruction loss with no effect on interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample complexity of data-driven tuning of model hyperparameters in
  neural networks with structured parameter-dependent dual function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13734v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13734v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria-Florina Balcan, Anh Tuan Nguyen, Dravyansh Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern machine learning algorithms, especially deep learning based
techniques, typically involve careful hyperparameter tuning to achieve the best
performance. Despite the surge of intense interest in practical techniques like
Bayesian optimization and random search based approaches to automating this
laborious and compute intensive task, the fundamental learning theoretic
complexity of tuning hyperparameters for deep neural networks is poorly
understood. Inspired by this glaring gap, we initiate the formal study of
hyperparameter tuning complexity in deep learning through a recently introduced
data driven setting. We assume that we have a series of deep learning tasks,
and we have to tune hyperparameters to do well on average over the distribution
of tasks. A major difficulty is that the utility function as a function of the
hyperparameter is very volatile and furthermore, it is given implicitly by an
optimization problem over the model parameters. To tackle this challenge, we
introduce a new technique to characterize the discontinuities and oscillations
of the utility function on any fixed problem instance as we vary the
hyperparameter; our analysis relies on subtle concepts including tools from
differential/algebraic geometry and constrained optimization. This can be used
to show that the learning theoretic complexity of the corresponding family of
utility functions is bounded. We instantiate our results and provide sample
complexity bounds for concrete applications tuning a hyperparameter that
interpolates neural activation functions and setting the kernel parameter in
graph neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matcha: Mitigating Graph Structure Shifts with Test-Time Adaptation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Bao, Zhichen Zeng, Zhining Liu, Hanghang Tong, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful as they are, graph neural networks (GNNs) are known to be vulnerable
to distribution shifts. Recently, test-time adaptation (TTA) has attracted
attention due to its ability to adapt a pre-trained model to a target domain,
without re-accessing the source domain. However, existing TTA algorithms are
primarily designed for attribute shifts in vision tasks, where samples are
independent. These methods perform poorly on graph data that experience
structure shifts, where node connectivity differs between source and target
graphs. We attribute this performance gap to the distinct impact of node
attribute shifts versus graph structure shifts: the latter significantly
degrades the quality of node representations and blurs the boundaries between
different node categories. To address structure shifts in graphs, we propose
Matcha, an innovative framework designed for effective and efficient adaptation
to structure shifts by adjusting the htop-aggregation parameters in GNNs. To
enhance the representation quality, we design a prediction-informed clustering
loss to encourage the formation of distinct clusters for different node
categories. Additionally, Matcha seamlessly integrates with existing TTA
algorithms, allowing it to handle attribute shifts effectively while improving
overall performance under combined structure and attribute shifts. We validate
the effectiveness of Matcha on both synthetic and real-world datasets,
demonstrating its robustness across various combinations of structure and
attribute shifts. Our code is available at https://github.com/baowenxuan/Matcha .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor-Var: Variational Data Assimilation in Tensor Product Feature
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Yang, Xiaoyuan Cheng, Daniel Giles, Sibo Cheng, Yi He, Xiao Xue, Boli Chen, Yukun Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational data assimilation estimates the dynamical system states by
minimizing a cost function that fits the numerical models with observational
data. The widely used method, four-dimensional variational assimilation
(4D-Var), has two primary challenges: (1) computationally demanding for complex
nonlinear systems and (2) relying on state-observation mappings, which are
often not perfectly known. Deep learning (DL) has been used as a more
expressive class of efficient model approximators to address these challenges.
However, integrating such models into 4D-Var remains challenging due to their
inherent nonlinearities and the lack of theoretical guarantees for consistency
in assimilation results. In this paper, we propose Tensor-Var to address these
challenges using kernel Conditional Mean Embedding (CME). Tensor-Var improves
optimization efficiency by characterizing system dynamics and state-observation
mappings as linear operators, leading to a convex cost function in the feature
space. Furthermore, our method provides a new perspective to incorporate CME
into 4D-Var, offering theoretical guarantees of consistent assimilation results
between the original and feature spaces. To improve scalability, we propose a
method to learn deep features (DFs) using neural networks within the Tensor-Var
framework. Experiments on chaotic systems and global weather prediction with
real-time observations show that Tensor-Var outperforms conventional and DL
hybrid 4D-Var baselines in accuracy while achieving efficiency comparable to
the static 3D-Var method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibault de Surrel, Fabien Lotte, Sylvain Chevallier, Florian Yger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Circular and non-flat data distributions are prevalent across diverse domains
of data science, yet their specific geometric structures often remain
underutilized in machine learning frameworks. A principled approach to
accounting for the underlying geometry of such data is pivotal, particularly
when extending statistical models, like the pervasive Gaussian distribution. In
this work, we tackle those issue by focusing on the manifold of symmetric
positive definite matrices, a key focus in information geometry. We introduced
a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive
theoretical properties of this distribution and propose a maximum likelihood
framework for parameter estimation. Furthermore, we reinterpret established
classifiers on SPD through a probabilistic lens and introduce new classifiers
based on the wrapped Gaussian model. Experiments on synthetic and real-world
datasets demonstrate the robustness and flexibility of this geometry-aware
distribution, underscoring its potential to advance manifold-based data
analysis. This work lays the groundwork for extending classical machine
learning and statistical methods to more complex and structured data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Performance of ChatGPT for Spam Email Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijing Si, Yuwei Wu, Le Tang, Yugui Zhang, Jedrek Wosik, Qinliang Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Email continues to be a pivotal and extensively utilized communication medium
within professional and commercial domains. Nonetheless, the prevalence of spam
emails poses a significant challenge for users, disrupting their daily routines
and diminishing productivity. Consequently, accurately identifying and
filtering spam based on content has become crucial for cybersecurity. Recent
advancements in natural language processing, particularly with large language
models like ChatGPT, have shown remarkable performance in tasks such as
question answering and text generation. However, its potential in spam
identification remains underexplored. To fill in the gap, this study attempts
to evaluate ChatGPT's capabilities for spam identification in both English and
Chinese email datasets. We employ ChatGPT for spam email detection using
in-context learning, which requires a prompt instruction with (or without) a
few demonstrations. We also investigate how the number of demonstrations in the
prompt affects the performance of ChatGPT. For comparison, we also implement
five popular benchmark methods, including naive Bayes, support vector machines
(SVM), logistic regression (LR), feedforward dense neural networks (DNN), and
BERT classifiers. Through extensive experiments, the performance of ChatGPT is
significantly worse than deep supervised learning methods in the large English
dataset, while it presents superior performance on the low-resourced Chinese
dataset. This study provides insights into the potential and limitations of
ChatGPT for spam identification, highlighting its potential as a viable
solution for resource-constrained language domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures; Accepted by Pacific Journal of Optimization
  (PJO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive incremental learning of generalizable skills with local
  trajectory modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Knauer, Alin Albu-Schäffer, Freek Stulp, João Silvério
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of generalization in learning from demonstration (LfD) has
received considerable attention over the years, particularly within the context
of movement primitives, where a number of approaches have emerged. Recently,
two important approaches have gained recognition. While one leverages
via-points to adapt skills locally by modulating demonstrated trajectories,
another relies on so-called task-parameterized models that encode movements
with respect to different coordinate systems, using a product of probabilities
for generalization. While the former are well-suited to precise, local
modulations, the latter aim at generalizing over large regions of the workspace
and often involve multiple objects. Addressing the quality of generalization by
leveraging both approaches simultaneously has received little attention. In
this work, we propose an interactive imitation learning framework that
simultaneously leverages local and global modulations of trajectory
distributions. Building on the kernelized movement primitives (KMP) framework,
we introduce novel mechanisms for skill modulation from direct human corrective
feedback. Our approach particularly exploits the concept of via-points to
incrementally and interactively 1) improve the model accuracy locally, 2) add
new objects to the task during execution and 3) extend the skill into regions
where demonstrations were not provided. We evaluate our method on a bearing
ring-loading task using a torque-controlled, 7-DoF, DLR SARA robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE Robotics and Automation Letters (RA-L), 16 pages, 19
  figures, 6 tables. See
  https://github.com/DLR-RM/interactive-incremental-learning for further
  information and video</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sketched Equivariant Imaging Regularization and Deep Internal Learning
  for Inverse Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05771v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05771v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guixian Xu, Jinglai Li, Junqi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivariant Imaging (EI) regularization has become the de-facto technique for
unsupervised training of deep imaging networks, without any need of
ground-truth data. Observing that the EI-based unsupervised training paradigm
currently has significant computational redundancy leading to inefficiency in
high-dimensional applications, we propose a sketched EI regularization which
leverages the randomized sketching techniques for acceleration. We then extend
our sketched EI regularization to develop an accelerated deep internal learning
framework, Sketched Equivariant Deep Image Prior (Sk-EI-DIP), which can be
efficiently applied for single-image and task-adapted reconstruction.
Additionally, for network adaptation tasks, we propose a parameter-efficient
approach for accelerating both EI-DIP and Sk-EI-DIP via optimizing only the
normalization layers. Our numerical study on X-ray CT and multi-coil MRI image
reconstruction tasks demonstrate that our approach can achieve significant
computational acceleration over standard EI-based counterpart in single-input
setting and network adaptation at test time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ chebgreen: Learning and Interpolating Continuous Empirical Green's
  Functions from Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harshwardhan Praveen, Jacob Brown, Christopher Earls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a mesh-independent, data-driven library, chebgreen,
to mathematically model one-dimensional systems, possessing an associated
control parameter, and whose governing partial differential equation is
unknown. The proposed method learns an Empirical Green's Function for the
associated, but hidden, boundary value problem, in the form of a Rational
Neural Network from which we subsequently construct a bivariate representation
in a Chebyshev basis. We uncover the Green's function, at an unseen control
parameter value, by interpolating the left and right singular functions within
a suitable library, expressed as points on a manifold of Quasimatrices, while
the associated singular values are interpolated with Lagrange polynomials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/hsharsh/chebgreen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Explainable Pipeline for Machine Learning with Functional Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katherine Goode, J. Derek Tucker, Daniel Ries, Heike Hofmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) models have shown success in applications with an
objective of prediction, but the algorithmic complexity of some models makes
them difficult to interpret. Methods have been proposed to provide insight into
these "black-box" models, but there is little research that focuses on
supervised ML when the model inputs are functional data. In this work, we
consider two applications from high-consequence spaces with objectives of
making predictions using functional data inputs. One application aims to
classify material types to identify explosive materials given hyperspectral
computed tomography scans of the materials. The other application considers the
forensics science task of connecting an inkjet printed document to the source
printer using color signatures extracted by Raman spectroscopy. An instinctive
route to consider for analyzing these data is a data driven ML model for
classification, but due to the high consequence nature of the applications, we
argue it is important to appropriately account for the nature of the data in
the analysis to not obscure or misrepresent patterns. As such, we propose the
Variable importance Explainable Elastic Shape Analysis (VEESA) pipeline for
training ML models with functional data that (1) accounts for the vertical and
horizontal variability in the functional data and (2) provides an explanation
in the original data space of how the model uses variability in the functional
data for prediction. The pipeline makes use of elastic functional principal
components analysis (efPCA) to generate uncorrelated model inputs and
permutation feature importance (PFI) to identify the principal components
important for prediction. The variability captured by the important principal
components in visualized the original data space. We ultimately discuss ideas
for natural extensions of the VEESA pipeline and challenges for future
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Oscillatory State-Space Models <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Konstantin Rusch, Daniela Rus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Linear Oscillatory State-Space models (LinOSS) for efficiently
learning on long sequences. Inspired by cortical dynamics of biological neural
networks, we base our proposed LinOSS model on a system of forced harmonic
oscillators. A stable discretization, integrated over time using fast
associative parallel scans, yields the proposed state-space model. We prove
that LinOSS produces stable dynamics only requiring nonnegative diagonal state
matrix. This is in stark contrast to many previous state-space models relying
heavily on restrictive parameterizations. Moreover, we rigorously show that
LinOSS is universal, i.e., it can approximate any continuous and causal
operator mapping between time-varying functions, to desired accuracy. In
addition, we show that an implicit-explicit discretization of LinOSS perfectly
conserves the symmetry of time reversibility of the underlying dynamics.
Together, these properties enable efficient modeling of long-range
interactions, while ensuring stable and accurate long-horizon forecasting.
Finally, our empirical results, spanning a wide range of time-series tasks from
mid-range to very long-range classification and regression, as well as
long-horizon forecasting, demonstrate that our proposed LinOSS model
consistently outperforms state-of-the-art sequence models. Notably, LinOSS
outperforms Mamba by nearly 2x and LRU by 2.5x on a sequence modeling task with
sequences of length 50k.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topological Blindspots: Understanding and Extending Topological Deep
  Learning Through the Lens of Expressivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yam Eitan, Yoav Gelberg, Guy Bar-Shalom, Fabrizio Frasca, Michael Bronstein, Haggai Maron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological deep learning (TDL) is a rapidly growing field that seeks to
leverage topological structure in data and facilitate learning from data
supported on topological objects, ranging from molecules to 3D shapes. Most TDL
architectures can be unified under the framework of higher-order
message-passing (HOMP), which generalizes graph message-passing to higher-order
domains. In the first part of the paper, we explore HOMP's expressive power
from a topological perspective, demonstrating the framework's inability to
capture fundamental topological and metric invariants such as diameter,
orientability, planarity, and homology. In addition, we demonstrate HOMP's
limitations in fully leveraging lifting and pooling methods on graphs. To the
best of our knowledge, this is the first work to study the expressivity of TDL
from a \emph{topological} perspective. In the second part of the paper, we
develop two new classes of architectures -- multi-cellular networks (MCN) and
scalable MCN (SMCN) -- which draw inspiration from expressive GNNs. MCN can
reach full expressivity, but scaling it to large data objects can be
computationally expansive. Designed as a more scalable alternative, SMCN still
mitigates many of HOMP's expressivity limitations. Finally, we create new
benchmarks for evaluating models based on their ability to learn topological
properties of complexes. We then evaluate SMCN on these benchmarks and on
real-world graph datasets, demonstrating improvements over both HOMP baselines
and expressive graph methods, highlighting the value of expressively leveraging
topological information. Code and data are available at
https://github.com/yoavgelberg/SMCN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Algorithms for Contextual Dynamic Pricing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11316v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11316v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matilde Tullii, Solenne Gaucher, Nadav Merlis, Vianney Perchet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contextual dynamic pricing, a seller sequentially prices goods based on
contextual information. Buyers will purchase products only if the prices are
below their valuations. The goal of the seller is to design a pricing strategy
that collects as much revenue as possible. We focus on two different valuation
models. The first assumes that valuations linearly depend on the context and
are further distorted by noise. Under minor regularity assumptions, our
algorithm achieves an optimal regret bound of $\tilde{\mathcal{O}}(T^{2/3})$,
improving the existing results. The second model removes the linearity
assumption, requiring only that the expected buyer valuation is
$\beta$-H\"older in the context. For this model, our algorithm obtains a regret
$\tilde{\mathcal{O}}(T^{d+2\beta/d+3\beta})$, where $d$ is the dimension of the
context space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differential equation and probability inspired graph neural networks for
  latent variable learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.13800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.13800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuangwei Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic theory and differential equation are powerful tools for the
interpretability and guidance of the design of machine learning models,
especially for illuminating the mathematical motivation of learning latent
variable from observation. Subspace learning maps high-dimensional features on
low-dimensional subspace to capture efficient representation. Graphs are widely
applied for modeling latent variable learning problems, and graph neural
networks implement deep learning architectures on graphs. Inspired by
probabilistic theory and differential equations, this paper conducts notes and
proposals about graph neural networks to solve subspace learning problems by
variational inference and differential equation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Stability Principle for Learning under Non-Stationarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18304v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18304v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengpiao Huang, Kaizheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a versatile framework for statistical learning in non-stationary
environments. In each time period, our approach applies a stability principle
to select a look-back window that maximizes the utilization of historical data
while keeping the cumulative bias within an acceptable range relative to the
stochastic error. Our theory and numerical experiments showcase the adaptivity
of this approach to unknown non-stationarity. We prove regret bounds that are
minimax optimal up to logarithmic factors when the population losses are
strongly convex, or Lipschitz only. At the heart of our analysis lie two novel
components: a measure of similarity between functions and a segmentation
technique for dividing the non-stationary data sequence into quasi-stationary
pieces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>65 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URSA: Understanding and Verifying Chain-of-thought Reasoning in
  Multimodal Mathematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04686v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04686v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical
reasoning capabilities of large language models (LLMs). The introduction of
process supervision for CoT trajectories has sparked discussions on improving
test-time scaling, thereby unlocking the System 2-style thinking capabilities
of these models. However, in multimodal mathematical reasoning, the scarcity of
high-quality CoT training data has hindered existing models from achieving both
deliberate reasoning and fine-grained verification. In this work, we propose a
novel framework that introduces System 2-style thinking to multimodal
mathematical reasoning. We introduce a three-module CoT data synthesis process
that integrates CoT distillation, trajectory-format rewriting, and format
unification. This process generates MMathCoT-1M, a high-quality CoT reasoning
instruction fine-tuning dataset. Furthermore, we implement a dual-view
trajectory labeling automation that targets both visual grounding fidelity and
deductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B
model, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance
among similarly sized multimodal LLMs on six popular reasoning benchmarks.
Training URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a
verifier that enhances URSA-8B's test-time performance and surpasses strong
closed-source multimodal MLLMs like GPT-4o. The model weights, training data,
and code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,
  training data and code have been open-sourced. Project url:
  https://ursa-math.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme
  Active-Site Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenao Li, Shuo Yan, Enyan Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enzyme-catalyzed protein cleavage is essential for many biological functions.
Accurate prediction of cleavage sites can facilitate various applications such
as drug development, enzyme design, and a deeper understanding of biological
mechanisms. However, most existing models are restricted to an individual
enzyme, which neglects shared knowledge of enzymes and fails generalize to
novel enzymes. Thus, we introduce a unified protein cleavage site predictor
named UniZyme, which can generalize across diverse enzymes. To enhance the
enzyme encoding for the protein cleavage site prediction, UniZyme employs a
novel biochemically-informed model architecture along with active-site
knowledge of proteolytic enzymes. Extensive experiments demonstrate that
UniZyme achieves high accuracy in predicting cleavage sites across a range of
proteolytic enzymes, including unseen enzymes. The code is available in
https://anonymous.4open.science/r/UniZyme-4A67.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages,8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Discovery from Conditionally Stationary Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06257v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06257v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carles Balsells-Rodas, Xavier Sumba, Tanmayee Narendra, Ruibo Tu, Gabriele Schweikert, Hedvig Kjellstrom, Yingzhen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery, i.e., inferring underlying causal relationships from
observational data, is highly challenging for AI systems. In a time series
modeling context, traditional causal discovery methods mainly consider
constrained scenarios with fully observed variables and/or data from stationary
time-series. We develop a causal discovery approach to handle a wide class of
nonstationary time series that are conditionally stationary, where the
nonstationary behaviour is modeled as stationarity conditioned on a set of
latent state variables. Named State-Dependent Causal Inference (SDCI), our
approach is able to recover the underlying causal dependencies, with provable
identifiablity for the state-dependent causal structures. Empirical experiments
on nonlinear particle interaction data and gene regulatory networks demonstrate
SDCI's superior performance over baseline causal discovery methods. Improved
results over non-causal RNNs on modeling NBA player movements demonstrate the
potential of our method and motivate the use of causality-driven methods for
forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Capability Discovery via Model Self-Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Lu, Shengran Hu, Jeff Clune
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have become general-purpose assistants, exhibiting diverse
capabilities across numerous domains through training on web-scale data. It
remains challenging to precisely characterize even a fraction of the full
spectrum of capabilities and potential risks in any new model. Existing
evaluation approaches often require significant human effort, and it is taking
increasing effort to design ever harder challenges for more capable models. We
introduce Automated Capability Discovery (ACD), a framework that designates one
foundation model as a scientist to systematically propose open-ended tasks
probing the abilities of a subject model (potentially itself). By combining
frontier models with ideas from the field of open-endedness, ACD automatically
and systematically uncovers both surprising capabilities and failures in the
subject model. We demonstrate ACD across a range of foundation models
(including the GPT, Claude, and Llama series), showing that it automatically
reveals thousands of capabilities that would be challenging for any single team
to uncover. We further validate our method's automated scoring with extensive
human surveys, observing high agreement between model-generated and human
evaluations. By leveraging foundation models' ability to both create tasks and
self-evaluate, ACD is a significant step toward scalable, automated evaluation
of novel AI systems. All code and evaluation logs are open-sourced at
https://github.com/conglu1997/ACD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the convergence rate of noisy Bayesian Optimization with Expected
  Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Wang, Haowei Wang, Nai-Yuan Chiang, Cosmin G. Petra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expected improvement (EI) is one of the most widely used acquisition
functions in Bayesian optimization (BO). Despite its proven success in
applications for decades, important open questions remain on the theoretical
convergence behaviors and rates for EI. In this paper, we contribute to the
convergence theory of EI in three novel and critical areas. First, we consider
objective functions that fit under the Gaussian process (GP) prior assumption,
whereas existing works mostly focus on functions in the reproducing kernel
Hilbert space (RKHS). Second, we establish for the first time the asymptotic
error bound and its corresponding rate for GP-EI with noisy observations under
the GP prior assumption. Third, by investigating the exploration and
exploitation properties of the non-convex EI function, we establish improved
error bounds of GP-EI for both the noise-free and noisy cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Code Models Understand Programming Concepts? Counterfactual
  Analysis for Code Predicates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05980v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05980v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Hooda, Mihai Christodorescu, Miltiadis Allamanis, Aaron Wilson, Kassem Fawaz, Somesh Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models' success on text generation has also made them better
at code generation and coding tasks. While a lot of work has demonstrated their
remarkable performance on tasks such as code completion and editing, it is
still unclear as to why. We help bridge this gap by exploring to what degree
auto-regressive models understand the logical constructs of the underlying
programs. We propose Counterfactual Analysis for Programming Concept Predicates
(CACP) as a counterfactual testing framework to evaluate whether Large Code
Models understand programming concepts. With only black-box access to the
model, we use CACP to evaluate ten popular Large Code Models for four different
programming concepts. Our findings suggest that current models lack
understanding of concepts such as data flow and control flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random ReLU Neural Networks as Non-Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Parhi, Pakshal Bohra, Ayoub El Biari, Mehrsa Pourya, Michael Unser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a large class of shallow neural networks with randomly
initialized parameters and rectified linear unit activation functions. We prove
that these random neural networks are well-defined non-Gaussian processes. As a
by-product, we demonstrate that these networks are solutions to stochastic
differential equations driven by impulsive white noise (combinations of random
Dirac measures). These processes are parameterized by the law of the weights
and biases as well as the density of activation thresholds in each bounded
region of the input domain. We prove that these processes are isotropic and
wide-sense self-similar with Hurst exponent 3/2. We also derive a remarkably
simple closed-form expression for their autocovariance function. Our results
are fundamentally different from prior work in that we consider a
non-asymptotic viewpoint: The number of neurons in each bounded region of the
input domain (i.e., the width) is itself a random variable with a Poisson law
with mean proportional to the density parameter. Finally, we show that, under
suitable hypotheses, as the expected width tends to infinity, these processes
can converge in law not only to Gaussian processes, but also to non-Gaussian
processes depending on the law of the weights. Our asymptotic results provide a
new take on several classical results (wide networks converge to Gaussian
processes) as well as some new ones (wide networks can converge to non-Gaussian
processes).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benign Overfitting in Single-Head Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roey Magen, Shuning Shang, Zhiwei Xu, Spencer Frei, Wei Hu, Gal Vardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The phenomenon of benign overfitting, where a trained neural network
perfectly fits noisy training data but still achieves near-optimal test
performance, has been extensively studied in recent years for linear models and
fully-connected/convolutional networks. In this work, we study benign
overfitting in a single-head softmax attention model, which is the fundamental
building block of Transformers. We prove that under appropriate conditions, the
model exhibits benign overfitting in a classification setting already after two
steps of gradient descent. Moreover, we show conditions where a
minimum-norm/maximum-margin interpolator exhibits benign overfitting. We study
how the overfitting behavior depends on the signal-to-noise ratio (SNR) of the
data distribution, namely, the ratio between norms of signal and noise tokens,
and prove that a sufficiently large SNR is both necessary and sufficient for
benign overfitting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representing Rule-based Chatbots with Transformers <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Friedman, Abhishek Panigrahi, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What kind of internal mechanisms might Transformers use to conduct fluid,
natural-sounding conversations? Prior work has illustrated by construction how
Transformers can solve various synthetic tasks, such as sorting a list or
recognizing formal languages, but it remains unclear how to extend this
approach to a conversational setting. In this work, we propose using ELIZA, a
classic rule-based chatbot, as a setting for formal, mechanistic analysis of
Transformer-based chatbots. ELIZA allows us to formally model key aspects of
conversation, including local pattern matching and long-term dialogue state
tracking. We first present a theoretical construction of a Transformer that
implements the ELIZA chatbot. Building on prior constructions, particularly
those for simulating finite-state automata, we show how simpler mechanisms can
be composed and extended to produce more sophisticated behavior. Next, we
conduct a set of empirical analyses of Transformers trained on synthetically
generated ELIZA conversations. Our analysis illustrates the kinds of mechanisms
these models tend to prefer--for example, models favor an induction head
mechanism over a more precise, position-based copying mechanism; and using
intermediate generations to simulate recurrent data structures, akin to an
implicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit
connection between neural chatbots and interpretable, symbolic mechanisms, our
results provide a new framework for the mechanistic analysis of conversational
agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025. Code and data are available at
  https://github.com/princeton-nlp/ELIZA-Transformer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphXAIN: Narratives to Explain Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02540v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02540v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Cedro, David Martens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are a powerful technique for machine learning on
graph-structured data, yet they pose challenges in interpretability. Existing
GNN explanation methods usually yield technical outputs, such as subgraphs and
feature importance scores, that are difficult for non-data scientists to
understand and thereby violate the purpose of explanations. Motivated by recent
Explainable AI (XAI) research, we propose GraphXAIN, a method that generates
natural language narratives explaining GNN predictions. GraphXAIN is a model-
and explainer-agnostic method that uses Large Language Models (LLMs) to
translate explanatory subgraphs and feature importance scores into coherent,
story-like explanations of GNN decision-making processes. Evaluations on
real-world datasets demonstrate GraphXAIN's ability to improve graph
explanations. A survey of machine learning researchers and practitioners
reveals that GraphXAIN enhances four explainability dimensions:
understandability, satisfaction, convincingness, and suitability for
communicating model predictions. When combined with another graph explainer
method, GraphXAIN further improves trustworthiness, insightfulness, confidence,
and usability. Notably, 95% of participants found GraphXAIN to be a valuable
addition to the GNN explanation method. By incorporating natural language
narratives, our approach serves both graph practitioners and non-expert users
by providing clearer and more effective explanations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoSAM: Local Search in Additive Noise Models with Mixed Mechanisms and
  General Noise for Global Causal Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11759v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11759v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujai Hiremath, Promit Ghosal, Kyra Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring causal relationships from observational data is crucial when
experiments are costly or infeasible. Additive noise models (ANMs) enable
unique directed acyclic graph (DAG) identification, but existing ANM methods
often rely on restrictive assumptions on the data generating process, limiting
their applicability to real-world settings. We propose local search in additive
noise models, LoSAM, a topological ordering method for learning a unique DAG in
ANMs with mixed causal mechanisms and general noise distributions. We introduce
new causal substructures and criteria for identifying roots and leaves,
enabling efficient top-down learning. We prove asymptotic consistency and
polynomial runtime, ensuring scalability and sample efficiency. We test LoSAM
on synthetic and real-world data, demonstrating state-of-the-art performance
across all mixed mechanism settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence of Message Passing Graph Neural Networks with Generic
  Aggregation On Large Random Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11140v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11140v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Cordonnier, Nicolas Keriven, Nicolas Tremblay, Samuel Vaiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the convergence of message passing graph neural networks on random
graph models to their continuous counterpart as the number of nodes tends to
infinity. Until now, this convergence was only known for architectures with
aggregation functions in the form of normalized means, or, equivalently, of an
application of classical operators like the adjacency matrix or the graph
Laplacian. We extend such results to a large class of aggregation functions,
that encompasses all classically used message passing graph neural networks,
such as attention-based message passing, max convolutional message passing,
(degree-normalized) convolutional message passing, or moment-based aggregation
message passing. Under mild assumptions, we give non-asymptotic bounds with
high probability to quantify this convergence. Our main result is based on the
McDiarmid inequality. Interestingly, this result does not apply to the case
where the aggregation is a coordinate-wise maximum. We treat this case
separately and obtain a different convergence rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fault Detection and Monitoring using a Data-Driven Information-Based
  Strategy: Method, Theory, and Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camilo Ramírez, Jorge F. Silva, Ferhat Tamssaouet, Tomás Rojas, Marcos E. Orchard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to detect when a system undergoes an incipient fault is of
paramount importance in preventing a critical failure. Classic methods for
fault detection (including model-based and data-driven approaches) rely on
thresholding error statistics or simple input-residual dependencies but face
difficulties with non-linear or non-Gaussian systems. Behavioral methods (e.g.,
those relying on digital twins) address these difficulties but still face
challenges when faulty data is scarce, decision guarantees are required, or
working with already-deployed models is required. In this work, we propose an
information-driven fault detection method based on a novel concept drift
detector, addressing these challenges. The method is tailored to identifying
drifts in input-output relationships of additive noise models (i.e., model
drifts) and is based on a distribution-free mutual information (MI) estimator.
Our scheme does not require prior faulty examples and can be applied
distribution-free over a large class of system models. Our core contributions
are twofold. First, we demonstrate the connection between fault detection,
model drift detection, and testing independence between two random variables.
Second, we prove several theoretical properties of the proposed MI-based fault
detection scheme: (i) strong consistency, (ii) exponentially fast detection of
the non-faulty case, and (iii) control of both significance levels and power of
the test. To conclude, we validate our theory with synthetic data and the
benchmark dataset N-CMAPSS of aircraft turbofan engines. These empirical
results support the usefulness of our methodology in many practical and
realistic settings, and the theoretical results show performance guarantees
that other methods cannot offer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 15 figures. This is the accepted manuscript for publication
  in Mechanical Systems and Signal Processing. The arXiv version has been
  updated accordingly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mask in the Mirror: Implicit Sparsification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Jacobs, Rebekka Burkholz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous sparsification strategies are among the most effective methods for
reducing the inference costs and memory demands of large-scale neural networks.
A key factor in their success is the implicit $L_1$ regularization induced by
jointly learning both mask and weight variables, which has been shown
experimentally to outperform explicit $L_1$ regularization. We provide a
theoretical explanation for this observation by analyzing the learning
dynamics, revealing that early continuous sparsification is governed by an
implicit $L_2$ regularization that gradually transitions to an $L_1$ penalty
over time. Leveraging this insight, we propose a method to dynamically control
the strength of this implicit bias. Through an extension of the mirror flow
framework, we establish convergence and optimality guarantees in the context of
underdetermined linear regression. Our theoretical findings may be of
independent interest, as we demonstrate how to enter the rich regime and show
that the implicit bias can be controlled via a time-dependent Bregman
potential. To validate these insights, we introduce PILoT, a continuous
sparsification approach with novel initialization and dynamic regularization,
which consistently outperforms baselines in standard experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence-based Estimators for Predictive Performance in Model
  Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.08649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.08649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juhani Kivimäki, Jakub Białek, Jukka K. Nurminen, Wojtek Kuberski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  After a machine learning model has been deployed into production, its
predictive performance needs to be monitored. Ideally, such monitoring can be
carried out by comparing the model's predictions against ground truth labels.
For this to be possible, the ground truth labels must be available relatively
soon after inference. However, there are many use cases where ground truth
labels are available only after a significant delay, or in the worst case, not
at all. In such cases, directly monitoring the model's predictive performance
is impossible.
  Recently, novel methods for estimating the predictive performance of a model
when ground truth is unavailable have been developed. Many of these methods
leverage model confidence or other uncertainty estimates and are experimentally
compared against a naive baseline method, namely Average Confidence (AC), which
estimates model accuracy as the average of confidence scores for a given set of
predictions. However, until now the theoretical properties of the AC method
have not been properly explored. In this paper, we try to fill this gap by
reviewing the AC method and show that under certain general assumptions, it is
an unbiased and consistent estimator of model accuracy with many desirable
properties. We also compare this baseline estimator against some more complex
estimators empirically and show that in many cases the AC method is able to
beat the others, although the comparative quality of the different estimators
is heavily case-dependent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version corresponds to the final published version in JAIR. The
  published article is available at [https://doi.org/10.1613/jair.1.16709]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Pre-Training in Tabular Data: A Neighborhood Embedding
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han-Jia Ye, Qi-Le Zhou, Huai-Hong Yin, De-Chuan Zhan, Wei-Lun Chao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training is prevalent in deep learning for vision and text data,
leveraging knowledge from other datasets to enhance downstream tasks. However,
for tabular data, the inherent heterogeneity in attribute and label spaces
across datasets complicates the learning of shareable knowledge. We propose
Tabular data Pre-Training via Meta-representation (TabPTM), aiming to pre-train
a general tabular model over diverse datasets. The core idea is to embed data
instances into a shared feature space, where each instance is represented by
its distance to a fixed number of nearest neighbors and their labels. This
''meta-representation'' transforms heterogeneous tasks into homogeneous local
prediction problems, enabling the model to infer labels (or scores for each
label) based on neighborhood information. As a result, the pre-trained TabPTM
can be applied directly to new datasets, regardless of their diverse attributes
and labels, without further fine-tuning. Extensive experiments on 101 datasets
confirm TabPTM's effectiveness in both classification and regression tasks,
with and without fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Sparse Attention Approximates Exact Attention? Your Attention is
  Naturally $n^C$-Sparse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Deng, Zhao Song, Jing Xiong, Chiwun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Attention is a technique that approximates standard attention
computation with sub-quadratic complexity. This is achieved by selectively
ignoring smaller entries in the attention matrix during the softmax function
computation. Variations of this technique, such as pruning KV cache,
sparsity-based fast attention, and Sparse Transformer, have been extensively
utilized for efficient Large Language Models (LLMs) deployment. Despite its
widespread use, a theoretical understanding of the conditions under which
sparse attention performs on par with traditional attention remains elusive.
This work aims to $\textbf{bridge this gap by examining the inherent sparsity
of standard attention processes}$. Our theoretical framework reveals several
brand-new key insights:
  $\bullet$ Attention is $n^{C}$-sparse, implying that considering only the
largest $\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse
attention to approximate the exact attention matrix with decreasing loss. Here,
$n$ represents the input length and $C \in (0, 1)$ is a constant.
  $\bullet$ Stable $o(\log(n))$-sparse attention, which approximates attention
computation with $\log(n)$ or fewer entries, may not be feasible since the
error will persist at a minimum of $O(1)$.
  $\bullet$ An adaptive strategy ($\alpha \cdot n^C, \alpha \in \mathbb{R}$)
for the window size of efficient attention methods rather than a fixed one is
guaranteed to perform more accurately and efficiently in a task for inference
on flexible context lengths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning in Chemical Engineering: A Tutorial on a Framework
  for Privacy-Preserving Collaboration Across Distributed Data Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Dutta, Iago Leal de Freitas, Pedro Maciel Xavier, Claudio Miceli de Farias, David Esteban Bernal Neira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a decentralized machine learning approach that has
gained attention for its potential to enable collaborative model training
across clients while protecting data privacy, making it an attractive solution
for the chemical industry. This work aims to provide the chemical engineering
community with an accessible introduction to the discipline. Supported by a
hands-on tutorial and a comprehensive collection of examples, it explores the
application of FL in tasks such as manufacturing optimization, multimodal data
integration, and drug discovery while addressing the unique challenges of
protecting proprietary information and managing distributed datasets. The
tutorial was built using key frameworks such as $\texttt{Flower}$ and
$\texttt{TensorFlow Federated}$ and was designed to provide chemical engineers
with the right tools to adopt FL in their specific needs. We compare the
performance of FL against centralized learning across three different datasets
relevant to chemical engineering applications, demonstrating that FL will often
maintain or improve classification performance, particularly for complex and
heterogeneous data. We conclude with an outlook on the open challenges in
federated learning to be tackled and current approaches designed to remediate
and improve this framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 Pages, 8 figures, Under review in ACS Industrial & Engineering
  Chemistry Research Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ APE: Faster and Longer Context-Augmented Generation via Adaptive
  Parallel Encoding <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Yang, Tianqi Chen, Beidi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context-augmented generation (CAG) techniques, including RAG and ICL, require
the efficient combination of multiple contexts to generate responses to user
queries. Directly inputting these contexts as a sequence introduces a
considerable computational burden by re-encoding the combined selection of
contexts for every request. To address this, we explore the promising potential
of parallel encoding to independently pre-compute and cache each context's KV
states. This approach enables the direct loading of cached states during
inference while accommodating more contexts through position reuse across
contexts. However, due to misalignments in attention distribution, directly
applying parallel encoding results in a significant performance drop. To enable
effective and efficient CAG, we propose Adaptive Parallel Encoding
($\textbf{APE}$), which brings shared prefix, attention temperature, and
scaling factor to align the distribution of parallel encoding with sequential
encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%
and 93% sequential encoding performance using the same inputs while
outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales
to many-shot CAG, effectively encoding hundreds of contexts in parallel.
Efficiency evaluation shows that APE can achieve an end-to-end 4.5$\times$
speedup by reducing 28$\times$ prefilling time for a 128K-length context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beam Prediction based on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08707v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08707v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Sheng, Kai Huang, Le Liang, Peng Liu, Shi Jin, Geoffrey Ye Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we use large language models (LLMs) to develop a
high-performing and robust beam prediction method. We formulate the millimeter
wave (mmWave) beam prediction problem as a time series forecasting task, where
the historical observations are aggregated through cross-variable attention and
then transformed into text-based representations using a trainable tokenizer.
By leveraging the prompt-as-prefix (PaP) technique for contextual enrichment,
our method harnesses the power of LLMs to predict future optimal beams.
Simulation results demonstrate that our LLM-based approach outperforms
traditional learning-based models in prediction accuracy as well as robustness,
highlighting the significant potential of LLMs in enhancing wireless
communication systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on <span class="highlight-title">Video</span> Analytics in Cloud-Edge-Terminal Collaborative Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linxiao Gong, Hao Yang, Gaoyun Fang, Bobo Ju, Juncen Guo, Xiaoguang Zhu, Yan Wang, Xiping Hu, Peng Sun, Azzedine Boukerche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive growth of video data has driven the development of distributed
video analytics in cloud-edge-terminal collaborative (CETC) systems, enabling
efficient video processing, real-time inference, and privacy-preserving
analysis. Among multiple advantages, CETC systems can distribute video
processing tasks and enable adaptive analytics across cloud, edge, and terminal
devices, leading to breakthroughs in video surveillance, autonomous driving,
and smart cities. In this survey, we first analyze fundamental architectural
components, including hierarchical, distributed, and hybrid frameworks,
alongside edge computing platforms and resource management mechanisms. Building
upon these foundations, edge-centric approaches emphasize on-device processing,
edge-assisted offloading, and edge intelligence, while cloud-centric methods
leverage powerful computational capabilities for complex video understanding
and model training. Our investigation also covers hybrid video analytics
incorporating adaptive task offloading and resource-aware scheduling techniques
that optimize performance across the entire system. Beyond conventional
approaches, recent advances in large language models and multimodal integration
reveal both opportunities and challenges in platform scalability, data
protection, and system reliability. Future directions also encompass
explainable systems, efficient processing mechanisms, and advanced video
analytics, offering valuable insights for researchers and practitioners in this
dynamic field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gramian Multimodal Representation Learning and Alignment <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giordano Cicchetti, Eleonora Grassucci, Luigi Sigillo, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human perception integrates multiple modalities, such as vision, hearing, and
language, into a unified understanding of the surrounding reality. While recent
multimodal models have achieved significant progress by aligning pairs of
modalities via contrastive learning, their solutions are unsuitable when
scaling to multiple modalities. These models typically align each modality to a
designated anchor without ensuring the alignment of all modalities with each
other, leading to suboptimal performance in tasks requiring a joint
understanding of multiple modalities. In this paper, we structurally rethink
the pairwise conventional approach to multimodal learning and we present the
novel Gramian Representation Alignment Measure (GRAM), which overcomes the
above-mentioned limitations. GRAM learns and then aligns $n$ modalities
directly in the higher-dimensional space in which modality embeddings lie by
minimizing the Gramian volume of the $k$-dimensional parallelotope spanned by
the modality vectors, ensuring the geometric alignment of all modalities
simultaneously. GRAM can replace cosine similarity in any downstream method,
holding for 2 to $n$ modalities and providing more meaningful alignment with
respect to previous similarity measures. The novel GRAM-based contrastive loss
function enhances the alignment of multimodal models in the higher-dimensional
embedding space, leading to new state-of-the-art performance in downstream
tasks such as video-audio-text retrieval and audio-video classification. The
project page, the code, and the pretrained models are available at
https://ispamm.github.io/GRAM/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Visual Representation Learning with Multi-modal Prior Knowledge
  for Image Classification Under Distribution Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongkuan Zhou, Lavdim Halilaj, Sebastian Monka, Stefan Schmid, Yuqicheng Zhu, Bo Xiong, Steffen Staab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable success of deep neural networks (DNNs) in computer
vision, they fail to remain high-performing when facing distribution shifts
between training and testing data. In this paper, we propose Knowledge-Guided
Visual representation learning (KGV) - a distribution-based learning approach
leveraging multi-modal prior knowledge - to improve generalization under
distribution shift. It integrates knowledge from two distinct modalities: 1) a
knowledge graph (KG) with hierarchical and association relationships; and 2)
generated synthetic images of visual elements semantically represented in the
KG. The respective embeddings are generated from the given modalities in a
common latent space, i.e., visual embeddings from original and synthetic images
as well as knowledge graph embeddings (KGEs). These embeddings are aligned via
a novel variant of translation-based KGE methods, where the node and relation
embeddings of the KG are modeled as Gaussian distributions and translations,
respectively. We claim that incorporating multi-model prior knowledge enables
more regularized learning of image representations. Thus, the models are able
to better generalize across different data distributions. We evaluate KGV on
different image classification tasks with major or minor distribution shifts,
namely road sign classification across datasets from Germany, China, and
Russia, image classification with the mini-ImageNet dataset and its variants,
as well as the DVM-CAR dataset. The results demonstrate that KGV consistently
exhibits higher accuracy and data efficiency across all experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Poincaré Inequality for Local Log-Polyak-Lojasiewicz Measures :
  Non-asymptotic Analysis in Low-temperature Regime 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Gong, Zebang Shen, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Potential functions in highly pertinent applications, such as deep learning
in over-parameterized regime, are empirically observed to admit non-isolated
minima. To understand the convergence behavior of stochastic dynamics in such
landscapes, we propose to study the class of \logPLmeasure\ measures
$\mu_\epsilon \propto \exp(-V/\epsilon)$, where the potential $V$ satisfies a
local Polyak-{\L}ojasiewicz (P\L) inequality, and its set of local minima is
provably \emph{connected}. Notably, potentials in this class can exhibit local
maxima and we characterize its optimal set S to be a compact $\mathcal{C}^2$
\emph{embedding submanifold} of $\mathbb{R}^d$ without boundary. The
\emph{non-contractibility} of S distinguishes our function class from the
classical convex setting topologically. Moreover, the embedding structure
induces a naturally defined Laplacian-Beltrami operator on S, and we show that
its first non-trivial eigenvalue provides an \emph{$\epsilon$-independent}
lower bound for the \Poincare\ constant in the \Poincare\ inequality of
$\mu_\epsilon$. As a direct consequence, Langevin dynamics with such non-convex
potential $V$ and diffusion coefficient $\epsilon$ converges to its equilibrium
$\mu_\epsilon$ at a rate of $\tilde{\mathcal{O}}(1/\epsilon)$, provided
$\epsilon$ is sufficiently small. Here $\tilde{\mathcal{O}}$ hides logarithmic
terms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a replacement version of arXiv:2501.00429</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Multivariate Time Series Imputation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wang, Wenjie Du, Yiyuan Yang, Linglong Qian, Wei Cao, Keli Zhang, Wenjia Wang, Yuxuan Liang, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Missing values are ubiquitous in multivariate time series (MTS) data, posing
significant challenges for accurate analysis and downstream applications. In
recent years, deep learning-based methods have successfully handled missing
data by leveraging complex temporal dependencies and learned data
distributions. In this survey, we provide a comprehensive summary of deep
learning approaches for multivariate time series imputation (MTSI) tasks. We
propose a novel taxonomy that categorizes existing methods based on two key
perspectives: imputation uncertainty and neural network architecture.
Furthermore, we summarize existing MTSI toolkits with a particular emphasis on
the PyPOTS Ecosystem, which provides an integrated and standardized foundation
for MTSI research. Finally, we discuss key challenges and future research
directions, which give insight for further MTSI research. This survey aims to
serve as a valuable resource for researchers and practitioners in the field of
time series analysis and missing data imputation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting DNA fragmentation: A non-destructive analogue to chemical
  assays using machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byron A Jacobs, Ifthakaar Shaik, Frando Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Globally, infertility rates are increasing, with 2.5\% of all births being
assisted by in vitro fertilisation (IVF) in 2022. Male infertility is the cause
for approximately half of these cases. The quality of sperm DNA has substantial
impact on the success of IVF. The assessment of sperm DNA is traditionally done
through chemical assays which render sperm cells ineligible for IVF. Many
compounding factors lead to the population crisis, with fertility rates
dropping globally in recent history. As such assisted reproductive technologies
(ART) have been the focus of recent research efforts. Simultaneously,
artificial intelligence has grown ubiquitous and is permeating more aspects of
modern life. With the advent of state-of-the-art machine learning and its
exceptional performance in many sectors, this work builds on these successes
and proposes a novel framework for the prediction of sperm cell DNA
fragmentation from images of unstained sperm. Rendering a predictive model
which preserves sperm integrity and allows for optimal selection of sperm for
IVF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning through <span class="highlight-title">Human</span>-Robot Interaction: <span class="highlight-title">Human</span> Perceptions of
  a Continual Learning Robot in Repeated Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ayub, Zachary De Francesco, Patrick Holthaus, Chrystopher L. Nehaniv, Kerstin Dautenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For long-term deployment in dynamic real-world environments, assistive robots
must continue to learn and adapt to their environments. Researchers have
developed various computational models for continual learning (CL) that can
allow robots to continually learn from limited training data, and avoid
forgetting previous knowledge. While these CL models can mitigate forgetting on
static, systematically collected datasets, it is unclear how human users might
perceive a robot that continually learns over multiple interactions with them.
In this paper, we developed a system that integrates CL models for object
recognition with a Fetch mobile manipulator robot and allows human participants
to directly teach and test the robot over multiple sessions. We conducted an
in-person study with 60 participants that interacted with our system in 300
sessions (5 sessions per participant). We conducted a between-subject study
with three different CL models to understand human perceptions of continual
learning robots over multiple sessions. Our results suggest that participants'
perceptions of trust, competence, and usability of a continual learning robot
significantly decrease over multiple sessions if the robot forgets previously
learned objects. However, the perceived task load on participants for teaching
and testing the robot remains the same over multiple sessions even if the robot
forgets previously learned objects. Our results also indicate that
state-of-the-art CL models might perform unreliably when applied on robots
interacting with human participants. Further, continual learning robots are not
perceived as very trustworthy or competent by human participants, regardless of
the underlying continual learning model or the session number.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Journal of Social Robotics (SoRo), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $C^2$: Scalable Auto-Feedback for LLM-based Chart Generation <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18652v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18652v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Woosung Koh, Jang Han Yoon, MinHyung Lee, Youngjin Song, Jaegwan Cho, Jaehyun Kang, Taehyeon Kim, Se-Young Yun, Youngjae Yu, Bongshin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating high-quality charts with Large Language Models (LLMs) presents
significant challenges due to limited data and the high cost of scaling through
human curation. $\langle \text{instruction}, \text{data}, \text{code} \rangle$
triplets are scarce and expensive to manually curate as their creation demands
technical expertise. To address this scalability challenge, we introduce a
reference-free automatic feedback generator, which eliminates the need for
costly human intervention. Our novel framework, C$^2$, consists of (1) an
automatic feedback provider (ChartAF) and (2) a diverse, reference-free dataset
(ChartUIE-8K). The results are compelling: in our first experiment, 74% of
respondents strongly preferred, and 10% preferred, the results after feedback.
The second post-feedback experiment demonstrates that ChartAF outperform nine
baselines. Moreover, ChartUIE-8K significantly improves data diversity by
increasing queries, datasets, and chart types by 5982%, 1936%, and 91%,
respectively, over benchmarks. Finally, a study of LLM users revealed that 94%
of participants preferred ChartUIE-8K's queries, with 93% deeming them aligned
with real-world use cases. Core contributions are available as open-source at
chartsquared.github.io, with ample qualitative examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Main (Long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRADES: Generating Realistic Market Simulations with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Berti, Bardh Prenkaj, Paola Velardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial markets are complex systems characterized by high statistical
noise, nonlinearity, and constant evolution. Thus, modeling them is extremely
hard. We address the task of generating realistic and responsive Limit Order
Book (LOB) market simulations, which are fundamental for calibrating and
testing trading strategies, performing market impact experiments, and
generating synthetic market data. Previous works lack realism, usefulness, and
responsiveness of the generated simulations. To bridge this gap, we propose a
novel TRAnsformer-based Denoising Diffusion Probabilistic Engine for LOB
Simulations (TRADES). TRADES generates realistic order flows conditioned on the
state of the market, leveraging a transformer-based architecture that captures
the temporal and spatial characteristics of high-frequency market data. There
is a notable absence of quantitative metrics for evaluating generative market
simulation models in the literature. To tackle this problem, we adapt the
predictive score, a metric measured as an MAE, by training a stock price
predictive model on synthetic data and testing it on real data. We compare
TRADES with previous works on two stocks, reporting an x3.27 and x3.47
improvement over SoTA according to the predictive score, demonstrating that we
generate useful synthetic market data for financial downstream tasks. We assess
TRADES's market simulation realism and responsiveness, showing that it
effectively learns the conditional data distribution and successfully reacts to
an experimental agent, giving sprout to possible calibrations and evaluations
of trading strategies and market impact experiments. We developed DeepMarket,
the first open-source Python framework for market simulation with deep
learning. Our repository includes a synthetic LOB dataset composed of TRADES's
generates simulations. We release the code at
github.com/LeonardoBerti00/DeepMarket.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TAID: Temporally Adaptive Interpolated Distillation for Efficient
  Knowledge Transfer in Language Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16937v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16937v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Shing, Kou Misaki, Han Bao, Sho Yokoi, Takuya Akiba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal language models have demonstrated remarkable capabilities, but their
size poses significant challenges for deployment in resource-constrained
environments. Knowledge distillation, a widely-used technique for transferring
knowledge from a large teacher model to a small student model, presents a
promising approach for model compression. A significant remaining issue lies in
the major differences between teacher and student models, namely the
substantial capacity gap, mode averaging, and mode collapse, which pose
barriers during distillation. To address these issues, we introduce
$\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel
knowledge distillation approach that dynamically interpolates student and
teacher distributions through an adaptive intermediate distribution, gradually
shifting from the student's initial distribution towards the teacher's
distribution. We provide a theoretical analysis demonstrating TAID's ability to
prevent mode collapse and empirically show its effectiveness in addressing the
capacity gap while balancing mode averaging and mode collapse. Our
comprehensive experiments demonstrate TAID's superior performance across
various model sizes and architectures in both instruction tuning and
pre-training scenarios. Furthermore, we showcase TAID's practical impact by
developing two state-of-the-art compact foundation models:
$\texttt{TAID-LLM-1.5B}$ for language tasks and $\texttt{TAID-VLM-2B}$ for
vision-language tasks. These results demonstrate TAID's effectiveness in
creating high-performing and efficient models, advancing the development of
more accessible AI technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the 13th International Conference on Learning
  Representations (ICLR 2025) as a Spotlight presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Learning Quantum States with the Logarithmic Loss via VB-FTRL <span class="chip">ALT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04237v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04237v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Fu Tseng, Kai-Chun Chen, Zi-Hong Xiao, Yen-Huan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online learning of quantum states with the logarithmic loss (LL-OLQS) is a
quantum generalization of online portfolio selection (OPS), a classic open
problem in online learning for over three decades. This problem also emerges in
designing stochastic optimization algorithms for maximum-likelihood quantum
state tomography. Recently, Jezequel et al. (arXiv:2209.13932) proposed the
VB-FTRL algorithm, the first regret-optimal algorithm for OPS with moderate
computational complexity. In this paper, we generalize VB-FTRL for LL-OLQS. Let
$d$ denote the dimension and $T$ the number of rounds. The generalized
algorithm achieves a regret rate of $O ( d^2 \log ( d + T ) )$ for LL-OLQS.
Each iteration of the algorithm consists of solving a semidefinite program that
can be implemented in polynomial time by, for example, cutting-plane methods.
For comparison, the best-known regret rate for LL-OLQS is currently $O ( d^2
\log T )$, achieved by an exponential weight method. However, no explicit
implementation is available for the exponential weight method for LL-OLQS. To
facilitate the generalization, we introduce the notion of VB-convexity.
VB-convexity is a sufficient condition for the volumetric barrier associated
with any function to be convex and is of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ALT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Training of Diffusion Models for Feasible Solution
  Generation in Neural Combinatorial Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00003v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00003v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seong-Hyun Hong, Hyun-Sung Kim, Zian Jang, Deunsol Yoon, Hyungseok Song, Byung-Jun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in neural combinatorial optimization (NCO) methods have
shown promising results in generating near-optimal solutions without the need
for expert-crafted heuristics. However, high performance of these approaches
often rely on problem-specific human-expertise-based search after generating
candidate solutions, limiting their applicability to commonly solved CO
problems such as Traveling Salesman Problem (TSP). In this paper, we present
IC/DC, an unsupervised CO framework that directly trains a diffusion model from
scratch. We train our model in a self-supervised way to minimize the cost of
the solution while adhering to the problem-specific constraints. IC/DC is
specialized in addressing CO problems involving two distinct sets of items, and
it does not need problem-specific search processes to generate valid solutions.
IC/DC employs a novel architecture capable of capturing the intricate
relationships between items, and thereby enabling effective optimization in
challenging CO scenarios. IC/DC achieves state-of-the-art performance relative
to existing NCO methods on the Parallel Machine Scheduling Problem (PMSP) and
Asymmetric Traveling Salesman Problem (ATSP).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Transparent and Accurate Diabetes Prediction Using Machine
  Learning and Explainable Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pir Bakhsh Khokhar, Viviana Pentangelo, Fabio Palomba, Carmine Gravino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diabetes mellitus (DM) is a global health issue of significance that must be
diagnosed as early as possible and managed well. This study presents a
framework for diabetes prediction using Machine Learning (ML) models,
complemented with eXplainable Artificial Intelligence (XAI) tools, to
investigate both the predictive accuracy and interpretability of the
predictions from ML models. Data Preprocessing is based on the Synthetic
Minority Oversampling Technique (SMOTE) and feature scaling used on the
Diabetes Binary Health Indicators dataset to deal with class imbalance and
variability of clinical features. The ensemble model provided high accuracy,
with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General
Health, Income, and Physical Activity were the most influential predictors
obtained from the model explanations. The results of this study suggest that ML
combined with XAI is a promising means of developing accurate and
computationally transparent tools for use in healthcare systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion
  Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyogon Ryu, NaHyeon Park, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread use of text-to-image diffusion models across various
tasks, their computational and memory demands limit practical applications. To
mitigate this issue, quantization of diffusion models has been explored. It
reduces memory usage and computational costs by compressing weights and
activations into lower-bit formats. However, existing methods often struggle to
preserve both image quality and text-image alignment, particularly in
lower-bit($<$ 8bits) quantization. In this paper, we analyze the challenges
associated with quantizing text-to-image diffusion models from a distributional
perspective. Our analysis reveals that activation outliers play a crucial role
in determining image quality. Additionally, we identify distinctive patterns in
cross-attention scores, which significantly affects text-image alignment. To
address these challenges, we propose Distribution-aware Group Quantization
(DGQ), a method that identifies and adaptively handles pixel-wise and
channel-wise outliers to preserve image quality. Furthermore, DGQ applies
prompt-specific logarithmic quantization scales to maintain text-image
alignment. Our method demonstrates remarkable performance on datasets such as
MS-COCO and PartiPrompts. We are the first to successfully achieve low-bit
quantization of text-to-image diffusion models without requiring additional
fine-tuning of weight quantization parameters. Code is available at
https://github.com/ugonfor/DGQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted ICLR 2025. Project page: https://ugonfor.kr/DGQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning without Forgetting for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Da-Wei Zhou, Yuanhan Zhang, Yan Wang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-Incremental Learning (CIL) or continual learning is a desired
capability in the real world, which requires a learning system to adapt to new
tasks without forgetting former ones. While traditional CIL methods focus on
visual information to grasp core features, recent advances in Vision-Language
Models (VLM) have shown promising capabilities in learning generalizable
representations with the aid of textual information. However, when continually
trained with new classes, VLMs often suffer from catastrophic forgetting of
former knowledge. Applying VLMs to CIL poses two major challenges: 1) how to
adapt the model without forgetting; and 2) how to make full use of the
multi-modal information. To this end, we propose PROjectiOn Fusion (PROOF) that
enables VLMs to learn without forgetting. To handle the first challenge, we
propose training task-specific projections based on the frozen image/text
encoders. When facing new tasks, new projections are expanded and former
projections are fixed, alleviating the forgetting of old concepts. For the
second challenge, we propose the fusion module to better utilize the
cross-modality information. By jointly adjusting visual and textual features,
the model can capture semantic information with stronger representation
ability. Extensive experiments on nine benchmark datasets validate PROOF
achieves state-of-the-art performance. Code is available at
https://github.com/zhoudw-zdw/PROOF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TPAMI. Code is available at
  https://github.com/zhoudw-zdw/PROOF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining a probabilistic prediction on the simplex with Shapley
  compositions <span class="chip">ECAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul-Gauthier Noé, Miquel Perelló-Nieto, Jean-François Bonastre, Peter Flach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Originating in game theory, Shapley values are widely used for explaining a
machine learning model's prediction by quantifying the contribution of each
feature's value to the prediction. This requires a scalar prediction as in
binary classification, whereas a multiclass probabilistic prediction is a
discrete probability distribution, living on a multidimensional simplex. In
such a multiclass setting the Shapley values are typically computed separately
on each class in a one-vs-rest manner, ignoring the compositional nature of the
output distribution. In this paper, we introduce Shapley compositions as a
well-founded way to properly explain a multiclass probabilistic prediction,
using the Aitchison geometry from compositional data analysis. We prove that
the Shapley composition is the unique quantity satisfying linearity, symmetry
and efficiency on the Aitchison simplex, extending the corresponding axiomatic
properties of the standard Shapley value. We demonstrate this proper multiclass
treatment in a range of scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ECAI2024's proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logarithmic Regret for Unconstrained Submodular Maximization Stochastic
  Bandit <span class="chip">ALT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Zhou, Pierre Gaillard, Thibaud Rahier, Julyan Arbel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the online unconstrained submodular maximization problem (Online
USM), in a setting with stochastic bandit feedback. In this framework, a
decision-maker receives noisy rewards from a non monotone submodular function
taking values in a known bounded interval. This paper proposes Double-Greedy -
Explore-then-Commit (DG-ETC), adapting the Double-Greedy approach from the
offline and online full-information settings. DG-ETC satisfies a $O(d\log(dT))$
problem-dependent upper bound for the $1/2$-approximate pseudo-regret, as well
as a $O(dT^{2/3}\log(dT)^{1/3})$ problem-free one at the same time,
outperforming existing approaches. In particular, we introduce a
problem-dependent notion of hardness characterizing the transition between
logarithmic and polynomial regime for the upper bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for ALT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning-Based Estimation Of Wave Direction For Unmanned Surface
  Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16205v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16205v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manele Ait Habouche, Mickaël Kerboeuf, Goulven Guillou, Jean-Philippe Babau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Surface Vehicles (USVs) have become critical tools for marine
exploration, environmental monitoring, and autonomous navigation. Accurate
estimation of wave direction is essential for improving USV navigation and
ensuring operational safety, but traditional methods often suffer from high
costs and limited spatial resolution. This paper proposes a machine
learning-based approach leveraging LSTM (Long Short-Term Memory) networks to
predict wave direction using sensor data collected from USVs. Experimental
results show the capability of the LSTM model to learn temporal dependencies
and provide accurate predictions, outperforming simpler baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Analysis of Combinatorial Gaussian Process Bandits <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12676v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12676v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Sandberg, Niklas Åkerblom, Morteza Haghir Chehreghani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the combinatorial volatile Gaussian process (GP) semi-bandit
problem. Each round, an agent is provided a set of available base arms and must
select a subset of them to maximize the long-term cumulative reward. We study
the Bayesian setting and provide novel Bayesian cumulative regret bounds for
three GP-based algorithms: GP-UCB, GP-BayesUCB and GP-TS. Our bounds extend
previous results for GP-UCB and GP-TS to the infinite, volatile and
combinatorial setting, and to the best of our knowledge, we provide the first
regret bound for GP-BayesUCB. Volatile arms encompass other widely considered
bandit problems such as contextual bandits. Furthermore, we employ our
framework to address the challenging real-world problem of online
energy-efficient navigation, where we demonstrate its effectiveness compared to
the alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 9 figures. Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Utility and Complexity of in- and out-of-Distribution Machine
  Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Allouah, Joshua Kazdan, Rachid Guerraoui, Sanmi Koyejo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning, the process of selectively removing data from trained
models, is increasingly crucial for addressing privacy concerns and knowledge
gaps post-deployment. Despite this importance, existing approaches are often
heuristic and lack formal guarantees. In this paper, we analyze the fundamental
utility, time, and space complexity trade-offs of approximate unlearning,
providing rigorous certification analogous to differential privacy. For
in-distribution forget data -- data similar to the retain set -- we show that a
surprisingly simple and general procedure, empirical risk minimization with
output perturbation, achieves tight unlearning-utility-complexity trade-offs,
addressing a previous theoretical gap on the separation from unlearning "for
free" via differential privacy, which inherently facilitates the removal of
such data. However, such techniques fail with out-of-distribution forget data
-- data significantly different from the retain set -- where unlearning time
complexity can exceed that of retraining, even for a single sample. To address
this, we propose a new robust and noisy gradient descent variant that provably
amortizes unlearning time complexity without compromising utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximum Mean Discrepancy on Exponential Windows for Online Change
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12706v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12706v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Kalinke, Marco Heyden, Georg Gntuni, Edouard Fouché, Klemens Böhm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting changes is of fundamental importance when analyzing data streams
and has many applications, e.g., in predictive maintenance, fraud detection, or
medicine. A principled approach to detect changes is to compare the
distributions of observations within the stream to each other via hypothesis
testing. Maximum mean discrepancy (MMD), a (semi-)metric on the space of
probability distributions, provides powerful non-parametric two-sample tests on
kernel-enriched domains. In particular, MMD is able to detect any disparity
between distributions under mild conditions. However, classical MMD estimators
suffer from a quadratic runtime complexity, which renders their direct use for
change detection in data streams impractical. In this article, we propose a new
change detection algorithm, called Maximum Mean Discrepancy on Exponential
Windows (MMDEW), that combines the benefits of MMD with an efficient
computation based on exponential windows. We prove that MMDEW enjoys
polylogarithmic runtime and logarithmic memory complexity and show empirically
that it outperforms the state of the art on benchmark data streams.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR 02/25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Falsification of Cyber-Physical Systems using Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zahra Ramezani, Kenan Šehić, Luigi Nardi, Knut Åkesson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cyber-physical systems (CPSs) are often complex and safety-critical, making
it both challenging and crucial to ensure that the system's specifications are
met. Simulation-based falsification is a practical testing technique for
increasing confidence in a CPS's correctness, as it only requires that the
system be simulated. Reducing the number of computationally intensive
simulations needed for falsification is a key concern. In this study, we
investigate Bayesian optimization (BO), a sample-efficient approach that learns
a surrogate model to capture the relationship between input signal
parameterization and specification evaluation. We propose two enhancements to
the basic BO for improving falsification: (1) leveraging local surrogate
models, and (2) utilizing the user's prior knowledge. Additionally, we address
the formulation of acquisition functions for falsification by proposing and
evaluating various alternatives. Our benchmark evaluation demonstrates
significant improvements when using local surrogate models in BO for falsifying
challenging benchmark examples. Incorporating prior knowledge is found to be
especially beneficial when the simulation budget is constrained. For some
benchmark problems, the choice of acquisition function noticeably impacts the
number of simulations required for successful falsification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACM Transactions on Embedded Computing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinyCL: An Efficient Hardware Architecture for Continual Learning on
  Autonomous Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09780v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09780v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugenio Ressa, Alberto Marchisio, Maurizio Martina, Guido Masera, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Continuous Learning (CL) paradigm consists of continuously evolving the
parameters of the Deep Neural Network (DNN) model to progressively learn to
perform new tasks without reducing the performance on previous tasks, i.e.,
avoiding the so-called catastrophic forgetting. However, the DNN parameter
update in CL-based autonomous systems is extremely resource-hungry. The
existing DNN accelerators cannot be directly employed in CL because they only
support the execution of the forward propagation. Only a few prior
architectures execute the backpropagation and weight update, but they lack the
control and management for CL. Towards this, we design a hardware architecture,
TinyCL, to perform CL on resource-constrained autonomous systems. It consists
of a processing unit that executes both forward and backward propagation, and a
control unit that manages memory-based CL workload. To minimize the memory
accesses, the sliding window of the convolutional layer moves in a snake-like
fashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at
runtime to execute different operations. As per our knowledge, our proposed
TinyCL represents the first hardware accelerator that executes CL on autonomous
systems. We synthesize the complete TinyCL architecture in a 65 nm CMOS
technology node with the conventional ASIC design flow. It executes 1 epoch of
training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while
1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,
thus achieving a 58x speedup, consuming 86 mW in a 4.74 mm2 die.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Directed Learning of Convex Labelings on Graphs <span class="chip">ALT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgy Sokolov, Maximilian Thiessen, Margarita Akhmejanova, Fabio Vitale, Francesco Orabona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of classifying the nodes of a given graph in the
self-directed learning setup. This learning setting is a variant of online
learning, where rather than an adversary determining the sequence in which
nodes are presented, the learner autonomously and adaptively selects them.
While self-directed learning of Euclidean halfspaces, linear functions, and
general multiclass hypothesis classes was recently considered, no results
previously existed specifically for self-directed node classification on
graphs. In this paper, we address this problem developing efficient algorithms
for it. More specifically, we focus on the case of (geodesically) convex
clusters, i.e., for every two nodes sharing the same label, all nodes on every
shortest path between them also share the same label. In particular, we devise
an algorithm with runtime polynomial in $n$ that makes only $3(h(G)+1)^4 \ln n$
mistakes on graphs with two convex clusters, where $n$ is the total number of
nodes and $h(G)$ is the Hadwiger number, i.e., the size of the largest clique
minor of the graph $G$. We also show that our algorithm is robust to the case
that clusters are slightly non-convex, still achieving a mistake bound
logarithmic in $n$. Finally, we devise a simple and efficient algorithm for
homophilic clusters, where strongly connected nodes tend to belong to the same
class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ALT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Generative Models with Hard Linear Equality Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05416v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05416v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyan Li, Dipti Ranjan Sahu, Guy Van den Broeck, Zhe Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep generative models~(DGMs) have demonstrated remarkable success in
capturing complex data distributions, they consistently fail to learn
constraints that encode domain knowledge and thus require constraint
integration. Existing solutions to this challenge have primarily relied on
heuristic methods and often ignore the underlying data distribution, harming
the generative performance. In this work, we propose a probabilistically sound
approach for enforcing the hard constraints into DGMs to generate
constraint-compliant and realistic data. This is achieved by our proposed
gradient estimators that allow the constrained distribution, the data
distribution conditioned on constraints, to be differentiably learned. We carry
out extensive experiments with various DGM model architectures over five image
datasets and three scientific applications in which domain knowledge is
governed by linear equality constraints. We validate that the standard DGMs
almost surely generate data violating the constraints. Among all the constraint
integration strategies, ours not only guarantees the satisfaction of
constraints in generation but also archives superior generative performance
than the other methods across every benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Potential and limitations of random Fourier features for dequantizing
  quantum machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Sweke, Erik Recio-Armengol, Sofiene Jerbi, Elies Gil-Fuster, Bryce Fuller, Jens Eisert, Johannes Jakob Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum machine learning is arguably one of the most explored applications of
near-term quantum devices. Much focus has been put on notions of variational
quantum machine learning where parameterized quantum circuits (PQCs) are used
as learning models. These PQC models have a rich structure which suggests that
they might be amenable to efficient dequantization via random Fourier features
(RFF). In this work, we establish necessary and sufficient conditions under
which RFF does indeed provide an efficient dequantization of variational
quantum machine learning for regression. We build on these insights to make
concrete suggestions for PQC architecture design, and to identify structures
which are necessary for a regression problem to admit a potential quantum
advantage via PQC based optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages (33+11). 6 Figures, with many clarifying figures added to
  this version from original version. Comments and feedback welcome. Now
  accepted in Quantum - this is the final version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RIDA: A Robust Attack Framework on Incomplete Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18170v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18170v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianke Yu, Hanchen Wang, Chen Chen, Xiaoyang Wang, Lu Qin, Wenjie Zhang, Ying Zhang, Xijuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are vital in data science but are increasingly
susceptible to adversarial attacks. To help researchers develop more robust GNN
models, it's essential to focus on designing strong attack models as
foundational benchmarks and guiding references. Among adversarial attacks,
gray-box poisoning attacks are noteworthy due to their effectiveness and fewer
constraints. These attacks exploit GNNs' need for retraining on updated data,
thereby impacting their performance by perturbing these datasets. However,
current research overlooks the real-world scenario of incomplete graphs. To
address this gap, we introduce the Robust Incomplete Deep Attack Framework
(RIDA). It is the first algorithm for robust gray-box poisoning attacks on
incomplete graphs. The approach innovatively aggregates distant vertex
information and ensures powerful data utilization. Extensive tests against 9
SOTA baselines on 3 real-world datasets demonstrate that RIDA's superiority in
handling incompleteness and high attack performance on the incomplete graph.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample-Efficient Reinforcement Learning from <span class="highlight-title">Human</span> Feedback via
  Information-Directed Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Qi, Haochen Yang, Qiaosheng Zhang, Zhuoran Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of reinforcement learning from human feedback (RLHF), a
critical problem in training large language models, from a theoretical
perspective. Our main contribution is the design of novel sample-efficient RLHF
algorithms based on information-directed sampling (IDS), an online
decision-making principle inspired by information theory. Our algorithms
maximize the sum of the value function and a mutual information term that
encourages exploration of the unknown environment (which quantifies the
information gained about the environment through observed human feedback data).
To tackle the challenge of large state spaces and improve sample efficiency, we
construct a simplified \emph{surrogate environment} and introduce a novel
distance measure (named the \emph{$\ell_g$-distance}), enabling our IDS-based
algorithm to achieve a Bayesian regret upper bound of order
$O(H^{\frac{3}{2}}\sqrt{\log(K(\epsilon)) T})$, where $H$ is the episode
length, $T$ is the number of episode and $K(\epsilon)$ is related to the
covering number of the environment. Specializing to the tabular settings, this
regret bound is of order $\tilde{O}(H^2\sqrt{SAT})$, where $S$ and $A$ are the
numbers of states and actions. Finally, we propose an Approximate-IDS algorithm
that is computationally more efficient while maintaining nearly the same sample
efficiency. The design principle of this approximate algorithm is not only
effective in RLHF settings but also applicable to the standard RL framework.
Moreover, our work showcases the value of information theory in reinforcement
learning and in the training of large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Parameter Update Balancing Algorithm for Multi-task Ranking Models in
  Recommendation Systems <span class="chip">ICDM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yuan, Guohao Cai, Zhenhua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task ranking models have become essential for modern real-world
recommendation systems. While most recommendation researches focus on designing
sophisticated models for specific scenarios, achieving performance improvement
for multi-task ranking models across various scenarios still remains a
significant challenge. Training all tasks naively can result in inconsistent
learning, highlighting the need for the development of multi-task optimization
(MTO) methods to tackle this challenge. Conventional methods assume that the
optimal joint gradient on shared parameters leads to optimal parameter updates.
However, the actual update on model parameters may deviates significantly from
gradients when using momentum based optimizers such as Adam, and we design and
execute statistical experiments to support the observation. In this paper, we
propose a novel Parameter Update Balancing algorithm for multi-task
optimization, denoted as PUB. In contrast to traditional MTO method which are
based on gradient level tasks fusion or loss level tasks fusion, PUB is the
first work to optimize multiple tasks through parameter update balancing.
Comprehensive experiments on benchmark multi-task ranking datasets demonstrate
that PUB consistently improves several multi-task backbones and achieves
state-of-the-art performance. Additionally, experiments on benchmark computer
vision datasets show the great potential of PUB in various multi-task learning
scenarios. Furthermore, we deployed our method for an industrial evaluation on
the real-world commercial platform, HUAWEI AppGallery, where PUB significantly
enhances the online multi-task ranking model, efficiently managing the primary
traffic of a crucial channel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDM'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VidCRAFT3: Camera, Object, and Lighting <span class="highlight-title">Control</span> for Image-to-<span class="highlight-title">Video</span>
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent image-to-video generation methods have demonstrated success in
enabling control over one or two visual elements, such as camera trajectory or
object motion. However, these methods are unable to offer control over multiple
visual elements due to limitations in data and network efficacy. In this paper,
we introduce VidCRAFT3, a novel framework for precise image-to-video generation
that enables control over camera motion, object motion, and lighting direction
simultaneously. To better decouple control over each visual element, we propose
the Spatial Triple-Attention Transformer, which integrates lighting direction,
text, and image in a symmetric way. Since most real-world video datasets lack
lighting annotations, we construct a high-quality synthetic video dataset, the
VideoLightingDirection (VLD) dataset. This dataset includes lighting direction
annotations and objects of diverse appearance, enabling VidCRAFT3 to
effectively handle strong light transmission and reflection effects.
Additionally, we propose a three-stage training strategy that eliminates the
need for training data annotated with multiple visual elements (camera motion,
object motion, and lighting direction) simultaneously. Extensive experiments on
benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing
high-quality video content, surpassing existing state-of-the-art methods in
terms of control granularity and visual coherence. All code and data will be
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning over Connected Modes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03333v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03333v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Grinwald, Philipp Wiesner, Shinichi Nakajima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical heterogeneity in federated learning poses two major challenges:
slow global training due to conflicting gradient signals, and the need of
personalization for local distributions. In this work, we tackle both
challenges by leveraging recent advances in \emph{linear mode connectivity} --
identifying a linearly connected low-loss region in the parameter space of
neural networks, which we call solution simplex. We propose federated learning
over connected modes (\textsc{Floco}), where clients are assigned local
subregions in this simplex based on their gradient signals, and together learn
the shared global solution simplex. This allows personalization of the client
models to fit their local distributions within the degrees of freedom in the
solution simplex and homogenizes the update signals for the global simplex
training. Our experiments show that \textsc{Floco} accelerates the global
training process, and significantly improves the local accuracy with minimal
computational overhead in cross-silo federated learning settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapt then Unlearn: Exploring Parameter Space Semantics for Unlearning
  in Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piyush Tiwary, Atri Guha, Subhodip Panda, Prathosh A. P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the growing concerns about privacy and regulatory compliance, it is
desirable to regulate the output of generative models. To that end, the
objective of this work is to prevent the generation of outputs containing
undesired features from a pre-trained Generative Adversarial Network (GAN)
where the underlying training data set is inaccessible. Our approach is
inspired by the observation that the parameter space of GANs exhibits
meaningful directions that can be leveraged to suppress specific undesired
features. However, such directions usually result in the degradation of the
quality of generated samples. Our proposed two-stage method, known as
'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also
maintaining the quality of generated samples. In the initial stage, we adapt a
pre-trained GAN on a set of negative samples (containing undesired features)
provided by the user. Subsequently, we train the original pre-trained GAN using
positive samples, along with a repulsion regularizer. This regularizer
encourages the learned model parameters to move away from the parameters of the
adapted model (first stage) while not degrading the generation quality. We
provide theoretical insights into the proposed method. To the best of our
knowledge, our approach stands as the first method addressing unlearning within
the realm of high-fidelity GANs (such as StyleGAN). We validate the
effectiveness of our method through comprehensive experiments, encompassing
both class-level unlearning on the MNIST and AFHQ dataset and feature-level
unlearning tasks on the CelebA-HQ dataset. Our code and implementation is
available at: https://github.com/atriguha/Adapt_Unlearn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KL-geodesics flow matching with a novel sampling scheme 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Egor Sevriugov, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-autoregressive language models generate all tokens simultaneously,
offering potential speed advantages over traditional autoregressive models, but
they face challenges in modeling the complex dependencies inherent in text
data. In this work, we investigate a conditional flow matching approach for
text generation. We represent tokens as one-hot vectors in a \(V\)-dimensional
simplex and utilize geodesics under the Kullback-Leibler (KL) divergence, which
correspond to linear interpolation in logit space. We provide a theoretical
justification that maximizing the conditional likelihood \(P_{\theta}(x_1 \mid
x_t, t)\) yields the exact flow matching velocity under logit interpolation. To
address the suboptimal performance of basic inference, we propose a novel
empirical sampling scheme that iteratively samples from the conditional
distribution and introduces additional noise, significantly improving results
despite lacking full theoretical underpinnings. Furthermore, we propose a
hybrid inference method that combines the basic approach with the sampling
scheme. This method demonstrates superior performance on both conditional and
unconditional text generation experiments compared to previous SOTA method for
discrete flow matching.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable and Class-Revealing Signal Feature Extraction via Scattering
  Transform and Constrained Zeroth-Order Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Saito, David Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new method to extract discriminant and explainable features from
a particular machine learning model, i.e., a combination of the scattering
transform and the multiclass logistic regression. Although this model is
well-known for its ability to learn various signal classes with high
classification rate, it remains elusive to understand why it can generate such
successful classification, mainly due to the nonlinearity of the scattering
transform. In order to uncover the meaning of the scattering transform
coefficients selected by the multiclass logistic regression (with the Lasso
penalty), we adopt zeroth-order optimization algorithms to search an input
pattern that maximizes the class probability of a class of interest given the
learned model. In order to do so, it turns out that imposing sparsity and
smoothness of input patterns is important. We demonstrate the effectiveness of
our proposed method using a couple of synthetic time-series classification
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages; 6 figures; submitted to 2025 IEEE Statistical Signal
  Processing Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proper <span class="highlight-title">Dataset</span> Valuation by Pointwise Mutual Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuran Zheng, Xuan Qi, Rui Ray Chen, Yongchan Kwon, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data plays a central role in the development of modern artificial
intelligence, with high-quality data emerging as a key driver of model
performance. This has prompted the development of various data curation methods
in recent years. However, measuring the effectiveness of these data curation
techniques remains a major challenge. Traditional evaluation methods, which
assess a trained model's performance on specific benchmarks, risk promoting
practices that merely make the data more similar to the test data. This issue
exemplifies Goodhart's law: when a measure becomes a target, it ceases to be a
good measure. To address this, we propose an information-theoretic framework
for evaluating data curation methods, where dataset quality is measured by its
informativeness about the true model parameters using the Blackwell ordering.
We compare informativeness by the Shannon mutual information of the evaluated
data and the test data, and we propose a novel method for estimating the mutual
information of datasets by training Bayesian models on embedded data and
computing the mutual information from the model's parameter posteriors.
Experiments on real-world data demonstrate that our mutual information-based
evaluation assigns appropriately lower scores to data curation strategies that
reduce dataset informativeness, while traditional test score-based evaluation
methods may favor data curation strategies that overfit to the test set but
compromise the training data's informativeness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Experience Replay Facilitates Safety Red-Teaming of
  Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Yi Chin, Mario Fritz, Pin-Yu Chen, Wei-Chen Chiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models have shown remarkable progress, but their
potential to generate harmful content remains a critical concern in the ML
community. While various safety mechanisms have been developed, the field lacks
systematic tools for evaluating their effectiveness against real-world misuse
scenarios. In this work, we propose ICER, a novel red-teaming framework that
leverages Large Language Models (LLMs) and a bandit optimization-based
algorithm to generate interpretable and semantic meaningful problematic prompts
by learning from past successful red-teaming attempts. Our ICER efficiently
probes safety mechanisms across different T2I models without requiring internal
access or additional training, making it broadly applicable to deployed
systems. Through extensive experiments, we demonstrate that ICER significantly
outperforms existing prompt attack methods in identifying model vulnerabilities
while maintaining high semantic similarity with intended content. By uncovering
that successful jailbreaking instances can systematically facilitate the
discovery of new vulnerabilities, our work provides crucial insights for
developing more robust safety mechanisms in T2I systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence of Distributed Adaptive Optimization with Local Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Cheng, Margalit Glasgow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study distributed adaptive algorithms with local updates (intermittent
communication). Despite the great empirical success of adaptive methods in
distributed training of modern machine learning models, the theoretical
benefits of local updates within adaptive methods, particularly in terms of
reducing communication complexity, have not been fully understood yet. In this
paper, for the first time, we prove that \em Local SGD \em with momentum (\em
Local \em SGDM) and \em Local \em Adam can outperform their minibatch
counterparts in convex and weakly convex settings in certain regimes,
respectively. Our analysis relies on a novel technique to prove contraction
during local iterations, which is a crucial yet challenging step to show the
advantages of local updates, under generalized smoothness assumption and
gradient clipping strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft Diffusion Actor-Critic: Efficient Online Reinforcement Learning for
  Diffusion Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitong Ma, Tianyi Chen, Kai Wang, Na Li, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion policies have achieved superior performance in imitation learning
and offline reinforcement learning (RL) due to their rich expressiveness.
However, the vanilla diffusion training procedure requires samples from target
distribution, which is impossible in online RL since we cannot sample from the
optimal policy, making training diffusion policies highly non-trivial in online
RL. Backpropagating policy gradient through the diffusion process incurs huge
computational costs and instability, thus being expensive and impractical. To
enable efficient diffusion policy training for online RL, we propose Soft
Diffusion Actor-Critic (SDAC), exploiting the viewpoint of diffusion models as
noise-perturbed energy-based models. The proposed SDAC relies solely on the
state-action value function as the energy functions to train diffusion
policies, bypassing sampling from the optimal policy while maintaining
lightweight computations. We conducted comprehensive comparisons on MuJoCo
benchmarks. The empirical results show that SDAC outperforms all recent
diffusion-policy online RLs on most tasks, and improves more than 120% over
soft actor-critic on complex locomotion tasks such as Humanoid and Ant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global Deep Forecasting with Patient-Specific Pharmacokinetics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13135v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13135v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Willa Potosnak, Cristian Challu, Kin G. Olivares, Keith A. Dufendach, Artur Dubrawski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting healthcare time series data is vital for early detection of
adverse outcomes and patient monitoring. However, it can be challenging in
practice due to variable medication administration and unique pharmacokinetic
(PK) properties of each patient. To address these challenges, we propose a
novel hybrid global-local architecture and a PK encoder that informs deep
learning models of patient-specific treatment effects. We showcase the efficacy
of our approach in achieving significant accuracy gains in a blood glucose
forecasting task using both realistically simulated and real-world data. Our PK
encoder surpasses baselines by up to 16.4% on simulated data and 4.9% on
real-world data for individual patients during critical events of severely high
and low glucose levels. Furthermore, our proposed hybrid global-local
architecture outperforms patient-specific PK models by 15.8%, on average.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against
  Jailbreak Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhan Zhao, Xiang Zheng, Lin Luo, Yige Li, Xingjun Ma, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on black-box defense for VLMs against jailbreak
attacks. Existing black-box defense methods are either unimodal or bimodal.
Unimodal methods enhance either the vision or language module of the VLM, while
bimodal methods robustify the model through text-image representation
realignment. However, these methods suffer from two limitations: 1) they fail
to fully exploit the cross-modal information, or 2) they degrade the model
performance on benign inputs. To address these limitations, we propose a novel
blue-team method BlueSuffix that defends target VLMs against jailbreak attacks
without compromising its performance under black-box setting. BlueSuffix
includes three key components: 1) a visual purifier against jailbreak images,
2) a textual purifier against jailbreak texts, and 3) a blue-team suffix
generator using reinforcement fine-tuning for enhancing cross-modal robustness.
We empirically show on four VLMs (LLaVA, MiniGPT-4, InstructionBLIP, and
Gemini) and four safety benchmarks (Harmful Instruction, AdvBench,
MM-SafetyBench, and RedTeam-2K) that BlueSuffix outperforms the baseline
defenses by a significant margin. Our BlueSuffix opens up a promising direction
for defending VLMs against jailbreak attacks. Code is available at
https://github.com/Vinsonzyh/BlueSuffix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DriveGPT: Scaling Autoregressive Behavior Models for Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14415v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14415v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Huang, Eric M. Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David S. Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, Xiaobai Chen, Chingiz Tairbekov, Pratik Agarwal, Tianshi Gao, Yuning Chai, Siddhartha Srinivasa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DriveGPT, a scalable behavior model for autonomous driving. We
model driving as a sequential decision-making task, and learn a transformer
model to predict future agent states as tokens in an autoregressive fashion. We
scale up our model parameters and training data by multiple orders of
magnitude, enabling us to explore the scaling properties in terms of dataset
size, model parameters, and compute. We evaluate DriveGPT across different
scales in a planning task, through both quantitative metrics and qualitative
examples, including closed-loop driving in complex real-world scenarios. In a
separate prediction task, DriveGPT outperforms state-of-the-art baselines and
exhibits improved performance by pretraining on a large-scale dataset, further
validating the benefits of data scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 16 figures, 8 tables, and 1 video link</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Human</span>ity's Last Exam 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Tung Nguyen, Daron Anderson, Imad Ali Shah, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Jaeho Lee, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, Robert Gerbicz, John-Clark Levin, Serguei Popov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Mstyslav Kazakov, Geoff Galgon, Johannes Schmitt, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Antrell Cheatom, Zachary Giboney, Gashaw M. Goshu, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, Jennifer Zampese, John Wydallis, John B. Wydallis, Ryan G. Hoerr, Mark Nandor, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Jungbae Nam, Edwin Taylor, Jun Jin, Gautier Abou Loume, Hangrui Cao, Alexis C Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Aras Bacho, Lianghui Li, Sumeet Motwani, Christian Schroeder de Witt, Alexei Kopylov, Johannes Veith, Eric Singer, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Ameya Prabhu, Longke Tang, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Joshua Robinson, Aleksandar Mikov, Julien Guillod, Yuqi Li, Ben Pageler, Joshua Vendrow, Vladyslav Kuchkin, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Andrew Gritsevskiy, Dakotah Martinez, Nick Crispino, Dimitri Zvonkine, Natanael Wildner Fraga, Saeed Soori, Ori Press, Henry Tang, Julian Salazar, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, T. Ryan Rogers, Wenjin Zhang, Ross Finocchio, Bikun Li, Jinzhou Yang, Arun Rao, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Ariel Ghislain Kemogne Kamdoum, Tad Hogg, Alvin Jin, Carlo Bosio, Gongbo Sun, Brian P Coppola, Haline Heidinger, Rafael Sayous, Stefan Ivanov, Joseph M Cavanagh, Jiawei Shen, Joseph Marvin Imperial, Philippe Schwaller, Shaipranesh Senthilkuma, Andres M Bran, Andres Algaba, Brecht Verbeken, Kelsey Van den Houte, Lynn Van Der Sypt, David Noever, Lisa Schut, Ilia Sucholutsky, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Shankar Sivarajan, Tong Yang, John Maar, Julian Wykowski, Martí Oller, Jennifer Sandlin, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Felipe Meneguitti Dias, Tobias Kreiman, Kaivalya Rawal, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Jeremy Nguyen, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Sergey Ivanov, Rafał Poświata, Chenguang Wang, Daofeng Li, Donato Crisostomi, Ali Dehghan, Andrea Achilleos, John Arnold Ambay, Benjamin Myklebust, Archan Sen, David Perrella, Nurdin Kaparov, Mark H Inlow, Allen Zang, Kalyan Ramakrishnan, Daniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Dan Bar Hava, Aleksey Kuchkin, Robert Lauff, David Holmes, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider, Daniel Pyda, Zakayo Kazibwe, Mukhwinder Singh, Don Clarke, Dae Hyun Kim, Sara Fish, Veit Elser, Victor Efren Guadarrama Vilchis, Immo Klose, Christoph Demian, Ujjwala Anantheswaran, Adam Zweiger, Guglielmo Albani, Jeffery Li, Nicolas Daans, Maksim Radionov, Václav Rozhoň, Vincent Ginis, Ziqiao Ma, Christian Stump, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Marco Piccardo, Niv Cohen, Virendra Singh, Josef Tkadlec, Paul Rosu, Alan Goldfarb, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Aline Menezes, Arkil Patel, Zixuan Wang, Jamie Tucker-Foltz, Jack Stade, Declan Grabb, Tom Goertzen, Fereshteh Kazemi, Jeremiah Milbauer, Abhishek Shukla, Hossam Elgnainy, Yan Carlos Leyva Labrador, Hao He, Ling Zhang, Alan Givré, Hew Wolff, Gözdenur Demir, Muhammad Fayez Aziz, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Elliott Thornley, Robin Zhang, Jiayi Pan, Antonio Terpin, Niklas Muennighoff, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Jainam Shah, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Andrew Ho, Shaul Barkan, Jiaqi Wang, Martin Stehberger, Egor Kretov, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Zaki Hossain, Ido Akov, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Micah Carroll, Orr Paradise, Jan Hendrik Kirchner, Stefan Steinerberger, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie, Paolo Giordano, Philipp Petersen, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Antonella Pinto, Shreyas Verma, Prashant Joshi, Eli Meril, Zheng-Xin Yong, Allison Tee, Jérémy Andréoletti, Orion Weller, Raghav Singhal, Gang Zhang, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Hamid Mostaghimi, Kunvar Thaman, Qijia Chen, Tran Quoc Khánh, Jacob Loader, Stefano Cavalleri, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Jonathan Roberts, William Alley, Kunyang Sun, Ryan Stendall, Max Lamparth, Anka Reuel, Ting Wang, Hanmeng Xu, Pablo Hernández-Cámara, Freddie Martin, Thomas Preu, Tomek Korbak, Marcus Abramovitch, Dominic Williamson, Ida Bosio, Ziye Chen, Biró Bálint, Eve J. Y. Lo, Maria Inês S. Nunes, Yibo Jiang, M Saiful Bari, Peyman Kassani, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Guillaume Douville, Daniel Tordera, George Balabanian, Earth Anderson, Lynna Kvistad, Alejandro José Moyano, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Isaac C. McAlister, Andrew Favre D. O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Ronald Clark, Sherwin Abdoli, Tim Santens, Harrison K Wang, Evan Chen, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Niels Mündler, Avi Semler, Emma Rodman, Jacob Drori, Carl J Fossum, Luk Gloor, Milind Jagota, Ronak Pradeep, Honglu Fan, Tej Shah, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciobâcă, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Siranut Usawasutsakorn, Mohammadreza Mofayezi, Alexander Piperski, Marc Carauleanu, David K. Zhang, Kostiantyn Dobarskyi, Dylan Ler, Roman Leventov, Ignat Soroko, Thorben Jansen, Scott Creighton, Pascal Lauer, Joshua Duersch, Vage Taamazyan, Dario Bezzi, Wiktor Morak, Wenjie Ma, William Held, Tran Đuc Huy, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle X Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Hossein Shahrtash, Edson Oliveira, Joseph W. Jackson, Daniel Espinosa Gonzalez, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Emilien Duc, Bita Golshani, David Stap, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Lukas Lewark, Miguel Orbegozo Rodriguez, Mátyás Vincze, Dustin Wehr, Colin Tang, Shaun Phillips, Fortuna Samuele, Jiang Muzhen, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Peñaflor, Haile Kassahun, Alena Friedrich, Claire Sparrow, Rayner Hernandez Perez, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber,  Handoko, Anton Peristyy, Stephen Malina, Samuel Albanie, Will Cai, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Jasdeep Sidhu, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Brian Weber, Harsh Kumar, Tong Jiang, Arunim Agarwal, Chiara Ceconello, Warren S. Vaz, Chao Zhuang, Haon Park, Andrew R. Tawfeek, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Shreen Gul, Gunjan Chhablani, Zhehang Du, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Florencia de la Rosa, Xiuyu Li, Guillaume Malod, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yiğit Yalın, Gbenga Daniel Obikoya, Luca Arnaboldi,  Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, Kaniuar Bacho, Pierre Clavier, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Denis Peskoff, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk,  Huanxu,  Liu, Olle Häggström, Emil Verkama, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Yiyang Fan, Gabriel Poesia Reis e Silva, Linwei Xin, Yosi Kratish, Jakub Łucki, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Justin Xu, Kevin Joseph Scaria, Freddie Vargus, Farzad Habibi,  Long,  Lian, Emanuele Rodolà, Jules Robins, Vincent Cheng, Tony Fruhauff, Brad Raynor, Hao Qi, Xi Jiang, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Sarah Hoback, Rodrigo De Oliveira Pena, Glen Sherman, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Sandra Mendoza, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Ashley Cartwright, Daphiny Pottmaier, Omid Taheri, David Outevsky, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi, Sam Ali, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Sk Md Salauddin, Murat Islam, Juan Gonzalez, Josh Ducey, Maja Somrak, Vasilios Mavroudis, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu, Jack Lindsey, Anil Radhakrishnan, Antoine Jallon, I. M. J. McInnis, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Javier Gimenez, Roselynn Grace Montecillo, Russell Campbell, Asankhaya Sharma, Khalida Meer, Xavier Alapont, Deepakkumar Patil, Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Sergei Bogdanov, Sören Möller, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Innocent Enyekwe, Ragavendran P V, Zienab EL-Wasif, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Song Bian, John Lai, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Alex Hoover, Joseph McGowan, Tejal Patwardhan, Summer Yue, Alexandr Wang, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarks are important tools for tracking the rapid advancements in large
language model (LLM) capabilities. However, benchmarks are not keeping pace in
difficulty: LLMs now achieve over 90\% accuracy on popular benchmarks like
MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In
response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at
the frontier of human knowledge, designed to be the final closed-ended academic
benchmark of its kind with broad subject coverage. HLE consists of 3,000
questions across dozens of subjects, including mathematics, humanities, and the
natural sciences. HLE is developed globally by subject-matter experts and
consists of multiple-choice and short-answer questions suitable for automated
grading. Each question has a known solution that is unambiguous and easily
verifiable, but cannot be quickly answered via internet retrieval.
State-of-the-art LLMs demonstrate low accuracy and calibration on HLE,
highlighting a significant gap between current LLM capabilities and the expert
human frontier on closed-ended academic questions. To inform research and
policymaking upon a clear understanding of model capabilities, we publicly
release HLE at https://lastexam.ai.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedAgentBench: A Realistic Virtual EHR Environment to Benchmark Medical
  LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixing Jiang, Kameron C. Black, Gloria Geng, Danny Park, James Zou, Andrew Y. Ng, Jonathan H. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) have demonstrated significant
advancements, particularly in their ability to serve as agents thereby
surpassing their traditional role as chatbots. These agents can leverage their
planning and tool utilization capabilities to address tasks specified at a high
level. However, a standardized dataset to benchmark the agent capabilities of
LLMs in medical applications is currently lacking, making the evaluation of
LLMs on complex tasks in interactive healthcare environments challenging. To
address this gap, we introduce MedAgentBench, a broad evaluation suite designed
to assess the agent capabilities of large language models within medical
records contexts. MedAgentBench encompasses 300 patient-specific
clinically-derived tasks from 10 categories written by human physicians,
realistic profiles of 100 patients with over 700,000 data elements, a
FHIR-compliant interactive environment, and an accompanying codebase. The
environment uses the standard APIs and communication infrastructure used in
modern EMR systems, so it can be easily migrated into live EMR systems.
MedAgentBench presents an unsaturated agent-oriented benchmark that current
state-of-the-art LLMs exhibit some ability to succeed at. The best model
(Claude 3.5 Sonnet v2) achieves a success rate of 69.67%. However, there is
still substantial space for improvement which gives the community a next
direction to optimize. Furthermore, there is significant variation in
performance across task categories. MedAgentBench establishes this and is
publicly available at https://github.com/stanfordmlgroup/MedAgentBench ,
offering a valuable framework for model developers to track progress and drive
continuous improvements in the agent capabilities of large language models
within the medical domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model
  Approach <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Zhang, Kaidi Xu, Ziqing Hu, Ren Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture of Experts (MoE) have shown remarkable success in leveraging
specialized expert networks for complex machine learning tasks. However, their
susceptibility to adversarial attacks presents a critical challenge for
deployment in robust applications. This paper addresses the critical question
of how to incorporate robustness into MoEs while maintaining high natural
accuracy. We begin by analyzing the vulnerability of MoE components, finding
that expert networks are notably more susceptible to adversarial attacks than
the router. Based on this insight, we propose a targeted robust training
technique that integrates a novel loss function to enhance the adversarial
robustness of MoE, requiring only the robustification of one additional expert
without compromising training or inference efficiency. Building on this, we
introduce a dual-model strategy that linearly combines a standard MoE model
with our robustified MoE model using a smoothing parameter. This approach
allows for flexible control over the robustness-accuracy trade-off. We further
provide theoretical foundations by deriving certified robustness bounds for
both the single MoE and the dual-model. To push the boundaries of robustness
and accuracy, we propose a novel joint training strategy JTDMoE for the
dual-model. This joint training enhances both robustness and accuracy beyond
what is achievable with separate models. Experimental results on CIFAR-10 and
TinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures
demonstrate the effectiveness of our proposed methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, submitted to ICML 2025 (under review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware
  Uncertainty Calibrator and Label Distribution Handler <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10784v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10784v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhun Kim, Taewon Kim, Seungyeon Woo, June Yong Yang, Eunho Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, tabular data often suffer from distribution shifts
that threaten the performance of machine learning models. Despite its
prevalence and importance, handling distribution shifts in the tabular domain
remains underexplored due to the inherent challenges within the tabular data
itself. In this sense, test-time adaptation (TTA) offers a promising solution
by adapting models to target data without accessing source data, crucial for
privacy-sensitive tabular domains. However, existing TTA methods either 1)
overlook the nature of tabular distribution shifts, often involving label
distribution shifts, or 2) impose architectural constraints on the model,
leading to a lack of applicability. To this end, we propose AdapTable, a novel
TTA framework for tabular data. AdapTable operates in two stages: 1)
calibrating model predictions using a shift-aware uncertainty calibrator, and
2) adjusting these predictions to match the target label distribution with a
label distribution handler. We validate the effectiveness of AdapTable through
theoretical analysis and extensive experiments on various distribution shift
scenarios. Our results demonstrate AdapTable's ability to handle various
real-world distribution shifts, achieving up to a 16% improvement on the HELOC
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS Workshop on Table Representation Learning (NeurIPSW-TRL),
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Calibration by Gaining Aware of Prediction Correctness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13016v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13016v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchi Liu, Lei Wang, Yuli Zou, James Zou, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model calibration aims to align confidence with prediction correctness. The
Cross-Entropy (CE) loss is widely used for calibrator training, which enforces
the model to increase confidence on the ground truth class. However, we find
the CE loss has intrinsic limitations. For example, for a narrow
misclassification (e.g., a test sample is wrongly classified and its softmax
score on the ground truth class is 0.4), a calibrator trained by the CE loss
often produces high confidence on the wrongly predicted class, which is
undesirable. In this paper, we propose a new post-hoc calibration objective
derived from the aim of calibration. Intuitively, the proposed objective
function asks that the calibrator decrease model confidence on wrongly
predicted samples and increase confidence on correctly predicted samples.
Because a sample itself has insufficient ability to indicate correctness, we
use its transformed versions (e.g., rotated, greyscaled, and color-jittered)
during calibrator training. Trained on an in-distribution validation set and
tested with isolated, individual test samples, our method achieves competitive
calibration performance on both in-distribution and out-of-distribution test
sets compared with the state of the art. Further, our analysis points out the
difference between our method and commonly used objectives such as CE loss and
Mean Square Error (MSE) loss, where the latters sometimes deviates from the
calibration aim.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Privacy Risk Measurement with Privacy Tokens for Gradient
  Leakage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02913v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02913v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Meng, Tao Huang, Hong Chen, Xin Shi, Qingyu Huang, Chen Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread deployment of deep learning models in privacy-sensitive
domains has amplified concerns regarding privacy risks, particularly those
stemming from gradient leakage during training. Current privacy assessments
primarily rely on post-training attack simulations. However, these methods are
inherently reactive, unable to encompass all potential attack scenarios, and
often based on idealized adversarial assumptions. These limitations underscore
the need for proactive approaches to privacy risk assessment during the
training process. To address this gap, we propose the concept of privacy
tokens, which are derived directly from private gradients during training.
Privacy tokens encapsulate gradient features and, when combined with data
features, offer valuable insights into the extent of private information
leakage from training data, enabling real-time measurement of privacy risks
without relying on adversarial attack simulations. Additionally, we employ
Mutual Information (MI) as a robust metric to quantify the relationship between
training data and gradients, providing precise and continuous assessments of
privacy leakage throughout the training process. Extensive experiments validate
our framework, demonstrating the effectiveness of privacy tokens and MI in
identifying and quantifying privacy risks. This proactive approach marks a
significant advancement in privacy monitoring, promoting the safer deployment
of deep learning models in sensitive applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Domain-adaptive Post-training for Financial LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Ke, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-adaptive post-training of large language models (LLMs) has emerged as
a promising approach for specialized domains such as medicine and finance.
However, significant challenges remain in identifying optimal adaptation
criteria and training strategies across varying data and model configurations.
To address these challenges, we introduce FINDAP, a systematic and fine-grained
investigation into domain adaptive post-training of LLMs for the finance
domain. Our approach consists of four key components: FinCap, which defines the
core capabilities required for the target domain; FinRec, an effective training
recipe that jointly optimizes continual pre-training and instruction-following,
along with a novel preference data distillation method leveraging process
signals from a generative reward model; FinTrain, a curated set of training
datasets supporting FinRec; and FinEval, a comprehensive evaluation suite
aligned with FinCap. The resulting model, Llama-Fin, achieves state-of-the-art
performance across a wide range of financial tasks. Our analysis also
highlights how each post-training stage contributes to distinct capabilities,
uncovering specific challenges and effective solutions, providing valuable
insights for domain adaptation of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start
  Cross-Domain Recommendation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15005v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15005v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hourun Li, Yifan Wang, Zhiping Xiao, Jia Yang, Changling Zhou, Ming Zhang, Wei Ju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used in various real-world applications, but
they often encounter the persistent challenge of the user cold-start problem.
Cross-domain recommendation (CDR), which leverages user interactions from one
domain to improve prediction performance in another, has emerged as a promising
solution. However, users with similar preferences in the source domain may
exhibit different interests in the target domain. Therefore, directly
transferring embeddings may introduce irrelevant source-domain collaborative
information. In this paper, we propose a novel graph-based disentangled
contrastive learning framework to capture fine-grained user intent and filter
out irrelevant collaborative information, thereby avoiding negative transfer.
Specifically, for each domain, we use a multi-channel graph encoder to capture
diverse user intents. We then construct the affinity graph in the embedding
space and perform multi-step random walks to capture high-order user similarity
relationships. Treating one domain as the target, we propose a disentangled
intent-wise contrastive learning approach, guided by user similarity, to refine
the bridging of user intents across domains. Extensive experiments on four
benchmark CDR datasets demonstrate that DisCo consistently outperforms existing
state-of-the-art baselines, thereby validating the effectiveness of both DisCo
and its components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Input Snapshots Fusion for Scalable Discrete-Time Dynamic Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        QingGuo Qi, Hongyang Chen, Minhao Cheng, Han Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a surge in research on dynamic graph
representation learning, primarily focusing on modeling the evolution of
temporal-spatial patterns in real-world applications. However, within the
domain of discrete-time dynamic graphs, the exploration of temporal edges
remains underexplored. Existing approaches often rely on additional sequential
models to capture dynamics, leading to high computational and memory costs,
particularly for large-scale graphs. To address this limitation, we propose the
Input {\bf S}napshots {\bf F}usion based {\bf Dy}namic {\bf G}raph Neural
Network (SFDyG), which combines Hawkes processes with graph neural networks to
capture temporal and structural patterns in dynamic graphs effectively. By
fusing multiple snapshots into a single temporal graph, SFDyG decouples
computational complexity from the number of snapshots, enabling efficient
full-batch and mini-batch training. Experimental evaluations on eight diverse
dynamic graph datasets for future link prediction tasks demonstrate that SFDyG
consistently outperforms existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large
  Language Models <span class="chip">ICLR'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02936v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02936v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Frank Yang, Hai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of pre-training data detection for large language models (LLMs)
has received growing attention due to its implications in critical issues like
copyright violation and test data contamination. Despite improved performance,
existing methods (including the state-of-the-art, Min-K%) are mostly developed
upon simple heuristics and lack solid, reasonable foundations. In this work, we
propose a novel and theoretically motivated methodology for pre-training data
detection, named Min-K%++. Specifically, we present a key insight that training
samples tend to be local maxima of the modeled distribution along each input
dimension through maximum likelihood training, which in turn allow us to
insightfully translate the problem into identification of local maxima. Then,
we design our method accordingly that works under the discrete distribution
modeled by LLMs, whose core idea is to determine whether the input forms a mode
or has relatively high probability under the conditional categorical
distribution. Empirically, the proposed method achieves new SOTA performance
across multiple settings. On the WikiMIA benchmark, Min-K%++ outperforms the
runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the
more challenging MIMIR benchmark, it consistently improves upon reference-free
methods while performing on par with reference-based method that requires an
extra reference model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR'25 Spotlight. Project page and code is available at
  https://zjysteven.github.io/mink-plus-plus/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks in EEG-based E<span class="highlight-title">motion</span> Recognition: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01138v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01138v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyu Liu, Xinliang Zhou, Yihao Wu, Ruizhi Yang, Zhongruo Wang, Liming Zhai, Ziyu Jia, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to other modalities, EEG-based emotion recognition can intuitively
respond to the emotional patterns in the human brain and, therefore, has become
one of the most concerning tasks in the brain-computer interfaces field. Since
dependencies within brain regions are closely related to emotion, a significant
trend is to develop Graph Neural Networks (GNNs) for EEG-based emotion
recognition. However, brain region dependencies in emotional EEG have
physiological bases that distinguish GNNs in this field from those in other
time series fields. Besides, there is neither a comprehensive review nor
guidance for constructing GNNs in EEG-based emotion recognition. In the survey,
our categorization reveals the commonalities and differences of existing
approaches under a unified framework of graph construction. We analyze and
categorize methods from three stages in the framework to provide clear guidance
on constructing GNNs in EEG-based emotion recognition. In addition, we discuss
several open challenges and future directions, such as Temporal full-connected
graph and Graph condensation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Music for All: Exploring Multicultural Representations in Music
  Generation Models <span class="chip">NAACL'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atharva Mehta, Shivam Chauhan, Amirbek Djanibekov, Atharva Kulkarni, Gus Xia, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Music-Language Models has greatly enhanced the automatic music
generation capability of AI systems, but they are also limited in their
coverage of the musical genres and cultures of the world. We present a study of
the datasets and research papers for music generation and quantify the bias and
under-representation of genres. We find that only 5.7% of the total hours of
existing music datasets come from non-Western genres, which naturally leads to
disparate performance of the models across genres. We then investigate the
efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating
this bias. Our experiments with two popular models -- MusicGen and Mustango,
for two underrepresented non-Western music traditions -- Hindustani Classical
and Turkish Makam music, highlight the promises as well as the non-triviality
of cross-genre adaptation of music through small datasets, implying the need
for more equitable baseline music-language models that are designed for
cross-cultural transfer learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures, accepted to NAACL'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiecheng Lu, Xu Han, Yan Sun, Shihao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a Weighted Autoregressive Varying gatE (WAVE) attention mechanism
equipped with both Autoregressive (AR) and Moving-average (MA) components. It
can adapt to various attention mechanisms, enhancing and decoupling their
ability to capture long-range and local temporal patterns in time series data.
In this paper, we first demonstrate that, for the time series forecasting (TSF)
task, the previously overlooked decoder-only autoregressive Transformer model
can achieve results comparable to the best baselines when appropriate
tokenization and training methods are applied. Moreover, inspired by the ARMA
model from statistics and recent advances in linear attention, we introduce the
full ARMA structure into existing autoregressive attention mechanisms. By using
an indirect MA weight generation method, we incorporate the MA term while
maintaining the time complexity and parameter size of the underlying efficient
attention models. We further explore how indirect parameter generation can
produce implicit MA weights that align with the modeling requirements for local
temporal impacts. Experimental results show that WAVE attention that
incorporates the ARMA structure consistently improves the performance of
various AR attentions on TSF tasks, achieving state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-aware Reward Model: Teaching Reward Models to Know What is
  Unknown 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie, Junge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) are essential for aligning large language models (LLM)
with human expectations. However, existing RMs struggle to capture the
stochastic and uncertain nature of human preferences and fail to assess the
reliability of reward predictions. To address these challenges, we introduce
the Uncertainty-aware Reward Model (URM) and its ensemble variant, URME. URM
employs a probabilistic value head to capture aleatoric uncertainty by modeling
the distribution of disentangled human preference attributes. URME further
quantifies epistemic uncertainty by examining discrepancies among individual
URMs within the ensemble, enabling identification of unreliable evaluations.
Our empirical evaluations demonstrate that URM achieves strong performance on
RewardBench, outperforming competitive large-scale models. Additionally,
extensive experiments, including best-of-n sampling (BoN), iterative direct
preference optimization (iterative DPO), and proximal policy optimization
(PPO), demonstrate that URM and URME significantly enhance LLMs' generation
quality. Notably, reward predictions with lower uncertainty are far more
reliable, demonstrate significantly higher quality, and result in substantially
improved alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Experts Meets Decoupled Message Passing: Towards General and
  Adaptive Node Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanze Chen, Jiajun Zhou, Shanqing Yu, Qi Xuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks excel at graph representation learning but struggle
with heterophilous data and long-range dependencies. And graph transformers
address these issues through self-attention, yet face scalability and noise
challenges on large-scale graphs. To overcome these limitations, we propose
GNNMoE, a universal model architecture for node classification. This
architecture flexibly combines fine-grained message-passing operations with a
mixture-of-experts mechanism to build feature encoding blocks. Furthermore, by
incorporating soft and hard gating layers to assign the most suitable expert
networks to each node, we enhance the model's expressive power and adaptability
to different graph types. In addition, we introduce adaptive residual
connections and an enhanced FFN module into GNNMoE, further improving the
expressiveness of node representation. Extensive experimental results
demonstrate that GNNMoE performs exceptionally well across various types of
graph data, effectively alleviating the over-smoothing issue and global noise,
enhancing model robustness and adaptability, while also ensuring computational
efficiency on large-scale graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Web Conference 2025 as a short paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the query complexity of sampling from non-log-concave distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen He, Chihao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of sampling from a $d$-dimensional distribution with
density $p(x)\propto e^{-f(x)}$, which does not necessarily satisfy good
isoperimetric conditions.
  Specifically, we show that for any $L,M$ satisfying $LM\ge d\ge 5$,
$\epsilon\in \left(0,\frac{1}{32}\right)$, and any algorithm with query
accesses to the value of $f(x)$ and $\nabla f(x)$, there exists an
$L$-log-smooth distribution with second moment at most $M$ such that the
algorithm requires $\left(\frac{LM}{d\epsilon}\right)^{\Omega(d)}$ queries to
compute a sample whose distribution is within $\epsilon$ in total variation
distance to the target distribution. We complement the lower bound with an
algorithm requiring $\left(\frac{LM}{d\epsilon}\right)^{\mathcal O(d)}$
queries, thereby characterizing the tight (up to the constant in the exponent)
query complexity for sampling from the family of non-log-concave distributions.
  Our results are in sharp contrast with the recent work of Huang et al.
(COLT'24), where an algorithm with quasi-polynomial query complexity was
proposed for sampling from a non-log-concave distribution when
$M=\mathtt{poly}(d)$. Their algorithm works under the stronger condition that
all distributions along the trajectory of the Ornstein-Uhlenbeck process,
starting from the target distribution, are $\mathcal O(1)$-log-smooth. We
investigate this condition and prove that it is strictly stronger than
requiring the target distribution to be $\mathcal O(1)$-log-smooth.
Additionally, we study this condition in the context of mixtures of Gaussians.
  Finally, we place our results within the broader theme of ``sampling versus
optimization'', as studied in Ma et al. (PNAS'19). We show that for a wide
range of parameters, sampling is strictly easier than optimization by a
super-exponential factor in the dimension $d$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Policy Evaluation Uncovers Policies of Generative Flow Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran He, Emmanuel Bengio, Qingpeng Cai, Ling Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Generative Flow Network (GFlowNet) is a probabilistic framework in which
an agent learns a stochastic policy and flow functions to sample objects with
probability proportional to an unnormalized reward function. GFlowNets share a
strong connection with reinforcement learning (RL) that typically aims to
maximize reward. A number of recent works explored connections between
GFlowNets and maximum entropy (MaxEnt) RL, which incorporates entropy
regularization into the standard RL objective. However, the relationship
between GFlowNets and standard RL remains largely unexplored, despite the
inherent similarities in their sequential decision-making nature. While
GFlowNets can discover diverse solutions through specialized flow-matching
objectives, connecting them to standard RL can simplify their implementation
through well-established RL principles and also improve RL's capabilities in
diverse solution discovery (a critical requirement in many real-world
applications), and bridging this gap can further unlock the potential of both
fields. In this paper, we bridge this gap by revealing a fundamental connection
between GFlowNets and one of the most basic components of RL -- policy
evaluation. Surprisingly, we find that the value function obtained from
evaluating a uniform policy is closely associated with the flow functions in
GFlowNets. Building upon these insights, we introduce a rectified random policy
evaluation (RPE) algorithm, which achieves the same reward-matching effect as
GFlowNets based on simply evaluating a fixed random policy, offering a new
perspective. Empirical results across extensive benchmarks demonstrate that RPE
achieves competitive results compared to previous approaches, shedding light on
the previously overlooked connection between (non-MaxEnt) RL and GFlowNets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Prompting: Time2Lang -- Bridging Time-Series Foundation Models
  and Large Language Models for Health Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arvind Pillai, Dimitris Spathis, Subigya Nepal, Amanda C Collins, Daniel M Mackin, Michael V Heinz, Tess Z Griffin, Nicholas C Jacobson, Andrew Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show promise for health applications when
combined with behavioral sensing data. Traditional approaches convert sensor
data into text prompts, but this process is prone to errors, computationally
expensive, and requires domain expertise. These challenges are particularly
acute when processing extended time series data. While time series foundation
models (TFMs) have recently emerged as powerful tools for learning
representations from temporal data, bridging TFMs and LLMs remains challenging.
Here, we present Time2Lang, a framework that directly maps TFM outputs to LLM
representations without intermediate text conversion. Our approach first trains
on synthetic data using periodicity prediction as a pretext task, followed by
evaluation on mental health classification tasks. We validate Time2Lang on two
longitudinal wearable and mobile sensing datasets: daily depression prediction
using step count data (17,251 days from 256 participants) and flourishing
classification based on conversation duration (46 participants over 10 weeks).
Time2Lang maintains near constant inference times regardless of input length,
unlike traditional prompting methods. The generated embeddings preserve
essential time-series characteristics such as auto-correlation. Our results
demonstrate that TFMs and LLMs can be effectively integrated while minimizing
information loss and enabling performance transfer across these distinct
modeling paradigms. To our knowledge, we are the first to integrate a TFM and
an LLM for health, thus establishing a foundation for future research combining
general-purpose large models for complex healthcare tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flat Posterior Does Matter For Bayesian Model Averaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15664v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15664v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungjun Lim, Jeyoon Yeom, Sooyon Kim, Hoyoon Byun, Jinho Kang, Yohan Jung, Jiyoung Jung, Kyungwoo Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian neural networks (BNNs) estimate the posterior distribution of model
parameters and utilize posterior samples for Bayesian Model Aver- aging (BMA)
in prediction. However, despite the crucial role of flatness in the loss
landscape in improving the generalization of neural networks, its impact on BMA
has been largely overlooked. In this work, we explore how posterior flatness
influences BMA generalization and empirically demonstrate that (1) most
approximate Bayesian inference methods fail to yield a flat posterior and (2)
BMA predictions, without considering posterior flatness, are less effective at
improving generalization. To address this, we propose Flat Posterior-aware
Bayesian Model Averaging (FP-BMA), a novel training objective that explicitly
encourages flat posteriors in a principled Bayesian manner. We also introduce a
Flat Posterior-aware Bayesian Transfer Learning scheme that enhances
generalization in downstream tasks. Empirically, we show that FP-BMA
successfully captures flat posteriors, improving generalization performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Watermarking Language Models with Error Correcting Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Chao, Yan Sun, Edgar Dobriban, Hamed Hassani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large language models enables the creation of realistic
machine-generated content. Watermarking is a promising approach to distinguish
machine-generated text from human text, embedding statistical signals in the
output that are ideally undetectable to humans. We propose a watermarking
framework that encodes such signals through an error correcting code. Our
method, termed robust binary code (RBC) watermark, introduces no distortion
compared to the original probability distribution, and no noticeable
degradation in quality. We evaluate our watermark on base and instruction
fine-tuned models and find our watermark is robust to edits, deletions, and
translations. We provide an information-theoretic perspective on watermarking,
a powerful statistical test for detection and for generating p-values, and
theoretical guarantees. Our empirical findings suggest our watermark is fast,
powerful, and robust, comparing favorably to the state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Hu, Weichen Yu, Yining Li, Kai Chen, Tianjun Yao, Xiang Li, Wenhe Liu, Lijun Yu, Zhiqiang Shen, Matt Fredrikson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research indicates that large language models (LLMs) are susceptible
to jailbreaking attacks that can generate harmful content. This paper
introduces a novel token-level attack method, Adaptive Dense-to-Sparse
Constrained Optimization (ADC), which has been shown to successfully jailbreak
multiple open-source LLMs. Drawing inspiration from the difficulties of
discrete token optimization, our method relaxes the discrete jailbreak
optimization into a continuous optimization process while gradually increasing
the sparsity of the optimizing vectors. This technique effectively bridges the
gap between discrete and continuous space optimization. Experimental results
demonstrate that our method is more effective and efficient than
state-of-the-art token-level methods. On Harmbench, our approach achieves the
highest attack success rate on seven out of eight LLMs compared to the latest
jailbreak methods. Trigger Warning: This paper contains model behavior that can
be offensive in nature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Algorithmic Fairness for <span class="highlight-title">Human</span>-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haosen Ge, Hamsa Bastani, Osbert Bastani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing approaches to algorithmic fairness aim to ensure equitable outcomes
if human decision-makers comply perfectly with algorithmic decisions. However,
perfect compliance with the algorithm is rarely a reality or even a desirable
outcome in human-AI collaboration. Yet, recent studies have shown that
selective compliance with fair algorithms can amplify discrimination relative
to the prior human policy. As a consequence, ensuring equitable outcomes
requires fundamentally different algorithmic design principles that ensure
robustness to the decision-maker's (a priori unknown) compliance pattern. We
define the notion of compliance-robustly fair algorithmic recommendations that
are guaranteed to (weakly) improve fairness in decisions, regardless of the
human's compliance pattern. We propose a simple optimization strategy to
identify the best performance-improving compliance-robustly fair policy.
However, we show that it may be infeasible to design algorithmic
recommendations that are simultaneously fair in isolation, compliance-robustly
fair, and more accurate than the human policy; thus, if our goal is to improve
the equity and accuracy of human-AI collaboration, it may not be desirable to
enforce traditional algorithmic fairness constraints. We illustrate the value
of our approach on criminal sentencing data before and after the introduction
of an algorithmic risk assessment tool in Virginia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DrivAerNet: A Parametric Car <span class="highlight-title">Dataset</span> for Data-Driven Aerodynamic Design
  and Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elrefaie, Angela Dai, Faez Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of
3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional
neural network model, both aimed at aerodynamic car design through machine
learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million
surface mesh faces and comprehensive aerodynamic performance data comprising of
full 3D pressure, velocity fields, and wall-shear stresses, addresses the
critical need for extensive datasets to train deep learning models in
engineering applications. It is 60\% larger than the previously available
largest public dataset of cars, and is the only open-source dataset that also
models wheels and underbody. RegDGCNN leverages this large-scale dataset to
provide high-precision drag estimates directly from 3D meshes, bypassing
traditional limitations such as the need for 2D image rendering or Signed
Distance Fields (SDF). By enabling fast drag estimation in seconds, RegDGCNN
facilitates rapid aerodynamic assessments, offering a substantial leap towards
integrating data-driven methods in automotive design. Together, DrivAerNet and
RegDGCNN promise to accelerate the car design process and contribute to the
development of more efficient cars. To lay the groundwork for future
innovations in the field, the dataset and code used in our study are publicly
accessible at https://github.com/Mohamedelrefaie/DrivAerNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Performance Analysis of Momentum Method: A Frequency Domain
  Perspective <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianliang Li, Jun Luo, Zhiwei Zheng, Hanxiao Wang, Li Luo, Lingkun Wen, Linlong Wu, Sheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Momentum-based optimizers are widely adopted for training neural networks.
However, the optimal selection of momentum coefficients remains elusive. This
uncertainty impedes a clear understanding of the role of momentum in stochastic
gradient methods. In this paper, we present a frequency domain analysis
framework that interprets the momentum method as a time-variant filter for
gradients, where adjustments to momentum coefficients modify the filter
characteristics. Our experiments support this perspective and provide a deeper
understanding of the mechanism involved. Moreover, our analysis reveals the
following significant findings: high-frequency gradient components are
undesired in the late stages of training; preserving the original gradient in
the early stages, and gradually amplifying low-frequency gradient components
during training both enhance performance. Based on these insights, we propose
Frequency Stochastic Gradient Descent with Momentum (FSGDM), a heuristic
optimizer that dynamically adjusts the momentum filtering characteristic with
an empirically effective dynamic magnitude response. Experimental results
demonstrate the superiority of FSGDM over conventional momentum optimizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand
  Graphs Better 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14427v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14427v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Chu, Hanlin Xue, Zhijie Tan, Bingce Wang, Tong Mo, Weiping Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Large Language Models (LLMs) in various domains has led
researchers to apply them to graph-related problems by converting graph data
into natural language text. However, unlike graph data, natural language
inherently has sequential order. We observe a counter-intuitive fact that when
the order of nodes or edges in the natural language description of a graph is
shuffled, despite describing the same graph, model performance fluctuates
between high performance and random guessing. Additionally, due to LLMs'
limited input context length, current methods typically randomly sample
neighbors of target nodes as representatives of their neighborhood, which may
not always be effective for accurate reasoning. To address these gaps, we
introduce GraphSOS (Graph Sampling and Order Selection). This novel model
framework features an Order Selector Module to ensure proper serialization
order of the graph and a Subgraph Sampling Module to sample subgraphs with
better structure for better reasoning. Furthermore, we propose Graph CoT
obtained through distillation, and enhance LLM's reasoning and zero-shot
learning capabilities for graph tasks through instruction tuning. Experiments
on multiple datasets for node classification and graph question-answering
demonstrate that GraphSOS improves LLMs' performance and generalization ability
on graph tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STRIDE: Automating Reward Design, Deep Reinforcement Learning Training
  and Feedback Optimization in <span class="highlight-title">Human</span>oid Robotics Loco<span class="highlight-title">motion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04692v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04692v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenwei Wu, Jinxiong Lu, Yuxiao Chen, Yunxin Liu, Yueting Zhuang, Luhui Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robotics presents significant challenges in artificial intelligence,
requiring precise coordination and control of high-degree-of-freedom systems.
Designing effective reward functions for deep reinforcement learning (DRL) in
this domain remains a critical bottleneck, demanding extensive manual effort,
domain expertise, and iterative refinement. To overcome these challenges, we
introduce STRIDE, a novel framework built on agentic engineering to automate
reward design, DRL training, and feedback optimization for humanoid robot
locomotion tasks. By combining the structured principles of agentic engineering
with large language models (LLMs) for code-writing, zero-shot generation, and
in-context optimization, STRIDE generates, evaluates, and iteratively refines
reward functions without relying on task-specific prompts or templates. Across
diverse environments featuring humanoid robot morphologies, STRIDE outperforms
the state-of-the-art reward design framework EUREKA, achieving an average
improvement of round 250% in efficiency and task performance. Using
STRIDE-generated rewards, simulated humanoid robots achieve sprint-level
locomotion across complex terrains, highlighting its ability to advance DRL
workflows and humanoid robotics research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trivialized Momentum Facilitates Diffusion Generative Modeling on Lie
  Groups <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Zhu, Tianrong Chen, Lingkai Kong, Evangelos A. Theodorou, Molei Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generative modeling of data on manifolds is an important task, for which
diffusion models in flat spaces typically need nontrivial adaptations. This
article demonstrates how a technique called `trivialization' can transfer the
effectiveness of diffusion models in Euclidean spaces to Lie groups. In
particular, an auxiliary momentum variable was algorithmically introduced to
help transport the position variable between data distribution and a fixed,
easy-to-sample distribution. Normally, this would incur further difficulty for
manifold data because momentum lives in a space that changes with the position.
However, our trivialization technique creates a new momentum variable that
stays in a simple fixed vector space. This design, together with a manifold
preserving integrator, simplifies implementation and avoids inaccuracies
created by approximations such as projections to tangent space and manifold,
which were typically used in prior work, hence facilitating generation with
high-fidelity and efficiency. The resulting method achieves state-of-the-art
performance on protein and RNA torsion angle generation and sophisticated torus
datasets. We also, arguably for the first time, tackle the generation of data
on high-dimensional Special Orthogonal and Unitary groups, the latter essential
for quantum problems. Code is available at
https://github.com/yuchen-zhu-zyc/TDM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offline to Online Learning for Real-Time Bandwidth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13481v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13481v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aashish Gottipati, Sami Khairy, Gabriel Mittag, Vishak Gopal, Ross Cutler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time video applications require accurate bandwidth estimation (BWE) to
maintain user experience across varying network conditions. However, increasing
network heterogeneity challenges general-purpose BWE algorithms, necessitating
solutions that adapt to end-user environments. While widely adopted,
heuristic-based methods are difficult to individualize without extensive domain
expertise. Conversely, online reinforcement learning (RL) offers ease of
customization but neglects prior domain expertise and suffers from sample
inefficiency. Thus, we present Merlin, an imitation learning-based solution
that replaces the manual parameter tuning of heuristic-based methods with
data-driven updates to streamline end-user personalization. Our key insight is
that transforming heuristic-based BWE algorithms into neural networks
facilitates data-driven personalization. Merlin utilizes Behavioral Cloning to
efficiently learn from offline telemetry logs, capturing heuristic policies
without live network interactions. The cloned policy can then be seamlessly
tailored to end user network conditions through online finetuning. In real
intercontinental videoconferencing calls, Merlin matches our heuristic's policy
with no statistically significant differences in user quality of experience
(QoE). Finetuning Merlin's control policy to end-user environments enables QoE
improvements of up to 7.8% compared to the heuristic policy. Lastly, our
IL-based design performs competitively with current state-of-the-art online RL
techniques but converges with 80% fewer videoconferencing samples, facilitating
practical end-user personalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, under review. Updated content, added finetuning evaluations,
  updated title, added IEEE copyright</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Step Time Series Inference Agent for Reasoning and Automated Task
  Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04047v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04047v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Ye, Yizhou Zhang, Wei Yang, Defu Cao, Lumingyuan Tang, Jie Cai, Yan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series analysis is crucial in real-world applications, yet traditional
methods focus on isolated tasks only, and recent studies on time series
reasoning remain limited to simple, single-step inference constrained to
natural language answer. In this work, we propose a practical novel task:
multi-step time series inference that demands both compositional reasoning and
computation precision of time series analysis. To address such challenge, we
propose a simple but effective program-aided inference agent that leverages
LLMs' reasoning ability to decompose complex tasks into structured execution
pipelines. By integrating in-context learning, self-correction, and
program-aided execution, our proposed approach ensures accurate and
interpretable results. To benchmark performance, we introduce a new dataset and
a unified evaluation framework with task-specific success criteria. Experiments
show that our approach outperforms standalone general purpose LLMs in both
basic time series concept understanding as well as multi-step time series
inference task, highlighting the importance of hybrid approaches that combine
reasoning with computational precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Learning With Sine-Activated Low-rank Matrices <span class="chip">ICLR
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank decomposition has emerged as a vital tool for enhancing parameter
efficiency in neural network architectures, gaining traction across diverse
applications in machine learning. These techniques significantly lower the
number of parameters, striking a balance between compactness and performance.
However, a common challenge has been the compromise between parameter
efficiency and the accuracy of the model, where reduced parameters often lead
to diminished accuracy compared to their full-rank counterparts. In this work,
we propose a novel theoretical framework that integrates a sinusoidal function
within the low-rank decomposition process. This approach not only preserves the
benefits of the parameter efficiency characteristic of low-rank methods but
also increases the decomposition's rank, thereby enhancing model performance.
Our method proves to be a plug in enhancement for existing low-rank models, as
evidenced by its successful application in Vision Transformers (ViT), Large
Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Paper accepted at ICLR
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Causal Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05935v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05935v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulun Wu, Layne C. Price, Zichen Wang, Vassilis N. Ioannidis, Robert A. Barton, George Karypis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating an individual's potential outcomes under counterfactual treatments
is a challenging task for traditional causal inference and supervised learning
approaches when the outcome is high-dimensional (e.g. gene expressions, impulse
responses, human faces) and covariates are relatively limited. In this case, to
construct one's outcome under a counterfactual treatment, it is crucial to
leverage individual information contained in its observed factual outcome on
top of the covariates. We propose a deep variational Bayesian framework that
rigorously integrates two main sources of information for outcome construction
under a counterfactual treatment: one source is the individual features
embedded in the high-dimensional factual outcome; the other source is the
response distribution of similar subjects (subjects with the same covariates)
that factually received this treatment of interest.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Human</span>-Centric Foundation Models: Perception, Generation and Agentic
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shixiang Tang, Yizhou Wang, Lu Chen, Yuan Wang, Sida Peng, Dan Xu, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human understanding and generation are critical for modeling digital humans
and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs)
inspired by the success of generalist models, such as large language and vision
models, have emerged to unify diverse human-centric tasks into a single
framework, surpassing traditional task-specific approaches. In this survey, we
present a comprehensive overview of HcFMs by proposing a taxonomy that
categorizes current approaches into four groups: (1) Human-centric Perception
Foundation Models that capture fine-grained features for multi-modal 2D and 3D
understanding. (2) Human-centric AIGC Foundation Models that generate
high-fidelity, diverse human-related content. (3) Unified Perception and
Generation Models that integrate these capabilities to enhance both human
understanding and synthesis. (4) Human-centric Agentic Foundation Models that
extend beyond perception and generation to learn human-like intelligence and
interactive behaviors for humanoid embodied tasks. We review state-of-the-art
techniques, discuss emerging challenges and future research directions. This
survey aims to serve as a roadmap for researchers and practitioners working
towards more robust, versatile, and intelligent digital human and embodiments
modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "You'll Be Alice Adventuring in Wonderland!" Processes, Challenges, and
  Opportunities of Creating Animated Virtual Reality Stories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin-Ping Yuan, Feilin Han, Liwenhan Xie, Junjie Zhang, Jian Zhao, Huamin Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animated virtual reality (VR) stories, combining the presence of VR and the
artistry of computer animation, offer a compelling way to deliver messages and
evoke emotions. Motivated by the growing demand for immersive narrative
experiences, more creators are creating animated VR stories. However, a
holistic understanding of their creation processes and challenges involved in
crafting these stories is still limited. Based on semi-structured interviews
with 21 animated VR story creators, we identify ten common stages in their
end-to-end creation processes, ranging from idea generation to evaluation,
which form diverse workflows that are story-driven or visual-driven.
Additionally, we highlight nine unique issues that arise during the creation
process, such as a lack of reference material for multi-element plots, the
absence of specific functionalities for story integration, and inadequate
support for audience evaluation. We compare the creation of animated VR stories
to general XR applications and distill several future research opportunities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conditionally accepted to the ACM Conference on Human Factors in
  Computing Systems (CHI'25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composite Sketch+Text Queries for Retrieving Objects with Elusive Names
  and Complex Interactions <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajwal Gatti, Kshitij Parikh, Dhriti Prasanna Paul, Manish Gupta, Anand Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-native speakers with limited vocabulary often struggle to name specific
objects despite being able to visualize them, e.g., people outside Australia
searching for numbats. Further, users may want to search for such elusive
objects with difficult-to-sketch interactions, e.g., numbat digging in the
ground. In such common but complex situations, users desire a search interface
that accepts composite multimodal queries comprising hand-drawn sketches of
difficult-to-name but easy-to-draw objects and text describing
difficult-to-sketch but easy-to-verbalize object attributes or interaction with
the scene. This novel problem statement distinctly differs from the previously
well-researched TBIR (text-based image retrieval) and SBIR (sketch-based image
retrieval) problems. To study this under-explored task, we curate a dataset,
CSTBIR (Composite Sketch+Text Based Image Retrieval), consisting of approx. 2M
queries and 108K natural scene images. Further, as a solution to this problem,
we propose a pretrained multimodal transformer-based baseline, STNET
(Sketch+Text Network), that uses a hand-drawn sketch to localize relevant
objects in the natural scene image, and encodes the text and image to perform
image retrieval. In addition to contrastive learning, we propose multiple
training objectives that improve the performance of our model. Extensive
experiments show that our proposed method outperforms several state-of-the-art
retrieval methods for text-only, sketch-only, and composite query modalities.
We make the dataset and code available at our project website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2024, 9 pages. Project Website:
  https://vl2g.github.io/projects/cstbir</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COutfitGAN: Learning to Synthesize Compatible Outfits Supervised by
  Silhouette Masks and Fashion Styles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Zhou, Haijun Zhang, Qun Li, Jianghong Ma, Xiaofei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How to recommend outfits has gained considerable attention in both academia
and industry in recent years. Many studies have been carried out regarding
fashion compatibility learning, to determine whether the fashion items in an
outfit are compatible or not. These methods mainly focus on evaluating the
compatibility of existing outfits and rarely consider applying such knowledge
to 'design' new fashion items. We propose the new task of generating
complementary and compatible fashion items based on an arbitrary number of
given fashion items. In particular, given some fashion items that can make up
an outfit, the aim of this paper is to synthesize photo-realistic images of
other, complementary, fashion items that are compatible with the given ones. To
achieve this, we propose an outfit generation framework, referred to as
COutfitGAN, which includes a pyramid style extractor, an outfit generator, a
UNet-based real/fake discriminator, and a collocation discriminator. To train
and evaluate this framework, we collected a large-scale fashion outfit dataset
with over 200K outfits and 800K fashion items from the Internet. Extensive
experiments show that COutfitGAN outperforms other baselines in terms of
similarity, authenticity, and compatibility measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by IEEE TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Learned Image Compression via Cross Window-based Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21144v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21144v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Mudgal, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, learned image compression methods have demonstrated superior
rate-distortion performance compared to traditional image compression methods.
Recent methods utilize convolutional neural networks (CNN), variational
autoencoders (VAE), invertible neural networks (INN), and transformers. Despite
their significant contributions, a main drawback of these models is their poor
performance in capturing local redundancy. Therefore, to leverage global
features along with local redundancy, we propose a CNN-based solution
integrated with a feature encoding module. The feature encoding module encodes
important features before feeding them to the CNN and then utilizes cross-scale
window-based attention, which further captures local redundancy. Cross-scale
window-based attention is inspired by the attention mechanism in transformers
and effectively enlarges the receptive field. Both the feature encoding module
and the cross-scale window-based attention module in our architecture are
flexible and can be incorporated into any other network architecture. We
evaluate our method on the Kodak and CLIC datasets and demonstrate that our
approach is effective and on par with state-of-the-art methods. Our code is
publicly available at https://github.com/prmudgal/CWAM_IC_ISVC. .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted and presented in ISVC'24. Copyrights stay with ISVC
  Our code is available at: https://github.com/prmudgal/CWAM_IC_ISVC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive
  Modality Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models, particularly following GPT-4o, have
sparked increasing interest in developing omni-modal models capable of
understanding more modalities. While some open-source alternatives have
emerged, there is still a notable lag behind specialized single-modality models
in performance. In this paper, we present Ola, an Omni-modal language model
that achieves competitive performance across image, video, and audio
understanding compared to specialized counterparts. The core design of Ola lies
in its progressive modality alignment strategy that extends the supporting
modality of the language model progressively. Our training pipeline begins with
the most distinct modalities: image and text, then gradually expands the skill
sets of the model using speech data that connects language and audio knowledge,
and video data that connects all modalities. The progressive learning pipeline
also enables us to maintain a relatively small size of the cross-modal
alignment data, making developing omni-modal from existing vision-language
models easy and less costly. Moreover, to unlock an advanced interactive
experience like GPT-4o, we further design a sentence-wise decoding solution for
streaming speech generation. Extensive experiments demonstrate that Ola
surpasses existing open omni-modal LLMs across all modalities while achieving
highly competitive performance compared to state-of-the-art specialized models
of similar sizes. We aim to make Ola a fully open omni-modal understanding
solution to advance future research in this emerging field. Model weights,
code, and data are open-sourced at https://github.com/Ola-Omni/Ola.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeSuite: Improving MLLMs for Long <span class="highlight-title">Video</span> Understanding via Grounded
  Tuning <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated impressive
performance in short video understanding. However, understanding long-form
videos still remains challenging for MLLMs. This paper proposes TimeSuite, a
collection of new designs to adapt the existing short-form video MLLMs for long
video understanding, including a simple yet efficient framework to process long
video sequence, a high-quality video dataset for grounded tuning of MLLMs, and
a carefully-designed instruction tuning task to explicitly incorporate the
grounding supervision in the traditional QA format. Specifically, based on
VideoChat, we propose our long-video MLLM, coined as VideoChat-T, by
implementing a token shuffling to compress long video tokens and introducing
Temporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of
visual representation. Meanwhile, we introduce the TimePro, a comprehensive
grounding-centric instruction tuning dataset composed of 9 tasks and 349k
high-quality grounded annotations. Notably, we design a new instruction tuning
task type, called Temporal Grounded Caption, to peform detailed video
descriptions with the corresponding time stamps prediction. This explicit
temporal location prediction will guide MLLM to correctly attend on the visual
content when generating description, and thus reduce the hallucination risk
caused by the LLMs. Experimental results demonstrate that our TimeSuite
provides a successful solution to enhance the long video understanding
capability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the
benchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T
exhibits robust zero-shot temporal grounding capabilities, significantly
outperforming the existing state-of-the-art MLLMs. After fine-tuning, it
performs on par with the traditional supervised expert models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Wu, Zhaoxi Ke, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, mixture of experts (MoE) has become a popular paradigm for
achieving the trade-off between modal capacity and efficiency of multi-modal
large language models (MLLMs). Different from previous efforts, we are
dedicated to exploring the dynamic expert path in an already exist MLLM and
show that a standard MLLM can be also a mixture of experts. To approach this
target, we propose a novel dynamic expert scheme for MLLMs, termed Routing
Experts (RoE), which can achieve example-dependent optimal path routing without
obvious structure tweaks. Meanwhile, a new regularization of structure sparsity
is also introduced to enforce MLLMs to learn more short-cut inference, ensuring
the efficiency. In addition, we also realize the first attempt of aligning the
training and inference schemes of MLLMs in terms of network routing. To
validate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,
LLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL
benchmarks. The experiment results not only show the great advantages of our
RoE in improving MLLMs' efficiency, but also yield obvious advantages than
MoE-LLaVA in both performance and speed, e.g., an average performance gain of
3.3% on 5 benchmarks while being faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VidCRAFT3: Camera, Object, and Lighting <span class="highlight-title">Control</span> for Image-to-<span class="highlight-title">Video</span>
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixiao Zheng, Zimian Peng, Yanpeng Zhou, Yi Zhu, Hang Xu, Xiangru Huang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent image-to-video generation methods have demonstrated success in
enabling control over one or two visual elements, such as camera trajectory or
object motion. However, these methods are unable to offer control over multiple
visual elements due to limitations in data and network efficacy. In this paper,
we introduce VidCRAFT3, a novel framework for precise image-to-video generation
that enables control over camera motion, object motion, and lighting direction
simultaneously. To better decouple control over each visual element, we propose
the Spatial Triple-Attention Transformer, which integrates lighting direction,
text, and image in a symmetric way. Since most real-world video datasets lack
lighting annotations, we construct a high-quality synthetic video dataset, the
VideoLightingDirection (VLD) dataset. This dataset includes lighting direction
annotations and objects of diverse appearance, enabling VidCRAFT3 to
effectively handle strong light transmission and reflection effects.
Additionally, we propose a three-stage training strategy that eliminates the
need for training data annotated with multiple visual elements (camera motion,
object motion, and lighting direction) simultaneously. Extensive experiments on
benchmark datasets demonstrate the efficacy of VidCRAFT3 in producing
high-quality video content, surpassing existing state-of-the-art methods in
terms of control granularity and visual coherence. All code and data will be
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Music for All: Exploring Multicultural Representations in Music
  Generation Models <span class="chip">NAACL'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atharva Mehta, Shivam Chauhan, Amirbek Djanibekov, Atharva Kulkarni, Gus Xia, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Music-Language Models has greatly enhanced the automatic music
generation capability of AI systems, but they are also limited in their
coverage of the musical genres and cultures of the world. We present a study of
the datasets and research papers for music generation and quantify the bias and
under-representation of genres. We find that only 5.7% of the total hours of
existing music datasets come from non-Western genres, which naturally leads to
disparate performance of the models across genres. We then investigate the
efficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating
this bias. Our experiments with two popular models -- MusicGen and Mustango,
for two underrepresented non-Western music traditions -- Hindustani Classical
and Turkish Makam music, highlight the promises as well as the non-triviality
of cross-genre adaptation of music through small datasets, implying the need
for more equitable baseline music-language models that are designed for
cross-cultural transfer learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures, accepted to NAACL'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-11T00:00:00Z">2025-02-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">311</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Initialization Matters: Unraveling the Impact of Pre-Training on
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyansh Jhunjhunwala, Pranay Sharma, Zheng Xu, Gauri Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Initializing with pre-trained models when learning on downstream tasks is
becoming standard practice in machine learning. Several recent works explore
the benefits of pre-trained initialization in a federated learning (FL)
setting, where the downstream training is performed at the edge clients with
heterogeneous data distribution. These works show that starting from a
pre-trained model can substantially reduce the adverse impact of data
heterogeneity on the test performance of a model trained in a federated
setting, with no changes to the standard FedAvg training algorithm. In this
work, we provide a deeper theoretical understanding of this phenomenon. To do
so, we study the class of two-layer convolutional neural networks (CNNs) and
provide bounds on the training error convergence and test error of such a
network trained with FedAvg. We introduce the notion of aligned and misaligned
filters at initialization and show that the data heterogeneity only affects
learning on misaligned filters. Starting with a pre-trained model typically
results in fewer misaligned filters at initialization, thus producing a lower
test error even when the model is trained in a federated setting with data
heterogeneity. Experiments in synthetic settings and practical FL training on
CNNs verify our theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Selection for Off-policy Evaluation: New Algorithms and
  Experimental Protocol 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pai Liu, Lingfeng Zhao, Shivangi Agarwal, Jinghan Liu, Audrey Huang, Philip Amortila, Nan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Holdout validation and hyperparameter tuning from data is a long-standing
problem in offline reinforcement learning (RL). A standard framework is to use
off-policy evaluation (OPE) methods to evaluate and select the policies, but
OPE either incurs exponential variance (e.g., importance sampling) or has
hyperparameters on their own (e.g., FQE and model-based). In this work we focus
on hyperparameter tuning for OPE itself, which is even more under-investigated.
Concretely, we select among candidate value functions ("model-free") or
dynamics ("model-based") to best assess the performance of a target policy. Our
contributions are two fold. We develop: (1) new model-free and model-based
selectors with theoretical guarantees, and (2) a new experimental protocol for
empirically evaluating them. Compared to the model-free protocol in prior
works, our new protocol allows for more stable generation of candidate value
functions, better control of misspecification, and evaluation of model-free and
model-based methods alike. We exemplify the protocol on a Gym environment, and
find that our new model-free selector, LSTD-Tournament, demonstrates promising
empirical performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Interactive Framework for Implementing Privacy-Preserving Federated
  Learning: Experiments on Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasra Ahmadi, Rouzbeh Behnia, Reza Ebrahimi, Mehran Mozaffari Kermani, Jeremiah Birrell, Jason Pacheco, Attila A Yavuz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enhances privacy by keeping user data on local
devices. However, emerging attacks have demonstrated that the updates shared by
users during training can reveal significant information about their data. This
has greatly thwart the adoption of FL methods for training robust AI models in
sensitive applications. Differential Privacy (DP) is considered the gold
standard for safeguarding user data. However, DP guarantees are highly
conservative, providing worst-case privacy guarantees. This can result in
overestimating privacy needs, which may compromise the model's accuracy.
Additionally, interpretations of these privacy guarantees have proven to be
challenging in different contexts. This is further exacerbated when other
factors, such as the number of training iterations, data distribution, and
specific application requirements, can add further complexity to this problem.
In this work, we proposed a framework that integrates a human entity as a
privacy practitioner to determine an optimal trade-off between the model's
privacy and utility. Our framework is the first to address the variable memory
requirement of existing DP methods in FL settings, where resource-limited
devices (e.g., cell phones) can participate. To support such settings, we adopt
a recent DP method with fixed memory usage to ensure scalable private FL. We
evaluated our proposed framework by fine-tuning a BERT-based LLM model using
the GLUE dataset (a common approach in literature), leveraging the new
accountant, and employing diverse data partitioning strategies to mimic
real-world conditions. As a result, we achieved stable memory usage, with an
average accuracy reduction of 1.33% for $\epsilon = 10$ and 1.9% for $\epsilon
= 6$, when compared to the state-of-the-art DP accountant which does not
support fixed memory usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Randomness in Stability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Hopkins, Shay Moran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stability is a central property in learning and statistics promising the
output of an algorithm $A$ does not change substantially when applied to
similar datasets $S$ and $S'$. It is an elementary fact that any sufficiently
stable algorithm (e.g.\ one returning the same result with high probability,
satisfying privacy guarantees, etc.) must be randomized. This raises a natural
question: can we quantify how much randomness is needed for algorithmic
stability?
  We study the randomness complexity of two influential notions of stability in
learning: replicability, which promises $A$ usually outputs the same result
when run over samples from the same distribution (and shared random coins), and
differential privacy, which promises the output distribution of $A$ remains
similar under neighboring datasets. The randomness complexity of these notions
was studied recently in (Dixon et al. ICML 2024) and (Cannone et al. ITCS 2024)
for basic $d$-dimensional tasks (e.g. estimating the bias of $d$ coins), but
little is known about the measures more generally or in complex settings like
classification.
  Toward this end, we prove a `weak-to-strong' boosting theorem for stability:
the randomness complexity of a task $M$ (either under replicability or DP) is
tightly controlled by the best replication probability of any deterministic
algorithm solving the task, a weak measure called `global stability' that is
universally capped at $\frac{1}{2}$ (Chase et al. FOCS 2023). Using this, we
characterize the randomness complexity of PAC Learning: a class has bounded
randomness complexity iff it has finite Littlestone dimension, and moreover
scales at worst logarithmically in the excess error of the learner. This
resolves a question of (Chase et al. STOC 2024) who asked for such a
characterization in the equivalent language of (error-dependent)
`list-replicability'.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Greed is Good: Guided Generation from a Greedy Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zander W. Blasingame, Chen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training-free guided generation is a widely used and powerful technique that
allows the end user to exert further control over the generative process of
diffusion models. In this work, we explore the guided generation from the
perspective of optimizing the solution trajectory of a neural differential
equation in a greedy manner. We present such a strategy as a unifying view on
training-free guidance by showing that the greedy strategy is a first-order
discretization of end-to-end optimization techniques. We show that a greedy
guidance strategy makes good decisions and compare it to a guidance strategy
using the ideal gradients found via the continuous adjoint equations. We then
show how other popular training-free guidance strategies can be viewed in a
unified manner from this perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initial preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Training One-Step Diffusion Models Without Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingtian Zhang, Jiajun He, Wenlin Chen, Zijing Ou, José Miguel Hernández-Lobato, Bernhard Schölkopf, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in one-step generative models typically follow a two-stage
process: first training a teacher diffusion model and then distilling it into a
one-step student model. This distillation process traditionally relies on both
the teacher model's score function to compute the distillation loss and its
weights for student initialization. In this paper, we explore whether one-step
generative models can be trained directly without this distillation process.
First, we show that the teacher's score function is not essential and propose a
family of distillation methods that achieve competitive results without relying
on score estimation. Next, we demonstrate that initialization from teacher
weights is indispensable in successful training. Surprisingly, we find that
this benefit is not due to improved ``input-output" mapping but rather the
learned feature representations, which dominate distillation quality. Our
findings provide a better understanding of the role of initialization in
one-step model training and its impact on distillation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Likelihoods via Mutual Information: Bridging Simulation-Based
  Inference and Bayesian Optimal Experimental Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent D. Zaballa, Elliot E. Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation-based inference (SBI) is a method to perform inference on a
variety of complex scientific models with challenging inference (inverse)
problems. Bayesian Optimal Experimental Design (BOED) aims to efficiently use
experimental resources to make better inferences. Various stochastic
gradient-based BOED methods have been proposed as an alternative to Bayesian
optimization and other experimental design heuristics to maximize information
gain from an experiment. We demonstrate a link via mutual information bounds
between SBI and stochastic gradient-based variational inference methods that
permits BOED to be used in SBI applications as SBI-BOED. This link allows
simultaneous optimization of experimental designs and optimization of amortized
inference functions. We evaluate the pitfalls of naive design optimization
using this method in a standard SBI task and demonstrate the utility of a
well-chosen design distribution in BOED. We compare this approach on SBI-based
models in real-world simulators in epidemiology and biology, showing notable
improvements in inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Multi-agent Multi-armed Bandits on Stochastic Block Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengfan Xu, Liren Shan, Fatemeh Ghaffari, Xuchuang Wang, Xutong Liu, Mohammad Hajiesmaili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a novel heterogeneous multi-agent multi-armed bandit problem with a
cluster structure induced by stochastic block models, influencing not only
graph topology, but also reward heterogeneity. Specifically, agents are
distributed on random graphs based on stochastic block models - a generalized
Erdos-Renyi model with heterogeneous edge probabilities: agents are grouped
into clusters (known or unknown); edge probabilities for agents within the same
cluster differ from those across clusters. In addition, the cluster structure
in stochastic block model also determines our heterogeneous rewards. Rewards
distributions of the same arm vary across agents in different clusters but
remain consistent within a cluster, unifying homogeneous and heterogeneous
settings and varying degree of heterogeneity, and rewards are independent
samples from these distributions. The objective is to minimize system-wide
regret across all agents. To address this, we propose a novel algorithm
applicable to both known and unknown cluster settings. The algorithm combines
an averaging-based consensus approach with a newly introduced information
aggregation and weighting technique, resulting in a UCB-type strategy. It
accounts for graph randomness, leverages both intra-cluster (homogeneous) and
inter-cluster (heterogeneous) information from rewards and graphs, and
incorporates cluster detection for unknown cluster settings. We derive optimal
instance-dependent regret upper bounds of order $\log{T}$ under sub-Gaussian
rewards. Importantly, our regret bounds capture the degree of heterogeneity in
the system (an additional layer of complexity), exhibit smaller constants,
scale better for large systems, and impose significantly relaxed assumptions on
edge probabilities. In contrast, prior works have not accounted for this
refined problem complexity, rely on more stringent assumptions, and exhibit
limited scalability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Client Privacy Leakage from Public <span class="highlight-title">Dataset</span> Usage in Federated
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Shi, Tu Ouyang, An Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Distillation (FD) has emerged as a popular federated training
framework, enabling clients to collaboratively train models without sharing
private data. Public Dataset-Assisted Federated Distillation (PDA-FD), which
leverages public datasets for knowledge sharing, has become widely adopted.
Although PDA-FD enhances privacy compared to traditional Federated Learning, we
demonstrate that the use of public datasets still poses significant privacy
risks to clients' private training data. This paper presents the first
comprehensive privacy analysis of PDA-FD in presence of an honest-but-curious
server. We show that the server can exploit clients' inference results on
public datasets to extract two critical types of private information: label
distributions and membership information of the private training dataset. To
quantify these vulnerabilities, we introduce two novel attacks specifically
designed for the PDA-FD setting: a label distribution inference attack and
innovative membership inference methods based on Likelihood Ratio Attack
(LiRA). Through extensive evaluation of three representative PDA-FD frameworks
(FedMD, DS-FL, and Cronus), our attacks achieve state-of-the-art performance,
with label distribution attacks reaching minimal KL-divergence and membership
inference attacks maintaining high True Positive Rates under low False Positive
Rate constraints. Our findings reveal significant privacy risks in current
PDA-FD frameworks and emphasize the need for more robust privacy protection
mechanisms in collaborative learning systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive kernel predictors from feature-learning infinite limits of
  neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clarissa Lauditi, Blake Bordelon, Cengiz Pehlevan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous influential work showed that infinite width limits of neural
networks in the lazy training regime are described by kernel machines. Here, we
show that neural networks trained in the rich, feature learning infinite-width
regime in two different settings are also described by kernel machines, but
with data-dependent kernels. For both cases, we provide explicit expressions
for the kernel predictors and prescriptions to numerically calculate them. To
derive the first predictor, we study the large-width limit of feature-learning
Bayesian networks, showing how feature learning leads to task-relevant
adaptation of layer kernels and preactivation densities. The saddle point
equations governing this limit result in a min-max optimization problem that
defines the kernel predictor. To derive the second predictor, we study gradient
flow training of randomly initialized networks trained with weight decay in the
infinite-width limit using dynamical mean field theory (DMFT). The fixed point
equations of the arising DMFT defines the task-adapted internal representations
and the kernel predictor. We compare our kernel predictors to kernels derived
from lazy regime and demonstrate that our adaptive kernels achieve lower test
loss on benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What is a Sketch-and-Precondition Derivation for Low-Rank Approximation?
  Inverse Power Error or Inverse Power Estimation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Xu, Yiping Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Randomized sketching accelerates large-scale numerical linear algebra by
reducing computa- tional complexity. While the traditional sketch-and-solve
approach reduces the problem size di- rectly through sketching, the
sketch-and-precondition method leverages sketching to construct a computational
friendly preconditioner. This preconditioner improves the convergence speed of
iterative solvers applied to the original problem, maintaining accuracy in the
full space. Further- more, the convergence rate of the solver improves at least
linearly with the sketch size. Despite its potential, developing a
sketch-and-precondition framework for randomized algorithms in low- rank matrix
approximation remains an open challenge. We introduce the Error-Powered
Sketched Inverse Iteration (EPSI) Method via run sketched Newton iteration for
the Lagrange form as a sketch-and-precondition variant for randomized low-rank
approximation. Our method achieves theoretical guarantees, including a
convergence rate that improves at least linearly with the sketch size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Effective Dynamics across Spatio-Temporal Scales of Complex
  Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Gao, Sebastian Kaltenbach, Petros Koumoutsakos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling and simulation of complex fluid flows with dynamics that span
multiple spatio-temporal scales is a fundamental challenge in many scientific
and engineering domains. Full-scale resolving simulations for systems such as
highly turbulent flows are not feasible in the foreseeable future, and
reduced-order models must capture dynamics that involve interactions across
scales. In the present work, we propose a novel framework, Graph-based Learning
of Effective Dynamics (Graph-LED), that leverages graph neural networks (GNNs),
as well as an attention-based autoregressive model, to extract the effective
dynamics from a small amount of simulation data. GNNs represent flow fields on
unstructured meshes as graphs and effectively handle complex geometries and
non-uniform grids. The proposed method combines a GNN based, dimensionality
reduction for variable-size unstructured meshes with an autoregressive temporal
attention model that can learn temporal dependencies automatically. We
evaluated the proposed approach on a suite of fluid dynamics problems,
including flow past a cylinder and flow over a backward-facing step over a
range of Reynolds numbers. The results demonstrate robust and effective
forecasting of spatio-temporal physics; in the case of the flow past a
cylinder, both small-scale effects that occur close to the cylinder as well as
its wake are accurately captured.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Parsimony and Learning (CPAL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning
  Capabilities of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lejla Skelic, Yan Xu, Matthew Cox, Wenjie Lu, Tao Yu, Ruonan Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The role of Large Language Models (LLMs) has not been extensively explored in
analog circuit design, which could benefit from a reasoning-based approach that
transcends traditional optimization techniques. In particular, despite their
growing relevance, there are no benchmarks to assess LLMs' reasoning capability
about circuits. Therefore, we created the CIRCUIT dataset consisting of 510
question-answer pairs spanning various levels of analog-circuit-related
subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04%
accuracy when evaluated on the final numerical answer. To evaluate the
robustness of LLMs on our dataset, we introduced a unique feature that enables
unit-test-like evaluation by grouping questions into unit tests. In this case,
GPT-4o can only pass 27.45% of the unit tests, highlighting that the most
advanced LLMs still struggle with understanding circuits, which requires
multi-level reasoning, particularly when involving circuit topologies. This
circuit-specific benchmark highlights LLMs' limitations, offering valuable
insights for advancing their application in analog integrated circuit design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of In-Context Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Moeini, Jiuqi Wang, Jacob Beck, Ethan Blaser, Shimon Whiteson, Rohan Chandra, Shangtong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) agents typically optimize their policies by
performing expensive backward passes to update their network parameters.
However, some agents can solve new tasks without updating any parameters by
simply conditioning on additional context such as their action-observation
histories. This paper surveys work on such behavior, known as in-context
reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RESIST: Resilient Decentralized Learning Using Consensus Gradient
  Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Fang, Rishabh Dixit, Waheed U. Bajwa, Mert Gurbuzbalaban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical risk minimization (ERM) is a cornerstone of modern machine learning
(ML), supported by advances in optimization theory that ensure efficient
solutions with provable algorithmic convergence rates, which measure the speed
at which optimization algorithms approach a solution, and statistical learning
rates, which characterize how well the solution generalizes to unseen data.
Privacy, memory, computational, and communications constraints increasingly
necessitate data collection, processing, and storage across network-connected
devices. In many applications, these networks operate in decentralized settings
where a central server cannot be assumed, requiring decentralized ML algorithms
that are both efficient and resilient. Decentralized learning, however, faces
significant challenges, including an increased attack surface for adversarial
interference during decentralized learning processes. This paper focuses on the
man-in-the-middle (MITM) attack, which can cause models to deviate
significantly from their intended ERM solutions. To address this challenge, we
propose RESIST (Resilient dEcentralized learning using conSensus gradIent
deScenT), an optimization algorithm designed to be robust against adversarially
compromised communication links. RESIST achieves algorithmic and statistical
convergence for strongly convex, Polyak-Lojasiewicz, and nonconvex ERM
problems. Experimental results demonstrate the robustness and scalability of
RESIST for real-world decentralized learning in adversarial environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint of a journal paper; 100 pages and 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sink equilibria and the attractors of learning in games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Biggar, Christos Papadimitriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterizing the limit behavior -- that is, the attractors -- of learning
dynamics is one of the most fundamental open questions in game theory. In
recent work in this front, it was conjectured that the attractors of the
replicator dynamic are in one-to-one correspondence with the sink equilibria of
the game -- the sink strongly connected components of a game's preference graph
-- , and it was established that they do stand in at least one-to-many
correspondence with them. We make threefold progress on the problem of
characterizing attractors. First, we show through a topological construction
that the one-to-one conjecture is false. Second, we make progress on the
attractor characterization problem for two-player games by establishing that
the one-to-one conjecture is true in the absence of a local pattern called a
weak local source -- a pattern that is absent from zero-sum games. Finally, we
look -- for the first time in this context -- at fictitious play, the
longest-studied learning dynamic, and examine to what extent the conjecture
generalizes there. We establish that under fictitious play, sink equilibria
always contain attractors (sometimes strictly), and every attractor corresponds
to a strongly connected set of nodes in the preference graph.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Hazard Identification to <span class="highlight-title">Control</span>ler Design: Proactive and
  LLM-Supported Safety Engineering for ML-Powered Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Hong, Christopher S. Timperley, Christian Kästner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) components are increasingly integrated into software
products, yet their complexity and inherent uncertainty often lead to
unintended and hazardous consequences, both for individuals and society at
large. Despite these risks, practitioners seldom adopt proactive approaches to
anticipate and mitigate hazards before they occur. Traditional safety
engineering approaches, such as Failure Mode and Effects Analysis (FMEA) and
System Theoretic Process Analysis (STPA), offer systematic frameworks for early
risk identification but are rarely adopted. This position paper advocates for
integrating hazard analysis into the development of any ML-powered software
product and calls for greater support to make this process accessible to
developers. By using large language models (LLMs) to partially automate a
modified STPA process with human oversight at critical steps, we expect to
address two key challenges: the heavy dependency on highly experienced safety
engineering experts, and the time-consuming, labor-intensive nature of
traditional hazard analysis, which often impedes its integration into
real-world development workflows. We illustrate our approach with a running
example, demonstrating that many seemingly unanticipated issues can, in fact,
be anticipated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the International Conference on AI
  Engineering (CAIN) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Gupta, Zichao Li, Tianyi Chen, Cem Subakan, Siva Reddy, Perouz Taslakian, Valentina Zantedeschi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document retrieval is a core component of question-answering systems, as it
enables conditioning answer generation on new and large-scale corpora. While
effective, the standard practice of encoding documents into high-dimensional
embeddings for similarity search entails large memory and compute footprints,
and also makes it hard to inspect the inner workings of the system. In this
paper, we propose a tree-based method for organizing and representing reference
documents at various granular levels, which offers the flexibility to balance
cost and utility, and eases the inspection of the corpus content and retrieval
operations. Our method, called ReTreever, jointly learns a routing function per
internal node of a binary tree such that query and reference documents are
assigned to similar tree branches, hence directly optimizing for retrieval
performance. Our evaluations show that ReTreever generally preserves full
representation accuracy. Its hierarchical structure further provides strong
coarse representations and enhances transparency by indirectly learning
meaningful semantic groupings. Among hierarchical retrieval methods, ReTreever
achieves the best retrieval accuracy at the lowest latency, proving that this
family of techniques can be viable in practical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Risk Minimization for Out-of-Distribution Generalization on
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Wang, Zhen Tan, Yaochen Zhu, Chuxu Zhang, Jundong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) generalization on graphs aims at dealing with
scenarios where the test graph distribution differs from the training graph
distributions. Compared to i.i.d. data like images, the OOD generalization
problem on graph-structured data remains challenging due to the non-i.i.d.
property and complex structural information on graphs. Recently, several works
on graph OOD generalization have explored extracting invariant subgraphs that
share crucial classification information across different distributions.
Nevertheless, such a strategy could be suboptimal for entirely capturing the
invariant information, as the extraction of discrete structures could
potentially lead to the loss of invariant information or the involvement of
spurious information. In this paper, we propose an innovative framework, named
Generative Risk Minimization (GRM), designed to generate an invariant subgraph
for each input graph to be classified, instead of extraction. To address the
challenge of optimization in the absence of optimal invariant subgraphs (i.e.,
ground truths), we derive a tractable form of the proposed GRM objective by
introducing a latent causal variable, and its effectiveness is validated by our
theoretical analysis. We further conduct extensive experiments across a variety
of real-world graph datasets for both node-level and graph-level OOD
generalization, and the results demonstrate the superiority of our framework
GRM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR 02/2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New tools for comparing classical and neural ODE models for tumor growth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony D. Blaom, Samuel Okon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new computational tool TumorGrowth.jl for modeling tumor growth is
introduced. The tool allows the comparison of standard textbook models, such as
General Bertalanffy and Gompertz, with some newer models, including, for the
first time, neural ODE models. As an application, we revisit a human meta-study
of non-small cell lung cancer and bladder cancer lesions, in patients
undergoing two different treatment options, to determine if previously reported
performance differences are statistically significant, and if newer, more
complex models perform any better. In a population of examples with at least
four time-volume measurements available for calibration, and an average of
about 6.3, our main conclusion is that the General Bertalanffy model has
superior performance, on average. However, where more measurements are
available, we argue that more complex models, capable of capturing rebound and
relapse behavior, may be better choices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures. Related software is archived at
  https://github.com/ablaom/TumorGrowth.jl</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESPFormer: Doubly-Stochastic Attention with Expected Sliced Transport
  Plans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashkan Shahbazi, Elaheh Akbari, Darian Salehi, Xinran Liu, Navid Naderializadeh, Soheil Kolouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While self-attention has been instrumental in the success of Transformers, it
can lead to over-concentration on a few tokens during training, resulting in
suboptimal information flow. Enforcing doubly-stochastic constraints in
attention matrices has been shown to improve structure and balance in attention
distributions. However, existing methods rely on iterative Sinkhorn
normalization, which is computationally costly. In this paper, we introduce a
novel, fully parallelizable doubly-stochastic attention mechanism based on
sliced optimal transport, leveraging Expected Sliced Transport Plans (ESP).
Unlike prior approaches, our method enforces double stochasticity without
iterative Sinkhorn normalization, significantly enhancing efficiency. To ensure
differentiability, we incorporate a temperature-based soft sorting technique,
enabling seamless integration into deep learning models. Experiments across
multiple benchmark datasets, including image classification, point cloud
classification, sentiment analysis, and neural machine translation, demonstrate
that our enhanced attention regularization consistently improves performance
across diverse applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Self-supervised Domain Generalization for Label-efficient
  Polyp Segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Tan, Jiacheng Wang, Liansheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing self-supervised learning (SSL) methodologies assumes par-amount
significance in handling unlabeled polyp datasets when building deep
learning-based automatic polyp segmentation models. However, the intricate
privacy dynamics surrounding medical data often preclude seamless data sharing
among disparate medical centers. Federated learning (FL) emerges as a
formidable solution to this privacy conundrum, yet within the realm of FL,
optimizing model generalization stands as a pressing imperative. Robust
generalization capabilities are imperative to ensure the model's efficacy
across diverse geographical domains post-training on localized client datasets.
In this paper, a Federated self-supervised Domain Generalization method is
proposed to enhance the generalization capacity of federated and
Label-efficient intestinal polyp segmentation, named LFDG. Based on a classical
SSL method, DropPos, LFDG proposes an adversarial learning-based data
augmentation method (SSADA) to enhance the data diversity. LFDG further
proposes a relaxation module based on Source-reconstruction and
Augmentation-masking (SRAM) to maintain stability in feature learning. We have
validated LFDG on polyp images from six medical centers. The performance of our
method achieves 3.80% and 3.92% better than the baseline and other recent FL
methods and SSL methods, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ADSMI @ MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VSC-RL: Advancing Autonomous Vision-Language Agents with Variational
  Subgoal-Conditioned Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyuan Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art (SOTA) reinforcement learning (RL) methods enable the
vision-language agents to learn from interactions with the environment without
human supervision. However, they struggle with learning inefficiencies in
tackling real-world complex sequential decision-making tasks, especially with
sparse reward signals and long-horizon dependencies. To effectively address the
issue, we introduce Variational Subgoal-Conditioned RL (VSC-RL), which
reformulates the vision-language sequential decision-making task as a
variational goal-conditioned RL problem, allowing us to leverage advanced
optimization methods to enhance learning efficiency. Specifically, VSC-RL
optimizes the SubGoal Evidence Lower BOund (SGC-ELBO), which consists of (a)
maximizing the subgoal-conditioned return via RL and (b) minimizing the
subgoal-conditioned difference with the reference policy. We theoretically
demonstrate that SGC-ELBO is equivalent to the original optimization objective,
ensuring improved learning efficiency without sacrificing performance
guarantees. Additionally, for real-world complex decision-making tasks, VSC-RL
leverages the vision-language model to autonomously decompose the goal into
feasible subgoals, enabling efficient learning. Across various benchmarks,
including challenging real-world mobile device control tasks, VSC-RL
significantly outperforms the SOTA vision-language agents, achieving superior
performance and remarkable improvement in learning efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SurGrID: <span class="highlight-title">Control</span>lable Surgical Simulation via Scene Graph to Image
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannik Frisch, Ssharvien Kumar Sivakumar, Çağhan Köksal, Elsa Böhm, Felix Wagner, Adrian Gericke, Ghazal Ghazaei, Anirban Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical simulation offers a promising addition to conventional surgical
training. However, available simulation tools lack photorealism and rely on
hardcoded behaviour. Denoising Diffusion Models are a promising alternative for
high-fidelity image synthesis, but existing state-of-the-art conditioning
methods fall short in providing precise control or interactivity over the
generated scenes.
  We introduce SurGrID, a Scene Graph to Image Diffusion Model, allowing for
controllable surgical scene synthesis by leveraging Scene Graphs. These graphs
encode a surgical scene's components' spatial and semantic information, which
are then translated into an intermediate representation using our novel
pre-training step that explicitly captures local and global information.
  Our proposed method improves the fidelity of generated images and their
coherence with the graph input over the state-of-the-art. Further, we
demonstrate the simulation's realism and controllability in a user assessment
study involving clinical experts.
  Scene Graphs can be effectively used for precise and interactive conditioning
of Denoising Diffusion Models for simulating surgical scenes, enabling high
fidelity and interactive control over the generated content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbiotic Cooperation for Web Agents: Harnessing Complementary Strengths
  of Large and Small LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruichen Zhang, Mufan Qiu, Zhen Tan, Mohan Zhang, Vincent Lu, Jie Peng, Kaidi Xu, Leandro Z. Agudelo, Peter Qian, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web browsing agents powered by large language models (LLMs) have shown
tremendous potential in automating complex web-based tasks. Existing approaches
typically rely on large LLMs (e.g., GPT-4o) to explore web environments and
generate trajectory data, which is then used either for demonstration retrieval
(for large LLMs) or to distill small LLMs (e.g., Llama3) in a process that
remains decoupled from the exploration. In this paper, we propose
AgentSymbiotic, an iterative framework that couples data synthesis with
task-performance, yielding a "symbiotic improvement" for both large and small
LLMs. Our study uncovers a complementary dynamic between LLM types: while large
LLMs excel at generating high-quality trajectories for distillation, the
distilled small LLMs-owing to their distinct reasoning capabilities-often
choose actions that diverge from those of their larger counterparts. This
divergence drives the exploration of novel trajectories, thereby enriching the
synthesized data. However, we also observe that the performance of small LLMs
becomes a bottleneck in this iterative enhancement process. To address this, we
propose two innovations in LLM distillation: a speculative data synthesis
strategy that mitigates off-policy bias, and a multi-task learning approach
designed to boost the reasoning capabilities of the student LLM. Furthermore,
we introduce a Hybrid Mode for Privacy Preservation to address user privacy
concerns. Evaluated on the WEBARENA benchmark, AgentSymbiotic achieves SOTA
performance with both LLM types. Our best Large LLM agent reaches 52%,
surpassing the previous best of 45%, while our 8B distilled model demonstrates
a competitive 49%, exceeding the prior best of 28%. Code will be released upon
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discrete Markov Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le-Tuyet-Nhi Pham, Dario Shariatian, Antonio Ocello, Giovanni Conforti, Alain Durmus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Discrete Markov Probabilistic Model (DMPM), a novel
algorithm for discrete data generation. The algorithm operates in the space of
bits $\{0,1\}^d$, where the noising process is a continuous-time Markov chain
that can be sampled exactly via a Poissonian clock that flips labels uniformly
at random. The time-reversal process, like the forward noise process, is a jump
process, with its intensity governed by a discrete analogue of the classical
score function. Crucially, this intensity is proven to be the conditional
expectation of a function of the forward process, strengthening its theoretical
alignment with score-based generative models while ensuring robustness and
efficiency. We further establish convergence bounds for the algorithm under
minimal assumptions and demonstrate its effectiveness through experiments on
low-dimensional Bernoulli-distributed datasets and high-dimensional binary
MNIST data. The results highlight its strong performance in generating discrete
structures. This work bridges theoretical foundations and practical
applications, advancing the development of effective and theoretically grounded
discrete generative modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Advantage-Aligned Online Reinforcement Learning with Offline Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuefeng Liu, Hung T. C. Le, Siyu Chen, Rick Stevens, Zhuoran Yang, Matthew R. Walter, Yuxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online reinforcement learning (RL) enhances policies through direct
interactions with the environment, but faces challenges related to sample
efficiency. In contrast, offline RL leverages extensive pre-collected data to
learn policies, but often produces suboptimal results due to limited data
coverage. Recent efforts have sought to integrate offline and online RL in
order to harness the advantages of both approaches. However, effectively
combining online and offline RL remains challenging due to issues that include
catastrophic forgetting, lack of robustness and sample efficiency. In an effort
to address these challenges, we introduce A3 RL , a novel method that actively
selects data from combined online and offline sources to optimize policy
improvement. We provide theoretical guarantee that validates the effectiveness
our active sampling strategy and conduct thorough empirical experiments showing
that our method outperforms existing state-of-the-art online RL techniques that
utilize offline data. Our code will be publicly available at:
https://github.com/xuefeng-cs/A3RL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sign Operator for Coping with Heavy-Tailed Noise: High Probability
  Convergence Bounds with Extensions to Distributed Optimization and Comparison
  Oracle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Kornilov, Philip Zmushko, Andrei Semenov, Alexander Gasnikov, Alexander Beznosikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing popularity of AI optimization problems involving severely
corrupted data has increased the demand for methods capable of handling
heavy-tailed noise, i.e., noise with bounded $\kappa$-th moment, $\kappa \in
(1,2]$. For the widely used clipping technique, effectiveness heavily depends
on the careful tuning of clipping levels throughout training. In this paper, we
demonstrate that using only the sign of the input, without introducing
additional hyperparameters, is sufficient to cope with heavy-tailed noise
effectively. For smooth non-convex functions, we prove that SignSGD achieves
optimal sample complexity $\tilde{O}\left(\varepsilon^{-\frac{3\kappa -
2}{\kappa - 1}}\right)$ with high probability for attaining an average gradient
norm accuracy of $\varepsilon$. Under the assumption of symmetric noise, we use
SignSGD with Majority Voting to extend this bound to the distributed
optimization or reduce the sample complexity to $\tilde{O}(\varepsilon^{-4})$
in the case of a single worker with arbitrary parameters. Furthermore, we
explore the application of the sign operator in zeroth-order optimization with
an oracle that can only compare function values at two different points. We
propose a novel method, MajorityVote-CompsSGD, and provide the first-known
high-probability bound $\tilde{O}(\varepsilon^{-6})$ for the number of
comparisons under symmetric noise assumption. Our theoretical findings are
supported by the superior performance of sign-based methods in training Large
Language Models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSeek on a Trip: Inducing Targeted Visual Hallucinations via
  Representation Vulnerabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chashi Mahiul Islam, Samuel Jacob Chacko, Preston Horne, Xiuwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) represent the cutting edge of AI
technology, with DeepSeek models emerging as a leading open-source alternative
offering competitive performance to closed-source systems. While these models
demonstrate remarkable capabilities, their vision-language integration
mechanisms introduce specific vulnerabilities. We implement an adapted
embedding manipulation attack on DeepSeek Janus that induces targeted visual
hallucinations through systematic optimization of image embeddings. Through
extensive experimentation across COCO, DALL-E 3, and SVIT datasets, we achieve
hallucination rates of up to 98.0% while maintaining high visual fidelity (SSIM
> 0.88) of the manipulated images on open-ended questions. Our analysis
demonstrates that both 1B and 7B variants of DeepSeek Janus are susceptible to
these attacks, with closed-form evaluation showing consistently higher
hallucination rates compared to open-ended questioning. We introduce a novel
multi-prompt hallucination detection framework using LLaMA-3.1 8B Instruct for
robust evaluation. The implications of these findings are particularly
concerning given DeepSeek's open-source nature and widespread deployment
potential. This research emphasizes the critical need for embedding-level
security measures in MLLM deployment pipelines and contributes to the broader
discussion of responsible AI implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Observational Partial Order of Causal Structures with Latent
  Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Maciel Ansanelli, Elie Wolfe, Robert W. Spekkens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For two causal structures with the same set of visible variables, one is said
to observationally dominate the other if the set of distributions over the
visible variables realizable by the first contains the set of distributions
over the visible variables realizable by the second. Knowing such dominance
relations is useful for adjudicating between these structures given
observational data. We here consider the problem of determining the partial
order of equivalence classes of causal structures with latent variables
relative to observational dominance. We provide a complete characterization of
the dominance order in the case of three visible variables, and a partial
characterization in the case of four visible variables. Our techniques also
help to identify which observational equivalence classes have a set of
realizable distributions that is characterized by nontrivial inequality
constraints, analogous to Bell inequalities and instrumental inequalities. We
find evidence that as one increases the number of visible variables, the
equivalence classes satisfying nontrivial inequality constraints become
ubiquitous. (Because such classes are the ones for which there can be a
difference in the distributions that are quantumly and classically realizable,
this implies that the potential for quantum-classical gaps is also ubiquitous.)
Furthermore, we find evidence that constraint-based causal discovery algorithms
that rely solely on conditional independence constraints have a significantly
weaker distinguishing power among observational equivalence classes than
algorithms that go beyond these (i.e., algorithms that also leverage nested
Markov constraints and inequality constraints).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 30 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A unifying account of warm start guarantees for patches of quantum
  landscapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hela Mhiri, Ricard Puig, Sacha Lerch, Manuel S. Rudolph, Thiparat Chotibut, Supanut Thanasilp, Zoë Holmes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Barren plateaus are fundamentally a statement about quantum loss landscapes
on average but there can, and generally will, exist patches of barren plateau
landscapes with substantial gradients. Previous work has studied certain
classes of parameterized quantum circuits and found example regions where
gradients vanish at worst polynomially in system size. Here we present a
general bound that unifies all these previous cases and that can tackle
physically-motivated ans\"atze that could not be analyzed previously.
Concretely, we analytically prove a lower-bound on the variance of the loss
that can be used to show that in a non-exponentially narrow region around a
point with curvature the loss variance cannot decay exponentially fast. This
result is complemented by numerics and an upper-bound that suggest that any
loss function with a barren plateau will have exponentially vanishing gradients
in any constant radius subregion. Our work thus suggests that while there are
hopes to be able to warm-start variational quantum algorithms, any
initialization strategy that cannot get increasingly close to the region of
attraction with increasing problem size is likely inadequate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curvature Tuning: Provable Training-free Model Steering From a Single
  Parameter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leyang Hu, Randall Balestriero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of model size and data size has reshaped the paradigm of AI. As a
result, the common protocol to leverage the latest models is to steer them
towards a specific downstream task of interest through {\em fine-tuning}.
Despite its importance, the main methods for fine-tuning remain limited to full
or low-rank adapters--containing countless hyper-parameters and lacking
interpretability. In this paper, we take a step back and demonstrate how novel
and explainable post-training steering solutions can be derived theoretically
from {\em spline operators}, a rich mathematical framing of Deep Networks that
was recently developed. Our method--coined \textbf{Curvature Tuning (CT)}--has
a single parameter that provably modulates the curvature of the model's
decision boundary henceforth allowing training-free steering. This makes CT
both more efficient and interpretable than conventional fine-tuning methods. We
empirically validate its effectiveness in improving generalization and
robustness of pretrained models. For example, CT improves out-of-distribution
transfer performances of ResNet-18/50 by 2.57\%/1.74\% across seventeen
downstream datasets, and improves RobustBench robust accuracy by
11.76\%/348.44\%. Additionally, we apply CT to ReLU-based Swin-T/S, improving
their generalization on nine downstream datasets by 2.43\%/3.33\%. Our code is
available at
\href{https://github.com/Leon-Leyang/curvature-tuning}{https://github.com/Leon-Leyang/curvature-tuning}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DarwinLM: Evolutionary Structured Pruning of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkun Tang, Oliver Sieberling, Eldar Kurtic, Zhiqiang Shen, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved significant success across various
NLP tasks. However, their massive computational costs limit their widespread
use, particularly in real-time applications. Structured pruning offers an
effective solution by compressing models and directly providing end-to-end
speed improvements, regardless of the hardware environment. Meanwhile,
different components of the model exhibit varying sensitivities towards
pruning, calling for \emph{non-uniform} model compression. However, a pruning
method should not only identify a capable substructure, but also account for
post-compression training. To this end, we propose \sysname, a method for
\emph{training-aware} structured pruning. \sysname builds upon an evolutionary
search process, generating multiple offspring models in each generation through
mutation, and selecting the fittest for survival. To assess the effect of
post-training, we incorporate a lightweight, multistep training process within
the offspring population, progressively increasing the number of tokens and
eliminating poorly performing models in each selection stage. We validate our
method through extensive experiments on Llama-2-7B, Llama-3.1-8B and
Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured
pruning. For instance, \sysname surpasses ShearedLlama while requiring
$5\times$ less training data during post-compression training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auditing Prompt Caching in Language Model APIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenchen Gu, Xiang Lisa Li, Rohith Kuditipudi, Percy Liang, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt caching in large language models (LLMs) results in data-dependent
timing variations: cached prompts are processed faster than non-cached prompts.
These timing differences introduce the risk of side-channel timing attacks. For
example, if the cache is shared across users, an attacker could identify cached
prompts from fast API response times to learn information about other users'
prompts. Because prompt caching may cause privacy leakage, transparency around
the caching policies of API providers is important. To this end, we develop and
conduct statistical audits to detect prompt caching in real-world LLM API
providers. We detect global cache sharing across users in seven API providers,
including OpenAI, resulting in potential privacy leakage about users' prompts.
Timing variations due to prompt caching can also result in leakage of
information about model architecture. Namely, we find evidence that OpenAI's
embedding model is a decoder-only Transformer, which was previously not
publicly known.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimistic Interior Point Methods for Sequential Hypothesis Testing by
  Betting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Chen, Jun-Kun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The technique of "testing by betting" frames nonparametric sequential
hypothesis testing as a multiple-round game, where a player bets on future
observations that arrive in a streaming fashion, accumulates wealth that
quantifies evidence against the null hypothesis, and rejects the null once the
wealth exceeds a specified threshold while controlling the false positive
error. Designing an online learning algorithm that achieves a small regret in
the game can help rapidly accumulate the bettor's wealth, which in turn can
shorten the time to reject the null hypothesis under the alternative $H_1$.
However, many of the existing works employ the Online Newton Step (ONS) to
update within a halved decision space to avoid a gradient explosion issue,
which is potentially conservative for rapid wealth accumulation. In this paper,
we introduce a novel strategy utilizing interior-point methods in optimization
that allows updates across the entire interior of the decision space without
the risk of gradient explosion. Our approach not only maintains strong
statistical guarantees but also facilitates faster null hypothesis rejection in
critical scenarios, overcoming the limitations of existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Down Bias: On The Limits of Generalizable Pruning Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sibo Ma, Alejandro Salinas, Peter Henderson, Julian Nyarko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We employ model pruning to examine how LLMs conceptualize racial biases, and
whether a generalizable mitigation strategy for such biases appears feasible.
Our analysis yields several novel insights. We find that pruning can be an
effective method to reduce bias without significantly increasing anomalous
model behavior. Neuron-based pruning strategies generally yield better results
than approaches pruning entire attention heads. However, our results also show
that the effectiveness of either approach quickly deteriorates as pruning
strategies become more generalized. For instance, a model that is trained on
removing racial biases in the context of financial decision-making poorly
generalizes to biases in commercial transactions. Overall, our analysis
suggests that racial biases are only partially represented as a general concept
within language models. The other part of these biases is highly
context-specific, suggesting that generalizable mitigation strategies may be of
limited effectiveness. Our findings have important implications for legal
frameworks surrounding AI. In particular, they suggest that an effective
mitigation strategy should include the allocation of legal responsibility on
those that deploy models in a specific use case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polynomial-Time Approximability of Constrained Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy McMahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the computational complexity of approximating general constrained
Markov decision processes. Our primary contribution is the design of a
polynomial time $(0,\epsilon)$-additive bicriteria approximation algorithm for
finding optimal constrained policies across a broad class of recursively
computable constraints, including almost-sure, chance, expectation, and their
anytime variants. Matching lower bounds imply our approximation guarantees are
optimal so long as $P \neq NP$. The generality of our approach results in
answers to several long-standing open complexity questions in the constrained
reinforcement learning literature. Specifically, we are the first to prove
polynomial-time approximability for the following settings: policies under
chance constraints, deterministic policies under multiple expectation
constraints, policies under non-homogeneous constraints (i.e., constraints of
different types), and policies under constraints for continuous-state
processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Fingerprinting of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshul Nasery, Jonathan Hayase, Creston Brooks, Peiyao Sheng, Himanshu Tyagi, Pramod Viswanath, Sewoong Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model fingerprinting has emerged as a powerful tool for model owners to
identify their shared model given API access. However, to lower false discovery
rate, fight fingerprint leakage, and defend against coalitions of model users
attempting to bypass detection, we argue that {\em scalability} is critical,
i.e., scaling up the number of fingerprints one can embed into a model. Hence,
we pose scalability as a crucial requirement for fingerprinting schemes. We
experiment with fingerprint design at a scale significantly larger than
previously considered, and introduce a new method, dubbed Perinucleus sampling,
to generate scalable, persistent, and harmless fingerprints. We demonstrate
that this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two
orders of magnitude more than existing schemes -- without degrading the model's
utility. Our inserted fingerprints persist even after supervised fine-tuning on
standard post-training data. We further address security risks for
fingerprinting, and theoretically and empirically show how a scalable
fingerprinting scheme like ours can mitigate these risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel computational workflows for natural and biomedical image
  processing based on hypercomplex algebras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nektarios A. Valous, Eckhard Hitzer, Dragoş Duşe, Rodrigo Rojas Moraleda, Ferdinand Popp, Meggy Suarez-Carmona, Anna Berthel, Ismini Papageorgiou, Carlo Fremd, Alexander Rölle, Christina C. Westhoff, Bénédicte Lenoir, Niels Halama, Inka Zörnig, Dirk Jäger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypercomplex image processing extends conventional techniques in a unified
paradigm encompassing algebraic and geometric principles. This work leverages
quaternions and the two-dimensional orthogonal planes split framework
(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D
planes) for natural/biomedical image analysis through the following
computational workflows and outcomes: natural/biomedical image re-colorization,
natural image de-colorization, natural/biomedical image contrast enhancement,
computational re-staining and stain separation in histological images, and
performance gains in machine/deep learning pipelines for histological images.
The workflows are analyzed separately for natural and biomedical images to
showcase the effectiveness of the proposed approaches. The proposed workflows
can regulate color appearance (e.g. with alternative renditions and grayscale
conversion) and image contrast, be part of automated image processing pipelines
(e.g. isolating stain components, boosting learning models), and assist in
digital pathology applications (e.g. enhancing biomarker visibility, enabling
colorblind-friendly renditions). Employing only basic arithmetic and matrix
operations, this work offers a computationally accessible methodology - in the
hypercomplex domain - that showcases versatility and consistency across image
processing tasks and a range of computer vision and biomedical applications.
The proposed non-data-driven methods achieve comparable or better results
(particularly in cases involving well-known methods) to those reported in the
literature, showcasing the potential of robust theoretical frameworks with
practical effectiveness. Results, methods, and limitations are detailed
alongside discussion of promising extensions, emphasizing the potential of
feature-rich mathematical/computational frameworks for natural and biomedical
images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 18 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient Optimizer Design for LLM via Structured Fisher
  Approximation with a Low-Rank Extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Gong, Meyer Scetbon, Chao Ma, Edward Meeds
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing efficient optimizers for large language models (LLMs) with
low-memory requirements and fast convergence is an important and challenging
problem. This paper makes a step towards the systematic design of such
optimizers through the lens of structured Fisher information matrix (FIM)
approximation. We show that many state-of-the-art efficient optimizers can be
viewed as solutions to FIM approximation (under the Frobenius norm) with
specific structural assumptions. Building on these insights, we propose two
design recommendations of practical efficient optimizers for LLMs, involving
the careful selection of structural assumptions to balance generality and
efficiency, and enhancing memory efficiency of optimizers with general
structures through a novel low-rank extension framework. We demonstrate how to
use each design approach by deriving new memory-efficient optimizers: Row and
Column Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation
(Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the
effectiveness, showing faster and better convergence than existing
memory-efficient baselines and Adam with little memory overhead. Notably, Alice
achieves better than 2x faster convergence over Adam, while RACS delivers
strong performance on the 1B model with SGD-like memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PFedDST: Personalized Federated Learning with Decentralized Selection
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengchen Fan, Keren Li, Tianyun Zhang, Qing Tian, Baocheng Geng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed Learning (DL) enables the training of machine learning models
across multiple devices, yet it faces challenges like non-IID data
distributions and device capability disparities, which can impede training
efficiency. Communication bottlenecks further complicate traditional Federated
Learning (FL) setups. To mitigate these issues, we introduce the Personalized
Federated Learning with Decentralized Selection Training (PFedDST) framework.
PFedDST enhances model training by allowing devices to strategically evaluate
and select peers based on a comprehensive communication score. This score
integrates loss, task similarity, and selection frequency, ensuring optimal
peer connections. This selection strategy is tailored to increase local
personalization and promote beneficial peer collaborations to strengthen the
stability and efficiency of the training process. Our experiments demonstrate
that PFedDST not only enhances model accuracy but also accelerates convergence.
This approach outperforms state-of-the-art methods in handling data
heterogeneity, delivering both faster and more effective training in diverse
and decentralized systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-Genome Phenotype Prediction with Machine Learning: Open Problems
  in Bacterial Genomics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamsin James, Ben Williamson, Peter Tino, Nicole Wheeler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we identify causal genetic mechanisms that govern bacterial traits?
Initial efforts entrusting machine learning models to handle the task of
predicting phenotype from genotype return high accuracy scores. However,
attempts to extract any meaning from the predictive models are found to be
corrupted by falsely identified "causal" features. Relying solely on pattern
recognition and correlations is unreliable, significantly so in bacterial
genomics settings where high-dimensionality and spurious associations are the
norm. Though it is not yet clear whether we can overcome this hurdle,
significant efforts are being made towards discovering potential high-risk
bacterial genetic variants. In view of this, we set up open problems
surrounding phenotype prediction from bacterial whole-genome datasets and
extending those to learning causal effects, and discuss challenges that impact
the reliability of a machine's decision-making when faced with datasets of this
nature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiPoNet: A Topology-Preserving Multi-View Neural Network For High
  Dimensional Point Cloud and Single-Cell Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Viswanath, Hiren Madhu, Dhananjay Bhaskar, Jake Kovalic, Dave Johnson, Rex Ying, Christopher Tape, Ian Adelstein, Michael Perlmutter, Smita Krishnaswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose HiPoNet, an end-to-end differentiable neural
network for regression, classification, and representation learning on
high-dimensional point clouds. Single-cell data can have high dimensionality
exceeding the capabilities of existing methods point cloud tailored for 3D
data. Moreover, modern single-cell and spatial experiments now yield entire
cohorts of datasets (i.e. one on every patient), necessitating models that can
process large, high-dimensional point clouds at scale. Most current approaches
build a single nearest-neighbor graph, discarding important geometric
information. In contrast, HiPoNet forms higher-order simplicial complexes
through learnable feature reweighting, generating multiple data views that
disentangle distinct biological processes. It then employs simplicial wavelet
transforms to extract multi-scale features - capturing both local and global
topology. We empirically show that these components preserve topological
information in the learned representations, and that HiPoNet significantly
outperforms state-of-the-art point-cloud and graph-based models on single cell.
We also show an application of HiPoNet on spatial transcriptomics datasets
using spatial co-ordinates as one of the views. Overall, HiPoNet offers a
robust and scalable solution for high-dimensional data analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing climate model interpretability: Feature attribution for Arctic
  melt anomalies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tolulope Ale, Nicole-Jeanne Schlegel, Vandana P. Janeja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The focus of our work is improving the interpretability of anomalies in
climate models and advancing our understanding of Arctic melt dynamics. The
Arctic and Antarctic ice sheets are experiencing rapid surface melting and
increased freshwater runoff, contributing significantly to global sea level
rise. Understanding the mechanisms driving snowmelt in these regions is
crucial. ERA5, a widely used reanalysis dataset in polar climate studies,
offers extensive climate variables and global data assimilation. However, its
snowmelt model employs an energy imbalance approach that may oversimplify the
complexity of surface melt. In contrast, the Glacier Energy and Mass Balance
(GEMB) model incorporates additional physical processes, such as snow
accumulation, firn densification, and meltwater percolation/refreezing,
providing a more detailed representation of surface melt dynamics. In this
research, we focus on analyzing surface snowmelt dynamics of the Greenland Ice
Sheet using feature attribution for anomalous melt events in ERA5 and GEMB
models. We present a novel unsupervised attribution method leveraging
counterfactual explanation method to analyze detected anomalies in ERA5 and
GEMB. Our anomaly detection results are validated using MEaSUREs ground-truth
data, and the attributions are evaluated against established feature ranking
methods, including XGBoost, Shapley values, and Random Forest. Our attribution
framework identifies the physics behind each model and the climate features
driving melt anomalies. These findings demonstrate the utility of our
attribution method in enhancing the interpretability of anomalies in climate
models and advancing our understanding of Arctic melt dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRP: High-Rank Preheating for Superior LoRA Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhu Chen, Yingjie Wang, Shi Fu, Li Shen, Yongcheng Jing, Xinmei Tian, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the crucial impact of initialization on the convergence
properties of Low-Rank Adaptation (LoRA). We theoretically demonstrate that
random initialization, a widely used schema, will likely lead LoRA to random
low-rank results, rather than the best low-rank result. While this issue can be
mitigated by adjusting initialization towards a well-informed direction, it
relies on prior knowledge of the target, which is typically unknown in
real-world scenarios. To approximate this well-informed initial direction, we
propose High-Rank Preheating (HRP), which fine-tunes high-rank LoRA for a few
steps and uses the singular value decomposition of the preheated result as a
superior initialization. HRP initialization is theory-supported to combine the
convergence strengths of high-rank LoRA and the generalization strengths of
low-rank LoRA. Extensive experiments demonstrate that HRP significantly
enhances LoRA's effectiveness across various models and tasks, achieving
performance comparable to full-parameter fine-tuning and outperforming other
initialization strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Non-Acyclic GFlowNets in Discrete Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Morozov, Ian Maksimov, Daniil Tiapkin, Sergey Samsonov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Flow Networks (GFlowNets) are a family of generative models that
learn to sample objects from a given probability distribution, potentially
known up to a normalizing constant. Instead of working in the object space,
GFlowNets proceed by sampling trajectories in an appropriately constructed
directed acyclic graph environment, greatly relying on the acyclicity of the
graph. In our paper, we revisit the theory that relaxes the acyclicity
assumption and present a simpler theoretical framework for non-acyclic
GFlowNets in discrete environments. Moreover, we provide various novel
theoretical insights related to training with fixed backward policies, the
nature of flow functions, and connections between entropy-regularized RL and
non-acyclic GFlowNets, which naturally generalize the respective concepts and
theoretical results from the acyclic setting. In addition, we experimentally
re-examine the concept of loss stability in non-acyclic GFlowNet training, as
well as validate our own theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Economics of Sourcing <span class="highlight-title">Human</span> Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastin Santy, Prasanta Bhattacharya, Manoel Horta Ribeiro, Kelsey Allen, Sewoong Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in AI has relied on human-generated data, from annotator
marketplaces to the wider Internet. However, the widespread use of large
language models now threatens the quality and integrity of human-generated data
on these very platforms. We argue that this issue goes beyond the immediate
challenge of filtering AI-generated content--it reveals deeper flaws in how
data collection systems are designed. Existing systems often prioritize speed,
scale, and efficiency at the cost of intrinsic human motivation, leading to
declining engagement and data quality. We propose that rethinking data
collection systems to align with contributors' intrinsic motivations--rather
than relying solely on external incentives--can help sustain high-quality data
sourcing at scale while maintaining contributor trust and long-term
participation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TMLC-Net: Transferable Meta Label Correction for Noisy Label Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevalence of noisy labels in real-world datasets poses a significant
impediment to the effective deployment of deep learning models. While
meta-learning strategies have emerged as a promising approach for addressing
this challenge, existing methods often suffer from limited transferability and
task-specific designs. This paper introduces TMLC-Net, a novel Transferable
Meta-Learner for Correcting Noisy Labels, designed to overcome these
limitations. TMLC-Net learns a general-purpose label correction strategy that
can be readily applied across diverse datasets and model architectures without
requiring extensive retraining or fine-tuning. Our approach integrates three
core components: (1) Normalized Noise Perception, which captures and normalizes
training dynamics to handle distribution shifts; (2) Time-Series Encoding,
which models the temporal evolution of sample statistics using a recurrent
neural network; and (3) Subclass Decoding, which predicts a corrected label
distribution based on the learned representations. We conduct extensive
experiments on benchmark datasets with various noise types and levels,
demonstrating that TMLC-Net consistently outperforms state-of-the-art methods
in terms of both accuracy and robustness to label noise. Furthermore, we
analyze the transferability of TMLC-Net, showcasing its adaptability to new
datasets and noise conditions, and establishing its potential as a broadly
applicable solution for robust deep learning in noisy environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise
  and Compute Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Wu, Kang Yang, Lance Kaplan, Mani Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal deep learning systems are deployed in dynamic scenarios due to the
robustness afforded by multiple sensing modalities. Nevertheless, they struggle
with varying compute resource availability (due to multi-tenancy, device
heterogeneity, etc.) and fluctuating quality of inputs (from sensor feed
corruption, environmental noise, etc.). Current multimodal systems employ
static resource provisioning and cannot easily adapt when compute resources
change over time. Additionally, their reliance on processing sensor data with
fixed feature extractors is ill-equipped to handle variations in modality
quality. Consequently, uninformative modalities, such as those with high noise,
needlessly consume resources better allocated towards other modalities. We
propose ADMN, a layer-wise Adaptive Depth Multimodal Network capable of
tackling both challenges - it adjusts the total number of active layers across
all modalities to meet compute resource constraints, and continually
reallocates layers across input modalities according to their modality quality.
Our evaluations showcase ADMN can match the accuracy of state-of-the-art
networks while reducing up to 75% of their floating-point operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BalanceKV: KV Cache Compression through Discrepancy Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Insu Han, Michael Kapralov, Ekaterina Kochetkova, Kshiteej Sheth, Amir Zandieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved impressive success, but their high
memory requirements present challenges for long-context token generation. The
memory complexity of long-context LLMs is primarily due to the need to store
Key-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache
compression method based on geometric sampling process stemming from
Banaszczyk's vector balancing theory, which introduces dependencies informed by
the geometry of keys and value tokens, and improves precision. BalanceKV offers
both theoretically proven and empirically validated performance improvements
over existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Near-Optimal Sample Complexity in Reward-Free Kernel-Based Reinforcement
  Learning <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aya Kayal, Sattar Vakili, Laura Toni, Alberto Bernacchia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) problems are being considered under increasingly
more complex structures. While tabular and linear models have been thoroughly
explored, the analytical study of RL under nonlinear function approximation,
especially kernel-based models, has recently gained traction for their strong
representational capacity and theoretical tractability. In this context, we
examine the question of statistical efficiency in kernel-based RL within the
reward-free RL framework, specifically asking: how many samples are required to
design a near-optimal policy? Existing work addresses this question under
restrictive assumptions about the class of kernel functions. We first explore
this question by assuming a generative model, then relax this assumption at the
cost of increasing the sample complexity by a factor of H, the length of the
episode. We tackle this fundamental problem using a broad class of kernels and
a simpler algorithm compared to prior work. Our approach derives new confidence
intervals for kernel ridge regression, specific to our RL setting, which may be
of broader applicability. We further validate our theoretical findings through
simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAAT: Mamba Adaptive Anomaly Transformer with association discrepancy
  for time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdellah Zakaria Sellam, Ilyes Benaissa, Abdelmalik Taleb-Ahmed, Luigi Patrono, Cosimo Distante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in time series is essential for industrial monitoring and
environmental sensing, yet distinguishing anomalies from complex patterns
remains challenging. Existing methods like the Anomaly Transformer and
DCdetector have progressed, but they face limitations such as sensitivity to
short-term contexts and inefficiency in noisy, non-stationary environments.
  To overcome these issues, we introduce MAAT, an improved architecture that
enhances association discrepancy modeling and reconstruction quality. MAAT
features Sparse Attention, efficiently capturing long-range dependencies by
focusing on relevant time steps, thereby reducing computational redundancy.
Additionally, a Mamba-Selective State Space Model is incorporated into the
reconstruction module, utilizing a skip connection and Gated Attention to
improve anomaly localization and detection performance.
  Extensive experiments show that MAAT significantly outperforms previous
methods, achieving better anomaly distinguishability and generalization across
various time series applications, setting a new standard for unsupervised time
series anomaly detection in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SNAP: Sequential Non-Ancestor Pruning for Targeted Causal Effect
  Estimation With an Unknown Graph <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mátyás Schubert, Tom Claassen, Sara Magliacane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery can be computationally demanding for large numbers of
variables. If we only wish to estimate the causal effects on a small subset of
target variables, we might not need to learn the causal graph for all
variables, but only a small subgraph that includes the targets and their
adjustment sets. In this paper, we focus on identifying causal effects between
target variables in a computationally and statistically efficient way. This
task combines causal discovery and effect estimation, aligning the discovery
objective with the effects to be estimated. We show that definite non-ancestors
of the targets are unnecessary to learn causal relations between the targets
and to identify efficient adjustments sets. We sequentially identify and prune
these definite non-ancestors with our Sequential Non-Ancestor Pruning (SNAP)
framework, which can be used either as a preprocessing step to standard causal
discovery methods, or as a standalone sound and complete causal discovery
algorithm. Our results on synthetic and real data show that both approaches
substantially reduce the number of independence tests and the computation time
without compromising the quality of causal effect estimations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partial-Label Learning with Conformal Candidate Cleaning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Fuchs, Florian Kalinke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world data is often ambiguous; for example, human annotation produces
instances with multiple conflicting class labels. Partial-label learning (PLL)
aims at training a classifier in this challenging setting, where each instance
is associated with a set of candidate labels and one correct, but unknown,
class label. A multitude of algorithms targeting this setting exists and, to
enhance their prediction quality, several extensions that are applicable across
a wide range of PLL methods have been introduced. While many of these
extensions rely on heuristics, this article proposes a novel enhancing method
that incrementally prunes candidate sets using conformal prediction. To work
around the missing labeled validation set, which is typically required for
conformal prediction, we propose a strategy that alternates between training a
PLL classifier to label the validation set, leveraging these predicted class
labels for calibration, and pruning candidate labels that are not part of the
resulting conformal sets. In this sense, our method alternates between
empirical risk minimization and candidate set pruning. We establish that our
pruning method preserves the conformal validity with respect to the unknown
ground truth. Our extensive experiments on artificial and real-world data show
that the proposed approach significantly improves the test set accuracies of
several state-of-the-art PLL classifiers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Private Low-Rank Approximation for Covariance Matrices, Dyson Brownian
  <span class="highlight-title">Motion</span>, and Eigenvalue-Gap Bounds for Gaussian Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oren Mangoubi, Nisheeth K. Vishnoi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of approximating a $d \times d$ covariance matrix $M$
with a rank-$k$ matrix under $(\varepsilon,\delta)$-differential privacy. We
present and analyze a complex variant of the Gaussian mechanism and obtain
upper bounds on the Frobenius norm of the difference between the matrix output
by this mechanism and the best rank-$k$ approximation to $M$. Our analysis
provides improvements over previous bounds, particularly when the spectrum of
$M$ satisfies natural structural assumptions. The novel insight is to view the
addition of Gaussian noise to a matrix as a continuous-time matrix Brownian
motion. This viewpoint allows us to track the evolution of eigenvalues and
eigenvectors of the matrix, which are governed by stochastic differential
equations discovered by Dyson. These equations enable us to upper bound the
Frobenius distance between the best rank-$k$ approximation of $M$ and that of a
Gaussian perturbation of $M$ as an integral that involves inverse eigenvalue
gaps of the stochastically evolving matrix, as opposed to a sum of perturbation
bounds obtained via Davis-Kahan-type theorems. Subsequently, again using the
Dyson Brownian motion viewpoint, we show that the eigenvalues of the matrix $M$
perturbed by Gaussian noise have large gaps with high probability. These
results also contribute to the analysis of low-rank approximations under
average-case perturbations, and to an understanding of eigenvalue gaps for
random matrices, both of which may be of independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Journal of the ACM. arXiv admin note: substantial text
  overlap with arXiv:2306.16648</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unifying Framework for Causal Imitation Learning with Hidden
  Confounders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daqian Shao, Thomas Kleine Buening, Marta Kwiatkowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a general and unifying framework for causal Imitation Learning
(IL) with hidden confounders that subsumes several existing confounded IL
settings from the literature. Our framework accounts for two types of hidden
confounders: (a) those observed by the expert, which thus influence the
expert's policy, and (b) confounding noise hidden to both the expert and the IL
algorithm. For additional flexibility, we also introduce a confounding noise
horizon and time-varying expert-observable hidden variables. We show that
causal IL in our framework can be reduced to a set of Conditional Moment
Restrictions (CMRs) by leveraging trajectory histories as instruments to learn
a history-dependent policy. We propose DML-IL, a novel algorithm that uses
instrumental variable regression to solve these CMRs and learn a policy. We
provide a bound on the imitation gap for DML-IL, which recovers prior results
as special cases. Empirical evaluation on a toy environment with continues
state-action spaces and multiple Mujoco tasks demonstrate that DML-IL
outperforms state-of-the-art causal IL algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guiding Time-Varying Generative Models with Natural Gradients on
  Exponential Family Manifold 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Liu, Leyang Wang, Yakun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimising probabilistic models is a well-studied field in statistics.
However, its connection with the training of generative models remains largely
under-explored. In this paper, we show that the evolution of time-varying
generative models can be projected onto an exponential family manifold,
naturally creating a link between the parameters of a generative model and
those of a probabilistic model. We then train the generative model by moving
its projection on the manifold according to the natural gradient descent
scheme. This approach also allows us to approximate the natural gradient of the
KL divergence efficiently without relying on MCMC for intractable models.
Furthermore, we propose particle versions of the algorithm, which feature
closed-form update rules for any parametric model within the exponential
family. Through toy and real-world experiments, we validate the effectiveness
of the proposed algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Additive Models with Unobserved Causal Paths and Backdoor Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thong Pham, Takashi Nicholas Maeda, Shohei Shimizu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal additive models have been employed as tractable yet expressive
frameworks for causal discovery involving hidden variables. State-of-the-art
methodologies suggest that determining the causal relationship between a pair
of variables is infeasible in the presence of an unobserved backdoor or an
unobserved causal path. Contrary to this assumption, we theoretically show that
resolving the causal direction is feasible in certain scenarios by
incorporating two novel components into the theory. The first component
introduces a novel characterization of regression sets within independence
between regression residuals. The second component leverages conditional
independence among the observed variables. We also provide a search algorithm
that integrates these innovations and demonstrate its competitive performance
against existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FoQA: A Faroese Question-Answering <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annika Simonsen, Dan Saattrup Nielsen, Hafsteinn Einarsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FoQA, a Faroese extractive question-answering (QA) dataset with
2,000 samples, created using a semi-automated approach combining Large Language
Models (LLMs) and human validation. The dataset was generated from Faroese
Wikipedia articles using GPT-4-turbo for initial QA generation, followed by
question rephrasing to increase complexity and native speaker validation to
ensure quality. We provide baseline performance metrics for FoQA across
multiple models, including LLMs and BERT, demonstrating its effectiveness in
evaluating Faroese QA performance. The dataset is released in three versions: a
validated set of 2,000 samples, a complete set of all 10,001 generated samples,
and a set of 2,395 rejected samples for error analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for RESOURCEFUL workshop, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goedel-Prover: A Frontier Model for Open-Source Automated Theorem
  Proving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Goedel-Prover, an open-source large language model (LLM) that
achieves the state-of-the-art (SOTA) performance in automated formal proof
generation for mathematical problems. The key challenge in this field is the
scarcity of formalized math statements and proofs, which we tackle in the
following ways. We train statement formalizers to translate the natural
language math problems from Numina into formal language (Lean 4), creating a
dataset of 1.64 million formal statements. LLMs are used to check that the
formal statements accurately preserve the content of the original natural
language problems. We then iteratively build a large dataset of formal proofs
by training a series of provers. Each prover succeeds in proving many
statements that the previous ones could not, and these new proofs are added to
the training set for the next prover. The final prover outperforms all existing
open-source models in whole-proof generation. On the miniF2F benchmark, it
achieves a 57.6% success rate (Pass@32), exceeding the previous best
open-source model by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7
problems (Pass@512), ranking first on the leaderboard. Furthermore, it
generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the
15.7K produced by earlier works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consistency Training with Physical Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che-Chia Chang, Chen-Yang Dai, Te-Sheng Lin, Ming-Chih Lai, Chieh-Hsin Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a physics-aware Consistency Training (CT) method that accelerates
sampling in Diffusion Models with physical constraints. Our approach leverages
a two-stage strategy: (1) learning the noise-to-data mapping via CT, and (2)
incorporating physics constraints as a regularizer. Experiments on toy examples
show that our method generates samples in a single step while adhering to the
imposed constraints. This approach has the potential to efficiently solve
partial differential equations (PDEs) using deep generative modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Value Decomposition Networks with Networked Agents <span class="chip">AAMAS
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guilherme S. Varela, Alberto Sardinha, Francisco S. Melo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of distributed training under partial
observability, whereby cooperative multi-agent reinforcement learning agents
(MARL) maximize the expected cumulative joint reward. We propose distributed
value decomposition networks (DVDN) that generate a joint Q-function that
factorizes into agent-wise Q-functions. Whereas the original value
decomposition networks rely on centralized training, our approach is suitable
for domains where centralized training is not possible and agents must learn by
interacting with the physical environment in a decentralized manner while
communicating with their peers. DVDN overcomes the need for centralized
training by locally estimating the shared objective. We contribute with two
innovative algorithms, DVDN and DVDN (GT), for the heterogeneous and
homogeneous agents settings respectively. Empirically, both algorithms
approximate the performance of value decomposition networks, in spite of the
information loss during communication, as demonstrated in ten MARL tasks in
three standard environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures, to be published in Proceedings of the 24th
  International Conference on Autonomous Agents and Multiagent Systems (AAMAS
  2025), Detroit, Michigan, USA, May 19 - 23, 2025, IFAAMAS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Timing Residuals: Advancing PET Detectors with Explicit TOF
  Corrections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephan Naunheim, Luis Lopes de Paiva, Vanessa Nadig, Yannick Kuhl, Stefan Gundacker, Florian Mueller, Volkmar Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PET is a functional imaging method that visualizes metabolic processes. TOF
information can be derived from coincident detector signals and incorporated
into image reconstruction to enhance the SNR. PET detectors are typically
assessed by their CTR, but timing performance is degraded by various factors.
Research on timing calibration seeks to mitigate these degradations and restore
accurate timing information. While many calibration methods use analytical
approaches, machine learning techniques have recently gained attention due to
their flexibility. We developed a residual physics-based calibration approach
that combines prior domain knowledge with the power of machine learning models.
This approach begins with an initial analytical calibration addressing
first-order skews. The remaining deviations, regarded as residual effects, are
used to train machine learning models to eliminate higher-order skews. The key
advantage is that the experimenter guides the learning process through the
definition of timing residuals. In earlier studies, we developed models that
directly predicted the expected time difference, which offered corrections only
implicitly (implicit correction models). In this study, we introduce a new
definition for timing residuals, enabling us to train models that directly
predict correction values (explicit correction models). The explicit correction
approach significantly simplifies data acquisition, improves linearity, and
enhances timing performance from $371 \pm 6$ ps to $281 \pm 5$ ps for
coincidences from 430 keV to 590 keV. Additionally, the new definition reduces
model size, making it suitable for high-throughput applications like PET
scanners. Experiments were conducted using two detector stacks composed of $4
\times 4$ LYSO:Ce,Ca crystals ($3.8\times 3.8\times 20$ mm$^{3}$) coupled to $4
\times 4$ Broadcom NUV-MT SiPMs and digitized with the TOFPET2 ASIC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal-Informed Contrastive Learning: Towards Bias-Resilient
  Pre-training under Concept Drift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Yang, Jie Lu, En Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of large-scale contrastive pre-training propelled by top-tier
datasets has reached a transition point in the scaling law. Consequently,
sustaining and enhancing a model's pre-training capabilities in drift
environments have surfaced as a notable challenge. In this paper, we initially
uncover that contrastive pre-training methods are significantly impacted by
concept drift wherein distributions change unpredictably, resulting in notable
biases in the feature space of the pre-trained model. Empowered by causal
inference, we construct a structural causal graph to analyze the impact of
concept drift to contrastive pre-training systemically, and propose the causal
interventional contrastive objective. Upon achieving this, we devise a
resilient contrastive pre-training approach to accommodate the data stream of
concept drift, with simple and scalable implementation. Extensive experiments
on various downstream tasks demonstrate our resilient contrastive pre-training
effectively mitigates the bias stemming from the concept drift data stream.
Codes are available at https://anonymous.4open.science/r/ResilientCL/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tractable Transformers for Flexible Conditional Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anji Liu, Xuejie Liu, Dayuan Zhao, Mathias Niepert, Yitao Liang, Guy Van den Broeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-autoregressive (NAR) generative models are valuable because they can
handle diverse conditional generation tasks in a more principled way than their
autoregressive (AR) counterparts, which are constrained by sequential
dependency requirements. Recent advancements in NAR models, such as diffusion
language models, have demonstrated superior performance in unconditional
generation compared to AR models (e.g., GPTs) of similar sizes. However, such
improvements do not always lead to improved conditional generation performance.
We show that a key reason for this gap is the difficulty in generalizing to
conditional probability queries unseen during training. As a result, strong
unconditional generation performance does not guarantee high-quality
conditional generation. This paper proposes Tractable Transformers
(Tracformer), a Transformer-based generative model that is more robust to
different conditional generation tasks. Unlike existing models that rely solely
on global contextual features derived from full inputs, Tracformers incorporate
a sparse Transformer encoder to capture both local and global contextual
information. This information is routed through a decoder for conditional
generation. Empirical results demonstrate that Tracformers achieve
state-of-the-art conditional generation performance on text modeling compared
to recent diffusion and AR model baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithmic Aspects of Strategic Trading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Kearns, Mirah Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic trading in modern financial markets is widely acknowledged to
exhibit strategic, game-theoretic behaviors whose complexity can be difficult
to model. A recent series of papers (Chriss, 2024b,c,a, 2025) has made progress
in the setting of trading for position building. Here parties wish to buy or
sell a fixed number of shares in a fixed time period in the presence of both
temporary and permanent market impact, resulting in exponentially large
strategy spaces. While these papers primarily consider the existence and
structural properties of equilibrium strategies, in this work we focus on the
algorithmic aspects of the proposed model. We give an efficient algorithm for
computing best responses, and show that while the temporary impact only setting
yields a potential game, best response dynamics do not generally converge for
the general setting, for which no fast algorithm for (Nash) equilibrium
computation is known. This leads us to consider the broader notion of Coarse
Correlated Equilibria (CCE), which we show can be computed efficiently via an
implementation of Follow the Perturbed Leader (FTPL). We illustrate the model
and our results with an experimental investigation, where FTPL exhibits
interesting behavior in different regimes of the relative weighting between
temporary and permanent market impact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DMWM: Dual-Mind World Model with Long-Term Imagination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imagination in world models is crucial for enabling agents to learn
long-horizon policy in a sample-efficient manner. Existing recurrent
state-space model (RSSM)-based world models depend on single-step statistical
inference to capture the environment dynamics, and, hence, they are unable to
perform long-term imagination tasks due to the accumulation of prediction
errors. Inspired by the dual-process theory of human cognition, we propose a
novel dual-mind world model (DMWM) framework that integrates logical reasoning
to enable imagination with logical consistency. DMWM is composed of two
components: an RSSM-based System 1 (RSSM-S1) component that handles state
transitions in an intuitive manner and a logic-integrated neural network-based
System 2 (LINN-S2) component that guides the imagination process through
hierarchical deep logical reasoning. The inter-system feedback mechanism is
designed to ensure that the imagination process follows the logical rules of
the real environment. The proposed framework is evaluated on benchmark tasks
that require long-term planning from the DMControl suite. Extensive
experimental results demonstrate that the proposed framework yields significant
improvements in terms of logical coherence, trial efficiency, data efficiency
and long-term imagination over the state-of-the-art world models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEMU: Singular Value Decomposition for Efficient Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Sendera, Łukasz Struski, Kamil Książek, Kryspin Musiol, Jacek Tabor, Dawid Rymarczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the capabilities of generative foundational models have advanced
rapidly in recent years, methods to prevent harmful and unsafe behaviors remain
underdeveloped. Among the pressing challenges in AI safety, machine unlearning
(MU) has become increasingly critical to meet upcoming safety regulations. Most
existing MU approaches focus on altering the most significant parameters of the
model. However, these methods often require fine-tuning substantial portions of
the model, resulting in high computational costs and training instabilities,
which are typically mitigated by access to the original training dataset.
  In this work, we address these limitations by leveraging Singular Value
Decomposition (SVD) to create a compact, low-dimensional projection that
enables the selective forgetting of specific data points. We propose Singular
Value Decomposition for Efficient Machine Unlearning (SEMU), a novel approach
designed to optimize MU in two key aspects. First, SEMU minimizes the number of
model parameters that need to be modified, effectively removing unwanted
knowledge while making only minimal changes to the model's weights. Second,
SEMU eliminates the dependency on the original training dataset, preserving the
model's previously acquired knowledge without additional data requirements.
  Extensive experiments demonstrate that SEMU achieves competitive performance
while significantly improving efficiency in terms of both data usage and the
number of modified parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Generalization Error of Markov algorithms through
  Poissonization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Dupuis, Maxime Haddouche, George Deligiannidis, Umut Simsekli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using continuous-time stochastic differential equation (SDE) proxies to
stochastic optimization algorithms has proven fruitful for understanding their
generalization abilities. A significant part of these approaches are based on
the so-called ``entropy flows'', which greatly simplify the generalization
analysis. Unfortunately, such well-structured entropy flows cannot be obtained
for most discrete-time algorithms, and the existing SDE approaches remain
limited to specific noise and algorithmic structures. We aim to alleviate this
issue by introducing a generic framework for analyzing the generalization error
of Markov algorithms through `Poissonization', a continuous-time approximation
of discrete-time processes with formal approximation guarantees. Through this
approach, we first develop a novel entropy flow, which directly leads to
PAC-Bayesian generalization bounds. We then draw novel links to modified
versions of the celebrated logarithmic Sobolev inequalities (LSI), identify
cases where such LSIs are satisfied, and obtain improved bounds. Beyond its
generality, our framework allows exploiting specific properties of learning
algorithms. In particular, we incorporate the noise structure of different
algorithm types - namely, those with additional noise injections (noisy) and
those without (non-noisy) - through various technical tools. This illustrates
the capacity of our methods to achieve known (yet, Poissonized) and new
generalization bounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Modeling with Bayesian Sample Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marten Lienen, Marcel Kollovieh, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We derive a novel generative model from the simple act of Gaussian posterior
inference. Treating the generated sample as an unknown variable to infer lets
us formulate the sampling process in the language of Bayesian probability. Our
model uses a sequence of prediction and posterior update steps to narrow down
the unknown sample from a broad initial belief. In addition to a rigorous
theoretical analysis, we establish a connection between our model and diffusion
models and show that it includes Bayesian Flow Networks (BFNs) as a special
case. In our experiments, we demonstrate improved performance over both BFNs
and Variational Diffusion Models, achieving competitive likelihood scores on
CIFAR10 and ImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single-Step Consistent Diffusion Samplers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Jutras-Dubé, Patrick Pynadath, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling from unnormalized target distributions is a fundamental yet
challenging task in machine learning and statistics. Existing sampling
algorithms typically require many iterative steps to produce high-quality
samples, leading to high computational costs that limit their practicality in
time-sensitive or resource-constrained settings. In this work, we introduce
consistent diffusion samplers, a new class of samplers designed to generate
high-fidelity samples in a single step. We first develop a distillation
algorithm to train a consistent diffusion sampler from a pretrained diffusion
model without pre-collecting large datasets of samples. Our algorithm leverages
incomplete sampling trajectories and noisy intermediate states directly from
the diffusion process. We further propose a method to train a consistent
diffusion sampler from scratch, fully amortizing exploration by training a
single model that both performs diffusion sampling and skips intermediate steps
using a self-consistency loss. Through extensive experiments on a variety of
unnormalized distributions, we show that our approach yields high-fidelity
samples using less than 1% of the network evaluations required by traditional
diffusion samplers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its
  Hybrid 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weigao Sun, Disen Lan, Yiran Zhong, Xiaoye Qu, Yu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear sequence modeling approaches, such as linear attention, provide
advantages like linear-time training and constant-memory inference over
sequence lengths. However, existing sequence parallelism (SP) methods are
either not optimized for the right-product-first feature of linear attention or
use a ring-style communication strategy, which results in lower computation
parallelism, limits their scalability for longer sequences in distributed
systems. In this paper, we introduce LASP-2, a new SP method to enhance both
communication and computation parallelism when training linear attention
transformer models with very-long input sequences. Compared to previous work
LASP, LASP-2 rethinks the minimal communication requirement for SP on linear
attention layers, reorganizes the whole communication-computation workflow of
LASP. In this way, only one single AllGather collective communication is needed
on intermediate memory states, whose sizes are independent of the sequence
length, leading to significant improvements of both communication and
computation parallelism, as well as their overlap. Additionally, we extend
LASP-2 to LASP-2H by applying similar communication redesign to standard
attention modules, offering an efficient SP solution for hybrid models that
blend linear and standard attention layers. Our evaluation on a Linear-Llama3
model, a variant of Llama3 with linear attention replacing standard attention,
demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2
achieves training speed improvements of 15.2% over LASP and 36.6% over Ring
Attention, with a sequence length of 2048K across 64 GPUs. The Code is released
as a part of: https://github.com/OpenSparseLLMs/Linear-MoE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, 17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Learning is Needed to Efficiently Learn Parity Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaomengxi Han, Debarghya Ghoshdastidar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers, with their attention mechanisms, have emerged as the
state-of-the-art architectures of sequential modeling and empirically
outperform feed-forward neural networks (FFNNs) across many fields, such as
natural language processing and computer vision. However, their generalization
ability, particularly for low-sensitivity functions, remains less studied. We
bridge this gap by analyzing transformers on the $k$-parity problem. Daniely
and Malach (NeurIPS 2020) show that FFNNs with one hidden layer and $O(nk^7
\log k)$ parameters can learn $k$-parity, where the input length $n$ is
typically much larger than $k$. In this paper, we prove that FFNNs require at
least $\Omega(n)$ parameters to learn $k$-parity, while transformers require
only $O(k)$ parameters, surpassing the theoretical lower bound needed by FFNNs.
We further prove that this parameter efficiency cannot be achieved with fixed
attention heads. Our work establishes transformers as theoretically superior to
FFNNs in learning parity function, showing how their attention mechanisms
enable parameter-efficient generalization in functions with low sensitivity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Early Stopping Against Label Noise Without Validation Data <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suqin Yuan, Lei Feng, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early stopping methods in deep learning face the challenge of balancing the
volume of training and validation data, especially in the presence of label
noise. Concretely, sparing more data for validation from training data would
limit the performance of the learned model, yet insufficient validation data
could result in a sub-optimal selection of the desired model. In this paper, we
propose a novel early stopping method called Label Wave, which does not require
validation data for selecting the desired model in the presence of label noise.
It works by tracking the changes in the model's predictions on the training set
during the training process, aiming to halt training before the model unduly
fits mislabeled data. This method is empirically supported by our observation
that minimum fluctuations in predictions typically occur at the training epoch
before the model excessively fits mislabeled data. Through extensive
experiments, we show both the effectiveness of the Label Wave method across
various settings and its capability to enhance the performance of existing
methods for learning with noisy labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HGTUL: A Hypergraph-based Model For Trajectory User Linking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengjie Chang, Xinning Zhu, Zheng Hu, Yang Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory User Linking (TUL), which links anonymous trajectories with users
who generate them, plays a crucial role in modeling human mobility. Despite
significant advancements in this field, existing studies primarily neglect the
high-order inter-trajectory relationships, which represent complex associations
among multiple trajectories, manifested through multi-location co-occurrence
patterns emerging when trajectories intersect at various Points of Interest
(POIs). Furthermore, they also overlook the variable influence of POIs on
different trajectories, as well as the user class imbalance problem caused by
disparities in user activity levels and check-in frequencies. To address these
limitations, we propose a novel HyperGraph-based multi-perspective Trajectory
User Linking model (HGTUL). Our model learns trajectory representations from
both relational and spatio-temporal perspectives: (1) it captures high-order
associations among trajectories by constructing a trajectory hypergraph and
leverages a hypergraph attention network to learn the variable impact of POIs
on trajectories; (2) it models the spatio-temporal characteristics of
trajectories by incorporating their temporal and spatial information into a
sequential encoder. Moreover, we design a data balancing method to effectively
address the user class imbalance problem and experimentally validate its
significance in TUL. Extensive experiments on three real-world datasets
demonstrate that HGTUL outperforms state-of-the-art baselines, achieving
improvements of 2.57%~20.09% and 5.68%~26.00% in ACC@1 and Macro-F1 metrics,
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instance-dependent Early Stopping <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suqin Yuan, Runqi Lin, Lei Feng, Bo Han, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In machine learning practice, early stopping has been widely used to
regularize models and can save computational costs by halting the training
process when the model's performance on a validation set stops improving.
However, conventional early stopping applies the same stopping criterion to all
instances without considering their individual learning statuses, which leads
to redundant computations on instances that are already well-learned. To
further improve the efficiency, we propose an Instance-dependent Early Stopping
(IES) method that adapts the early stopping mechanism from the entire training
set to the instance level, based on the core principle that once the model has
mastered an instance, the training on it should stop. IES considers an instance
as mastered if the second-order differences of its loss value remain within a
small range around zero. This offers a more consistent measure of an instance's
learning status compared with directly using the loss value, and thus allows
for a unified threshold to determine when an instance can be excluded from
further backpropagation. We show that excluding mastered instances from
backpropagation can increase the gradient norms, thereby accelerating the
decrease of the training loss and speeding up the training process. Extensive
experiments on benchmarks demonstrate that IES method can reduce
backpropagation instances by 10%-50% while maintaining or even slightly
improving the test accuracy and transfer learning performance of a model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Heat Demand Forecasting with Attention Mechanisms:
  Opportunities and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adithya Ramachandran, Thorkil Flensmark B. Neergaard, Andreas Maier, Siming Bayer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global leaders and policymakers are unified in their unequivocal commitment
to decarbonization efforts in support of Net-Zero agreements. District Heating
Systems (DHS), while contributing to carbon emissions due to the continued
reliance on fossil fuels for heat production, are embracing more sustainable
practices albeit with some sense of vulnerability as it could constrain their
ability to adapt to dynamic demand and production scenarios. As demographic
demands grow and renewables become the central strategy in decarbonizing the
heating sector, the need for accurate demand forecasting has intensified.
Advances in digitization have paved the way for Machine Learning (ML) based
solutions to become the industry standard for modeling complex time series
patterns. In this paper, we focus on building a Deep Learning (DL) model that
uses deconstructed components of independent and dependent variables that
affect heat demand as features to perform multi-step ahead forecasting of head
demand. The model represents the input features in a time-frequency space and
uses an attention mechanism to generate accurate forecasts. The proposed method
is evaluated on a real-world dataset and the forecasting performance is
assessed against LSTM and CNN-based forecasting models. Across different supply
zones, the attention-based models outperforms the baselines quantitatively and
qualitatively, with an Mean Absolute Error (MAE) of 0.105 with a standard
deviation of 0.06kW h and a Mean Absolute Percentage Error (MAPE) of 5.4% with
a standard deviation of 2.8%, in comparison the second best model with a MAE of
0.10 with a standard deviation of 0.06kW h and a MAPE of 5.6% with a standard
deviation of 3%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Deep Learning Models with Norm-Constrained LMOs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study optimization methods that leverage the linear
minimization oracle (LMO) over a norm-ball. We propose a new stochastic family
of algorithms that uses the LMO to adapt to the geometry of the problem and,
perhaps surprisingly, show that they can be applied to unconstrained problems.
The resulting update rule unifies several existing optimization methods under a
single framework. Furthermore, we propose an explicit choice of norm for deep
architectures, which, as a side benefit, leads to the transferability of
hyperparameters across model sizes. Experimentally, we demonstrate significant
speedups on nanoGPT training without any reliance on Adam. The proposed method
is memory-efficient, requiring only one set of model weights and one set of
gradients, which can be stored in half-precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting the future development in quality and value of professional
  football players for applications in team management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koen W. van Arem, Floris Goes-Smit, Jakob Söhl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfers in professional football (soccer) are risky investments because of
the large transfer fees and high risks involved. Although data-driven models
can be used to improve transfer decisions, existing models focus on describing
players' historical progress, leaving their future performance unknown.
Moreover, recent developments have called for the use of explainable models
combined with uncertainty quantification of predictions. This paper assesses
explainable machine learning models based on predictive accuracy and
uncertainty quantification methods for the prediction of the future development
in quality and transfer value of professional football players. Using a
historical data set of data-driven indicators describing player quality and the
transfer value of a football player, the models are trained to forecast player
quality and player value one year ahead. These two prediction problems
demonstrate the efficacy of tree-based models, particularly random forest and
XGBoost, in making accurate predictions. In general, the random forest model is
found to be the most suitable model because it provides accurate predictions as
well as an uncertainty quantification method that naturally arises from the
bagging procedure of the random forest model. Additionally, our research shows
that the development of player performance contains nonlinear patterns and
interactions between variables, and that time series information can provide
useful information for the modeling of player performance metrics. Our research
provides models to help football clubs make more informed, data-driven transfer
decisions by forecasting player quality and transfer value.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article itself is on the pages 1-27. The data set used in this
  article is described in the appendix at the pages 28-35</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NatureLM: Deciphering the Language of Nature for Scientific Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, Zekun Guo, Yeqi Bai, Pan Deng, Yaosen Min, Ziheng Lu, Hongxia Hao, Han Yang, Jielan Li, Chang Liu, Jia Zhang, Jianwei Zhu, Kehan Wu, Wei Zhang, Kaiyuan Gao, Qizhi Pei, Qian Wang, Xixian Liu, Yanting Li, Houtian Zhu, Yeqing Lu, Mingqian Ma, Zun Wang, Tian Xie, Krzysztof Maziarz, Marwin Segler, Zhao Yang, Zilong Chen, Yu Shi, Shuxin Zheng, Lijun Wu, Chen Hu, Peggy Dai, Tie-Yan Liu, Haiguang Liu, Tao Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have revolutionized natural language processing and
artificial intelligence, significantly enhancing how machines comprehend and
generate human languages. Inspired by the success of these foundation models,
researchers have developed foundation models for individual scientific domains,
including small molecules, materials, proteins, DNA, and RNA. However, these
models are typically trained in isolation, lacking the ability to integrate
across different scientific domains. Recognizing that entities within these
domains can all be represented as sequences, which together form the "language
of nature", we introduce Nature Language Model (briefly, NatureLM), a
sequence-based science foundation model designed for scientific discovery.
Pre-trained with data from multiple scientific domains, NatureLM offers a
unified, versatile model that enables various applications including: (i)
generating and optimizing small molecules, proteins, RNA, and materials using
text instructions; (ii) cross-domain generation/design, such as
protein-to-molecule and protein-to-RNA generation; and (iii) achieving
state-of-the-art performance in tasks like SMILES-to-IUPAC translation and
retrosynthesis on USPTO-50k. NatureLM offers a promising generalist approach
for various scientific tasks, including drug discovery (hit
generation/optimization, ADMET optimization, synthesis), novel material design,
and the development of therapeutic proteins or nucleotides. We have developed
NatureLM models in different sizes (1 billion, 8 billion, and 46.7 billion
parameters) and observed a clear improvement in performance as the model size
increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>81 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Off-Policy Reinforcement Learning with Batch and Weight
  Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Palenicek, Florian Vogt, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning has achieved significant milestones, but sample
efficiency remains a bottleneck for real-world applications. Recently, CrossQ
has demonstrated state-of-the-art sample efficiency with a low update-to-data
(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with
higher UTD ratios. We identify challenges in the training dynamics, which are
emphasized by higher UTD ratios. To address these, we integrate weight
normalization into the CrossQ framework, a solution that stabilizes training,
has been shown to prevent potential loss of plasticity and keeps the effective
learning rate constant. Our proposed approach reliably scales with increasing
UTD ratios, achieving competitive performance across 25 challenging continuous
control tasks on the DeepMind Control Suite and Myosuite benchmarks, notably
the complex dog and humanoid environments. This work eliminates the need for
drastic interventions, such as network resets, and offers a simple yet robust
pathway for improving sample efficiency and scalability in model-free
reinforcement learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Devil is in the Prompts: De-Identification Traces Enhance
  Memorization Risks in Synthetic Chest X-Ray Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raman Dutt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models, particularly text-to-image (T2I) diffusion models, play a
crucial role in medical image analysis. However, these models are prone to
training data memorization, posing significant risks to patient privacy.
Synthetic chest X-ray generation is one of the most common applications in
medical image analysis with the MIMIC-CXR dataset serving as the primary data
repository for this task. This study adopts a data-driven approach and presents
the first systematic attempt to identify prompts and text tokens in MIMIC-CXR
that contribute the most to training data memorization. Our analysis reveals an
unexpected finding: prompts containing traces of de-identification procedures
are among the most memorized, with de-identification markers contributing the
most. Furthermore, we also find existing inference-time memorization mitigation
strategies are ineffective and fail to sufficiently reduce the model's reliance
on memorized text tokens highlighting a broader issue in T2I synthesis with
MIMIC-CXR. On this front, we propose actionable strategies to enhance privacy
and improve the reliability of generative models in medical imaging. Finally,
our results provide a foundation for future work on developing and benchmarking
memorization mitigation techniques for synthetic chest X-ray generation using
the MIMIC-CXR dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Near-optimal, Scalable and Corruption-tolerant Framework for
  Stochastic Bandits: From Single-Agent to Multi-Agent and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Hu, Cheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate various stochastic bandit problems in the presence of
adversarial corruption. A seminal contribution to this area is the
BARBAR~\citep{gupta2019better} algorithm, which is both simple and efficient,
tolerating significant levels of corruption with nearly no degradation in
performance. However, its regret upper bound exhibits a complexity of $O(KC)$,
while the lower bound is $\Omega(C)$. In this paper, we enhance the BARBAR
algorithm by proposing a novel framework called BARBAT, which eliminates the
factor of $K$ and achieves an optimal regret bound up to a logarithmic factor.
We also demonstrate how BARBAT can be extended to various settings, including
graph bandits, combinatorial semi-bandits, batched bandits and multi-agent
bandits. In comparison to the Follow-The-Regularized-Leader (FTRL) family of
methods, which provide a best-of-both-worlds guarantee, our approach is more
efficient and parallelizable. Notably, FTRL-based methods face challenges in
scaling to batched and multi-agent settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Metric Space Embedding by Unbalanced OT with Gromov-Wasserstein
  Marginal Penalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Beier, Moritz Piening, Robert Beinert, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for unsupervised alignment of heterogeneous
datasets, which maps data from two different domains without any known
correspondences to a common metric space. Our method is based on an unbalanced
optimal transport problem with Gromov-Wasserstein marginal penalization. It can
be seen as a counterpart to the recently introduced joint multidimensional
scaling method. We prove that there exists a minimizer of our functional and
that for penalization parameters going to infinity, the corresponding sequence
of minimizers converges to a minimizer of the so-called embedded Wasserstein
distance. Our model can be reformulated as a quadratic, multi-marginal,
unbalanced optimal transport problem, for which a bi-convex relaxation admits a
numerical solver via block-coordinate descent. We provide numerical examples
for joint embeddings in Euclidean as well as non-Euclidean spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Language's Fractal Geometry with Recursive Inference Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Alabdulmohsin, Xiaohua Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research in language modeling reveals two scaling effects: the
well-known improvement from increased training compute, and a lesser-known
boost from applying more sophisticated or computationally intensive inference
methods. Inspired by recent findings on the fractal geometry of language, we
introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe
for scaling inference time. For a given fixed model architecture and training
compute budget, RINS substantially improves language modeling performance. It
also generalizes beyond pure language tasks, delivering gains in multimodal
systems, including a +2% improvement in 0-shot ImageNet accuracy for
SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS
improves both the asymptotic performance limits and the scaling exponents.
These advantages are maintained even when compared to state-of-the-art
recursive techniques like the "repeat-all-over" (RAO) strategy in Mobile LLM.
Finally, stochastic RINS not only can enhance performance further but also
provides the flexibility to optionally forgo increased inference computation at
test time with minimal performance degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Graph Networks (UGN): A Deep Neural Framework for Solving Graph
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rudrajit Dawn, Madhusudan Ghosh, Partha Basuchowdhuri, Sudip Kumar Naskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have enabled researchers to create powerful generalized
frameworks, such as transformers, that can be used to solve well-studied
problems in various application domains, such as text and image. However, such
generalized frameworks are not available for solving graph problems. Graph
structures are ubiquitous in many applications around us and many graph
problems have been widely studied over years. In recent times, there has been a
surge in deep neural network based approaches to solve graph problems, with
growing availability of graph structured datasets across diverse domains.
Nevertheless, existing methods are mostly tailored to solve a specific task and
lack the capability to create a generalized model leading to solutions for
different downstream tasks. In this work, we propose a novel,
resource-efficient framework named \emph{U}nified \emph{G}raph \emph{N}etwork
(UGN) by leveraging the feature extraction capability of graph convolutional
neural networks (GCN) and 2-dimensional convolutional neural networks (Conv2D).
UGN unifies various graph learning tasks, such as link prediction, node
classification, community detection, graph-to-graph translation, knowledge
graph completion, and more, within a cohesive framework, while exercising
minimal task-specific extensions (e.g., formation of supernodes for coarsening
massive networks to increase scalability, use of \textit{mean target
connectivity matrix} (MTCM) representation for achieving scalability in graph
translation task, etc.) to enhance the generalization capability of graph
learning and analysis. We test the novel UGN framework for six uncorrelated
graph problems, using twelve different datasets. Experimental results show that
UGN outperforms the state-of-the-art baselines by a significant margin on ten
datasets, while producing comparable results on the remaining dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Training-Conditional Conformal Prediction and Binomial Proportion
  Confidence Intervals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rudi Coppola, Manuel Mazo Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the expectation of a Bernoulli random variable based on N
independent trials is a classical problem in statistics, typically addressed
using Binomial Proportion Confidence Intervals (BPCI). In the control systems
community, many critical tasks-such as certifying the statistical safety of
dynamical systems-can be formulated as BPCI problems. Conformal Prediction
(CP), a distribution-free technique for uncertainty quantification, has gained
significant attention in recent years and has been applied to various control
systems problems, particularly to address uncertainties in learned dynamics or
controllers. A variant known as training-conditional CP was recently employed
to tackle the problem of safety certification. In this note, we highlight that
the use of training-conditional CP in this context does not provide valid
safety guarantees. We demonstrate why CP is unsuitable for BPCI problems and
argue that traditional BPCI methods are better suited for statistical safety
certification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Sketch: Enhancing Network Sketches with LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanpeng Li, Zhen Xu, Zongwei Lv, Yannan Hu, Yong Cui, Tong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network stream mining is fundamental to many network operations. Sketches, as
compact data structures that offer low memory overhead with bounded accuracy,
have emerged as a promising solution for network stream mining. Recent studies
attempt to optimize sketches using machine learning; however, these approaches
face the challenges of lacking adaptivity to dynamic networks and incurring
high training costs. In this paper, we propose LLM-Sketch, based on the insight
that fields beyond the flow IDs in packet headers can also help infer flow
sizes. By using a two-tier data structure and separately recording large and
small flows, LLM-Sketch improves accuracy while minimizing memory usage.
Furthermore, it leverages fine-tuned large language models (LLMs) to reliably
estimate flow sizes. We evaluate LLM-Sketch on three representative tasks, and
the results demonstrate that LLM-Sketch outperforms state-of-the-art methods by
achieving a $7.5\times$ accuracy improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Patterns Behind Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Chengcheng Ma, XuanQi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive framework for time series prediction
using a hybrid model that combines ARIMA and LSTM. The model incorporates
feature engineering techniques, including embedding and PCA, to transform raw
data into a lower-dimensional representation while retaining key information.
The embedding technique is used to convert categorical data into continuous
vectors, facilitating the capture of complex relationships. PCA is applied to
reduce dimensionality and extract principal components, enhancing model
performance and computational efficiency. To handle both linear and nonlinear
patterns in the data, the ARIMA model captures linear trends, while the LSTM
model models complex nonlinear dependencies. The hybrid model is trained on
historical data and achieves high accuracy, as demonstrated by low RMSE and MAE
scores. Additionally, the paper employs the run test to assess the randomness
of sequences, providing insights into the underlying patterns. Ablation studies
are conducted to validate the roles of different components in the model,
demonstrating the significance of each module. The paper also utilizes the SHAP
method to quantify the impact of traditional advantages on the predicted
results, offering a detailed understanding of feature importance. The KNN
method is used to determine the optimal prediction interval, further enhancing
the model's accuracy. The results highlight the effectiveness of combining
traditional statistical methods with modern deep learning techniques for robust
time series forecasting in Sports.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn
  More 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xialie Zhuang, Zhikai Jia, Jianjin Li, Zhenyu Zhang, Li Shen, Zheng Cao, Shiwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are discovered to suffer from accurately
retrieving key information. To address this, we propose Mask-Enhanced
Autoregressive Prediction (MEAP), a simple yet effective training paradigm that
seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction
(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,
MEAP first randomly masks a small fraction of input tokens and then directly
performs the standard next-token prediction autoregressive using a decoder-only
Transformer. MEAP eliminates the need for bidirectional attention or
encoder-decoder architectures for MLM, incurring no additional computational
overhead during pre-training or inference. Intensive experiments demonstrate
that MEAP substantially outperforms NTP on key information retrieval and
long-context reasoning tasks, while performing on par or better on commonsense
reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,
where it shows remarkable advantages in lost-in-the-middle scenarios,
outperforming NTP by 11.77 percentage points. Our analysis indicates that
MEAP's effectiveness arises from its ability to promote more distinguishable
attention scores by concentrating on a reduced set of non-masked tokens. This
mechanism improves the model's focus on task-relevant signals while mitigating
the influence of peripheral context. These findings position MEAP as a
promising training paradigm for large language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physiome-ODE: A Benchmark for Irregularly Sampled Multivariate Time
  Series Forecasting Based on Biological ODEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Klötergens, Vijaya Krishna Yalavarthi, Randolf Scholz, Maximilian Stubbemann, Stefan Born, Lars Schmidt-Thieme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for forecasting irregularly sampled time series with
missing values predominantly rely on just four datasets and a few small toy
examples for evaluation. While ordinary differential equations (ODE) are the
prevalent models in science and engineering, a baseline model that forecasts a
constant value outperforms ODE-based models from the last five years on three
of these existing datasets. This unintuitive finding hampers further research
on ODE-based models, a more plausible model family. In this paper, we develop a
methodology to generate irregularly sampled multivariate time series (IMTS)
datasets from ordinary differential equations and to select challenging
instances via rejection sampling. Using this methodology, we create
Physiome-ODE, a large and sophisticated benchmark of IMTS datasets consisting
of 50 individual datasets, derived from real-world ordinary differential
equations from research in biology. Physiome-ODE is the first benchmark for
IMTS forecasting that we are aware of and an order of magnitude larger than the
current evaluation setting of four datasets. Using our benchmark Physiome-ODE,
we show qualitatively completely different results than those derived from the
current four datasets: on Physiome-ODE ODE-based models can play to their
strength and our benchmark can differentiate in a meaningful way between
different IMTS forecasting models. This way, we expect to give a new impulse to
research on ODE-based time series modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Adaptive Moment Optimization via Preconditioner
  Diagonalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Son Nguyen, Bo Liu, Lizhang Chen, Qiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern adaptive optimization methods, such as Adam and its variants, have
emerged as the most widely used tools in deep learning over recent years. These
algorithms offer automatic mechanisms for dynamically adjusting the update step
based on estimates of gradient statistics. Compared to traditional algorithms
like Stochastic Gradient Descent, these adaptive methods are typically more
robust to model scale and hyperparameter tuning. However, the gradient
statistics employed by these methods often do not leverage sufficient gradient
covariance information, leading to suboptimal updates in certain directions of
the parameter space and potentially slower convergence. In this work, we keep
track of such covariance statistics in the form of a structured preconditioner
matrix. Unlike other works, our approach does not apply direct approximations
to estimate this matrix. We instead implement an invertible transformation that
maps the preconditioner matrix into a new space where it becomes approximately
diagonal. This enables a diagonal approximation of the preconditioner matrix in
the transformed space, offering several computational advantages. Empirical
results show that our approach can substantially enhance the convergence speed
of modern adaptive optimizers. Notably, for large language models like LLaMA,
we can achieve a speedup of 2x compared to the baseline Adam. Additionally, our
method can be integrated with memory-efficient optimizers like Adafactor to
manage computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overfitting Regimes of Nadaraya-Watson Interpolators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Barzilai, Guy Kornowski, Ohad Shamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been much interest in understanding the
generalization behavior of interpolating predictors, which overfit on noisy
training data. Whereas standard analyses are concerned with whether a method is
consistent or not, recent observations have shown that even inconsistent
predictors can generalize well. In this work, we revisit the classic
interpolating Nadaraya-Watson (NW) estimator (also known as Shepard's method),
and study its generalization capabilities through this modern viewpoint. In
particular, by varying a single bandwidth-like hyperparameter, we prove the
existence of multiple overfitting behaviors, ranging non-monotonically from
catastrophic, through benign, to tempered. Our results highlight how even
classical interpolating methods can exhibit intricate generalization behaviors.
Numerical experiments complement our theory, demonstrating the same phenomena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma
  Turbulence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Galletti, Fabian Paischer, Paul Setinek, William Hornsby, Lorenzo Zanisi, Naomi Carey, Stanislas Pamela, Johannes Brandstetter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nuclear fusion plays a pivotal role in the quest for reliable and sustainable
energy production. A major roadblock to achieving commercially viable fusion
power is understanding plasma turbulence, which can significantly degrade
plasma confinement. Modelling turbulence is crucial to design performing plasma
scenarios for next-generation reactor-class devices and current experimental
machines. The nonlinear gyrokinetic equation underpinning turbulence modelling
evolves a 5D distribution function over time. Solving this equation numerically
is extremely expensive, requiring up to weeks for a single run to converge,
making it unfeasible for iterative optimisation and control studies. In this
work, we propose a method for training neural surrogates for 5D gyrokinetic
simulations. Our method extends a hierarchical vision transformer to five
dimensions and is trained on the 5D distribution function for the adiabatic
electron approximation. We demonstrate that our model can accurately infer
downstream physical quantities such as heat flux time trace and electrostatic
potentials for single-step predictions two orders of magnitude faster than
numerical codes. Our work paves the way towards neural surrogates for plasma
turbulence simulations to accelerate deployment of commercial energy production
via nuclear fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages (+ references and appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logarithmic Regret for Online KL-Regularized Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heyang Zhao, Chenlu Ye, Wei Xiong, Quanquan Gu, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Reinforcement Learning from Human Feedback (RLHF) have
shown that KL-regularization plays a pivotal role in improving the efficiency
of RL fine-tuning for large language models (LLMs). Despite its empirical
advantage, the theoretical difference between KL-regularized RL and standard RL
remains largely under-explored. While there is a recent line of work on the
theoretical analysis of KL-regularized objective in decision making
\citep{xiong2024iterative, xie2024exploratory,zhao2024sharp}, these analyses
either reduce to the traditional RL setting or rely on strong coverage
assumptions. In this paper, we propose an optimism-based KL-regularized online
contextual bandit algorithm, and provide a novel analysis of its regret. By
carefully leveraging the benign optimization landscape induced by the
KL-regularization and the optimistic reward estimation, our algorithm achieves
an $\mathcal{O}\big(\eta\log (N_{\mathcal R} T)\cdot d_{\mathcal R}\big)$
logarithmic regret bound, where $\eta, N_{\mathcal R},T,d_{\mathcal R}$ denote
the KL-regularization parameter, the cardinality of the reward function class,
number of rounds, and the complexity of the reward function class. Furthermore,
we extend our algorithm and analysis to reinforcement learning by developing a
novel decomposition over transition steps and also obtain a similar logarithmic
regret bound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedAPA: Server-side Gradient-Based Adaptive Personalized Aggregation for
  Federated Learning on Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxia Sun, Aoxiang Sun, Siyi Pan, Zhixiao Fu, Jingcai Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized federated learning (PFL) tailors models to clients' unique data
distributions while preserving privacy. However, existing
aggregation-weight-based PFL methods often struggle with heterogeneous data,
facing challenges in accuracy, computational efficiency, and communication
overhead. We propose FedAPA, a novel PFL method featuring a server-side,
gradient-based adaptive aggregation strategy to generate personalized models,
by updating aggregation weights based on gradients of client-parameter changes
with respect to the aggregation weights in a centralized manner. FedAPA
guarantees theoretical convergence and achieves superior accuracy and
computational efficiency compared to 10 PFL competitors across three datasets,
with competitive communication overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurit Cohen-Inger, Yehonatan Elisha, Bracha Shapira, Lior Rokach, Seffi Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often appear to excel on public benchmarks, but
these high scores may mask an overreliance on dataset-specific surface cues
rather than true language understanding. We introduce the Chameleon Benchmark
Overfit Detector (C-BOD), a meta-evaluation framework that systematically
distorts benchmark prompts via a parametric transformation and detects
overfitting of LLMs. By rephrasing inputs while preserving their semantic
content and labels, C-BOD exposes whether a model's performance is driven by
memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our
method reveals an average performance degradation of 2.15% under modest
perturbations, with 20 out of 26 models exhibiting statistically significant
differences. Notably, models with higher baseline accuracy exhibit larger
performance differences under perturbation, and larger LLMs tend to be more
sensitive to rephrasings indicating that both cases may overrely on fixed
prompt patterns. In contrast, the Llama family and models with lower baseline
accuracy show insignificant degradation, suggesting reduced dependency on
superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows
easy integration into training pipelines to promote more robust language
understanding. Our findings challenge the community to look beyond leaderboard
scores and prioritize resilience and generalization in LLM evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Classifier-Free Guidance: High-Dimensional Theory and
  Non-Linear Generalizations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krunoslav Lehman Pavasovic, Jakob Verbeek, Giulio Biroli, Marc Mezard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have raised concerns about the effectiveness of
Classifier-Free Guidance (CFG), indicating that in low-dimensional settings, it
can lead to overshooting the target distribution and reducing sample diversity.
In this work, we demonstrate that in infinite and sufficiently high-dimensional
contexts CFG effectively reproduces the target distribution, revealing a
blessing-of-dimensionality result. Additionally, we explore finite-dimensional
effects, precisely characterizing overshoot and variance reduction. Based on
our analysis, we introduce non-linear generalizations of CFG. Through numerical
simulations on Gaussian mixtures and experiments on class-conditional and
text-to-image diffusion models, we validate our analysis and show that our
non-linear CFG offers improved flexibility and generation quality without
additional computation cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CapyMOA: Efficient Machine Learning for Data Streams in Python 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heitor Murilo Gomes, Anton Lee, Nuwan Gunasekara, Yibin Sun, Guilherme Weigert Cassales, Justin Liu, Marco Heyden, Vitor Cerqueira, Maroua Bahri, Yun Sing Koh, Bernhard Pfahringer, Albert Bifet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CapyMOA is an open-source library designed for efficient machine learning on
streaming data. It provides a structured framework for real-time learning and
evaluation, featuring a flexible data representation. CapyMOA includes an
extensible architecture that allows integration with external frameworks such
as MOA and PyTorch, facilitating hybrid learning approaches that combine
traditional online algorithms with deep learning techniques. By emphasizing
adaptability, scalability, and usability, CapyMOA allows researchers and
practitioners to tackle dynamic learning challenges across various domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Foundation Model for Physics-Informed Neural Networks:
  Multi-PDE Learning with Active Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keon Vin Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs) by embedding physical laws
into neural network training. However, traditional PINN models are typically
designed for single PDEs, limiting their generalizability across different
physical systems. In this work, we explore the potential of a foundation PINN
model capable of solving multiple PDEs within a unified architecture. We
investigate the efficacy of a single PINN framework trained on four distinct
PDEs-the Simple Harmonic Oscillator (SHO), the 1D Heat Equation, the 1D Wave
Equation, and the 2D Laplace Equation, demonstrating its ability to learn
diverse physical dynamics.
  To enhance sample efficiency, we incorporate Active Learning (AL) using Monte
Carlo (MC) Dropout-based uncertainty estimation, selecting the most informative
training samples iteratively. We evaluate different active learning strategies,
comparing models trained on 10%, 20%, 30%, 40%, and 50% of the full dataset,
and analyze their impact on solution accuracy. Our results indicate that
targeted uncertainty sampling significantly improves performance with fewer
training samples, leading to efficient learning across multiple PDEs.
  This work highlights the feasibility of a generalizable PINN-based foundation
model, capable of adapting to different physics-based problems without
redesigning network architectures. Our findings suggest that multi-PDE PINNs
with active learning can serve as an effective approach for reducing
computational costs while maintaining high accuracy in physics-based deep
learning applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Technical note on calibrating vision-language models under covariate
  shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behraj Khan, Rizwan Qureshi, Tahir Syed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite being a successful example of emerging capability, vision-language
foundation models for low-shot vision classification have a limited ability to
sufficiently generalize to the target data distribution due to sample poverty,
leading to sensitivity to variations in the data. A popular mitigation strategy
is finetuning over multiple datasets, but domain generalization is expensive
when practiced in this manner. This work examines both covariate shift between
pre-training data and the underspecified target data, and \textit{confidence
misalignment}, where the model's prediction confidence amplified by the limited
data availability. We propose \textit{Confidence-Calibrated Covariate Shift
Correction ($C3SC$)}, a unified framework to mitigate both covariate shift and
confidence misalignment. $C3SC$ leverages Fisher information penalty for
covariate shift correction and confidence misalignment penalty (CMP) to lower
confidence on misclassified examples. Experimental results across various
vision and covariate shift datasets demonstrates that $C3SC$ significantly
improves in calibration (ECE) by $5.82\%$ at maximum. $C3SC$ shows better
robustness as well by showing $3.5\%$ improvement in accuracy metric on
challenging covariate shift datasets, making $C3SC$ a promising solution for
reliable real-world vision-language low-shot applications under distribution
shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoENAS: Mixture-of-Expert based Neural Architecture Search for jointly
  Accurate, Fair, and Robust Edge Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lotfi Abdelkrim Mecharbat, Alberto Marchisio, Muhammad Shafique, Mohammad M. Ghassemi, Tuka Alhanai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a surge in optimizing edge Deep Neural Networks (DNNs) for
accuracy and efficiency using traditional optimization techniques such as
pruning, and more recently, employing automatic design methodologies. However,
the focus of these design techniques has often overlooked critical metrics such
as fairness, robustness, and generalization. As a result, when evaluating SOTA
edge DNNs' performance in image classification using the FACET dataset, we
found that they exhibit significant accuracy disparities (14.09%) across 10
different skin tones, alongside issues of non-robustness and poor
generalizability. In response to these observations, we introduce
Mixture-of-Experts-based Neural Architecture Search (MoENAS), an automatic
design technique that navigates through a space of mixture of experts to
discover accurate, fair, robust, and general edge DNNs. MoENAS improves the
accuracy by 4.02% compared to SOTA edge DNNs and reduces the skin tone accuracy
disparities from 14.09% to 5.60%, while enhancing robustness by 3.80% and
minimizing overfitting to 0.21%, all while keeping model size close to
state-of-the-art models average size (+0.4M). With these improvements, MoENAS
establishes a new benchmark for edge DNN design, paving the way for the
development of more inclusive and robust edge DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantification of model error for inverse problems in the Weak Neural
  Variational Inference framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent C. Scholz, P. S. Koutsourelakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel extension of the Weak Neural Variational Inference (WNVI)
framework for probabilistic material property estimation that explicitly
quantifies model errors in PDE-based inverse problems. Traditional approaches
assume the correctness of all governing equations, including potentially
unreliable constitutive laws, which can lead to biased estimates and
misinterpretations. Our proposed framework addresses this limitation by
distinguishing between reliable governing equations, such as conservation laws,
and uncertain constitutive relationships. By treating all state variables as
latent random variables, we enforce these equations through separate sets of
residuals, leveraging a virtual likelihood approach with weighted residuals.
This formulation not only identifies regions where constitutive laws break down
but also improves robustness against model uncertainties without relying on a
fully trustworthy forward model. We demonstrate the effectiveness of our
approach in the context of elastography, showing that it provides a structured,
interpretable, and computationally efficient alternative to traditional model
error correction techniques. Our findings suggest that the proposed framework
enhances the accuracy and reliability of material property estimation by
offering a principled way to incorporate uncertainty in constitutive modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory Analysis on the Training Course of DeepSeek Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Zhang, Lei Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a theoretical analysis of GPU memory consumption during the
training of DeepSeek models such as DeepSeek-v2 and DeepSeek-v3. Our primary
objective is to clarify the device-level memory requirements associated with
various distributed training configurations. Specifically, we examine critical
factors influencing memory usage, including micro-batch size, activation
recomputation policies, 3D parallelism, and ZeRO optimizations. It is important
to emphasize that the training policies discussed in this report are not
representative of DeepSeek's official configurations. Instead, they are
explored to provide a deeper understanding of memory dynamics in training of
large-scale mixture-of-experts model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample Weight Averaging for Stable Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yu, Yue He, Renzhe Xu, Dongbai Li, Jiayin Zhang, Wenchao Zou, Peng Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of Out-of-Distribution (OOD) generalization poses a
foundational concern for the application of machine learning algorithms to
risk-sensitive areas. Inspired by traditional importance weighting and
propensity weighting methods, prior approaches employ an independence-based
sample reweighting procedure. They aim at decorrelating covariates to
counteract the bias introduced by spurious correlations between unstable
variables and the outcome, thus enhancing generalization and fulfilling stable
prediction under covariate shift. Nonetheless, these methods are prone to
experiencing an inflation of variance, primarily attributable to the reduced
efficacy in utilizing training samples during the reweighting process. Existing
remedies necessitate either environmental labels or substantially higher time
costs along with additional assumptions and supervised information. To mitigate
this issue, we propose SAmple Weight Averaging (SAWA), a simple yet efficacious
strategy that can be universally integrated into various sample reweighting
algorithms to decrease the variance and coefficient estimation error, thus
boosting the covariate-shift generalization and achieving stable prediction
across different environments. We prove its rationality and benefits
theoretically. Experiments across synthetic datasets and real-world datasets
consistently underscore its superiority against covariate shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for
  Few-Shot WSI Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh-Tien Nguyen, Duy Minh Ho Nguyen, Nghiem Tuong Diep, Trung Quoc Nguyen, Nhat Ho, Jacqueline Michelle Metsch, Miriam Cindy Maurer, Daniel Sonntag, Hanibal Bohnenberger, Anne-Christin Hauschild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide pathology image classification presents challenges due to
gigapixel image sizes and limited annotation labels, hindering model
generalization. This paper introduces a prompt learning method to adapt large
vision-language models for few-shot pathology classification. We first extend
the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology
image tiles, into a vision-language model by adding adaptors and aligning it
with medical text encoders via contrastive learning on 923K image-text pairs.
The model is then used to extract visual features and text embeddings from
few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike
prior methods that combine prompts with frozen features using prefix embeddings
or self-attention, we propose multi-granular attention that compares
interactions between learnable prompts with individual image patches and groups
of them. This approach improves the model's ability to capture both
fine-grained details and broader context, enhancing its recognition of complex
patterns across sub-regions. To further improve accuracy, we leverage
(unbalanced) optimal transport-based visual-text distance to secure model
robustness by mitigating perturbations that might occur during the data
augmentation process. Empirical experiments on lung, kidney, and breast
pathology modalities validate the effectiveness of our approach; thereby, we
surpass several of the latest competitors and consistently improve performance
across diverse architectures, including CLIP, PLIP, and Prov-GigaPath
integrated PLIP. We release our implementations and pre-trained models at this
MGPATH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>first version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Data, No Optimization: A Lightweight Method To Disrupt Neural
  Networks With Sign-Flips 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ido Galil, Moshe Kimhi, Ran El-Yaniv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) can be catastrophically disrupted by flipping
only a handful of sign bits in their parameters. We introduce Deep Neural
Lesion (DNL), a data-free, lightweight method that locates these critical
parameters and triggers massive accuracy drops. We validate its efficacy on a
wide variety of computer vision models and datasets. The method requires no
training data or optimization and can be carried out via common exploits
software, firmware or hardware based attack vectors. An enhanced variant that
uses a single forward and backward pass further amplifies the damage beyond
DNL's zero-pass approach. Flipping just two sign bits in ResNet50 on ImageNet
reduces accuracy by 99.8\%. We also show that selectively protecting a small
fraction of vulnerable sign bits provides a practical defense against such
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Multimodal Machine Learning for Revealing Structure-Property
  Relationships in Carbon Nanotube Fibers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daisuke Kimura, Naoko Tajima, Toshiya Okazaki, Shun Muroga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose Explainable Multimodal Machine Learning (EMML),
which integrates the analysis of diverse data types (multimodal data) using
factor analysis for feature extraction with Explainable AI (XAI), for carbon
nanotube (CNT) fibers prepared from aqueous dispersions. This method is a
powerful approach to elucidate the mechanisms governing material properties,
where multi-stage fabrication conditions and multiscale structures have complex
influences. Thus, in our case, this approach helps us understand how different
processing steps and structures at various scales impact the final properties
of CNT fibers. The analysis targeted structures ranging from the nanoscale to
the macroscale, including aggregation size distributions of CNT dispersions and
the effective length of CNTs. Furthermore, because some types of data were
difficult to interpret using standard methods, challenging-to-interpret
distribution data were analyzed using Negative Matrix Factorization (NMF) for
extracting key features that determine the outcome. Contribution analysis with
SHapley Additive exPlanations (SHAP) demonstrated that small, uniformly
distributed aggregates are crucial for improving fracture strength, while CNTs
with long effective lengths are significant factors for enhancing electrical
conductivity. The analysis also identified thresholds and trends for these key
factors to assist in defining the conditions needed to optimize CNT fiber
properties. EMML is not limited to CNT fibers but can be applied to the design
of other materials derived from nanomaterials, making it a useful tool for
developing a wide range of advanced materials. This approach provides a
foundation for advancing data-driven materials research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bandit Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Croissant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive progress in statistical Optimal Transport (OT) in
recent years, there has been little interest in the study of the
\emph{sequential learning} of OT. Surprisingly so, as this problem is both
practically motivated and a challenging extension of existing settings such as
linear bandits. This article considers (for the first time) the stochastic
bandit problem of learning to solve generic Kantorovich and entropic OT
problems from repeated interactions when the marginals are known but the cost
is unknown. We provide $\tilde{\mathcal O}(\sqrt{T})$ regret algorithms for
both problems by extending linear bandits on Hilbert spaces. These results
provide a reduction to infinite-dimensional linear bandits. To deal with the
dimension, we provide a method to exploit the intrinsic regularity of the cost
to learn, yielding corresponding regret bounds which interpolate between
$\tilde{\mathcal O}(\sqrt{T})$ and $\tilde{\mathcal O}(T)$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Rules for Online Failure Prediction: A Case Study on the
  Metro do Porto <span class="highlight-title">dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Jakobs, Bruno Veloso, Joao Gama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their high predictive performance, predictive maintenance applications
have increasingly been approached with Deep Learning techniques in recent
years. However, as in other real-world application scenarios, the need for
explainability is often stated but not sufficiently addressed. This study will
focus on predicting failures on Metro trains in Porto, Portugal. While recent
works have found high-performing deep neural network architectures that feature
a parallel explainability pipeline, the generated explanations are fairly
complicated and need help explaining why the failures are happening. This work
proposes a simple online rule-based explainability approach with interpretable
features that leads to straightforward, interpretable rules. We showcase our
approach on MetroPT2 and find that three specific sensors on the Metro do Porto
trains suffice to predict the failures present in the dataset with simple
rules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under submission at Information Fusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EvoFlow: Evolving Diverse Agentic Workflows On The Fly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guibin Zhang, Kaijie Chen, Guancheng Wan, Heng Chang, Hong Cheng, Kun Wang, Shuyue Hu, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The past two years have witnessed the evolution of large language model
(LLM)-based multi-agent systems from labor-intensive manual design to partial
automation (\textit{e.g.}, prompt engineering, communication topology) and
eventually to fully automated design. However, existing agentic automation
pipelines often lack LLM heterogeneity and focus on single-objective
performance optimization, limiting their potential to combine weaker models for
more customized and cost-effective solutions. To address this challenge, we
propose EvoFlow, a niching evolutionary algorithm-based framework to
automatically search a population of heterogeneous and complexity-adaptive
agentic workflows, rather than a single homogeneous, complex workflow.
Technically, EvoFlow performs \textit{(1) tag-based retrieval} to extract
parent workflows from an agentic population, evolves new workflows through
\textit{(2) crossover} and \textit{(3) mutation}, and employs \textit{(4)
niching-based selection} to maintain population diversity and quality.
Extensive evaluations across seven benchmarks demonstrate that EvoFlow is:
\textbf{(I) diverse}, evolving a population of workflows ranging from simple
I/O tasks to complex multi-turn interactions; \textbf{(II) high-performing},
outperforming previous handcrafted and automated workflows by
$1.23\%\sim29.86\%$; \textbf{(III) economical}, surpassing powerful
\llmname{o1-preview} at $12.4\%$ of its inference cost using weaker open-source
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uniform Kernel Prober 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumya Mukherjee, Bharath K. Sriperumbudur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to identify useful features or representations of the input data
based on training data that achieves low prediction error on test data across
multiple prediction tasks is considered the key to multitask learning success.
In practice, however, one faces the issue of the choice of prediction tasks and
the availability of test data from the chosen tasks while comparing the
relative performance of different features. In this work, we develop a class of
pseudometrics called Uniform Kernel Prober (UKP) for comparing features or
representations learned by different statistical models such as neural networks
when the downstream prediction tasks involve kernel ridge regression. The
proposed pseudometric, UKP, between any two representations, provides a uniform
measure of prediction error on test data corresponding to a general class of
kernel ridge regression tasks for a given choice of a kernel without access to
test data. Additionally, desired invariances in representations can be
successfully captured by UKP only through the choice of the kernel function and
the pseudometric can be efficiently estimated from $n$ input data samples with
$O(\frac{1}{\sqrt{n}})$ estimation error. We also experimentally demonstrate
the ability of UKP to discriminate between different types of features or
representations based on their generalization performance on downstream kernel
ridge regression tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongReD: Mitigating Short-Text Degradation of Long-Context Large
  Language Models via Restoration Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zican Dong, Junyi Li, Jinhao Jiang, Mingyu Xu, Wayne Xin Zhao, Bingning Wang, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have gained extended context windows through
scaling positional encodings and lightweight continual pre-training. However,
this often leads to degraded performance on short-text tasks, while the reasons
for this degradation remain insufficiently explored. In this work, we identify
two primary factors contributing to this issue: distribution drift in hidden
states and attention scores, and catastrophic forgetting during continual
pre-training. To address these challenges, we propose Long Context Pre-training
with Restoration Distillation (LongReD), a novel approach designed to mitigate
short-text performance degradation through minimizing the distribution
discrepancy between the extended and original models. Besides training on long
texts, LongReD distills the hidden state of selected layers from the original
model on short texts. Additionally, LongReD also introduces a short-to-long
distillation, aligning the output distribution on short texts with that on long
texts by leveraging skipped positional indices. Experiments on common text
benchmarks demonstrate that LongReD effectively preserves the model's
short-text performance while maintaining comparable or even better capacity to
handle long texts than baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effects of Random Edge-Dropping on Over-Squashing in Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jasraj Singh, Keyue Jiang, Brooks Paige, Laura Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message Passing Neural Networks (MPNNs) are a class of Graph Neural Networks
(GNNs) that leverage the graph topology to propagate messages across
increasingly larger neighborhoods. The message-passing scheme leads to two
distinct challenges: over-smoothing and over-squashing. While several
algorithms, e.g. DropEdge and its variants -- DropNode, DropAgg and DropGNN --
have successfully addressed the over-smoothing problem, their impact on
over-squashing remains largely unexplored. This represents a critical gap in
the literature as failure to mitigate over-squashing would make these methods
unsuitable for long-range tasks. In this work, we take the first step towards
closing this gap by studying the aforementioned algorithms in the context of
over-squashing. We present novel theoretical results that characterize the
negative effects of DropEdge on sensitivity between distant nodes, suggesting
its unsuitability for long-range tasks. Our findings are easily extended to its
variants, allowing us to build a comprehensive understanding of how they affect
over-squashing. We evaluate these methods using real-world datasets,
demonstrating their detrimental effects. Specifically, we show that while
DropEdge-variants improve test-time performance in short range tasks, they
deteriorate performance in long-range ones. Our theory explains these results
as follows: random edge-dropping lowers the effective receptive field of GNNs,
which although beneficial for short-range tasks, misaligns the models on
long-range ones. This forces the models to overfit to short-range artefacts in
the training set, resulting in poor generalization. Our conclusions highlight
the need to re-evaluate various methods designed for training deep GNNs, with a
renewed focus on modelling long-range interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Physics and Data-Driven Approaches: An Explainable and
  Uncertainty-Aware Hybrid Model for Wind Turbine Power Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfonso Gijón, Simone Eiraudo, Antonio Manjavacas, Daniele Salvatore Schiera, Miguel Molina-Solana, Juan Gómez-Romero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of the wind energy sector underscores the urgent need to
optimize turbine operations and ensure effective maintenance through early
fault detection systems. While traditional empirical and physics-based models
offer approximate predictions of power generation based on wind speed, they
often fail to capture the complex, non-linear relationships between other input
variables and the resulting power output. Data-driven machine learning methods
present a promising avenue for improving wind turbine modeling by leveraging
large datasets, enhancing prediction accuracy but often at the cost of
interpretability. In this study, we propose a hybrid semi-parametric model that
combines the strengths of both approaches, applied to a dataset from a wind
farm with four turbines. The model integrates a physics-inspired submodel,
providing a reasonable approximation of power generation, with a non-parametric
submodel that predicts the residuals. This non-parametric submodel is trained
on a broader range of variables to account for phenomena not captured by the
physics-based component. The hybrid model achieves a 37% improvement in
prediction accuracy over the physics-based model. To enhance interpretability,
SHAP values are used to analyze the influence of input features on the residual
submodel's output. Additionally, prediction uncertainties are quantified using
a conformalized quantile regression method. The combination of these
techniques, alongside the physics grounding of the parametric submodel,
provides a flexible, accurate, and reliable framework. Ultimately, this study
opens the door for evaluating the impact of unmodeled variables on wind turbine
power generation, offering a basis for potential optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E<span class="highlight-title">motion</span>al EEG Classification using Upscaled Connectivity Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chae-Won Lee, Jong-Seok Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies of emotional EEG classification, connectivity matrices have
been successfully employed as input to convolutional neural networks (CNNs),
which can effectively consider inter-regional interaction patterns in EEG.
However, we find that such an approach has a limitation that important patterns
in connectivity matrices may be lost during the convolutional operations in
CNNs. To resolve this issue, we propose and validate an idea to upscale the
connectivity matrices to strengthen the local patterns. Experimental results
demonstrate that this simple idea can significantly enhance the classification
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Flow Samplers with Shortcut Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuhao Chen, Zijing Ou, Yingzhen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling from unnormalized densities is a fundamental task across various
domains. Flow-based samplers generate samples by learning a velocity field that
satisfies the continuity equation, but this requires estimating the intractable
time derivative of the partition function. While importance sampling provides
an approximation, it suffers from high variance. To mitigate this, we introduce
a velocity-driven Sequential Monte Carlo method combined with control variates
to reduce variance. Additionally, we incorporate a shortcut model to improve
efficiency by minimizing the number of sampling steps. Empirical results on
both synthetic datasets and $n$-body system targets validate the effectiveness
of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PICTS: A Novel Deep Reinforcement Learning Approach for Dynamic P-I
  <span class="highlight-title">Control</span> in Scanning Probe Microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Wei, Shuming Wei, Qibin Zeng, Wanheng Lu, Huajun Liu, Kaiyang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have developed a Parallel Integrated Control and Training System,
leveraging the deep reinforcement learning to dynamically adjust the control
strategies in real time for scanning probe microscopy techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-term simulation of physical and mechanical behaviors using
  curriculum-transfer-learning based physics-informed neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Guo, Zhuojia Fu, Jian Min, Shiyu Lin, Xiaoting Liu, Youssef F. Rashed, Xiaoying Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a Curriculum-Transfer-Learning based physics-informed
neural network (CTL-PINN) for long-term simulation of physical and mechanical
behaviors. The main innovation of CTL-PINN lies in decomposing long-term
problems into a sequence of short-term subproblems. Initially, the standard
PINN is employed to solve the first sub-problem. As the simulation progresses,
subsequent time-domain problems are addressed using a curriculum learning
approach that integrates information from previous steps. Furthermore, transfer
learning techniques are incorporated, allowing the model to effectively utilize
prior training data and solve sequential time domain transfer problems.
CTL-PINN combines the strengths of curriculum learning and transfer learning,
overcoming the limitations of standard PINNs, such as local optimization
issues, and addressing the inaccuracies over extended time domains encountered
in CL-PINN and the low computational efficiency of TL-PINN. The efficacy and
robustness of CTL-PINN are demonstrated through applications to nonlinear wave
propagation, Kirchhoff plate dynamic response, and the hydrodynamic model of
the Three Gorges Reservoir Area, showcasing its superior capability in
addressing long-term computational challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject
  Batch Editing for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilu Dong, Xiangqing Shen, Rui Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models continue to scale up, knowledge editing techniques
that modify models' internal knowledge without full retraining have gained
significant attention. MEMIT, a prominent batch editing algorithm, stands out
for its capability to perform mass knowledge modifications. However, we uncover
a critical limitation that MEMIT's editing efficacy significantly deteriorates
when processing batches containing multiple edits sharing the same subject. Our
analysis reveals that the root cause lies in MEMIT's key value modeling
framework: When multiple facts with the same subject in a batch are modeled
through MEMIT's key value mechanism, identical keys (derived from the shared
subject) are forced to represent different values (corresponding to different
knowledge), resulting in updates conflicts during editing. Addressing this
issue, we propose MEMIT-Merge, an enhanced approach that merges value
computation processes for facts sharing the same subject, effectively resolving
the performance degradation in same-subject batch editing scenarios.
Experimental results demonstrate that when MEMIT's edit success rate drops to
around 50% at larger batch sizes, MEMIT-Merge maintains a success rate
exceeding 90%, showcasing remarkable robustness to subject entity collisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learnable Residual-based Latent Denoising in Semantic Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkai Xu, Yongpeng Wu, Yuxuan Shi, Xiang-Gen Xia, Wenjun Zhang, Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A latent denoising semantic communication (SemCom) framework is proposed for
robust image transmission over noisy channels. By incorporating a learnable
latent denoiser into the receiver, the received signals are preprocessed to
effectively remove the channel noise and recover the semantic information,
thereby enhancing the quality of the decoded images. Specifically, a latent
denoising mapping is established by an iterative residual learning approach to
improve the denoising efficiency while ensuring stable performance. Moreover,
channel signal-to-noise ratio (SNR) is utilized to estimate and predict the
latent similarity score (SS) for conditional denoising, where the number of
denoising steps is adapted based on the predicted SS sequence, further reducing
the communication latency. Finally, simulations demonstrate that the proposed
framework can effectively and efficiently remove the channel noise at various
levels and reconstruct visual-appealing images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE Wireless Communications Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenGrok: Enhancing SNS Data Processing with Distilled Knowledge and
  Mask-like Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lumen AI, Zaozhuang No. 28 Middle School, Shihao Ji, Zihui Song, Fucheng Zhong, Jisen Jia, Zhaobo Wu, Zheyi Cao, Tianhao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report details Lumen Labs' novel approach to processing Social
Networking Service (SNS) data. We leverage knowledge distillation, specifically
a simple distillation method inspired by DeepSeek-R1's CoT acquisition,
combined with prompt hacking, to extract valuable training data from the Grok
model. This data is then used to fine-tune a Phi-3-mini model, augmented with a
mask-like mechanism specifically designed for handling the nuances of SNS data.
Our method demonstrates state-of-the-art (SOTA) performance on several SNS data
processing tasks, outperforming existing models like Grok, Phi-3, and GPT-4. We
provide a comprehensive analysis of our approach, including mathematical
formulations, engineering details, ablation studies, and comparative
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navid Rajabi, Jana Kosecka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a modular approach for the Vision-Language
Navigation (VLN) task by decomposing the problem into four sub-modules that use
state-of-the-art Large Language Models (LLMs) and Vision-Language Models (VLMs)
in a zero-shot setting. Given navigation instruction in natural language, we
first prompt LLM to extract the landmarks and the order in which they are
visited. Assuming the known model of the environment, we retrieve the top-k
locations of the last landmark and generate $k$ path hypotheses from the
starting location to the last landmark using the shortest path algorithm on the
topological map of the environment. Each path hypothesis is represented by a
sequence of panoramas. We then use dynamic programming to compute the alignment
score between the sequence of panoramas and the sequence of landmark names,
which match scores obtained from VLM. Finally, we compute the nDTW metric
between the hypothesis that yields the highest alignment score to evaluate the
path fidelity. We demonstrate superior performance compared to other approaches
that use joint semantic maps like VLMaps \cite{vlmaps} on the complex
R2R-Habitat \cite{r2r} instruction dataset and quantify in detail the effect of
visual grounding on navigation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Liu, Siyuan Li, Zhiyuan Chen, Lei Xin, Fang Wu, Chang Yu, Qirong Yang, Yucheng Guo, Yujie Yang, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interactions between DNA, RNA, and proteins are fundamental to biological
processes, as illustrated by the central dogma of molecular biology. While
modern biological pre-trained models have achieved great success in analyzing
these macromolecules individually, their interconnected nature remains
under-explored. In this paper, we follow the guidance of the central dogma to
redesign both the data and model pipeline and offer a comprehensive framework,
Life-Code, that spans different biological functions. As for data flow, we
propose a unified pipeline to integrate multi-omics data by
reverse-transcribing RNA and reverse-translating amino acids into
nucleotide-based sequences. As for the model, we design a codon tokenizer and a
hybrid long-sequence architecture to encode the interactions of both coding and
non-coding regions with masked modeling pre-training. To model the translation
and folding process with coding sequences, Life-Code learns protein structures
of the corresponding amino acids by knowledge distillation from off-the-shelf
protein language models. Such designs enable Life-Code to capture complex
interactions within genetic sequences, providing a more comprehensive
understanding of multi-omics with the central dogma. Extensive Experiments show
that Life-Code achieves state-of-the-art performance on various tasks across
three omics, highlighting its potential for advancing multi-omics analysis and
interpretation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages main text with 6 pages Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation of Drug-Induced Cardiac Reactions towards Virtual Clinical
  Trials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Shao, Bang Du, Zepeng Li, Qiyuan Chen, Hongxia Xu, Jimeng Sun, Jian Wu, Jintai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical trials are pivotal in cardiac drug development, yet they often fail
due to inadequate efficacy and unexpected safety issues, leading to significant
financial losses. Using in-silico trials to replace a part of physical clinical
trials, e.g., leveraging advanced generative models to generate drug-influenced
electrocardiograms (ECGs), seems an effective method to reduce financial risk
and potential harm to trial participants. While existing generative models have
demonstrated progress in ECG generation, they fall short in modeling drug
reactions due to limited fidelity and inability to capture individualized drug
response patterns. In this paper, we propose a Drug-Aware Diffusion Model
(DADM), which could simulate individualized drug reactions while ensuring
fidelity. To ensure fidelity, we construct a set of ordinary differential
equations to provide external physical knowledge (EPK) of the realistic ECG
morphology. The EPK is used to adaptively constrain the morphology of the
generated ECGs through a dynamic cross-attention (DCA) mechanism. Furthermore,
we propose an extension of ControlNet to incorporate demographic and drug data,
simulating individual drug reactions. We compare DADM with the other eight
state-of-the-art ECG generative models on two real-world databases covering 8
types of drug regimens. The results demonstrate that DADM can more accurately
simulate drug-induced changes in ECGs, improving the accuracy by at least 5.79%
and recall by 8%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Treatment Effect Estimation for Exponential Family Outcomes using Neural
  Networks with Targeted Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahong Li, Zeqin Yang, Jiayi Dan, Jixing Xu, Zhichao Zou, Peng Zhen, Jiecheng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Networks (NNs) have became a natural choice for treatment effect
estimation due to their strong approximation capabilities. Nevertheless, how to
design NN-based estimators with desirable properties, such as low bias and
doubly robustness, still remains a significant challenge. A common approach to
address this is targeted regularization, which modifies the objective function
of NNs. However, existing works on targeted regularization are limited to
Gaussian-distributed outcomes, significantly restricting their applicability in
real-world scenarios. In this work, we aim to bridge this blank by extending
this framework to the boarder exponential family outcomes. Specifically, we
first derive the von-Mises expansion of the Average Dose function of Canonical
Functions (ADCF), which inspires us how to construct a doubly robust estimator
with good properties. Based on this, we develop a NN-based estimator for ADCF
by generalizing functional targeted regularization to exponential families, and
provide the corresponding theoretical convergence rate. Extensive experimental
results demonstrate the effectiveness of our proposed model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Universal Scaling and Ultra-Small Parameterization in Machine
  Learning Interatomic Potentials with Super-Linearity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxiao Hu, Ye Sheng, Jing Huang, Xiaoxin Xu, Yuyan Yang, Mingqiang Zhang, Yabei Wu, Caichao Ye, Jiong Yang, Wenqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using machine learning (ML) to construct interatomic interactions and thus
potential energy surface (PES) has become a common strategy for materials
design and simulations. However, those current models of machine learning
interatomic potential (MLIP) provide no relevant physical constrains, and thus
may owe intrinsic out-of-domain difficulty which underlies the challenges of
model generalizability and physical scalability. Here, by incorporating
physics-informed Universal-Scaling law and nonlinearity-embedded interaction
function, we develop a Super-linear MLIP with both Ultra-Small parameterization
and greatly expanded expressive capability, named SUS2-MLIP. Due to the global
scaling rooting in universal equation of state (UEOS), SUS2-MLIP not only has
significantly-reduced parameters by decoupling the element space from
coordinate space, but also naturally outcomes the out-of-domain difficulty and
endows the potentials with inherent generalizability and scalability even with
relatively small training dataset. The nonlinearity-enbeding transformation for
interaction function expands the expressive capability and make the potentials
super-linear. The SUS2-MLIP outperforms the state-of-the-art MLIP models with
its exceptional computational efficiency especially for multiple-element
materials and physical scalability in property prediction. This work not only
presents a highly-efficient universal MLIP model but also sheds light on
incorporating physical constraints into artificial-intelligence-aided materials
simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negative Dependence as a toolbox for machine learning : <span class="highlight-title">review</span> and new
  developments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang-Son Tran, Vladimir Petrovic, Remi Bardenet, Subhroshekhar Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negative dependence is becoming a key driver in advancing learning
capabilities beyond the limits of traditional independence. Recent developments
have evidenced support towards negatively dependent systems as a learning
paradigm in a broad range of fundamental machine learning challenges including
optimization, sampling, dimensionality reduction and sparse signal recovery,
often surpassing the performance of current methods based on statistical
independence. The most popular negatively dependent model has been that of
determinantal point processes (DPPs), which have their origins in quantum
theory. However, other models, such as perturbed lattice models, strongly
Rayleigh measures, zeros of random functions have gained salience in various
learning applications. In this article, we review this burgeoning field of
research, as it has developed over the past two decades or so. We also present
new results on applications of DPPs to the parsimonious representation of
neural networks. In the limited scope of the article, we mostly focus on
aspects of this area to which the authors contributed over the recent years,
including applications to Monte Carlo methods, coresets and stochastic gradient
descent, stochastic networks, signal processing and connections to quantum
computation. However, starting from basics of negative dependence for the
uninitiated reader, extensive references are provided to a broad swath of
related developments which could not be covered within our limited scope. While
existing works and reviews generally focus on specific negatively dependent
models (e.g. DPPs), a notable feature of this article is that it addresses
negative dependence as a machine learning methodology as a whole. In this vein,
it covers within its span an array of negatively dependent models and their
applications well beyond DPPs, thereby putting forward a very general and
rather unique perspective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dedicated to the memory of Prof K.R. Parthasarathy: visionary, guru,
  and scientist par excellence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Contrastive Block Disentanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taro Makino, Ji Won Park, Natasa Tagasovska, Takamasa Kudo, Paula Coelho, Jan-Christian Huetter, Heming Yao, Burkhard Hoeckendorf, Ana Carolina Leote, Stephen Ra, David Richmond, Kyunghyun Cho, Aviv Regev, Romain Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world datasets often combine data collected under different experimental
conditions. This yields larger datasets, but also introduces spurious
correlations that make it difficult to model the phenomena of interest. We
address this by learning two embeddings to independently represent the
phenomena of interest and the spurious correlations. The embedding representing
the phenomena of interest is correlated with the target variable $y$, and is
invariant to the environment variable $e$. In contrast, the embedding
representing the spurious correlations is correlated with $e$. The invariance
to $e$ is difficult to achieve on real-world datasets. Our primary contribution
is an algorithm called Supervised Contrastive Block Disentanglement (SCBD) that
effectively enforces this invariance. It is based purely on Supervised
Contrastive Learning, and applies to real-world data better than existing
approaches. We empirically validate SCBD on two challenging problems. The first
problem is domain generalization, where we achieve strong performance on a
synthetic dataset, as well as on Camelyon17-WILDS. We introduce a single
hyperparameter $\alpha$ to control the degree of invariance to $e$. When we
increase $\alpha$ to strengthen the degree of invariance, out-of-distribution
performance improves at the expense of in-distribution performance. The second
problem is batch correction, in which we apply SCBD to preserve biological
signal and remove inter-well batch effects when modeling single-cell
perturbations from 26 million Optical Pooled Screening images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIGT: Memory Instance Gated Transformer Framework for Financial
  Portfolio Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengchen Gu, Angelos Stefanidis, Ángel García-Fernández, Jionglong Su, Huakang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (DRL) has been applied in financial portfolio
management to improve returns in changing market conditions. However, unlike
most fields where DRL is widely used, the stock market is more volatile and
dynamic as it is affected by several factors such as global events and investor
sentiment. Therefore, it remains a challenge to construct a DRL-based portfolio
management framework with strong return capability, stable training, and
generalization ability. This study introduces a new framework utilizing the
Memory Instance Gated Transformer (MIGT) for effective portfolio management. By
incorporating a novel Gated Instance Attention module, which combines a
transformer variant, instance normalization, and a Lite Gate Unit, our approach
aims to maximize investment returns while ensuring the learning process's
stability and reducing outlier impacts. Tested on the Dow Jones Industrial
Average 30, our framework's performance is evaluated against fifteen other
strategies using key financial metrics like the cumulative return and
risk-return ratios (Sharpe, Sortino, and Omega ratios). The results highlight
MIGT's advantage, showcasing at least a 9.75% improvement in cumulative returns
and a minimum 2.36% increase in risk-return ratios over competing strategies,
marking a significant advancement in DRL for portfolio management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploratory Diffusion Policy for Unsupervised Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyang Ying, Huayu Chen, Xinning Zhou, Zhongkai Hao, Hang Su, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised reinforcement learning (RL) aims to pre-train agents by
exploring states or skills in reward-free environments, facilitating the
adaptation to downstream tasks. However, existing methods often overlook the
fitting ability of pre-trained policies and struggle to handle the
heterogeneous pre-training data, which are crucial for achieving efficient
exploration and fast fine-tuning. To address this gap, we propose Exploratory
Diffusion Policy (EDP), which leverages the strong expressive ability of
diffusion models to fit the explored data, both boosting exploration and
obtaining an efficient initialization for downstream tasks. Specifically, we
estimate the distribution of collected data in the replay buffer with the
diffusion policy and propose a score intrinsic reward, encouraging the agent to
explore unseen states. For fine-tuning the pre-trained diffusion policy on
downstream tasks, we provide both theoretical analyses and practical
algorithms, including an alternating method of Q function optimization and
diffusion policy distillation. Extensive experiments demonstrate the
effectiveness of EDP in efficient exploration during pre-training and fast
adaptation during fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> Ownership Verification in Contrastive Pre-trained Models <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuechen Xie, Jie Song, Mengqi Xue, Haofei Zhang, Xingen Wang, Bingde Hu, Genlang Chen, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality open-source datasets, which necessitate substantial efforts for
curation, has become the primary catalyst for the swift progress of deep
learning. Concurrently, protecting these datasets is paramount for the
well-being of the data owner. Dataset ownership verification emerges as a
crucial method in this domain, but existing approaches are often limited to
supervised models and cannot be directly extended to increasingly popular
unsupervised pre-trained models. In this work, we propose the first dataset
ownership verification method tailored specifically for self-supervised
pre-trained models by contrastive learning. Its primary objective is to
ascertain whether a suspicious black-box backbone has been pre-trained on a
specific unlabeled dataset, aiding dataset owners in upholding their rights.
The proposed approach is motivated by our empirical insights that when models
are trained with the target dataset, the unary and binary instance
relationships within the embedding space exhibit significant variations
compared to models trained without the target dataset. We validate the efficacy
of this approach across multiple contrastive pre-trained models including
SimCLR, BYOL, SimSiam, MOCO v3, and DINO. The results demonstrate that our
method rejects the null hypothesis with a $p$-value markedly below $0.05$,
surpassing all previous methodologies. Our code is available at
https://github.com/xieyc99/DOV4CL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-Efficient Continual Learning with Sufficient Exemplar Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongkyu Cho, Taesup Moon, Rumi Chunara, Kyunghyun Cho, Sungmin Cha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning (CL) research typically assumes highly constrained
exemplar memory resources. However, in many real-world scenarios-especially in
the era of large foundation models-memory is abundant, while GPU computational
costs are the primary bottleneck. In this work, we investigate CL in a novel
setting where exemplar memory is ample (i.e., sufficient exemplar memory).
Unlike prior methods designed for strict exemplar memory constraints, we
propose a simple yet effective approach that directly operates in the model's
weight space through a combination of weight resetting and averaging
techniques. Our method achieves state-of-the-art performance while reducing the
computational cost to a quarter or third of existing methods. These findings
challenge conventional CL assumptions and provide a practical baseline for
computationally efficient CL applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Learning Induces Adaptive Label Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sin-Han Yang, Zhedong Liu, Gian Maria Marconi, Mohammad Emtiyaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that variational learning naturally induces an adaptive label
smoothing where label noise is specialized for each example. Such
label-smoothing is useful to handle examples with labeling errors and
distribution shifts, but designing a good adaptivity strategy is not always
easy. We propose to skip this step and simply use the natural adaptivity
induced during the optimization of a variational objective. We show empirical
results where a variational algorithm called IVON outperforms traditional label
smoothing and yields adaptivity strategies similar to those of an existing
approach. By connecting Bayesian methods to label smoothing, our work provides
a new way to handle overconfident predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Column-wise Quantization of Weights and Partial Sums for Accurate and
  Efficient Compute-In-Memory Accelerators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyoon Kim, Kang Eun Jeon, Yulhwa Kim, Jong Hwan Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compute-in-memory (CIM) is an efficient method for implementing deep neural
networks (DNNs) but suffers from substantial overhead from analog-to-digital
converters (ADCs), especially as ADC precision increases. Low-precision ADCs
can re- duce this overhead but introduce partial-sum quantization errors
degrading accuracy. Additionally, low-bit weight constraints, im- posed by cell
limitations and the need for multiple cells for higher- bit weights, present
further challenges. While fine-grained partial- sum quantization has been
studied to lower ADC resolution effectively, weight granularity, which limits
overall partial-sum quantized accuracy, remains underexplored. This work
addresses these challenges by aligning weight and partial-sum quantization
granularities at the column-wise level. Our method improves accuracy while
maintaining dequantization overhead, simplifies training by removing two-stage
processes, and ensures robustness to memory cell variations via independent
column-wise scale factors. We also propose an open-source CIM-oriented
convolution framework to handle fine-grained weights and partial-sums effi-
ciently, incorporating a novel tiling method and group convolution.
Experimental results on ResNet-20 (CIFAR-10, CIFAR-100) and ResNet-18
(ImageNet) show accuracy improvements of 0.99%, 2.69%, and 1.01%, respectively,
compared to the best-performing related works. Additionally, variation analysis
reveals the robust- ness of our method against memory cell variations. These
findings highlight the effectiveness of our quantization scheme in enhancing
accuracy and robustness while maintaining hardware efficiency in CIM-based DNN
implementations. Our code is available at
https://github.com/jiyoonkm/ColumnQuant.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When More is Less: Understanding Chain-of-Thought Length in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) reasoning enhances the multi-step reasoning
capabilities of large language models (LLMs) by breaking complex tasks into
smaller, manageable sub-tasks. Researchers have been exploring ways to guide
models to generate more complex CoT processes to improve the reasoning ability
of LLMs, such as long CoT and the test-time scaling law. However, for most
models and tasks, does an increase in CoT length consistently lead to improved
reasoning accuracy? In this paper, we observe a nuanced relationship: as the
number of reasoning steps increases, performance initially improves but
eventually decreases. To understand this phenomenon, we provide a piece of
evidence that longer reasoning processes are increasingly susceptible to noise.
We theoretically prove the existence of an optimal CoT length and derive a
scaling law for this optimal length based on model capability and task
difficulty. Inspired by our theory, we conduct experiments on both synthetic
and real world datasets and propose Length-filtered Vote to alleviate the
effects of excessively long or short CoTs. Our findings highlight the critical
need to calibrate CoT length to align with model capabilities and task demands,
offering a principled framework for optimizing multi-step reasoning in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Riemannian Proximal Sampler for High-accuracy Sampling on Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunrui Guan, Krishnakumar Balasubramanian, Shiqian Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Riemannian Proximal Sampler, a method for sampling from
densities defined on Riemannian manifolds. The performance of this sampler
critically depends on two key oracles: the Manifold Brownian Increments (MBI)
oracle and the Riemannian Heat-kernel (RHK) oracle. We establish high-accuracy
sampling guarantees for the Riemannian Proximal Sampler, showing that
generating samples with $\varepsilon$-accuracy requires
$O(\log(1/\varepsilon))$ iterations in Kullback-Leibler divergence assuming
access to exact oracles and $O(\log^2(1/\varepsilon))$ iterations in the total
variation metric assuming access to sufficiently accurate inexact oracles.
Furthermore, we present practical implementations of these oracles by
leveraging heat-kernel truncation and Varadhan's asymptotics. In the latter
case, we interpret the Riemannian Proximal Sampler as a discretization of the
entropy-regularized Riemannian Proximal Point Method on the associated
Wasserstein space. We provide preliminary numerical results that illustrate the
effectiveness of the proposed methodology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flat U-Net: An Efficient Ultralightweight Model for Solar Filament
  Segmentation in Full-disk H$α$ Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        GaoFei Zhu, GangHua Lin, Xiao Yang, Cheng Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solar filaments are one of the most prominent features observed on the Sun,
and their evolutions are closely related to various solar activities, such as
flares and coronal mass ejections. Real-time automated identification of solar
filaments is the most effective approach to managing large volumes of data.
Existing models of filament identification are characterized by large parameter
sizes and high computational costs, which limit their future applications in
highly integrated and intelligent ground-based and space-borne observation
devices. Consequently, the design of more lightweight models will facilitate
the advancement of intelligent observation equipment. In this study, we
introduce Flat U-Net, a novel and highly efficient ultralightweight model that
incorporates simplified channel attention (SCA) and channel self-attention
(CSA) convolutional blocks for the segmentation of solar filaments in full-disk
H$\alpha$ images. Feature information from each network layer is fully
extracted to reconstruct interchannel feature representations. Each block
effectively optimizes the channel features from the previous layer,
significantly reducing parameters. The network architecture presents an elegant
flattening, improving its efficiency, and simplifying the overall design.
Experimental validation demonstrates that a model composed of pure SCAs
achieves a precision of approximately 0.93, with dice similarity coefficient
(DSC) and recall rates of 0.76 and 0.64, respectively, significantly
outperforming the classical U-Net. Introducing a certain number of CSA blocks
improves the DSC and recall rates to 0.82 and 0.74, respectively, which
demonstrates a pronounced advantage, particularly concerning model weight size
and detection effectiveness. The data set, models, and code are available as
open-source resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, 3 tables, accepted for publication in ApJ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NARCE: A Mamba-Based Neural Algorithmic Reasoner Framework for Online
  Complex Event Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liying Han, Gaofeng Dong, Xiaomin Ouyang, Lance Kaplan, Federico Cerutti, Mani Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current machine learning models excel in short-span perception tasks but
struggle to derive high-level insights from long-term observation, a capability
central to understanding complex events (CEs). CEs, defined as sequences of
short-term atomic events (AEs) governed by spatiotemporal rules, are
challenging to detect online due to the need to extract meaningful patterns
from long and noisy sensor data while ignoring irrelevant events. We
hypothesize that state-based methods are well-suited for CE detection, as they
capture event progression through state transitions without requiring long-term
memory. Baseline experiments validate this, demonstrating that the state-space
model Mamba outperforms existing architectures. However, Mamba's reliance on
extensive labeled data, which are difficult to obtain, motivates our second
hypothesis: decoupling CE rule learning from noisy sensor data can reduce data
requirements. To address this, we propose NARCE, a framework that combines
Neural Algorithmic Reasoning (NAR) to split the task into two components: (i)
learning CE rules independently of sensor data using synthetic concept traces
generated by LLMs and (ii) mapping sensor inputs to these rules via an adapter.
Our results show that NARCE outperforms baselines in accuracy, generalization
to unseen and longer sensor data, and data efficiency, significantly reducing
annotation costs while advancing robust CE detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear Transformers as VAR Models: Aligning Autoregressive Attention
  Mechanisms with Autoregressive Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiecheng Lu, Shihao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive attention-based time series forecasting (TSF) has drawn
increasing interest, with mechanisms like linear attention sometimes
outperforming vanilla attention. However, deeper Transformer architectures
frequently misalign with autoregressive objectives, obscuring the underlying
VAR structure embedded within linear attention and hindering their ability to
capture the data generative processes in TSF. In this work, we first show that
a single linear attention layer can be interpreted as a dynamic vector
autoregressive (VAR) structure. We then explain that existing multi-layer
Transformers have structural mismatches with the autoregressive forecasting
objective, which impair interpretability and generalization ability. To address
this, we show that by rearranging the MLP, attention, and input-output flow,
multi-layer linear attention can also be aligned as a VAR model. Then, we
propose Structural Aligned Mixture of VAR (SAMoVAR), a linear Transformer
variant that integrates interpretable dynamic VAR weights for multivariate TSF.
By aligning the Transformer architecture with autoregressive objectives,
SAMoVAR delivers improved performance, interpretability, and computational
efficiency, comparing to SOTA TSF models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DrugImproverGPT: A Large Language Model for Drug Optimization with
  Fine-Tuning via Structured Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuefeng Liu, Songhao Jiang, Siyu Chen, Zhuoran Yang, Yuxin Chen, Ian Foster, Rick Stevens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finetuning a Large Language Model (LLM) is crucial for generating results
towards specific objectives. This research delves into the realm of drug
optimization and introduce a novel reinforcement learning algorithm to finetune
a drug optimization LLM-based generative model, enhancing the original drug
across target objectives, while retains the beneficial chemical properties of
the original drug. This work is comprised of two primary components: (1)
DrugImprover: A framework tailored for improving robustness and efficiency in
drug optimization. It includes a LLM designed for drug optimization and a novel
Structured Policy Optimization (SPO) algorithm, which is theoretically
grounded. This algorithm offers a unique perspective for fine-tuning the
LLM-based generative model by aligning the improvement of the generated
molecule with the input molecule under desired objectives. (2) A dataset of 1
million compounds, each with OEDOCK docking scores on 5 human proteins
associated with cancer cells and 24 binding sites from SARS-CoV-2 virus. We
conduct a comprehensive evaluation of SPO and demonstrate its effectiveness in
improving the original drug across target properties. Our code and dataset will
be publicly available at: https://github.com/xuefeng-cs/DrugImproverGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simplifying Adversarially Robust PAC Learning with Tolerance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassan Ashtiani, Vinayak Pathak, Ruth Urner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarially robust PAC learning has proved to be challenging, with the
currently best known learners [Montasser et al., 2021a] relying on improper
methods based on intricate compression schemes, resulting in sample complexity
exponential in the VC-dimension. A series of follow up work considered a
slightly relaxed version of the problem called adversarially robust learning
with tolerance [Ashtiani et al., 2023, Bhattacharjee et al., 2023, Raman et
al., 2024] and achieved better sample complexity in terms of the VC-dimension.
However, those algorithms were either improper and complex, or required
additional assumptions on the hypothesis class H. We prove, for the first time,
the existence of a simpler learner that achieves a sample complexity linear in
the VC-dimension without requiring additional assumptions on H. Even though our
learner is improper, it is "almost proper" in the sense that it outputs a
hypothesis that is "similar" to a hypothesis in H.
  We also use the ideas from our algorithm to construct a semi-supervised
learner in the tolerant setting. This simple algorithm achieves comparable
bounds to the previous (non-tolerant) semi-supervised algorithm of Attias et
al. [2022a], but avoids the use of intricate subroutines from previous works,
and is "almost proper."
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Memory Efficient Randomized Subspace Optimization Method for Training
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Chen, Yuan Zhang, Yin Liu, Kun Yuan, Zaiwen Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The memory challenges associated with training Large Language Models (LLMs)
have become a critical concern, particularly when using the Adam optimizer. To
address this issue, numerous memory-efficient techniques have been proposed,
with GaLore standing out as a notable example designed to reduce the memory
footprint of optimizer states. However, these approaches do not alleviate the
memory burden imposed by activations, rendering them unsuitable for scenarios
involving long context sequences or large mini-batches. Moreover, their
convergence properties are still not well-understood in the literature. In this
work, we introduce a Randomized Subspace Optimization framework for
pre-training and fine-tuning LLMs. Our approach decomposes the high-dimensional
training problem into a series of lower-dimensional subproblems. At each
iteration, a random subspace is selected, and the parameters within that
subspace are optimized. This structured reduction in dimensionality allows our
method to simultaneously reduce memory usage for both activations and optimizer
states. We establish comprehensive convergence guarantees and derive rates for
various scenarios, accommodating different optimization strategies to solve the
subproblems. Extensive experiments validate the superior memory and
communication efficiency of our method, achieving performance comparable to
GaLore and Adam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LUNAR: LLM Unlearning via Neural Activation Redirection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William F. Shen, Xinchi Qiu, Meghdad Kurmanji, Alex Iacob, Lorenzo Sani, Yihong Chen, Nicola Cancedda, Nicholas D. Lane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) benefit from training on ever larger amounts of
textual data, but as a result, they increasingly incur the risk of leaking
private information. The ability to selectively remove knowledge from LLMs is,
therefore, a highly desirable capability. In this paper, we propose LUNAR, a
novel unlearning methodology grounded in the Linear Representation Hypothesis.
LUNAR operates by redirecting the representations of unlearned data to regions
that trigger the model's inherent ability to express its inability to answer.
LUNAR achieves state-of-the-art unlearning performance while significantly
enhancing the controllability of the unlearned model during inference.
Specifically, LUNAR achieves between 2.9x to 11.7x improvements on combined
"unlearning efficacy" and "model utility" score ("Deviation Score") on the
PISTOL dataset across various base models. We also demonstrate, through
quantitative analysis and qualitative examples, LUNAR's superior
controllability in generating coherent and contextually aware responses,
mitigating undesired side effects of existing methods. Moreover, we demonstrate
that LUNAR is robust against white-box adversarial attacks and versatile in
handling real-world scenarios, such as processing sequential unlearning
requests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pareto Optimal Algorithmic Recourse in Multi-cost Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Ling Chen, Hong-Chang Huang, Kai-Hung Lin, Shang-Wei Hwang, Hao-Tsung Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In decision-making systems, algorithmic recourse aims to identify
minimal-cost actions to alter an individual features, thereby obtaining a
desired outcome. This empowers individuals to understand, question, or alter
decisions that negatively affect them. However, due to the variety and
sensitivity of system environments and individual personalities, quantifying
the cost of a single function is nearly impossible while considering multiple
criteria situations. Most current recourse mechanisms use gradient-based
methods that assume cost functions are differentiable, often not applicable in
real-world scenarios, resulting in sub-optimal solutions that compromise
various criteria. These solutions are typically intractable and lack rigorous
theoretical foundations, raising concerns regarding interpretability,
reliability, and transparency from the explainable AI (XAI) perspective.
  To address these issues, this work proposes an algorithmic recourse framework
that handles non-differentiable and discrete multi-cost functions. By
formulating recourse as a multi-objective optimization problem and assigning
weights to different criteria based on their importance, our method identifies
Pareto optimal recourse recommendations. To demonstrate scalability, we
incorporate the concept of epsilon-net, proving the ability to find
approximated Pareto optimal actions. Experiments show the trade-off between
different criteria and the methods scalability in large graphs. Compared to
current heuristic practices, our approach provides a stronger theoretical
foundation and better aligns recourse suggestions with real-world requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation for Regression Analyses on Evolving Data Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Sun, Heitor Murilo Gomes, Bernhard Pfahringer, Albert Bifet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper explores the challenges of regression analysis in evolving data
streams, an area that remains relatively underexplored compared to
classification. We propose a standardized evaluation process for regression and
prediction interval tasks in streaming contexts. Additionally, we introduce an
innovative drift simulation strategy capable of synthesizing various drift
types, including the less-studied incremental drift. Comprehensive experiments
with state-of-the-art methods, conducted under the proposed process, validate
the effectiveness and robustness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 Pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve the Training Efficiency of DRL for Wireless Communication
  Resource Allocation: The Role of Generative Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinren Zhang, Jiadong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic resource allocation in mobile wireless networks involves complex,
time-varying optimization problems, motivating the adoption of deep
reinforcement learning (DRL). However, most existing works rely on pre-trained
policies, overlooking dynamic environmental changes that rapidly invalidate the
policies. Periodic retraining becomes inevitable but incurs prohibitive
computational costs and energy consumption-critical concerns for
resource-constrained wireless systems. We identify three root causes of
inefficient retraining: high-dimensional state spaces, suboptimal action spaces
exploration-exploitation trade-offs, and reward design limitations. To overcome
these limitations, we propose Diffusion-based Deep Reinforcement Learning
(D2RL), which leverages generative diffusion models (GDMs) to holistically
enhance all three DRL components. Iterative refinement process and distribution
modelling of GDMs enable (1) the generation of diverse state samples to improve
environmental understanding, (2) balanced action space exploration to escape
local optima, and (3) the design of discriminative reward functions that better
evaluate action quality. Our framework operates in two modes: Mode I leverages
GDMs to explore reward spaces and design discriminative reward functions that
rigorously evaluate action quality, while Mode II synthesizes diverse state
samples to enhance environmental understanding and generalization. Extensive
experiments demonstrate that D2RL achieves faster convergence and reduced
computational costs over conventional DRL methods for resource allocation in
wireless communications while maintaining competitive policy performance. This
work underscores the transformative potential of GDMs in overcoming fundamental
DRL training bottlenecks for wireless networks, paving the way for practical,
real-time deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Physics-Informed Neural Networks Through Feature Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaghayegh Fazliani, Zachary Frangella, Madeleine Udell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-Informed Neural Networks (PINNs) seek to solve partial differential
equations (PDEs) with deep learning. Mainstream approaches that deploy
fully-connected multi-layer deep learning architectures require prolonged
training to achieve even moderate accuracy, while recent work on feature
engineering allows higher accuracy and faster convergence. This paper
introduces SAFE-NET, a Single-layered Adaptive Feature Engineering NETwork that
achieves orders-of-magnitude lower errors with far fewer parameters than
baseline feature engineering methods. SAFE-NET returns to basic ideas in
machine learning, using Fourier features, a simplified single hidden layer
network architecture, and an effective optimizer that improves the conditioning
of the PINN optimization problem. Numerical results show that SAFE-NET
converges faster and typically outperforms deeper networks and more complex
architectures. It consistently uses fewer parameters -- on average, 65% fewer
than the competing feature engineering methods -- while achieving comparable
accuracy in less than 30% of the training epochs. Moreover, each SAFE-NET epoch
is 95% faster than those of competing feature engineering approaches. These
findings challenge the prevailing belief that modern PINNs effectively learn
features in these scientific applications and highlight the efficiency gains
possible through feature engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on the Importance of Features in Detecting Advanced Persistent
  Threats Using Machine Learning <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Hallaji, Roozbeh Razavi-Far, Mehrdad Saif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced Persistent Threats (APTs) pose a significant security risk to
organizations and industries. These attacks often lead to severe data breaches
and compromise the system for a long time. Mitigating these sophisticated
attacks is highly challenging due to the stealthy and persistent nature of
APTs. Machine learning models are often employed to tackle this challenge by
bringing automation and scalability to APT detection. Nevertheless, these
intelligent methods are data-driven, and thus, highly affected by the quality
and relevance of input data. This paper aims to analyze measurements considered
when recording network traffic and conclude which features contribute more to
detecting APT samples. To do this, we study the features associated with
various APT cases and determine their importance using a machine learning
framework. To ensure the generalization of our findings, several feature
selection techniques are employed and paired with different classifiers to
evaluate their effectiveness. Our findings provide insights into how APT
detection can be enhanced in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the 2024 International Conference on
  Computational Science and Computational Intelligence (CSCI'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement
  Learning <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyu Wang, Jialu Li, Ling Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing prevalence of autonomous vehicles (AVs), their
vulnerability to various types of attacks has grown, presenting significant
security challenges. In this paper, we propose a reinforcement learning
(RL)-based approach for designing optimal stealthy integrity attacks on AV
actuators. We also analyze the limitations of state-of-the-art RL-based secure
controllers developed to counter such attacks. Through extensive simulation
experiments, we demonstrate the effectiveness and efficiency of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS) Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VINP: Variational Bayesian Inference with Neural Speech Prior for Joint
  ASR-Effective Speech Dereverberation and Blind RIR Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyu Wang, Ying Fang, Xiaofei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reverberant speech, denoting the speech signal degraded by the process of
reverberation, contains crucial knowledge of both anechoic source speech and
room impulse response (RIR). This work proposes a variational Bayesian
inference (VBI) framework with neural speech prior (VINP) for joint speech
dereverberation and blind RIR identification. In VINP, a probabilistic signal
model is constructed in the time-frequency (T-F) domain based on convolution
transfer function (CTF) approximation. For the first time, we propose using an
arbitrary discriminative dereverberation deep neural network (DNN) to predict
the prior distribution of anechoic speech within a probabilistic model. By
integrating both reverberant speech and the anechoic speech prior, VINP yields
the maximum a posteriori (MAP) and maximum likelihood (ML) estimations of the
anechoic speech spectrum and CTF filter, respectively. After simple
transformations, the waveforms of anechoic speech and RIR are estimated.
Moreover, VINP is effective for automatic speech recognition (ASR) systems,
which sets it apart from most deep learning (DL)-based single-channel
dereverberation approaches. Experiments on single-channel speech
dereverberation demonstrate that VINP reaches an advanced level in most metrics
related to human perception and displays unquestionable state-of-the-art (SOTA)
performance in ASR-related metrics. For blind RIR identification, experiments
indicate that VINP attains the SOTA level in blind estimation of reverberation
time at 60 dB (RT60) and direct-to-reverberation ratio (DRR). Codes and audio
samples are available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/ACM Trans. on TASLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Monte Carlo Tree Diffusion for System 2 Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaesik Yoon, Hyeonseo Cho, Doojin Baek, <span class="highlight-author">Yoshua Bengio</span>, Sungjin Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently emerged as a powerful tool for planning.
However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally
improves with additional test-time computation (TTC), standard diffusion-based
planners offer only limited avenues for TTC scalability. In this paper, we
introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates
the generative strength of diffusion models with the adaptive search
capabilities of MCTS. Our method reconceptualizes denoising as a
tree-structured process, allowing partially denoised plans to be iteratively
evaluated, pruned, and refined. By selectively expanding promising trajectories
while retaining the flexibility to revisit and improve suboptimal branches,
MCTD achieves the benefits of MCTS such as controlling exploration-exploitation
trade-offs within the diffusion framework. Empirical results on challenging
long-horizon tasks show that MCTD outperforms diffusion baselines, yielding
higher-quality solutions as TTC increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fixed-Confidence Best Arm Identification with Decreasing Variance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamojeet Roychowdhury, Kota Srinivas Reddy, Krishna P Jagannathan, Sharayu Moharir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We focus on the problem of best-arm identification in a stochastic multi-arm
bandit with temporally decreasing variances for the arms' rewards. We model arm
rewards as Gaussian random variables with fixed means and variances that
decrease with time. The cost incurred by the learner is modeled as a weighted
sum of the time needed by the learner to identify the best arm, and the number
of samples of arms collected by the learner before termination. Under this cost
function, there is an incentive for the learner to not sample arms in all
rounds, especially in the initial rounds. On the other hand, not sampling
increases the termination time of the learner, which also increases cost. This
trade-off necessitates new sampling strategies. We propose two policies. The
first policy has an initial wait period with no sampling followed by continuous
sampling. The second policy samples periodically and uses a weighted average of
the rewards observed to identify the best arm. We provide analytical guarantees
on the performance of both policies and supplement our theoretical results with
simulations which show that our polices outperform the state-of-the-art
policies for the classical best arm identification problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, accepted in the National Conference on
  Communications 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Efficient RLHF Pipeline: A Unified View from Contextual Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long-Fei Li, Yu-Yang Qian, Peng Zhao, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning from Human Feedback (RLHF) is a widely used approach
for aligning Large Language Models (LLMs) with human preferences. While recent
advancements have provided valuable insights into various stages and settings
of RLHF, a comprehensive theoretical understanding of the entire RLHF pipeline
remains lacking. Towards this end, we propose a unified framework for the RLHF
pipeline from the view of contextual bandits and provide provable efficiency
guarantees. In particular, we decompose the RLHF process into two distinct
stages: (post-)training and deployment, exploring both passive and active data
collection strategies during the training phase. By employing the Bradley-Terry
preference model with a linearly parameterized reward function, we reformulate
RLHF as a contextual preference bandit problem. We then develop novel
algorithms for each stage, demonstrating significant improvements over existing
approaches in both statistical and computational efficiency. Finally, we apply
our method to train and deploy Llama-3-8B-Instruct on the
Ultrafeedback-binarized dataset, and empirical results confirm the
effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Neural Network Pruning with Screening Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyuan Wang, Yangzi Guo, Sida Liu, Yanwen Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) such as convolutional neural networks (CNNs) for
visual tasks, recurrent neural networks (RNNs) for sequence data, and
transformer models for rich linguistic or multimodal tasks, achieved
unprecedented performance on a wide range of tasks. The impressive performance
of modern DNNs is partially attributed to their sheer scale. The latest deep
learning models have tens to hundreds of millions of parameters which makes the
inference processes resource-intensive. The high computational complexity of
these networks prevents their deployment on resource-limited devices such as
mobile platforms, IoT devices, and edge computing systems because these devices
require energy-efficient and real-time processing capabilities. This paper
proposes and evaluates a network pruning framework that eliminates
non-essential parameters based on a statistical analysis of network component
significance across classification categories. The proposed method uses
screening methods coupled with a weighted scheme to assess connection and
channel contributions for unstructured and structured pruning which allows for
the elimination of unnecessary network elements without significantly degrading
model performance. Extensive experimental validation on real-world vision
datasets for both fully connected neural networks (FNNs) and CNNs has shown
that the proposed framework produces competitive lean networks compared to the
original networks. Moreover, the proposed framework outperforms state-of-art
network pruning methods in two out of three cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Regularizers Are Not Transductive Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sky Jafar, Julian Asilis, Shaddin Dughmi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We partly resolve an open question raised by Asilis et al. (COLT 2024):
whether the algorithmic template of local regularization -- an intriguing
generalization of explicit regularization, a.k.a. structural risk minimization
-- suffices to learn all learnable multiclass problems. Specifically, we
provide a negative answer to this question in the transductive model of
learning. We exhibit a multiclass classification problem which is learnable in
both the transductive and PAC models, yet cannot be learned transductively by
any local regularizer. The corresponding hypothesis class, and our proof, are
based on principles from cryptographic secret sharing. We outline challenges in
extending our negative result to the PAC model, leaving open the tantalizing
possibility of a PAC/transductive separation with respect to local
regularization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceived Confidence Scoring for Data Annotation with Zero-Shot LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Salimian, Gias Uddin, Most Husne Jahan, Shaina Raza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot LLMs are now also used for textual classification tasks, e.g.,
sentiment/emotion detection of a given input as a sentence/article. However,
their performance can be suboptimal in such data annotation tasks. We introduce
a novel technique Perceived Confidence Scoring (PCS) that evaluates LLM's
confidence for its classification of an input by leveraging Metamorphic
Relations (MRs). The MRs generate semantically equivalent yet textually mutated
versions of the input. Following the principles of Metamorphic Testing (MT),
the mutated versions are expected to have annotation labels similar to the
input. By analyzing the consistency of LLM responses across these variations,
PCS computes a confidence score based on the frequency of predicted labels. PCS
can be used both for single LLM and multiple LLM settings (e.g., majority
voting). We introduce an algorithm Perceived Differential Evolution (PDE) that
determines the optimal weights assigned to the MRs and the LLMs for a
classification task. Empirical evaluation shows PCS significantly improves
zero-shot accuracy for Llama-3-8B-Instruct (4.96%) and Mistral-7B-Instruct-v0.3
(10.52%), with Gemma-2-9b-it showing a 9.39% gain. When combining all three
models, PCS significantly outperforms majority voting by 7.75%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoboBERT: An End-to-end Multimodal Robotic Manipulation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Wang, Jianhua Shan, Jianwei Zhang, Haozhang Gao, Hailiang Han, Yipeng Chen, Kang Wei, Chengkun Zhang, Kairos Wong, Jie Zhao, Lei Zhao, Bin Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied intelligence integrates multiple modalities, enabling agents to
understand images, language, and actions simultaneously. However, existing
models always depend on additional datasets or extensive pre-training to
maximize performance improvements, consuming abundant training time and
expensive hardware cost. To tackle this issue, we present RoboBERT, a novel
end-to-end robotic manipulation model integrated with a unique training
strategy. This model utilizes a CNN-based diffusion policy, enhancing and
stabilizing the effectiveness of this model by separating training processes
for different modalities. It also underscores the importance of data
augmentation, verifying various techniques to significantly boost performance.
Unlike models that depend on extra data or large foundation models, RoboBERT
achieves a highly competitive success rate while using only language-labeled
expert demonstrations and maintaining a relatively smaller model size.
Specifically, RoboBERT achieves an average length of 4.52 on the CALVIN
benchmark for \(ABCD \rightarrow D\) task, setting a new state-of-the-art
(SOTA) record. Furthermore, when tested on a real robot, the model demonstrates
superior performance, achieving a higher success rate than other methods
trained with the same data. We propose that these concepts and methodologies of
RoboBERT demonstrate extensive versatility and compatibility, contributing
significantly to the development of lightweight multimodal robotic models. The
code can be accessed on https://github.com/PeterWangsicheng/RoboBERT
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tab2Visual: Overcoming Limited Data in Tabular Data Classification Using
  Deep Learning with Visual Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Mamdouh, Moumen El-Melegy, Samia Ali, Ron Kikinis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the challenge of limited data in tabular data
classification, particularly prevalent in domains with constraints like
healthcare. We propose Tab2Visual, a novel approach that transforms
heterogeneous tabular data into visual representations, enabling the
application of powerful deep learning models. Tab2Visual effectively addresses
data scarcity by incorporating novel image augmentation techniques and
facilitating transfer learning. We extensively evaluate the proposed approach
on diverse tabular datasets, comparing its performance against a wide range of
machine learning algorithms, including classical methods, tree-based ensembles,
and state-of-the-art deep learning models specifically designed for tabular
data. We also perform an in-depth analysis of factors influencing Tab2Visual's
performance. Our experimental results demonstrate that Tab2Visual outperforms
other methods in classification problems with limited tabular data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MatrixKAN: Parallelized Kolmogorov-Arnold Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cale Coffman, Lizhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kolmogorov-Arnold Networks (KAN) are a new class of neural network
architecture representing a promising alternative to the Multilayer Perceptron
(MLP), demonstrating improved expressiveness and interpretability. However,
KANs suffer from slow training and inference speeds relative to MLPs due in
part to the recursive nature of the underlying B-spline calculations. This
issue is particularly apparent with respect to KANs utilizing high-degree
B-splines, as the number of required non-parallelizable recursions is
proportional to B-spline degree. We solve this issue by proposing MatrixKAN, a
novel optimization that parallelizes B-spline calculations with matrix
representation and operations, thus significantly improving effective
computation time for models utilizing high-degree B-splines. In this paper, we
demonstrate the superior scaling of MatrixKAN's computation time relative to
B-spline degree. Further, our experiments demonstrate speedups of approximately
40x relative to KAN, with significant additional speedup potential for larger
datasets or higher spline degrees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Precision Oncology Through Modeling of Longitudinal and
  Multimodal Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luoting Zhuang, Stephen H. Park, Steven J. Skates, Ashley E. Prosper, Denise R. Aberle, William Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer evolves continuously over time through a complex interplay of genetic,
epigenetic, microenvironmental, and phenotypic changes. This dynamic behavior
drives uncontrolled cell growth, metastasis, immune evasion, and therapy
resistance, posing challenges for effective monitoring and treatment. However,
today's data-driven research in oncology has primarily focused on
cross-sectional analysis using data from a single modality, limiting the
ability to fully characterize and interpret the disease's dynamic
heterogeneity. Advances in multiscale data collection and computational methods
now enable the discovery of longitudinal multimodal biomarkers for precision
oncology. Longitudinal data reveal patterns of disease progression and
treatment response that are not evident from single-timepoint data, enabling
timely abnormality detection and dynamic treatment adaptation. Multimodal data
integration offers complementary information from diverse sources for more
precise risk assessment and targeting of cancer therapy. In this review, we
survey methods of longitudinal and multimodal modeling, highlighting their
synergy in providing multifaceted insights for personalized care tailored to
the unique characteristics of a patient's cancer. We summarize the current
challenges and future directions of longitudinal multimodal analysis in
advancing precision oncology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE RBME for potential
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Robustness Of Digital Shadow For CO2 Storage Monitoring With
  Augmented Rock Physics Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Prakash Gahlot, Felix J. Herrmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To meet climate targets, the IPCC underscores the necessity of technologies
capable of removing gigatonnes of CO2 annually, with Geological Carbon Storage
(GCS) playing a central role. GCS involves capturing CO2 and injecting it into
deep geological formations for long-term storage, requiring precise monitoring
to ensure containment and prevent leakage. Time-lapse seismic imaging is
essential for tracking CO2 migration but often struggles to capture the
complexities of multi-phase subsurface flow. Digital Shadows (DS), leveraging
machine learning-driven data assimilation techniques such as nonlinear Bayesian
filtering and generative AI, provide a more detailed, uncertainty-aware
monitoring approach. By incorporating uncertainties in reservoir properties, DS
frameworks improve CO2 migration forecasts, reducing risks in GCS operations.
However, data assimilation depends on assumptions regarding reservoir
properties, rock physics models, and initial conditions, which, if inaccurate,
can compromise prediction reliability. This study demonstrates that augmenting
forecast ensembles with diverse rock physics models mitigates the impact of
incorrect assumptions and improves predictive accuracy, particularly in
differentiating uniform versus patchy saturation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Geological Carbon Storage Monitoring With 3d Digital Shadow
  Technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Prakash Gahlot, Rafael Orozco, Felix J. Herrmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geological Carbon Storage (GCS) is a key technology for achieving global
climate goals by capturing and storing CO2 in deep geological formations. Its
effectiveness and safety rely on accurate monitoring of subsurface CO2
migration using advanced time-lapse seismic imaging. A Digital Shadow framework
integrates field data, including seismic and borehole measurements, to track
CO2 saturation over time. Machine learning-assisted data assimilation
techniques, such as generative AI and nonlinear ensemble Bayesian filtering,
update a digital model of the CO2 plume while incorporating uncertainties in
reservoir properties. Compared to 2D approaches, 3D monitoring enhances the
spatial accuracy of GCS assessments, capturing the full extent of CO2
migration. This study extends the uncertainty-aware 2D Digital Shadow framework
by incorporating 3D seismic imaging and reservoir modeling, improving
decision-making and risk mitigation in CO2 storage projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimization for Building Social-Influence-Free Consensus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masaki Adachi, Siu Lun Chau, Wenjie Xu, Anurag Singh, Michael A. Osborne, Krikamol Muandet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Social Bayesian Optimization (SBO), a vote-efficient algorithm
for consensus-building in collective decision-making. In contrast to
single-agent scenarios, collective decision-making encompasses group dynamics
that may distort agents' preference feedback, thereby impeding their capacity
to achieve a social-influence-free consensus -- the most preferable decision
based on the aggregated agent utilities. We demonstrate that under mild
rationality axioms, reaching social-influence-free consensus using noisy
feedback alone is impossible. To address this, SBO employs a dual voting
system: cheap but noisy public votes (e.g., show of hands in a meeting), and
more accurate, though expensive, private votes (e.g., one-to-one interview). We
model social influence using an unknown social graph and leverage the dual
voting system to efficiently learn this graph. Our theoretical findigns show
that social graph estimation converges faster than the black-box estimation of
agents' utilities, allowing us to reduce reliance on costly private votes early
in the process. This enables efficient consensus-building primarily through
noisy public votes, which are debiased based on the estimated social graph to
infer social-influence-free feedback. We validate the efficacy of SBO across
multiple real-world applications, including thermal comfort, team building,
travel negotiation, and energy trading collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Training on Synthetic Data Make Models Less Robust? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingze Zhang, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An increasingly common practice is to train large language models (LLMs)
using synthetic data. Often this synthetic data is produced by the same or
similar LLMs as those it is being used to train. This raises the question of
whether the synthetic data might in fact exacerbate certain "blindspots" by
reinforcing heuristics that the LLM already encodes. In this paper, we conduct
simulated experiments on the natural language inference (NLI) task with
Llama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted
evaluation set designed to measure the presence of specific heuristic
strategies for NLI, as our "blindspot" task. Our goal is to determine whether
performance disparities between the general and blind spot tasks emerge. Our
results indicate that synthetic data does not reinforce blindspots in the way
we expected. Specifically, we see that, while fine-tuning with synthetic data
doesn't necessarily reduce the use of the heuristic, it also does not make it
worse as we hypothesized.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Early Risk Prediction of Pediatric Cardiac Arrest from Electronic Health
  Records via Multimodal Fused Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaying Lu, Stephanie R. Brown, Songyuan Liu, Shifan Zhao, Kejun Dong, Del Bold, Michael Fundora, Alaa Aljiffry, Alex Fedorov, Jocelyn Grunwell, Xiao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early prediction of pediatric cardiac arrest (CA) is critical for timely
intervention in high-risk intensive care settings. We introduce PedCA-FT, a
novel transformer-based framework that fuses tabular view of EHR with the
derived textual view of EHR to fully unleash the interactions of
high-dimensional risk factors and their dynamics. By employing dedicated
transformer modules for each modality view, PedCA-FT captures complex temporal
and contextual patterns to produce robust CA risk estimates. Evaluated on a
curated pediatric cohort from the CHOA-CICU database, our approach outperforms
ten other artificial intelligence models across five key performance metrics
and identifies clinically meaningful risk factors. These findings underscore
the potential of multimodal fusion techniques to enhance early CA detection and
improve patient care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for
  Fully-Utilized In-Memory Computing Architectures <span class="chip">DATE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Do Yeong Kang, Yeong Hwan Oh, Chanwook Hwang, Jinhee Kim, Kang Eun Jeon, Jong Hwan Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The implementation of Hyperdimensional Computing (HDC) on In-Memory Computing
(IMC) architectures faces significant challenges due to the mismatch between
highdimensional vectors and IMC array sizes, leading to inefficient memory
utilization and increased computation cycles. This paper presents MEMHD, a
Memory-Efficient Multi-centroid HDC framework designed to address these
challenges. MEMHD introduces a clustering-based initialization method and
quantization aware iterative learning for multi-centroid associative memory.
Through these approaches and its overall architecture, MEMHD achieves a
significant reduction in memory requirements while maintaining or improving
classification accuracy. Our approach achieves full utilization of IMC arrays
and enables one-shot (or few-shot) associative search. Experimental results
demonstrate that MEMHD outperforms state-of-the-art binary HDC models,
achieving up to 13.69% higher accuracy with the same memory usage, or 13.25x
more memory efficiency at the same accuracy level. Moreover, MEMHD reduces
computation cycles by up to 80x and array usage by up to 71x compared to
baseline IMC mapping methods when mapped to 128x128 IMC arrays, while
significantly improving energy and computation cycle efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear at DATE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting
  Confidence Improves Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Chen, Allan Raventos, Nan Cheng, Surya Ganguli, Shaul Druckmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in large language models (LLMs) highlights the power of
scaling test-time compute to achieve strong performance on complex tasks, such
as mathematical reasoning and code generation. This raises a critical question:
how should model training be modified to optimize performance under a
subsequent test-time compute strategy and budget? To explore this, we focus on
pass@N, a simple test-time strategy that searches for a correct answer in $N$
independent samples. We show, surprisingly, that training with cross-entropy
(CE) loss can be ${\it misaligned}$ with pass@N in that pass@N accuracy ${\it
decreases}$ with longer training. We explain the origins of this misalignment
in terms of model overconfidence induced by CE, and experimentally verify our
prediction of overconfidence as an impediment to scaling test-time compute via
pass@N. Furthermore we suggest a principled, modified training loss that is
better aligned to pass@N by limiting model confidence and rescuing pass@N test
performance. Our algorithm demonstrates improved mathematical reasoning on MATH
and MiniF2F benchmarks under several scenarios: (1) providing answers to math
questions; and (2) proving theorems by searching over proof trees of varying
shapes. Overall our work underscores the importance of co-designing two
traditionally separate phases of LLM development: training-time protocols and
test-time search and reasoning strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feature Importance Depends on Properties of the Data: Towards Choosing
  the Correct Explanations for Your Data and Decision Trees based Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Célia Wafa Ayad, Thomas Bonnier, Benjamin Bosch, Sonali Parbhoo, Jesse Read
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to ensure the reliability of the explanations of machine learning
models, it is crucial to establish their advantages and limits and in which
case each of these methods outperform. However, the current understanding of
when and how each method of explanation can be used is insufficient. To fill
this gap, we perform a comprehensive empirical evaluation by synthesizing
multiple datasets with the desired properties. Our main objective is to assess
the quality of feature importance estimates provided by local explanation
methods, which are used to explain predictions made by decision tree-based
models. By analyzing the results obtained from synthetic datasets as well as
publicly available binary classification datasets, we observe notable
disparities in the magnitude and sign of the feature importance estimates
generated by these methods. Moreover, we find that these estimates are
sensitive to specific properties present in the data. Although some model
hyper-parameters do not significantly influence feature importance assignment,
it is important to recognize that each method of explanation has limitations in
specific contexts. Our assessment highlights these limitations and provides
valuable insight into the suitability and reliability of different explanatory
methods in various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Distribution Quantization in Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blaise Delattre, Sylvain Delattre, Alexandre Vérine, Alexandre Allauzen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional expectation \mathbb{E}(Y \mid X) often fails to capture the
complexity of multimodal conditional distributions \mathcal{L}(Y \mid X). To
address this, we propose using n-point conditional quantizations--functional
mappings of X that are learnable via gradient descent--to approximate
\mathcal{L}(Y \mid X). This approach adapts Competitive Learning Vector
Quantization (CLVQ), tailored for conditional distributions. It goes beyond
single-valued predictions by providing multiple representative points that
better reflect multimodal structures. It enables the approximation of the true
conditional law in the Wasserstein distance. The resulting framework is
theoretically grounded and useful for uncertainty quantification and multimodal
data generation tasks. For example, in computer vision inpainting tasks,
multiple plausible reconstructions may exist for the same partially observed
input image X. We demonstrate the effectiveness of our approach through
experiments on synthetic and real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHARP: Accelerating Language Model Inference by SHaring Adjacent layers
  with Recovery Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Wang, Hanxian Huang, Yifang Chen, Jishen Zhao, Simon Shaolei Du, Yuandong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large language models (LLMs) have advanced natural language processing
tasks, their growing computational and memory demands make deployment on
resource-constrained devices like mobile phones increasingly challenging. In
this paper, we propose SHARP (SHaring Adjacent Layers with Recovery
Parameters), a novel approach to accelerate LLM inference by sharing parameters
across adjacent layers, thus reducing memory load overhead, while introducing
low-rank recovery parameters to maintain performance. Inspired by observations
that consecutive layers have similar outputs, SHARP employs a two-stage
recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT).
The SLW stage aligns the outputs of the shared layers using L_2 loss, providing
a good initialization for the following SFT stage to further restore the model
performance. Extensive experiments demonstrate that SHARP can recover the
model's perplexity on various in-distribution tasks using no more than 50k
fine-tuning data while reducing the number of stored MLP parameters by 38% to
65%. We also conduct several ablation studies of SHARP and show that replacing
layers towards the later parts of the model yields better performance
retention, and that different recovery parameterizations perform similarly when
parameter counts are matched. Furthermore, SHARP saves 42.8% in model storage
and reduces the total inference time by 42.2% compared to the original
Llama2-7b model on mobile devices. Our results highlight SHARP as an efficient
solution for reducing inference costs in deploying LLMs without the need for
pretraining-scale resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small steps no more: Global convergence of stochastic gradient bandits
  for arbitrary learning rates <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Mei, Bo Dai, Alekh Agarwal, Sharan Vaswani, Anant Raj, Csaba Szepesvari, Dale Schuurmans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a new understanding of the stochastic gradient bandit algorithm by
showing that it converges to a globally optimal policy almost surely using
\emph{any} constant learning rate. This result demonstrates that the stochastic
gradient algorithm continues to balance exploration and exploitation
appropriately even in scenarios where standard smoothness and noise control
assumptions break down. The proofs are based on novel findings about action
sampling rates and the relationship between cumulative progress and noise, and
extend the current understanding of how simple stochastic gradient methods
behave in bandit settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version for a paper published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Captured by Captions: On Memorization and its Mitigation in CLIP Models <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Wang, Adam Dziedzic, Grace C. Kim, Michael Backes, Franziska Boenisch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal models, such as CLIP, have demonstrated strong performance in
aligning visual and textual representations, excelling in tasks like image
retrieval and zero-shot classification. Despite this success, the mechanisms by
which these models utilize training data, particularly the role of
memorization, remain unclear. In uni-modal models, both supervised and
self-supervised, memorization has been shown to be essential for
generalization. However, it is not well understood how these findings would
apply to CLIP, which incorporates elements from both supervised learning via
captions that provide a supervisory signal similar to labels, and from
self-supervised learning via the contrastive objective. To bridge this gap in
understanding, we propose a formal definition of memorization in CLIP (CLIPMem)
and use it to quantify memorization in CLIP models. Our results indicate that
CLIP's memorization behavior falls between the supervised and self-supervised
paradigms, with "mis-captioned" samples exhibiting highest levels of
memorization. Additionally, we find that the text encoder contributes more to
memorization than the image encoder, suggesting that mitigation strategies
should focus on the text domain. Building on these insights, we propose
multiple strategies to reduce memorization while at the same time improving
utility--something that had not been shown before for traditional learning
paradigms where reducing memorization typically results in utility decrease.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-TPP: Integrating Temporal Point Processes with Language Models
  for Event Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quyu Kong, Yixuan Zhang, Yang Liu, Panrong Tong, Enqi Liu, Feng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Point Processes (TPPs) have been widely used for event sequence
modeling, but they often struggle to incorporate rich textual event
descriptions effectively. Conversely, while Large Language Models (LLMs) have
been shown remarkable capabilities in processing textual data, they lack
mechanisms for handling temporal dynamics. To bridge this gap, we introduce
Language-TPP, a unified framework that integrates TPPs with LLMs for enhanced
event sequence modeling. Language-TPP introduces a novel temporal encoding
mechanism that converts continuous time intervals into specialized byte-tokens,
enabling seamless integration with standard LLM architectures. This approach
allows Language-TPP to achieve state-of-the-art performance across multiple TPP
tasks, including event time prediction, type prediction, and intensity
estimation, on five datasets. Additionally, we demonstrate that incorporating
temporal information significantly improves the quality of generated event
descriptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Robust Framework for Multimodal Hate Detection: A Study on
  <span class="highlight-title">Video</span> vs. Image-based Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Girish A. Koushik, Diptesh Kanojia, Helen Treharne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms enable the propagation of hateful content across
different modalities such as textual, auditory, and visual, necessitating
effective detection methods. While recent approaches have shown promise in
handling individual modalities, their effectiveness across different modality
combinations remains unexplored. This paper presents a systematic analysis of
fusion-based approaches for multimodal hate detection, focusing on their
performance across video and image-based content. Our comprehensive evaluation
reveals significant modality-specific limitations: while simple embedding
fusion achieves state-of-the-art performance on video content (HateMM dataset)
with a 9.9% points F1-score improvement, it struggles with complex image-text
relationships in memes (Hateful Memes dataset). Through detailed ablation
studies and error analysis, we demonstrate how current fusion approaches fail
to capture nuanced cross-modal interactions, particularly in cases involving
benign confounders. Our findings provide crucial insights for developing more
robust hate detection systems and highlight the need for modality-specific
architectural considerations. The code is available at
https://github.com/gak97/Video-vs-Meme-Hate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the MM4SG Workshop at the WebConf 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Cellular Responses with Variational Causal Inference and
  Refined Relational Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00116v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00116v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulun Wu, Robert A. Barton, Zichen Wang, Vassilis N. Ioannidis, Carlo De Donno, Layne C. Price, Luis F. Voloch, George Karypis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the responses of a cell under perturbations may bring important
benefits to drug discovery and personalized therapeutics. In this work, we
propose a novel graph variational Bayesian causal inference framework to
predict a cell's gene expressions under counterfactual perturbations
(perturbations that this cell did not factually receive), leveraging
information representing biological knowledge in the form of gene regulatory
networks (GRNs) to aid individualized cellular response predictions. Aiming at
a data-adaptive GRN, we also developed an adjacency matrix updating technique
for graph convolutional networks and used it to refine GRNs during
pre-training, which generated more insights on gene relations and enhanced
model performance. Additionally, we propose a robust estimator within our
framework for the asymptotically efficient estimation of marginal perturbation
effect, which is yet to be carried out in previous works. With extensive
experiments, we exhibited the advantage of our approach over state-of-the-art
deep learning models for individual response prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Generative Modeling with Variational Causal Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulun Wu, Louie McConnell, Claudia Iriondo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating an individual's counterfactual outcomes under interventions is a
challenging task for traditional causal inference and supervised learning
approaches when the outcome is high-dimensional (e.g. gene expressions, facial
images) and covariates are relatively limited. In this case, to predict one's
outcomes under counterfactual treatments, it is crucial to leverage individual
information contained in the observed outcome in addition to the covariates.
Prior works using variational inference in counterfactual generative modeling
have been focusing on neural adaptations and model variants within the
conditional variational autoencoder formulation, which we argue is
fundamentally ill-suited to the notion of counterfactual in causal inference.
In this work, we present a novel variational Bayesian causal inference
framework and its theoretical backings to properly handle counterfactual
generative modeling tasks, through which we are able to conduct counterfactual
supervision end-to-end during training without any counterfactual samples, and
encourage disentangled exogenous noise abduction that aids the correct
identification of causal effect in counterfactual generations. In experiments,
we demonstrate the advantage of our framework compared to state-of-the-art
models in counterfactual generative modeling on multiple benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Probabilistic Inference Approach to Inference-Time Scaling of LLMs
  using Particle-Based Monte Carlo Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01618v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01618v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Puri, Shivchander Sudalairaj, Guangxuan Xu, Kai Xu, Akash Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved significant performance gains via
scaling up model sizes and/or data. However, recent evidence suggests
diminishing returns from such approaches, motivating scaling the computation
spent at inference time. Existing inference-time scaling methods, usually with
reward models, cast the task as a search problem, which tends to be vulnerable
to reward hacking as a consequence of approximation errors in reward models. In
this paper, we instead cast inference-time scaling as a probabilistic inference
task and leverage sampling-based techniques to explore the typical set of the
state distribution of a state-space model with an approximate likelihood,
rather than optimize for its mode directly. We propose a novel inference-time
scaling approach by adapting particle-based Monte Carlo methods to this task.
Our empirical evaluation demonstrates that our methods have a 4-16x better
scaling rate over our deterministic search counterparts on various challenging
mathematical reasoning tasks. Using our approach, we show that
Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts,
while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts.
Our work not only presents an effective method to inference-time scaling, but
also connects the rich literature in probabilistic inference with
inference-time scaling of LLMs to develop more robust algorithms in future
work. Code, videos, and further information available at
https://probabilistic-inference-scaling.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ More Experts Than Galaxies: Conditionally-overlapping Experts With
  Biologically-Inspired Fixed Routing <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08003v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08003v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagi Shaier, Francisco Pereira, Katharina von der Wense, Lawrence E Hunter, Matt Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of biological neural systems has led to both modularity and
sparse coding, which enables energy efficiency and robustness across the
diversity of tasks in the lifespan. In contrast, standard neural networks rely
on dense, non-specialized architectures, where all model parameters are
simultaneously updated to learn multiple tasks, leading to interference.
Current sparse neural network approaches aim to alleviate this issue but are
hindered by limitations such as 1) trainable gating functions that cause
representation collapse, 2) disjoint experts that result in redundant
computation and slow learning, and 3) reliance on explicit input or task IDs
that limit flexibility and scalability. In this paper we propose Conditionally
Overlapping Mixture of ExperTs (COMET), a general deep learning method that
addresses these challenges by inducing a modular, sparse architecture with an
exponential number of overlapping experts. COMET replaces the trainable gating
function used in Sparse Mixture of Experts with a fixed, biologically inspired
random projection applied to individual input representations. This design
causes the degree of expert overlap to depend on input similarity, so that
similar inputs tend to share more parameters. This results in faster learning
per update step and improved out-of-sample generalization. We demonstrate the
effectiveness of COMET on a range of tasks, including image classification,
language modeling, and regression, using several popular deep learning
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMem: A Modular Approach for Ultra-Efficient Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Tang, Peng Sun, Zhenglin Cheng, Tao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies indicate that the denoising process in deep generative
diffusion models implicitly learns and memorizes semantic information from the
data distribution. These findings suggest that capturing more complex data
distributions requires larger neural networks, leading to a substantial
increase in computational demands, which in turn become the primary bottleneck
in both training and inference of diffusion models. To this end, we introduce
GMem: A Modular Approach for Ultra-Efficient Generative Models. Our approach
GMem decouples the memory capacity from model and implements it as a separate,
immutable memory set that preserves the essential semantic information in the
data. The results are significant: GMem enhances both training, sampling
efficiency, and diversity generation. This design on one hand reduces the
reliance on network for memorize complex data distribution and thus enhancing
both training and sampling efficiency. On ImageNet at $256 \times 256$
resolution, GMem achieves a $50\times$ training speedup compared to SiT,
reaching FID $=7.66$ in fewer than $28$ epochs ($\sim 4$ hours training time),
while SiT requires $1400$ epochs. Without classifier-free guidance, GMem
achieves state-of-the-art (SoTA) performance FID $=1.53$ in $160$ epochs with
only $\sim 20$ hours of training, outperforming LightningDiT which requires
$800$ epochs and $\sim 95$ hours to attain FID $=2.17$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision Foundation Models in Remote Sensing: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Lu, Junlin Guo, James R Zimmer-Dauphinee, Jordan M Nieusma, Xiao Wang, Parker VanValkenburgh, Steven A Wernke, Yuankai Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) technologies have profoundly transformed the
field of remote sensing, revolutionizing data collection, processing, and
analysis. Traditionally reliant on manual interpretation and task-specific
models, remote sensing research has been significantly enhanced by the advent
of foundation models-large-scale, pre-trained AI models capable of performing a
wide array of tasks with unprecedented accuracy and efficiency. This paper
provides a comprehensive survey of foundation models in the remote sensing
domain. We categorize these models based on their architectures, pre-training
datasets, and methodologies. Through detailed performance comparisons, we
highlight emerging trends and the significant advancements achieved by those
foundation models. Additionally, we discuss technical challenges, practical
implications, and future research directions, addressing the need for
high-quality data, computational resources, and improved model generalization.
Our research also finds that pre-training methods, particularly self-supervised
learning techniques like contrastive learning and masked autoencoders,
remarkably enhance the performance and robustness of foundation models. This
survey aims to serve as a resource for researchers and practitioners by
providing a panorama of advances and promising pathways for continued
development and application of foundation models in remote sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closure Discovery for Coarse-Grained Partial Differential Equations
  Using Grid-based Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00972v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00972v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Philipp von Bassewitz, Sebastian Kaltenbach, Petros Koumoutsakos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable predictions of critical phenomena, such as weather, wildfires and
epidemics often rely on models described by Partial Differential Equations
(PDEs). However, simulations that capture the full range of spatio-temporal
scales described by such PDEs are often prohibitively expensive. Consequently,
coarse-grained simulations are usually deployed that adopt various heuristics
and empirical closure terms to account for the missing information. We propose
a novel and systematic approach for identifying closures in under-resolved PDEs
using grid-based Reinforcement Learning. This formulation incorporates
inductive bias and exploits locality by deploying a central policy represented
efficiently by a Fully Convolutional Network (FCN). We demonstrate the
capabilities and limitations of our framework through numerical solutions of
the advection equation and the Burgers' equation. Our results show accurate
predictions for in- and out-of-distribution test cases as well as a significant
speedup compared to resolving all scales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Parsimony and Learning (CPAL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnvId: A Metric Learning Approach for Forensic Few-Shot Identification
  of Unseen Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denise Moussa, Germans Hirsch, Christian Riess
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio recordings may provide important evidence in criminal investigations.
One such case is the forensic association of a recorded audio to its recording
location. For example, a voice message may be the only investigative cue to
narrow down the candidate sites for a crime. Up to now, several works provide
supervised classification tools for closed-set recording environment
identification under relatively clean recording conditions. However, in
forensic investigations, the candidate locations are case-specific. Thus,
supervised learning techniques are not applicable without retraining a
classifier on a sufficient amount of training samples for each case and
respective candidate set. In addition, a forensic tool has to deal with audio
material from uncontrolled sources with variable properties and quality. In
this work, we therefore attempt a major step towards practical forensic
application scenarios. We propose a representation learning framework called
EnvId, short for environment identification. EnvId avoids case-specific
retraining by modeling the task as a few-shot classification problem. We
demonstrate that EnvId can handle forensically challenging material. It
provides good quality predictions even under unseen signal degradations,
out-of-distribution reverberation characteristics or recording position
mismatches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TIFS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Characterization of point-source transient events with a rolling-shutter
  compressed sensing system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Qiu, Joshua Michalenko, Lilian K. Casias, Cameron J. Radosevich, Jon Slater, Eric A. Shields
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-source transient events (PSTEs) - optical events that are both
extremely fast and extremely small - pose several challenges to an imaging
system. Due to their speed, accurately characterizing such events often
requires detectors with very high frame rates. Due to their size, accurately
detecting such events requires maintaining coverage over an extended
field-of-view, often through the use of imaging focal plane arrays (FPA) with a
global shutter readout. Traditional imaging systems that meet these
requirements are costly in terms of price, size, weight, power consumption, and
data bandwidth, and there is a need for cheaper solutions with adequate
temporal and spatial coverage. To address these issues, we develop a novel
compressed sensing algorithm adapted to the rolling shutter readout of an
imaging system. This approach enables reconstruction of a PSTE signature at the
sampling rate of the rolling shutter, offering a 1-2 order of magnitude
temporal speedup and a proportional reduction in data bandwidth. We present
empirical results demonstrating accurate recovery of PSTEs using measurements
that are spatially undersampled by a factor of 25, and our simulations show
that, relative to other compressed sensing algorithms, our algorithm is both
faster and yields higher quality reconstructions. We also present theoretical
results characterizing our algorithm and corroborating simulations. The
potential impact of our work includes the development of much faster, cheaper
sensor solutions for PSTE detection and characterization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Algorithms for Structured Linear and Kernel Support Vector
  Machines <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhou Gu, Zhao Song, Lichen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadratic programming is a ubiquitous prototype in convex programming. Many
machine learning problems can be formulated as quadratic programming, including
the famous Support Vector Machines (SVMs). Linear and kernel SVMs have been
among the most popular models in machine learning over the past three decades,
prior to the deep learning era.
  Generally, a quadratic program has an input size of $\Theta(n^2)$, where $n$
is the number of variables. Assuming the Strong Exponential Time Hypothesis
($\textsf{SETH}$), it is known that no $O(n^{2-o(1)})$ time algorithm exists
when the quadratic objective matrix is positive semidefinite (Backurs, Indyk,
and Schmidt, NeurIPS'17). However, problems such as SVMs usually admit much
smaller input sizes: one is given $n$ data points, each of dimension $d$, and
$d$ is oftentimes much smaller than $n$. Furthermore, the SVM program has only
$O(1)$ equality linear constraints. This suggests that faster algorithms are
feasible, provided the program exhibits certain structures.
  In this work, we design the first nearly-linear time algorithm for solving
quadratic programs whenever the quadratic objective admits a low-rank
factorization, and the number of linear constraints is small. Consequently, we
obtain results for SVMs:
  * For linear SVM when the input data is $d$-dimensional, our algorithm runs
in time $\widetilde O(nd^{(\omega+1)/2}\log(1/\epsilon))$ where $\omega\approx
2.37$ is the fast matrix multiplication exponent;
  * For Gaussian kernel SVM, when the data dimension $d = {\color{black}O(\log
n)}$ and the squared dataset radius is sub-logarithmic in $n$, our algorithm
runs in time $O(n^{1+o(1)}\log(1/\epsilon))$. We also prove that when the
squared dataset radius is at least $\Omega(\log^2 n)$, then
$\Omega(n^{2-o(1)})$ time is required. This improves upon the prior best lower
bound in both the dimension $d$ and the squared dataset radius.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Matters in Hierarchical Search for Combinatorial Reasoning
  Problems? <span class="chip">ICLR
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03361v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03361v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michał Zawalski, Gracjan Góral, Michał Tyrolski, Emilia Wiśnios, Franciszek Budrowski, Marek Cygan, Łukasz Kuciński, Piotr Miłoś
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently tackling combinatorial reasoning problems, particularly the
notorious NP-hard tasks, remains a significant challenge for AI research.
Recent efforts have sought to enhance planning by incorporating hierarchical
high-level search strategies, known as subgoal methods. While promising, their
performance against traditional low-level planners is inconsistent, raising
questions about their application contexts. In this study, we conduct an
in-depth exploration of subgoal-planning methods for combinatorial reasoning.
We identify the attributes pivotal for leveraging the advantages of high-level
search: hard-to-learn value functions, complex action spaces, presence of dead
ends in the environment, or using data collected from diverse experts. We
propose a consistent evaluation methodology to achieve meaningful comparisons
between methods and reevaluate the state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Generative Models for Decision Making Workshop at ICLR
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science
  Research Repositories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Xiao, Runhui Wang, Luyang Kong, Davor Golac, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity of computer science research projects demands more
effective tools for deploying code repositories. Large Language Models (LLMs),
such as Anthropic Claude and Meta Llama, have demonstrated significant
advancements across various fields of computer science research, including the
automation of diverse software engineering tasks. To evaluate the effectiveness
of LLMs in handling complex code development tasks of research projects,
particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark
for Computer Science Research projects. This benchmark assesses LLMs from
various aspects including accuracy, efficiency, and deployment script quality,
aiming to explore their potential in conducting computer science research
autonomously. We also introduce a novel framework, CSR-Agents, that utilizes
multiple LLM agents to automate the deployment of GitHub code repositories of
computer science research projects. Specifically, by checking instructions from
markdown files and interpreting repository structures, the model generates and
iteratively improves bash commands that set up the experimental environments
and deploy the code to conduct research tasks. Preliminary results from
CSR-Bench indicate that LLM agents can significantly enhance the workflow of
repository deployment, thereby boosting developer productivity and improving
the management of developmental workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emulators for stellar profiles in binary population modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Teng, Ugur Demir, Zoheyr Doctor, Philipp M. Srivastava, Shamal Lalvani, Vicky Kalogera, Aggelos Katsaggelos, Jeff J. Andrews, Simone S. Bavera, Max M. Briel, Seth Gossage, Konstantinos Kovlakas, Matthias U. Kruckow, Kyle Akira Rocha, Meng Sun, Zepei Xing, Emmanouil Zapartas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge about the internal physical structure of stars is crucial to
understanding their evolution. The novel binary population synthesis code
POSYDON includes a module for interpolating the stellar and binary properties
of any system at the end of binary MESA evolution based on a pre-computed set
of models. In this work, we present a new emulation method for predicting
stellar profiles, i.e., the internal stellar structure along the radial axis,
using machine learning techniques. We use principal component analysis for
dimensionality reduction and fully-connected feed-forward neural networks for
making predictions. We find accuracy to be comparable to that of nearest
neighbor approximation, with a strong advantage in terms of memory and storage
efficiency. By providing a versatile framework for modeling stellar internal
structure, the emulation method presented here will enable faster simulations
of higher physical fidelity, offering a foundation for a wide range of
large-scale population studies of stellar and binary evolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures. Accepted for publication by Astronomy and
  Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Unlearning via Information Theoretic Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shizhou Xu, Thomas Strohmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we effectively remove or "unlearn" undesirable information, such as
specific features or individual data points, from a learning outcome while
minimizing utility loss and ensuring rigorous guarantees? We introduce a
mathematical framework based on information-theoretic regularization to address
both feature and data point unlearning. For feature unlearning, we derive a
unified solution that simultaneously optimizes diverse learning objectives,
including entropy, conditional entropy, KL-divergence, and the energy of
conditional probability. For data point unlearning, we first propose a novel
definition that serves as a practical condition for unlearning via retraining,
is easy to verify, and aligns with the principles of differential privacy from
an inference perspective. Then, we provide provable guarantees for our
framework on data point unlearning. By combining flexibility in learning
objectives with simplicity in regularization design, our approach is highly
adaptable and practical for a wide range of machine learning and AI
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EIA: Environmental Injection Attack on Generalist Web Agents for Privacy
  Leakage <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11295v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11295v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Liao, Lingbo Mo, Chejian Xu, Mintong Kang, Jiawei Zhang, Chaowei Xiao, Yuan Tian, Bo Li, Huan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist web agents have demonstrated remarkable potential in autonomously
completing a wide range of tasks on real websites, significantly boosting human
productivity. However, web tasks, such as booking flights, usually involve
users' PII, which may be exposed to potential privacy risks if web agents
accidentally interact with compromised websites, a scenario that remains
largely unexplored in the literature. In this work, we narrow this gap by
conducting the first study on the privacy risks of generalist web agents in
adversarial environments. First, we present a realistic threat model for
attacks on the website, where we consider two adversarial targets: stealing
users' specific PII or the entire user request. Then, we propose a novel attack
method, termed Environmental Injection Attack (EIA). EIA injects malicious
content designed to adapt well to environments where the agents operate and our
work instantiates EIA specifically for privacy scenarios in web environments.
We collect 177 action steps that involve diverse PII categories on realistic
websites from the Mind2Web, and conduct experiments using one of the most
capable generalist web agent frameworks to date. The results demonstrate that
EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user
request. Additionally, by accessing the stealthiness and experimenting with a
defensive system prompt, we indicate that EIA is hard to detect and mitigate.
Notably, attacks that are not well adapted for a webpage can be detected via
human inspection, leading to our discussion about the trade-off between
security and autonomy. However, extra attackers' efforts can make EIA
seamlessly adapted, rendering such supervision ineffective. Thus, we further
discuss the defenses at the pre- and post-deployment stages of the websites
without relying on human supervision and call for more advanced defense
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Learning of Causal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyat Mahajan, Jannes Gladrow, Agrin Hilmkil, Cheng Zhang, Meyer Scetbon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing acquisition of datasets over time, we now have access to
precise and varied descriptions of the world, encompassing a broad range of
phenomena. These datasets can be seen as observations from unknown causal
generative processes, commonly described by Structural Causal Models (SCMs).
Recovering SCMs from observations poses formidable challenges, and often
requires us to learn a specific generative model for each dataset. In this
work, we propose to learn a \emph{single} model capable of inferring the SCMs
in a zero-shot manner. Rather than learning a specific SCM for each dataset, we
enable the Fixed-Point Approach (FiP)~\citep{scetbon2024fip} to infer the
generative SCMs conditionally on their empirical representations. As a
by-product, our approach can perform zero-shot generation of new dataset
samples and intervened samples. We demonstrate via experiments that our
amortized procedure achieves performances on par with SoTA methods trained
specifically for each dataset on both in and out-of-distribution problems. To
the best of our knowledge, this is the first time that SCMs are inferred in a
zero-shot manner from observations, paving the way for a paradigmatic shift
toward the assimilation of causal knowledge across datasets. The code is
available on Github.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logicbreaks: A Framework for Understanding Subversion of Rule-based
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00075v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00075v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Xue, Avishree Khare, Rajeev Alur, Surbhi Goel, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how to subvert large language models (LLMs) from following
prompt-specified rules. We first formalize rule-following as inference in
propositional Horn logic, a mathematical system in which rules have the form
"if $P$ and $Q$, then $R$" for some propositions $P$, $Q$, and $R$. Next, we
prove that although small transformers can faithfully follow such rules,
maliciously crafted prompts can still mislead both theoretical constructions
and models learned from data. Furthermore, we demonstrate that popular attack
algorithms on LLMs find adversarial prompts and induce attention patterns that
align with our theory. Our novel logic-based framework provides a foundation
for studying LLMs in rule-based settings, enabling a formal analysis of tasks
like logical reasoning and jailbreak attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EfficientLLM: Scalable Pruning-Aware Pretraining for
  Architecture-Agnostic Edge Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingrun Xing, Zheng Liu, Shitao Xiao, Boyan Gao, Yiming Liang, Wanpeng Zhang, Haokun Lin, Guoqi Li, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models (LLMs) driven by scaling laws, achieve
intelligence emergency in large model sizes. Recently, the increasing concerns
about cloud costs, latency, and privacy make it an urgent requirement to
develop compact edge language models. Distinguished from direct pretraining
that bounded by the scaling law, this work proposes the pruning-aware
pretraining, focusing on retaining performance of much larger optimized models.
It features following characteristics: 1) Data-scalable: we introduce minimal
parameter groups in LLM and continuously optimize structural pruning, extending
post-training pruning methods like LLM-Pruner and SparseGPT into the
pretraining phase. 2) Architecture-agnostic: the LLM architecture is
auto-designed using saliency-driven pruning, which is the first time to exceed
SoTA human-designed LLMs in modern pretraining. We reveal that it achieves
top-quality edge language models, termed EfficientLLM, by scaling up LLM
compression and extending its boundary. EfficientLLM significantly outperforms
SoTA baselines with $100M \sim 1B$ parameters, such as MobileLLM, SmolLM,
Qwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first
attempt, EfficientLLM bridges the performance gap between traditional LLM
compression and direct pretraining methods, and we will fully open source at
https://github.com/Xingrun-Xing2/EfficientLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Language Models on Synthetic Edit Sequences Improves Code
  Synthesis <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02749v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02749v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ulyana Piterbarg, Lerrel Pinto, Rob Fergus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software engineers mainly write code by editing existing programs. In
contrast, language models (LMs) autoregressively synthesize programs in a
single pass. One explanation for this is the scarcity of sequential edit data.
While high-quality instruction data for code synthesis is scarce, edit data for
synthesis is even scarcer. To fill this gap, we develop a synthetic data
generation algorithm called LintSeq. This algorithm refactors programs into
sequences of synthetic edits by using a linter to procedurally sample across
interdependent lines of source code. Synthetic edits sampled with LintSeq
reflect the syntax and semantics of their programming language. To test the
algorithm, we use it to refactor a dataset of instruction + program pairs into
instruction + program-diff-sequence tuples. Then, we fine-tune a series of
smaller LMs ranging from 2.6B to 14B parameters on both the re-factored and
original versions of this dataset. We perform comprehensive evaluations
comparing edit sequence code LMs against baselines on HumanEval, MBPP(+),
CodeContests, DS-1000, and BigCodeBench. We show that models fine-tuned to
iteratively synthesize code match or outperform baselines on pass@1, and
exhibit better scaling across higher pass@k as a function of total test-time
FLOPs. Finally, we also pretrain our own tiny LMs for code understanding. We
show that fine-tuning these models to synthesize code edit-by-edit results in
strong performance on HumanEval and MBPP(+) compared to existing code language
models of similar scale such as CodeT5+, AlphaCode, and Codex.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ENFORCE: Exact Nonlinear Constrained Learning with Adaptive-depth Neural
  Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Lastrucci, Artur M. Schweidtmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring neural networks adhere to domain-specific constraints is crucial for
addressing safety and ethical concerns while also enhancing prediction
accuracy. Despite the nonlinear nature of most real-world tasks, existing
methods are predominantly limited to affine or convex constraints. We introduce
ENFORCE, a neural network architecture that guarantees predictions to satisfy
nonlinear constraints exactly. ENFORCE is trained with standard unconstrained
gradient-based optimizers (e.g., Adam) and leverages autodifferentiation and
local neural projections to enforce any $\mathcal{C}^1$ constraint to arbitrary
tolerance $\epsilon$. We build an adaptive-depth neural projection (AdaNP)
module that dynamically adjusts its complexity to suit the specific problem and
the required tolerance levels. ENFORCE guarantees satisfaction of equality
constraints that are nonlinear in both inputs and outputs of the neural network
with minimal (and adjustable) computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cache Me If You Must: Adaptive Key-Value Quantization for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alina Shutova, Vladimir Malinovskii, Vage Egiazarian, Denis Kuznedelev, Denis Mazur, Nikita Surkov, Ivan Ermakov, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient real-world deployments of large language models (LLMs) rely on
Key-Value (KV) caching for processing and generating long outputs, reducing the
need for repetitive computation. For large contexts, Key-Value caches can take
up tens of gigabytes of device memory, as they store vector representations for
each token and layer. Recent work has shown that the cached vectors can be
compressed through quantization, pruning or merging, but these techniques often
compromise quality towards higher compression rates. In this work, we aim to
improve Key & Value compression by exploiting two observations: 1) the inherent
dependencies between keys and values across different layers, and 2)
high-compression mechanisms for internal network states. We propose AQUA-KV, an
adaptive quantization for Key-Value caches that relies on compact adapters to
exploit existing dependencies between Keys and Values, and aims to "optimally"
compress the information that cannot be predicted. AQUA-KV significantly
improves compression rates, while maintaining high accuracy on state-of-the-art
LLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5
bits per value with under $1\%$ relative error in perplexity and LongBench
scores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a
single GPU within 1-6 hours, even for 70B models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accessing Vision Foundation Models via ImageNet-1K <span class="chip">ICLR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitian Zhang, Xu Ma, Yue Bai, Huan Wang, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision foundation models are renowned for the generalization ability due to
massive training data. Nevertheless, they demand tremendous training resources,
and the training data is often inaccessible, e.g., CLIP, DINOv2, posing great
challenges to developing derivatives that could facilitate the research. In
this work, we offer a very simple and general solution, named \textit{Proteus},
to distill foundation models into smaller equivalents on ImageNet-1K without
access to the original training data. Specifically, we remove the designs from
conventional knowledge distillation settings that result in dataset bias and
present three levels of training objectives, i.e., token, patch, and feature,
to maximize the efficacy of knowledge transfer. In this manner, Proteus is
trained at ImageNet-level costs with surprising ability, facilitating the
accessibility of training foundation models for the broader research community.
When leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the
performance of the Oracle method DINOv2-L/14 (142M training data) across 19
benchmarks and outperforms other vision foundation models including CLIP-L/14
(400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M) with a significantly
smaller training set of 1.2M images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation
  during Multi-stage Fine-tuning <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08096v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08096v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Bai, Chih-Kuan Yeh, Cho-Jui Hsieh, Ankur Taly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incrementally fine-tuning foundational models on new tasks or domains is now
the de facto approach in NLP. A known pitfall of this approach is the
\emph{catastrophic forgetting} of prior knowledge that happens during
fine-tuning. A common approach to alleviate such forgetting is to rehearse
samples from prior tasks during fine-tuning. Several existing works assume a
fixed memory buffer to store prior task examples, while relying on inferences
(forward passes) with the model at hand for choosing examples for rehearsal
from the buffer. However, given the increasing computational cost of model
inference, and decreasing cost of data storage, we focus on the setting to
rehearse samples with a fixed computational budget instead of a fixed memory
budget. We propose a sampling scheme, \texttt{\bf mix-cd}, that prioritizes
rehearsal of ``collateral damage'' samples, which are samples predicted
correctly by the prior model but forgotten by the incrementally tuned one. The
crux of our scheme is a procedure to efficiently estimate the density of
collateral damage samples without incurring additional model inferences. Our
approach is computationally efficient, easy to implement, and outperforms
several leading continual learning methods in compute-constrained settings. All
the code will be publicly available at
https://github.com/jybai/mix-cd-rehearsal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures. Published in NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning from <span class="highlight-title">Human</span> Feedback with Active Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixuan Ji, Jiafan He, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLM) with human preference plays a key role
in building modern generative models and can be achieved by reinforcement
learning from human feedback (RLHF). Despite their superior performance,
current RLHF approaches often require a large amount of human-labelled
preference data, which is expensive to collect. In this paper, inspired by the
success of active learning, we address this problem by proposing
query-efficient RLHF methods. We first formalize the alignment problem as a
contextual dueling bandit problem and design an active-query-based proximal
policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$
instance-dependent regret bound and an $\tilde{O}(d^2/\Delta^2)$ query
complexity, where $d$ is the dimension of feature space and $\Delta$ is the
sub-optimality gap over all the contexts. We then propose ADPO, a practical
version of our algorithm based on direct preference optimization (DPO) and
apply it to fine-tuning LLMs. Our experiments show that ADPO, while only making
about half of queries for human preference, matches the performance of the
state-of-the-art DPO method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 1 figure, 4 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNSURE: self-supervised learning with Unknown Noise level and Stein's
  Unbiased Risk Estimate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01985v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01985v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julián Tachella, Mike Davies, Laurent Jacques
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, many self-supervised learning methods for image reconstruction have
been proposed that can learn from noisy data alone, bypassing the need for
ground-truth references. Most existing methods cluster around two classes: i)
Stein's Unbiased Risk Estimate (SURE) and similar approaches that assume full
knowledge of the noise distribution, and ii) Noise2Self and similar
cross-validation methods that require very mild knowledge about the noise
distribution. The first class of methods tends to be impractical, as the noise
level is often unknown in real-world applications, and the second class is
often suboptimal compared to supervised learning. In this paper, we provide a
theoretical framework that characterizes this expressivity-robustness trade-off
and propose a new approach based on SURE, but unlike the standard SURE, does
not require knowledge about the noise level. Throughout a series of
experiments, we show that the proposed estimator outperforms other existing
self-supervised methods on various imaging inverse problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Language Models to Reason Efficiently 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daman Arora, Andrea Zanette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling model size and training data has led to great advances in the
performance of Large Language Models (LLMs). However, the diminishing returns
of this approach necessitate alternative methods to improve model capabilities,
particularly in tasks requiring advanced reasoning. Large reasoning models,
which leverage long chain-of-thoughts, bring unprecedented breakthroughs in
problem-solving capabilities but at a substantial deployment cost associated to
longer generations. Reducing inference costs is crucial for the economic
feasibility, user experience, and environmental sustainability of these models.
  In this work, we propose to train large reasoning models to reason
efficiently. More precisely, we use reinforcement learning (RL) to train
reasoning models to dynamically allocate inference-time compute based on task
complexity. Our method incentivizes models to minimize unnecessary
computational overhead while maintaining accuracy, thereby achieving
substantial efficiency gains. It enables the derivation of a family of
reasoning models with varying efficiency levels, controlled via a single
hyperparameter. Experiments on two open-weight large reasoning models
demonstrate significant reductions in inference cost while preserving most of
the accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What makes math problems hard for reinforcement learning: a case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Shehper, Anibal M. Medina-Mardones, Lucas Fagan, Bartłomiej Lewandowski, Angus Gruen, Yang Qiu, Piotr Kucharski, Zhenghan Wang, Sergei Gukov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using a long-standing conjecture from combinatorial group theory, we explore,
from multiple perspectives, the challenges of finding rare instances carrying
disproportionately high rewards. Based on lessons learned in the context
defined by the Andrews-Curtis conjecture, we propose algorithmic enhancements
and a topological hardness measure with implications for a broad class of
search problems. As part of our study, we also address several open
mathematical questions. Notably, we demonstrate the length reducibility of all
but two presentations in the Akbulut-Kirby series (1981), and resolve various
potential counterexamples in the Miller-Schupp series (1991), including three
infinite subfamilies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages, 25 figures, 1 table. Try it:
  https://github.com/shehper/AC-Solver</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpaceMesh: A Continuous Representation for Learning Manifold Surface
  Meshes <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchang Shen, Zhaoshuo Li, Marc Law, Matan Atzmon, Sanja Fidler, James Lucas, Jun Gao, Nicholas Sharp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meshes are ubiquitous in visual computing and simulation, yet most existing
machine learning techniques represent meshes only indirectly, e.g. as the level
set of a scalar field or deformation of a template, or as a disordered triangle
soup lacking local structure. This work presents a scheme to directly generate
manifold, polygonal meshes of complex connectivity as the output of a neural
network. Our key innovation is to define a continuous latent connectivity space
at each mesh vertex, which implies the discrete mesh. In particular, our vertex
embeddings generate cyclic neighbor relationships in a halfedge mesh
representation, which gives a guarantee of edge-manifoldness and the ability to
represent general polygonal meshes. This representation is well-suited to
machine learning and stochastic optimization, without restriction on
connectivity or topology. We first explore the basic properties of this
representation, then use it to fit distributions of meshes from large datasets.
The resulting models generate diverse meshes with tessellation structure
learned from the dataset population, with concise details and high-quality mesh
elements. In applications, this approach not only yields high-quality outputs
from generative models, but also enables directly learning challenging geometry
processing tasks such as mesh repair.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>published at SIGGRAPH Asia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TopoTune : A Framework for Generalized Combinatorial Complex Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06530v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06530v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathilde Papillon, Guillermo Bernárdez, Claudio Battiloro, Nina Miolane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) excel in learning from relational datasets,
processing node and edge features in a way that preserves the symmetries of the
graph domain. However, many complex systems -- such as biological or social
networks--involve multiway complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Topological Deep Learning (TDL) aims to accommodate and leverage these
higher-order structures. Combinatorial Complex Neural Networks (CCNNs), fairly
general TDL models, have been shown to be more expressive and better performing
than GNNs. However, differently from the GNN ecosystem, TDL lacks a principled
and standardized framework for easily defining new architectures, restricting
its accessibility and applicability. To address this issue, we introduce
Generalized CCNNs (GCCNs), a novel simple yet powerful family of TDL models
that can be used to systematically transform any (graph) neural network into
its TDL counterpart. We prove that GCCNs generalize and subsume CCNNs, while
extensive experiments on a diverse class of GCCNs show that these architectures
consistently match or outperform CCNNs, often with less model complexity. In an
effort to accelerate and democratize TDL, we introduce TopoTune, a lightweight
software for defining, building, and training GCCNs with unprecedented
flexibility and ease.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Benefits of Balance: From Information Projections to Variance
  Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lang Liu, Ronak Mehta, Soumik Pal, Zaid Harchaoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data balancing across multiple modalities and sources appears in various
forms in foundation models in machine learning and AI, e.g. in CLIP and DINO.
We show that data balancing across modalities and sources actually offers an
unsuspected benefit: variance reduction. We present a non-asymptotic
statistical bound that quantifies this variance reduction effect and relates it
to the eigenvalue decay of Markov operators. Furthermore, we describe how
various forms of data balancing in contrastive multimodal learning and
self-supervised clustering can be better understood, and even improved upon,
owing to our variance reduction viewpoint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Faiss library 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08281v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08281v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, Hervé Jégou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector databases typically manage large collections of embedding vectors.
Currently, AI applications are growing rapidly, and so is the number of
embeddings that need to be stored and indexed. The Faiss library is dedicated
to vector similarity search, a core functionality of vector databases. Faiss is
a toolkit of indexing methods and related primitives used to search, cluster,
compress and transform vectors. This paper describes the trade-off space of
vector search and the design principles of Faiss in terms of structure,
approach to optimization and interfacing. We benchmark key features of the
library and discuss a few selected applications to highlight its broad
applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Novelty Detection in Reinforcement Learning with World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08731v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08731v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geigh Zollicoffer, Kenneth Eaton, Jonathan Balloch, Julia Kim, Wei Zhou, Robert Wright, Mark O. Riedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) using world models has found significant recent
successes. However, when a sudden change to world mechanics or properties
occurs then agent performance and reliability can dramatically decline. We
refer to the sudden change in visual properties or state transitions as
novelties. Implementing novelty detection within generated world model
frameworks is a crucial task for protecting the agent when deployed. In this
paper, we propose straightforward bounding approaches to incorporate novelty
detection into world model RL agents, by utilizing the misalignment of the
world model's hallucinated states and the true observed states as an anomaly
score. We provide effective approaches to detecting novelties in a distribution
of transitions learned by an agent in a world model. Finally, we show the
advantage of our work in a novel environment compared to traditional machine
learning novelty detection methods as well as currently accepted RL focused
novelty detection algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RLC Safety 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Glinthawk: A Two-Tiered Architecture for Offline LLM Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pouya Hamadanian, Sadjad Fouladi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Glinthawk, an architecture for offline Large Language Model
(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the
utilization of the high-end accelerators ("Tier 1") by offloading the attention
mechanism to lower-end compute tier ("Tier 2"). This separation allows the
memory demand of the attention, known as the key-value cache, to scale
independently from the model weights, enabling larger batch sizes and more
efficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU
VMs, Glinthawk improves throughput by $5.9\times$ and reduces cost of
generation by $2.8\times$, compared to paged attention baselines. For long
sequence lengths, it achieves $16.3\times$ throughput improvement at
$2.4\times$ less cost. Our evaluation shows that this architecture can tolerate
moderate network latency with minimal performance degradation, making it highly
effective for latency-tolerant, throughput-focused applications such as batch
processing. The prototype is publicly available at
https://github.com/microsoft/glinthawk.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Variational Annealing for Multimodal Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tâm Le Minh, Julyan Arbel, Thomas Möllenhoff, Mohammad Emtiyaz Khan, Florence Forbes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new multimodal optimization approach called Natural
Variational Annealing (NVA) that combines the strengths of three foundational
concepts to simultaneously search for multiple global and local modes of
black-box nonconvex objectives. First, it implements a simultaneous search by
using variational posteriors, such as, mixtures of Gaussians. Second, it
applies annealing to gradually trade off exploration for exploitation. Finally,
it learns the variational search distribution using natural-gradient learning
where updates resemble well-known and easy-to-implement algorithms. The three
concepts come together in NVA giving rise to new algorithms and also allowing
us to incorporate "fitness shaping", a core concept from evolutionary
algorithms. We assess the quality of search on simulations and compare them to
methods using gradient descent and evolution strategies. We also provide an
application to a real-world inverse problem in planetary science.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Continual Instruction Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Qiao, Zhizhong Zhang, Xin Tan, Yanyun Qu, Shouhong Ding, Yuan Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Instruction Tuning (CIT) is adopted to continually instruct Large
Models to follow human intent data by data. It is observed that existing
gradient update would heavily destroy the performance on previous datasets
during CIT process. Instead, Exponential Moving Average (EMA), owns the ability
to trace previous parameters, which can aid in decreasing forgetting.
Nonetheless, its stable balance weight fails to deal with the ever-changing
datasets, leading to the out-of-balance between plasticity and stability. In
this paper, we propose a general continual instruction tuning framework to
address the challenge. Starting from the trade-off prerequisite and EMA update,
we propose the plasticity and stability ideal condition. Based on Taylor
expansion in the loss function, we find the optimal balance weight can be
automatically determined by the gradients and learned parameters. Therefore, we
propose a stable-plasticity balanced coefficient to avoid knowledge confusion.
Based on the semantic similarity of the instructions, we can determine whether
to retrain or expand the training parameters and allocate the most suitable
parameters for the testing instances. Extensive experiments across multiple
continual instruction tuning benchmarks demonstrate that our approach not only
enhances anti-forgetting capabilities but also significantly improves overall
continual tuning performance. For example, based on LLaVA-7B, the forgetting is
reduced from 5.42 to 1.93. Our code will be made publicly available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drago: Primal-Dual Coupled Variance Reduction for Faster
  Distributionally Robust Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronak Mehta, Jelena Diakonikolas, Zaid Harchaoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the penalized distributionally robust optimization (DRO) problem
with a closed, convex uncertainty set, a setting that encompasses learning
using $f$-DRO and spectral/$L$-risk minimization. We present Drago, a
stochastic primal-dual algorithm that combines cyclic and randomized components
with a carefully regularized primal update to achieve dual variance reduction.
Owing to its design, Drago enjoys a state-of-the-art linear convergence rate on
strongly convex-strongly concave DRO problems with a fine-grained dependency on
primal and dual condition numbers. Theoretical results are supported by
numerical benchmarks on regression and classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPO Meets PPO: Reinforced Token Optimization for RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhong, Zikang Shan, Guhao Feng, Wei Xiong, Xinle Cheng, Li Zhao, Di He, Jiang Bian, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,
Proximal Policy Optimization (PPO) is employed to learn from sparse,
sentence-level rewards -- a challenging scenario in traditional deep
reinforcement learning. Despite the great successes of PPO in the alignment of
large language models, its open-source implementation is still largely
sub-optimal. To address these issues, we introduce a framework that models RLHF
problems as a Markov decision process (MDP), enabling the capture of
fine-grained token-wise information. Under this framework, we introduce an
algorithm Reinforced Token Optimization (\texttt{RTO}), which learns the
token-wise reward function from preference data and performs policy
optimization based on this learned token-wise reward signal. Theoretically,
\texttt{RTO} is proven to have the capability of finding the near-optimal
policy sample-efficiently. For its practical implementation, \texttt{RTO}
innovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,
originally derived from sparse sentence rewards, surprisingly provides us with
a token-wise characterization of response quality, which is seamlessly
incorporated into our subsequent PPO training stage. Extensive experiments
demonstrate that \texttt{RTO} performs better than PPO and other direct
preference learning algorithms. In particular, RTO outperforms PPO by 7.5
points on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code
and models are available at
\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ pFedGPA: Diffusion-based Generative Parameter Aggregation for
  Personalized Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Lai, Jiaqi Li, Jian Xu, Yanru Wu, Boshi Tang, Siqi Chen, Yongfeng Huang, Wenbo Ding, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) offers a decentralized approach to model training,
where data remains local and only model parameters are shared between the
clients and the central server. Traditional methods, such as Federated
Averaging (FedAvg), linearly aggregate these parameters which are usually
trained on heterogeneous data distributions, potentially overlooking the
complex, high-dimensional nature of the parameter space. This can result in
degraded performance of the aggregated model. While personalized FL approaches
can mitigate the heterogeneous data issue to some extent, the limitation of
linear aggregation remains unresolved. To alleviate this issue, we investigate
the generative approach of diffusion model and propose a novel generative
parameter aggregation framework for personalized FL, \texttt{pFedGPA}. In this
framework, we deploy a diffusion model on the server to integrate the diverse
parameter distributions and propose a parameter inversion method to efficiently
generate a set of personalized parameters for each client. This inversion
method transforms the uploaded parameters into a latent code, which is then
aggregated through denoising sampling to produce the final personalized
parameters. By encoding the dependence of a client's model parameters on the
specific data distribution using the high-capacity diffusion model,
\texttt{pFedGPA} can effectively decouple the complexity of the overall
distribution of all clients' model parameters from the complexity of each
individual client's parameter distribution. Our experimental results
consistently demonstrate the superior performance of the proposed method across
multiple datasets, surpassing baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ (Ir)rationality in AI: State of the Art, Research Challenges and Open
  Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17165v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17165v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivia Macmillan-Scott, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The concept of rationality is central to the field of artificial
intelligence. Whether we are seeking to simulate human reasoning, or the goal
is to achieve bounded optimality, we generally seek to make artificial agents
as rational as possible. Despite the centrality of the concept within AI, there
is no unified definition of what constitutes a rational agent. This article
provides a survey of rationality and irrationality in artificial intelligence,
and sets out the open questions in this area. The understanding of rationality
in other fields has influenced its conception within artificial intelligence,
in particular work in economics, philosophy and psychology. Focusing on the
behaviour of artificial agents, we consider irrational behaviours that can
prove to be optimal in certain scenarios. Some methods have been developed to
deal with irrational agents, both in terms of identification and interaction,
however work in this area remains limited. Methods that have up to now been
developed for other purposes, namely adversarial scenarios, may be adapted to
suit interactions with artificial agents. We further discuss the interplay
between human and artificial agents, and the role that rationality plays within
this interaction; many questions remain in this area, relating to potentially
irrational behaviour of both humans and artificial agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards scientific discovery with dictionary learning: Extracting
  biological concepts from microscopy foundation models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Donhauser, Kristina Ulicna, Gemma Elyse Moran, Aditya Ravuri, Kian Kenyon-Dean, Cian Eastwood, Jason Hartford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dictionary learning (DL) has emerged as a powerful interpretability tool for
large language models. By extracting known concepts (e.g., Golden-Gate Bridge)
from human-interpretable data (e.g., text), sparse DL can elucidate a model's
inner workings. In this work, we ask if DL can also be used to discover unknown
concepts from less human-interpretable scientific data (e.g., cell images),
ultimately enabling modern approaches to scientific discovery. As a first step,
we use DL algorithms to study microscopy foundation models trained on
multi-cell image data, where little prior knowledge exists regarding which
high-level concepts should arise. We show that sparse dictionaries indeed
extract biologically-meaningful concepts such as cell type and genetic
perturbation type. We also propose Iterative Codebook Feature Learning~(ICFL)
and combine it with a pre-processing step which uses PCA whitening from a
control dataset. In our experiments, we demonstrate that both ICFL and PCA
improve the selectivity of extracted features compared to TopK sparse
autoencoders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPCore: Dynamic Prompt Coreset for Continual Test-Time Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10737v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10737v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunbei Zhang, Akshay Mehra, Shuaicheng Niu, Jihun Hamm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Test-Time Adaptation (CTTA) seeks to adapt source pre-trained
models to continually changing, unseen target domains. While existing CTTA
methods assume structured domain changes with uniform durations, real-world
environments often exhibit dynamic patterns where domains recur with varying
frequencies and durations. Current approaches, which adapt the same parameters
across different domains, struggle in such dynamic conditions-they face
convergence issues with brief domain exposures, risk forgetting previously
learned knowledge, or misapplying it to irrelevant domains. To remedy this, we
propose DPCore, a method designed for robust performance across diverse domain
change patterns while ensuring computational efficiency. DPCore integrates
three key components: Visual Prompt Adaptation for efficient domain alignment,
a Prompt Coreset for knowledge preservation, and a Dynamic Update mechanism
that intelligently adjusts existing prompts for similar domains while creating
new ones for substantially different domains. Extensive experiments on four
benchmarks demonstrate that DPCore consistently outperforms various CTTA
methods, achieving state-of-the-art performance in both structured and dynamic
settings while reducing trainable parameters by 99% and computation time by 64%
compared to previous approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Demonstration with Implicit Nonlinear Dynamics Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18768v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18768v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter David Fagan, Subramanian Ramamoorthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from Demonstration (LfD) is a useful paradigm for training policies
that solve tasks involving complex motions, such as those encountered in
robotic manipulation. In practice, the successful application of LfD requires
overcoming error accumulation during policy execution, i.e. the problem of
drift due to errors compounding over time and the consequent
out-of-distribution behaviours. Existing works seek to address this problem
through scaling data collection, correcting policy errors with a
human-in-the-loop, temporally ensembling policy predictions or through learning
a dynamical system model with convergence guarantees. In this work, we propose
and validate an alternative approach to overcoming this issue. Inspired by
reservoir computing, we develop a recurrent neural network layer that includes
a fixed nonlinear dynamical system with tunable dynamical properties for
modelling temporal dynamics. We validate the efficacy of our neural network
layer on the task of reproducing human handwriting motions using the LASA Human
Handwriting Dataset. Through empirical experiments we demonstrate that
incorporating our layer into existing neural network architectures addresses
the issue of compounding errors in LfD. Furthermore, we perform a comparative
evaluation against existing approaches including a temporal ensemble of policy
predictions and an Echo State Network (ESN) implementation. We find that our
approach yields greater policy precision and robustness on the handwriting task
while also generalising to multiple dynamics regimes and maintaining
competitive latency scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Initial Steps in Adaptive Gradient Descent Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abulikemu Abuduweili, Changliu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive gradient optimization methods, such as Adam, are prevalent in
training deep neural networks across diverse machine learning tasks due to
their ability to achieve faster convergence. However, these methods often
suffer from suboptimal generalization compared to stochastic gradient descent
(SGD) and exhibit instability, particularly when training Transformer models.
In this work, we show the standard initialization of the second-order moment
estimation ($v_0 =0$) as a significant factor contributing to these
limitations. We introduce simple yet effective solutions: initializing the
second-order moment estimation with non-zero values, using either data-driven
or random initialization strategies. Empirical evaluations demonstrate that our
approach not only stabilizes convergence but also enhances the final
performance of adaptive gradient optimizers. Furthermore, by adopting the
proposed initialization strategies, Adam achieves performance comparable to
many recently proposed variants of adaptive gradient optimization methods. Our
code is available at https://github.com/Walleclipse/Adam_Initialization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Parsimony and Learning (CPAL) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Programming Refusal with Conditional Activation Steering <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruce W. Lee, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Erik Miehling, Pierre Dognin, Manish Nagireddy, Amit Dhurandhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs have shown remarkable capabilities, but precisely controlling their
response behavior remains challenging. Existing activation steering methods
alter LLM behavior indiscriminately, limiting their practical applicability in
settings where selective responses are essential, such as content moderation or
domain-specific assistants. In this paper, we propose Conditional Activation
Steering (CAST), which analyzes LLM activation patterns during inference to
selectively apply or withhold activation steering based on the input context.
Our method is based on the observation that different categories of prompts
activate distinct patterns in the model's hidden states. Using CAST, one can
systematically control LLM behavior with rules like "if input is about hate
speech or adult content, then refuse" or "if input is not about legal advice,
then refuse." This allows for selective modification of responses to specific
content while maintaining normal responses to other content, all without
requiring weight optimization. We release an open-source implementation of our
framework at <github.com/IBM/activation-steering>.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025, Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Pixels to Components: Eigenvector Masking for Visual Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Bizeul, Thomas Sutter, Alain Ryser, Bernhard Schölkopf, Julius von Kügelgen, Julia E. Vogt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting masked from visible parts of an image is a powerful
self-supervised approach for visual representation learning. However, the
common practice of masking random patches of pixels exhibits certain failure
modes, which can prevent learning meaningful high-level features, as required
for downstream tasks. We propose an alternative masking strategy that operates
on a suitable transformation of the data rather than on the raw pixels.
Specifically, we perform principal component analysis and then randomly mask a
subset of components, which accounts for a fixed ratio of the data variance.
The learning task then amounts to reconstructing the masked components from the
visible ones. Compared to local patches of pixels, the principal components of
images carry more global information. We thus posit that predicting masked from
visible components involves more high-level features, allowing our masking
strategy to extract more useful representations. This is corroborated by our
empirical findings which demonstrate improved image classification performance
for component over pixel masking. Our method thus constitutes a simple and
robust data-driven alternative to traditional masked image modeling approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Practical Method for Generating String Counterfactuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11355v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11355v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matan Avitan, Ryan Cotterell, Yoav Goldberg, Shauli Ravfogel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interventions targeting the representation space of language models (LMs)
have emerged as an effective means to influence model behavior. Such methods
are employed, for example, to eliminate or alter the encoding of demographic
information such as gender within the model's representations and, in so doing,
create a counterfactual representation. However, because the intervention
operates within the representation space, understanding precisely what aspects
of the text it modifies poses a challenge. In this paper, we give a method to
convert representation counterfactuals into string counterfactuals. We
demonstrate that this approach enables us to analyze the linguistic alterations
corresponding to a given representation space intervention and to interpret the
features utilized to encode a specific concept. Moreover, the resulting
counterfactuals can be used to mitigate bias in classification through data
augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Optimize for Mixed-Integer Non-linear Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11061v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11061v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Tang, Elias B. Khalil, Ján Drgoňa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed-integer nonlinear programs (MINLPs) arise in diverse domains such as
energy systems and transportation but are notoriously difficult to solve,
particularly on a large scale. While learning-to-optimize methods have been
successful at continuous optimization, extending them to MINLPs is still
challenging due to the integer constraints. To overcome this, we propose a
novel deep-learning approach with two learnable correction layers to ensure
solution integrality and a post-processing step to improve solution
feasibility. Our experiments show that this is the first general method capable
of efficiently solving large-scale MINLPs with up to tens of thousands of
variables in milliseconds, delivering high-quality solutions even when
traditional solvers and heuristics fail. This is the first general learning
method for MINLP, successfully solving some of the largest instances reported
to date.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12042v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12042v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Ganjdanesh, Reza Shirkavand, Shangqian Gao, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) diffusion models have demonstrated impressive image
generation capabilities. Still, their computational intensity prohibits
resource-constrained organizations from deploying T2I models after fine-tuning
them on their internal target data. While pruning techniques offer a potential
solution to reduce the computational burden of T2I models, static pruning
methods use the same pruned model for all input prompts, overlooking the
varying capacity requirements of different prompts. Dynamic pruning addresses
this issue by utilizing a separate sub-network for each prompt, but it prevents
batch parallelism on GPUs. To overcome these limitations, we introduce Adaptive
Prompt-Tailored Pruning (APTP), a novel prompt-based pruning method designed
for T2I diffusion models. Central to our approach is a prompt router model,
which learns to determine the required capacity for an input text prompt and
routes it to an architecture code, given a total desired compute budget for
prompts. Each architecture code represents a specialized model tailored to the
prompts assigned to it, and the number of codes is a hyperparameter. We train
the prompt router and architecture codes using contrastive learning, ensuring
that similar prompts are mapped to nearby codes. Further, we employ optimal
transport to prevent the codes from collapsing into a single one. We
demonstrate APTP's effectiveness by pruning Stable Diffusion (SD) V2.1 using
CC3M and COCO as target datasets. APTP outperforms the single-model pruning
baselines in terms of FID, CLIP, and CMMD scores. Our analysis of the clusters
learned by APTP reveals they are semantically meaningful. We also show that
APTP can automatically discover previously empirically found challenging
prompts for SD, e.g. prompts for generating text images, assigning them to
higher capacity codes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Isotonic Mechanism for Exponential Family Estimation in Machine Learning
  Peer <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11160v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11160v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuling Yan, Weijie J. Su, Jianqing Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 2023, the International Conference on Machine Learning (ICML) required
authors with multiple submissions to rank their submissions based on perceived
quality. In this paper, we aim to employ these author-specified rankings to
enhance peer review in machine learning and artificial intelligence conferences
by extending the Isotonic Mechanism to exponential family distributions. This
mechanism generates adjusted scores that closely align with the original scores
while adhering to author-specified rankings. Despite its applicability to a
broad spectrum of exponential family distributions, implementing this mechanism
does not require knowledge of the specific distribution form. We demonstrate
that an author is incentivized to provide accurate rankings when her utility
takes the form of a convex additive function of the adjusted review scores. For
a certain subclass of exponential family distributions, we prove that the
author reports truthfully only if the question involves only pairwise
comparisons between her submissions, thus indicating the optimality of ranking
in truthful information elicitation. Moreover, we show that the adjusted scores
improve dramatically the estimation accuracy compared to the original scores
and achieve nearly minimax optimality when the ground-truth scores have bounded
total variation. We conclude with a numerical analysis of the ICML 2023 ranking
data, showing substantial estimation gains in approximating a proxy
ground-truth quality of the papers using the Isotonic Mechanism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to the Journal of the Royal Statistical Society: Series B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Your Absorbing Discrete Diffusion Secretly Models the Conditional
  Distributions of Clean Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, Chongxuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discrete diffusion models with absorbing processes have shown promise in
language modeling. The key quantities to be estimated are the ratios between
the marginal probabilities of two transitive states at all timesteps, called
the concrete score. In this paper, we reveal that the concrete score in
absorbing diffusion can be expressed as conditional probabilities of clean
data, multiplied by a time-dependent scalar in an analytic form. Motivated by
this finding, we propose reparameterized absorbing discrete diffusion (RADD), a
dedicated diffusion model without time-condition that characterizes the
time-independent conditional probabilities. Besides its simplicity, RADD can
reduce the number of function evaluations (NFEs) by caching the output of the
time-independent network when the noisy sample remains unchanged in a sampling
interval, which enables sampling acceleration. Built upon the new perspective
of conditional distributions, we further unify absorbing discrete diffusion and
any-order autoregressive models (AO-ARMs), showing that the upper bound on the
negative log-likelihood for the diffusion model can be interpreted as an
expected negative log-likelihood for AO-ARMs. Further, our RADD models achieve
SOTA performance among diffusion models on 5 zero-shot language modeling
benchmarks (measured by perplexity) at the GPT-2 scale. Our code is available
at https://github.com/ML-GSAI/RADD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable and consistent embedding of probability measures into Hilbert
  spaces via measure quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erell Gachon, Elsa Cazelles, Jérémie Bigot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is focused on statistical learning from data that come as
probability measures. In this setting, popular approaches consist in embedding
such data into a Hilbert space with either Linearized Optimal Transport or
Kernel Mean Embedding. However, the cost of computing such embeddings prohibits
their direct use in large-scale settings. We study two methods based on measure
quantization for approximating input probability measures with discrete
measures of small-support size. The first one is based on optimal quantization
of each input measure, while the second one relies on mean-measure
quantization. We study the consistency of such approximations, and its
implication for scalable embeddings of probability measures into a Hilbert
space at a low computational cost. We finally illustrate our findings with
various numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Convergence with Less Communication: Broadcast-Based Subgraph
  Sampling for Decentralized Learning over Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Pérez Herrera, Zheng Chen, Erik G. Larsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely
adopted algorithm for decentralized training of machine learning models across
networked agents. A crucial part of D-SGD is the consensus-based model
averaging, which heavily relies on information exchange and fusion among the
nodes. Specifically, for consensus averaging over wireless networks,
communication coordination is necessary to determine when and how a node can
access the channel and transmit (or receive) information to (or from) its
neighbors. In this work, we propose $\texttt{BASS}$, a broadcast-based subgraph
sampling method designed to accelerate the convergence of D-SGD while
considering the actual communication cost per iteration. $\texttt{BASS}$
creates a set of mixing matrix candidates that represent sparser subgraphs of
the base topology. In each consensus iteration, one mixing matrix is sampled,
leading to a specific scheduling decision that activates multiple
collision-free subsets of nodes. The sampling occurs in a probabilistic manner,
and the elements of the mixing matrices, along with their sampling
probabilities, are jointly optimized. Simulation results demonstrate that
$\texttt{BASS}$ enables faster convergence with fewer transmission slots
compared to existing link-based scheduling methods. In conclusion, the inherent
broadcasting nature of wireless channels offers intrinsic advantages in
accelerating the convergence of decentralized optimization and learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures, accepted for publication at IEEE Open Journals
  of Communication. arXiv admin note: text overlap with arXiv:2310.16106</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Least Squares Kernelized Tensor Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07041v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07041v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengying Lei, Lijun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Completing multidimensional tensor-structured data with missing entries is a
fundamental task for many real-world applications involving incomplete or
corrupted datasets. For data with spatial or temporal side information,
low-rank factorization models with smoothness constraints have demonstrated
strong performance. Although effective at capturing global and long-range
correlations, these models often struggle to capture short-scale,
high-frequency variations in the data. To address this limitation, we propose
the Generalized Least Squares Kernelized Tensor Factorization (GLSKF) framework
for tensor completion. GLSKF integrates smoothness-constrained low-rank
factorization with a locally correlated residual process; the resulting
additive structure enables effective characterization of both global
dependencies and local variations. Specifically, we define the covariance norm
to enforce the smoothness of factor matrices in the global low-rank
factorization, and use structured covariance/kernel functions to model the
local processes. For model estimation, we develop an alternating least squares
(ALS) procedure with closed-form solutions for each subproblem. GLSKF utilizes
zero-padding and slicing operations based on projection matrices which preserve
the Kronecker structure of covariances, facilitating efficient computations
through the conjugate gradient (CG) method. The proposed framework is evaluated
on four real-world datasets across diverse tasks. Experimental results
demonstrate that GLSKF achieves superior performance and scalability,
establishing it as a novel solution for multidimensional tensor completion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalisation under gradient descent via deterministic PAC-Bayes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02525v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02525v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugenio Clerico, Tyler Farghly, George Deligiannidis, Benjamin Guedj, Arnaud Doucet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish disintegrated PAC-Bayesian generalisation bounds for models
trained with gradient descent methods or continuous gradient flows. Contrary to
standard practice in the PAC-Bayesian setting, our result applies to
optimisation algorithms that are deterministic, without requiring any
de-randomisation step. Our bounds are fully computable, depending on the
density of the initial distribution and the Hessian of the training objective
over the trajectory. We show that our framework can be applied to a variety of
iterative optimisation algorithms, including stochastic gradient descent (SGD),
momentum-based schemes, and damped Hamiltonian dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Particle Algorithm for Mean-Field Variational Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Du, Kaizheng Wang, Edith Zhang, Chenyang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational inference is a fast and scalable alternative to Markov chain
Monte Carlo and has been widely applied to posterior inference tasks in
statistics and machine learning. A traditional approach for implementing
mean-field variational inference (MFVI) is coordinate ascent variational
inference (CAVI), which relies crucially on parametric assumptions on complete
conditionals. In this paper, we introduce a novel particle-based algorithm for
mean-field variational inference, which we term PArticle VI (PAVI). Notably,
our algorithm does not rely on parametric assumptions on complete conditionals,
and it applies to the nonparametric setting. We provide non-asymptotic
finite-particle convergence guarantee for our algorithm. To our knowledge, this
is the first end-to-end guarantee for particle-based MFVI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Networks and (Virtual) Extended Formulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Hertrich, Georg Loho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks with piecewise linear activation functions, such as rectified
linear units (ReLU) or maxout, are among the most fundamental models in modern
machine learning. We make a step towards proving lower bounds on the size of
such neural networks by linking their representative capabilities to the notion
of the extension complexity $\mathrm{xc}(P)$ of a polytope $P$. This is a
well-studied quantity in combinatorial optimization and polyhedral geometry
describing the number of inequalities needed to model $P$ as a linear program.
We show that $\mathrm{xc}(P)$ is a lower bound on the size of any monotone or
input-convex neural network that solves the linear optimization problem over
$P$. This implies exponential lower bounds on such neural networks for a
variety of problems, including the polynomially solvable maximum weight
matching problem.
  In an attempt to prove similar bounds also for general neural networks, we
introduce the notion of virtual extension complexity $\mathrm{vxc}(P)$, which
generalizes $\mathrm{xc}(P)$ and describes the number of inequalities needed to
represent the linear optimization problem over $P$ as a difference of two
linear programs. We prove that $\mathrm{vxc}(P)$ is a lower bound on the size
of any neural network that optimizes over $P$. While it remains an open
question to derive useful lower bounds on $\mathrm{vxc}(P)$, we argue that this
quantity deserves to be studied independently from neural networks by proving
that one can efficiently optimize over a polytope $P$ using a small virtual
extended formulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChameleonLLM: Batch-Aware Dynamic Low-Rank Adaptation via Inference-Time
  Clusters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04315v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04315v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamer Ali Yuksel, Hassan Sawaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have shown remarkable
performance across diverse tasks. However, these models are typically deployed
with fixed weights, which limits their ability to adapt dynamically to the
variability inherent in real-world data during inference. This paper introduces
ChameleonLLM, a novel framework that enables inference-time adaptation of LLMs
by leveraging batch-aware clustering and on-the-fly generation of low-rank
updates. Unlike traditional fine-tuning approaches such as Low-Rank Adaptation
(LoRA) or methods that rely on a fixed set of pre-learned uniforms (changeable
masks), our method dynamically generates adaptive modifications to the decoder
weights based on the aggregated statistics of clustered batches. By
intelligently grouping similar inputs and computing context-aware low-rank
updates via a hyper-network, ChameleonLLM achieves significant performance
gains, outperforming conventional LoRA methods while eliminating the overhead
of maintaining multiple expert models. Our experiments highlight the potential
of our approach to serve as a versatile and highly adaptive solution for
language model inference. ChameleonLLM is open-sourced to ensure the
reproducibility of our experiments:
https://anonymous.4open.science/r/ChamaleonLLM/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Causal Information Bottleneck and Optimal Causal Variable
  Abstractions <span class="chip">UAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco N. F. Q. Simoes, Mehdi Dastani, Thijs van Ommen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To effectively study complex causal systems, it is often useful to construct
abstractions of parts of the system by discarding irrelevant details while
preserving key features. The Information Bottleneck (IB) method is a widely
used approach to construct variable abstractions by compressing random
variables while retaining predictive power over a target variable. Traditional
methods like IB are purely statistical and ignore underlying causal structures,
making them ill-suited for causal tasks. We propose the Causal Information
Bottleneck (CIB), a causal extension of the IB, which compresses a set of
chosen variables while maintaining causal control over a target variable. This
method produces abstractions of (sets of) variables which are causally
interpretable, give us insight about the interactions between the abstracted
variables and the target variable, and can be used when reasoning about
interventions. We present experimental results demonstrating that the learned
abstractions accurately capture causal relations as intended.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to UAI 2025. Code available at
  github.com/francisco-simoes/cib-optimization-psagd</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fault Localization via Fine-tuning Large Language Models with Mutation
  Generated Stack Traces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18005v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18005v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neetha Jambigi, Bartosz Bogacz, Moritz Mueller, Thomas Bach, Michael Felderer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abrupt and unexpected terminations of software are termed as software
crashes. They can be challenging to analyze. Finding the root cause requires
extensive manual effort and expertise to connect information sources like stack
traces, source code, and logs. Typical approaches to fault localization require
either test failures or source code. Crashes occurring in production
environments, such as that of SAP HANA, provide solely crash logs and stack
traces. We present a novel approach to localize faults based only on the stack
trace information and no additional runtime information, by fine-tuning large
language models (LLMs). We address complex cases where the root cause of a
crash differs from the technical cause, and is not located in the innermost
frame of the stack trace. As the number of historic crashes is insufficient to
fine-tune LLMs, we augment our dataset by leveraging code mutators to inject
synthetic crashes into the code base. By fine-tuning on 64,369 crashes
resulting from 4.1 million mutations of the HANA code base, we can correctly
predict the root cause location of a crash with an accuracy of 66.9\% while
baselines only achieve 12.6% and 10.6%. We substantiate the generalizability of
our approach by evaluating on two additional open-source databases, SQLite and
DuckDB, achieving accuracies of 63% and 74%, respectively. Across all our
experiments, fine-tuning consistently outperformed prompting non-finetuned LLMs
for localizing faults in our datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I do not have the necessary approvals to out the paper on Arxiv from
  my organization yet. I was too soon to do this</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeoRL: Efficient Exploration for Nonepisodic RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01175v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01175v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavya Sukhija, Lenart Treven, Florian Dörfler, Stelian Coros, Andreas Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of nonepisodic reinforcement learning (RL) for nonlinear
dynamical systems, where the system dynamics are unknown and the RL agent has
to learn from a single trajectory, i.e., without resets. We propose Nonepisodic
Optimistic RL (NeoRL), an approach based on the principle of optimism in the
face of uncertainty. NeoRL uses well-calibrated probabilistic models and plans
optimistically w.r.t. the epistemic uncertainty about the unknown dynamics.
Under continuity and bounded energy assumptions on the system, we provide a
first-of-its-kind regret bound of $O(\Gamma_T \sqrt{T})$ for general nonlinear
systems with Gaussian process dynamics. We compare NeoRL to other baselines on
several deep RL environments and empirically demonstrate that NeoRL achieves
the optimal average cost while incurring the least regret.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PLMTrajRec: A Scalable and Generalizable Trajectory Recovery Method with
  Pre-trained Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tonglong Wei, Yan Lin, Youfang Lin, Shengnan Guo, Jilin Hu, Haitao Yuan, Gao Cong, Huaiyu Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal trajectory data is crucial for various applications. However,
issues such as device malfunctions and network instability often cause sparse
trajectories, leading to lost detailed movement information. Recovering the
missing points in sparse trajectories to restore the detailed information is
thus essential. Despite recent progress, several challenges remain. First, the
lack of large-scale dense trajectory data makes it difficult to train a
trajectory recovery model from scratch. Second, the varying spatiotemporal
correlations in sparse trajectories make it hard to generalize recovery across
different sampling intervals. Third, the lack of location information
complicates the extraction of road conditions for missing points.
  To address these challenges, we propose a novel trajectory recovery model
called PLMTrajRec. It leverages the scalability of a pre-trained language model
(PLM) and can be fine-tuned with only a limited set of dense trajectories. To
handle different sampling intervals in sparse trajectories, we first convert
each trajectory's sampling interval and movement features into natural language
representations, allowing the PLM to recognize its interval. We then introduce
a trajectory encoder to unify trajectories of varying intervals into a single
interval and capture their spatiotemporal relationships. To obtain road
conditions for missing points, we propose an area flow-guided implicit
trajectory prompt, which models road conditions by collecting traffic flows in
each region. We also introduce a road condition passing mechanism that uses
observed points' road conditions to infer those of the missing points.
Experiments on two public trajectory datasets with three sampling intervals
each demonstrate the effectiveness, scalability, and generalization ability of
PLMTrajRec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeMo: Meaningful, Modular <span class="highlight-title">Control</span>lers via Noise Injection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan Tjandrasuwita, Jie Xu, Armando Solar-Lezama, Wojciech Matusik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots are often built from standardized assemblies, (e.g. arms, legs, or
fingers), but each robot must be trained from scratch to control all the
actuators of all the parts together. In this paper we demonstrate a new
approach that takes a single robot and its controller as input and produces a
set of modular controllers for each of these assemblies such that when a new
robot is built from the same parts, its control can be quickly learned by
reusing the modular controllers. We achieve this with a framework called MeMo
which learns (Me)aningful, (Mo)dular controllers. Specifically, we propose a
novel modularity objective to learn an appropriate division of labor among the
modules. We demonstrate that this objective can be optimized simultaneously
with standard behavior cloning loss via noise injection. We benchmark our
framework in locomotion and grasping environments on simple to complex robot
morphology transfer. We also show that the modules help in task transfer. On
both structure and task transfer, MeMo achieves improved training efficiency to
graph neural network and Transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; 29 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Linear Quadratic Regulator for Robotic <span class="highlight-title">Control</span> Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhang, Shaohui Yang, Toshiyuki Ohtsuka, Colin Jones, Joschka Boedecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model predictive control (MPC) has played a more crucial role in various
robotic control tasks, but its high computational requirements are concerning,
especially for nonlinear dynamical models. This paper presents a
$\textbf{la}$tent $\textbf{l}$inear $\textbf{q}$uadratic $\textbf{r}$egulator
(LaLQR) that maps the state space into a latent space, on which the dynamical
model is linear and the cost function is quadratic, allowing the efficient
application of LQR. We jointly learn this alternative system by imitating the
original MPC. Experiments show LaLQR's superior efficiency and generalization
compared to other baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at RSS 2024 workshop on Koopman Operators in Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Holistic Semantic Representation for Navigational Trajectory Generation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Cao, Tongya Zheng, Qinghong Guo, Yu Wang, Junshu Dai, Shunyu Liu, Jie Yang, Jie Song, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory generation has garnered significant attention from researchers in
the field of spatio-temporal analysis, as it can generate substantial
synthesized human mobility trajectories that enhance user privacy and alleviate
data scarcity. However, existing trajectory generation methods often focus on
improving trajectory generation quality from a singular perspective, lacking a
comprehensive semantic understanding across various scales. Consequently, we
are inspired to develop a HOlistic SEmantic Representation (HOSER) framework
for navigational trajectory generation. Given an origin-and-destination (OD)
pair and the starting time point of a latent trajectory, we first propose a
Road Network Encoder to expand the receptive field of road- and zone-level
semantics. Second, we design a Multi-Granularity Trajectory Encoder to
integrate the spatio-temporal semantics of the generated trajectory at both the
point and trajectory levels. Finally, we employ a Destination-Oriented
Navigator to seamlessly integrate destination-oriented guidance. Extensive
experiments on three real-world datasets demonstrate that HOSER outperforms
state-of-the-art baselines by a significant margin. Moreover, the model's
performance in few-shot learning and zero-shot learning scenarios further
verifies the effectiveness of our holistic semantic representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-invariant Clinical Representation Learning by Bridging Data
  Distribution Shift across EMR <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07799v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07799v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongji Zhang, Yuhang Wang, Yinghao Zhu, Xinyu Ma, Yasha Wang, Junyi Gao, Liantao Ma, Wen Tang, Xiaoyun Zhang, Ling Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging diseases present challenges in symptom recognition and timely
clinical intervention due to limited available information. An effective
prognostic model could assist physicians in making accurate diagnoses and
designing personalized treatment plans to prevent adverse outcomes. However, in
the early stages of disease emergence, several factors hamper model
development: limited data collection, insufficient clinical experience, and
privacy and ethical concerns restrict data availability and complicate accurate
label assignment. Furthermore, Electronic Medical Record (EMR) data from
different diseases or sources often exhibit significant cross-dataset feature
misalignment, severely impacting the effectiveness of deep learning models. We
present a domain-invariant representation learning method that constructs a
transition model between source and target datasets. By constraining the
distribution shift of features generated across different domains, we capture
domain-invariant features specifically relevant to downstream tasks, developing
a unified domain-invariant encoder that achieves better feature representation
across various task domains. Experimental results across multiple target tasks
demonstrate that our proposed model surpasses competing baseline methods and
achieves faster training convergence, particularly when working with limited
data. Extensive experiments validate our method's effectiveness in providing
more accurate predictions for emerging pandemics and other diseases. Code is
publicly available at https://github.com/wang1yuhang/domain_invariant_network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAGFormer: Learning Semantic Attributes and Topological Structure for
  Fraud Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17472v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17472v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Li, Shuyang Jiang, Lifeng Zhang, Siyuan Du, Guangnan Ye, Hongfeng Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fraud detection remains a challenging task due to the complex and deceptive
nature of fraudulent activities. Current approaches primarily concentrate on
learning only one perspective of the graph: either the topological structure of
the graph or the attributes of individual nodes. However, we conduct empirical
studies to reveal that these two types of features, while nearly orthogonal,
are each independently effective. As a result, previous methods can not fully
capture the comprehensive characteristics of the fraud graph. To address this
dilemma, we present a novel framework called Relation-Aware GNN with
transFormer~(RAGFormer) which simultaneously embeds both semantic and
topological features into a target node. The simple yet effective network
consists of a semantic encoder, a topology encoder, and an attention fusion
module. The semantic encoder utilizes Transformer to learn semantic features
and node interactions across different relations. We introduce Relation-Aware
GNN as the topology encoder to learn topological features and node interactions
within each relation. These two complementary features are interleaved through
an attention fusion module to support prediction by both orthogonal features.
Extensive experiments on two popular public datasets demonstrate that RAGFormer
achieves state-of-the-art performance. The significant improvement of RAGFormer
in an industrial credit card fraud detection dataset further validates the
applicability of our method in real-world business scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00134v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00134v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Andreychuk, Konstantin Yakovlev, Aleksandr Panov, Alexey Skrynnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent pathfinding (MAPF) is a problem that generally requires finding
collision-free paths for multiple agents in a shared environment. Solving MAPF
optimally, even under restrictive assumptions, is NP-hard, yet efficient
solutions for this problem are critical for numerous applications, such as
automated warehouses and transportation systems. Recently, learning-based
approaches to MAPF have gained attention, particularly those leveraging deep
reinforcement learning. Typically, such learning-based MAPF solvers are
augmented with additional components like single-agent planning or
communication. Orthogonally, in this work we rely solely on imitation learning
that leverages a large dataset of expert MAPF solutions and transformer-based
neural network to create a foundation model for MAPF called MAPF-GPT. The
latter is capable of generating actions without additional heuristics or
communication. MAPF-GPT demonstrates zero-shot learning abilities when solving
the MAPF problems that are not present in the training dataset. We show that
MAPF-GPT notably outperforms the current best-performing learnable MAPF solvers
on a diverse range of problem instances and is computationally efficient during
inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating crossmodal gene expression from cancer histopathology
  improves multimodal AI predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00568v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00568v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samiran Dey, Christopher R. S. Banerji, Partha Basuchowdhuri, Sanjoy K. Saha, Deepak Parashar, Tapabrata Chakraborti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging research has highlighted that artificial intelligence based
multimodal fusion of digital pathology and transcriptomic features can improve
cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction.
However, such direct fusion for joint decision is impractical in real clinical
settings, where histopathology is still the gold standard for diagnosis and
transcriptomic tests are rarely requested, at least in the public healthcare
system. With our novel diffusion based crossmodal generative AI model PathGen,
we show that genomic expressions synthesized from digital histopathology
jointly predicts cancer grading and patient survival risk with high accuracy
(state-of-the-art performance), certainty (through conformal coverage
guarantee) and interpretability (through distributed attention maps). PathGen
code is available for open use by the research community through GitHub at
https://github.com/Samiran-Dey/PathGen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Confident Classifiers in the Presence of Label Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00524v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00524v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asma Ahmed Hashmi, Aigerim Zhumabayeva, Nikita Kotelevskii, Artem Agafonov, Mohammad Yaqub, Maxim Panov, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Deep Neural Network (DNN) models significantly depends on the
quality of provided annotations. In medical image segmentation, for example,
having multiple expert annotations for each data point is common to minimize
subjective annotation bias. Then, the goal of estimation is to filter out the
label noise and recover the ground-truth masks, which are not explicitly given.
This paper proposes a probabilistic model for noisy observations that allows us
to build a confident classification and segmentation models. To accomplish it,
we explicitly model label noise and introduce a new information-based
regularization that pushes the network to recover the ground-truth labels. In
addition, for segmentation task we adjust the loss function by prioritizing
learning in high-confidence regions where all the annotators agree on labeling.
We evaluate the proposed method on a series of classification tasks such as
noisy versions of MNIST, CIFAR-10, Fashion-MNIST datasets as well as CIFAR-10N,
which is real-world dataset with noisy human annotations. Additionally, for
segmentation task, we consider several medical imaging datasets, such as, LIDC
and RIGA that reflect real-world inter-variability among multiple annotators.
Our experiments show that our algorithm outperforms state-of-the-art solutions
for the considered classification and segmentation problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind the Gap: Towards Generalizable Autonomous Penetration Testing via
  Domain Randomization and Meta-Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Zhou, Jingju Liu, Yuliang Lu, Jiahai Yang, Yue Zhang, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With increasing numbers of vulnerabilities exposed on the internet,
autonomous penetration testing (pentesting) has emerged as a promising research
area. Reinforcement learning (RL) is a natural fit for studying this topic.
However, two key challenges limit the applicability of RL-based autonomous
pentesting in real-world scenarios: (a) training environment dilemma --
training agents in simulated environments is sample-efficient while ensuring
their realism remains challenging; (b) poor generalization ability -- agents'
policies often perform poorly when transferred to unseen scenarios, with even
slight changes potentially causing significant generalization gap. To this end,
we propose GAP, a generalizable autonomous pentesting framework that aims to
realizes efficient policy training in realistic environments and train
generalizable agents capable of drawing inferences about other cases from one
instance. GAP introduces a Real-to-Sim-to-Real pipeline that (a) enables
end-to-end policy learning in unknown real environments while constructing
realistic simulations; (b) improves agents' generalization ability by
leveraging domain randomization and meta-RL learning.Specially, we are among
the first to apply domain randomization in autonomous pentesting and propose a
large language model-powered domain randomization method for synthetic
environment generation. We further apply meta-RL to improve agents'
generalization ability in unseen environments by leveraging synthetic
environments. The combination of two methods effectively bridges the
generalization gap and improves agents' policy adaptation
performance.Experiments are conducted on various vulnerable virtual machines,
with results showing that GAP can enable policy learning in various realistic
environments, achieve zero-shot policy transfer in similar environments, and
realize rapid policy adaptation in dissimilar environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and
  Uncertainty Based Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunfeng Lai, Zhenheng Tang, Xinglin Pan, Peijie Dong, Xiang Liu, Haolan Chen, Li Shen, Bo Li, Xiaowen Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging aggregates Large Language Models (LLMs) finetuned on different
tasks into a stronger one. However, parameter conflicts between models leads to
performance degradation in averaging. While model routing addresses this issue
by selecting individual models during inference, it imposes excessive storage
and compute costs, and fails to leverage the common knowledge from different
models. In this work, we observe that different layers exhibit varying levels
of parameter conflicts. Building on this insight, we average layers with
minimal parameter conflicts and use a novel task-level expert routing for
layers with significant conflicts. To further reduce storage costs, inspired by
task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense
expert and several sparse experts. Considering the out-of-distribution samples,
we select and merge appropriate experts based on the task uncertainty of the
input data. We conduct extensive experiments on both LLaMA and Qwen with
varying parameter scales, and evaluate on real-world reasoning tasks. Results
demonstrate that our method consistently achieves significant performance
improvements while requiring less system cost compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress. arXiv admin note: text overlap with
  arXiv:2405.09673 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The AI off-switch problem as a signalling game: bounded rationality and
  incomparability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio benavoli, Alessandro facchini, Marco Zaffalon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The off-switch problem is a critical challenge in AI control: if an AI system
resists being switched off, it poses a significant risk. In this paper, we
model the off-switch problem as a signalling game, where a human decision-maker
communicates its preferences about some underlying decision problem to an AI
agent, which then selects actions to maximise the human's utility. We assume
that the human is a bounded rational agent and explore various bounded
rationality mechanisms. Using real machine learning models, we reprove prior
results and demonstrate that a necessary condition for an AI system to refrain
from disabling its off-switch is its uncertainty about the human's utility. We
also analyse how message costs influence optimal strategies and extend the
analysis to scenarios involving incomparability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization bounds for mixing processes via delayed online-to-PAC
  conversions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baptiste Abeles, Eugenio Clerico, Gergely Neu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the generalization error of statistical learning algorithms in a
non-i.i.d. setting, where the training data is sampled from a stationary mixing
process. We develop an analytic framework for this scenario based on a
reduction to online learning with delayed feedback. In particular, we show that
the existence of an online learning algorithm with bounded regret (against a
fixed statistical learning algorithm in a specially constructed game of online
learning with delayed feedback) implies low generalization error of said
statistical learning method even if the data sequence is sampled from a mixing
time series. The rates demonstrate a trade-off between the amount of delay in
the online learning game and the degree of dependence between consecutive data
points, with near-optimal rates recovered in a number of well-studied settings
when the delay is tuned appropriately as a function of the mixing time of the
process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GANQ: GPU-Adaptive Non-Uniform Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengxiang Zhao, Xiaoming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) face significant deployment challenges due to
their substantial resource requirements. While low-bit quantized weights can
reduce memory usage and improve inference efficiency, current hardware lacks
native support for mixed-precision General Matrix Multiplication (mpGEMM),
resulting in inefficient dequantization-based implementations. Moreover,
uniform quantization methods often fail to capture weight distributions
adequately, leading to performance degradation. We propose GANQ (GPU-Adaptive
Non-Uniform Quantization), a layer-wise post-training non-uniform quantization
framework optimized for hardware-efficient lookup table-based mpGEMM. GANQ
achieves superior quantization performance by utilizing a training-free,
GPU-adaptive optimization algorithm to efficiently reduce layer-wise
quantization errors. Extensive experiments demonstrate GANQ's ability to reduce
the perplexity gap from the FP16 baseline compared to state-of-the-art methods
for both 3-bit and 4-bit quantization. Furthermore, when deployed on a single
NVIDIA RTX 4090 GPU, GANQ's quantized models achieve up to 2.57$\times$ speedup
over the baseline, advancing memory and inference efficiency in LLM deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Limits to scalable evaluation at the frontier: LLM as Judge won't beat
  twice the data <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian E. Dorner, Vivian Y. Nastl, Moritz Hardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High quality annotations are increasingly a bottleneck in the explosively
growing machine learning ecosystem. Scalable evaluation methods that avoid
costly annotation have therefore become an important research ambition. Many
hope to use strong existing models in lieu of costly labels to provide cheap
model evaluations. Unfortunately, this method of using models as judges
introduces biases, such as self-preferencing, that can distort model
comparisons. An emerging family of debiasing tools promises to fix these issues
by using a few high quality labels to debias a large number of model judgments.
In this paper, we study how far such debiasing methods, in principle, can go.
Our main result shows that when the judge is no more accurate than the
evaluated model, no debiasing method can decrease the required amount of ground
truth labels by more than half. Our result speaks to the severe limitations of
the LLM-as-a-judge paradigm at the evaluation frontier where the goal is to
assess newly released models that are possibly better than the judge. Through
an empirical evaluation, we demonstrate that the sample size savings achievable
in practice are even more modest than what our theoretical limit suggests.
Along the way, our work provides new observations about debiasing methods for
model evaluation, and points out promising avenues for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025; 28 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Higher-Order Message Passing for Glycan Representation Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13467v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13467v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Joeres, Daniel Bojar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glycans are the most complex biological sequence, with monosaccharides
forming extended, non-linear sequences. As post-translational modifications,
they modulate protein structure, function, and interactions. Due to their
diversity and complexity, predictive models of glycan properties and functions
are still insufficient.
  Graph Neural Networks (GNNs) are deep learning models designed to process and
analyze graph-structured data. These architectures leverage the connectivity
and relational information in graphs to learn effective representations of
nodes, edges, and entire graphs. Iteratively aggregating information from
neighboring nodes, GNNs capture complex patterns within graph data, making them
particularly well-suited for tasks such as link prediction or graph
classification across domains.
  This work presents a new model architecture based on combinatorial complexes
and higher-order message passing to extract features from glycan structures
into a latent space representation. The architecture is evaluated on an
improved GlycanML benchmark suite, establishing a new state-of-the-art
performance. We envision that these improvements will spur further advances in
computational glycosciences and reveal the roles of glycans in biology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to MLSB Workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Object-centric proto-symbolic behavioural reasoning from pixels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruben van Bergen, Justus Hübotter, Pablo Lanillos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous intelligent agents must bridge computational challenges at
disparate levels of abstraction, from the low-level spaces of sensory input and
motor commands to the high-level domain of abstract reasoning and planning. A
key question in designing such agents is how best to instantiate the
representational space that will interface between these two levels -- ideally
without requiring supervision in the form of expensive data annotations. These
objectives can be efficiently achieved by representing the world in terms of
objects (grounded in perception and action). In this work, we present a novel,
brain-inspired, deep-learning architecture that learns from pixels to
interpret, control, and reason about its environment, using object-centric
representations. We show the utility of our approach through tasks in synthetic
environments that require a combination of (high-level) logical reasoning and
(low-level) continuous control. Results show that the agent can learn emergent
conditional behavioural reasoning, such as $(A \to B) \land (\neg A \to C)$, as
well as logical composition $(A \to B) \land (A \to C) \vdash A \to (B \land
C)$ and XOR operations, and successfully controls its environment to satisfy
objectives deduced from these logical rules. The agent can adapt online to
unexpected changes in its environment and is robust to mild violations of its
world model, thanks to dynamic internal desired goal generation. While the
present results are limited to synthetic settings (2D and 3D activated versions
of dSprites), which fall short of real-world levels of complexity, the proposed
architecture shows how to manipulate grounded object representations, as a key
inductive bias for unsupervised learning, to enable behavioral reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Conformal Prediction with Vectorized Non-Conformity Scores 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minxing Zheng, Shixiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction (CP) provides model-agnostic uncertainty quantification
with guaranteed coverage, but conventional methods often produce overly
conservative uncertainty sets, especially in multi-dimensional settings. This
limitation arises from simplistic non-conformity scores that rely solely on
prediction error, failing to capture the prediction error distribution's
complexity. To address this, we propose a generative conformal prediction
framework with vectorized non-conformity scores, leveraging a generative model
to sample multiple predictions from the fitted data distribution. By computing
non-conformity scores across these samples and estimating empirical quantiles
at different density levels, we construct adaptive uncertainty sets using
density-ranked uncertainty balls. This approach enables more precise
uncertainty allocation -- yielding larger prediction sets in high-confidence
regions and smaller or excluded sets in low-confidence regions -- enhancing
both flexibility and efficiency. We establish theoretical guarantees for
statistical validity and demonstrate through extensive numerical experiments
that our method outperforms state-of-the-art techniques on synthetic and
real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with
  Realistic Validation On Diverse Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11676v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11676v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanis Lalou, Théo Gnassounou, Antoine Collas, Antoine de Mathelin, Oleksii Kachaiev, Ambroise Odonnat, Alexandre Gramfort, Thomas Moreau, Rémi Flamary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a
labeled source domain to perform well on an unlabeled target domain with some
data distribution shift. While many methods have been proposed in the
literature, fair and realistic evaluation remains an open question,
particularly due to methodological difficulties in selecting hyperparameters in
the unsupervised setting. With SKADA-bench, we propose a framework to evaluate
DA methods on diverse modalities, beyond computer vision task that have been
largely explored in the literature. We present a complete and fair evaluation
of existing shallow algorithms, including reweighting, mapping, and subspace
alignment. Realistic hyperparameter selection is performed with nested
cross-validation and various unsupervised model selection scores, on both
simulated datasets with controlled shifts and real-world datasets across
diverse modalities, such as images, text, biomedical, and tabular data. Our
benchmark highlights the importance of realistic validation and provides
practical guidance for real-life applications, with key insights into the
choice and impact of model selection approaches. SKADA-bench is open-source,
reproducible, and can be easily extended with novel DA methods, datasets, and
model selection criteria without requiring re-evaluating competitors.
SKADA-bench is available on Github at
https://github.com/scikit-adaptation/skada-bench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformer Neural Processes - Kernel Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12502v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12502v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Jenson, Jhonathan Navott, Mengyan Zhang, Makkunda Sharma, Elizaveta Semenova, Seth Flaxman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Processes (NPs) are a rapidly evolving class of models designed to
directly model the posterior predictive distribution of stochastic processes.
Originally developed as a scalable alternative to Gaussian Processes (GPs),
which are limited by $O(n^3)$ runtime complexity, the most accurate modern NPs
can often rival GPs but still suffer from an $O(n^2)$ bottleneck due to their
attention mechanism. We introduce the Transformer Neural Process - Kernel
Regression (TNP-KR), a scalable NP featuring: (1) a Kernel Regression Block
(KRBlock), a simple, extensible, and parameter efficient transformer block with
complexity $O(n_c^2 + n_c n_t)$, where $n_c$ and $n_t$ are the number of
context and test points, respectively; (2) a kernel-based attention bias; and
(3) two novel attention mechanisms: scan attention (SA), a memory-efficient
scan-based attention that when paired with a kernel-based bias can make TNP-KR
translation invariant, and deep kernel attention (DKA), a Performer-style
attention that implicitly incoporates a distance bias and further reduces
complexity to $O(n_c)$. These enhancements enable both TNP-KR variants to
perform inference with 100K context points on over 1M test points in under a
minute on a single 24GB GPU. On benchmarks spanning meta regression, Bayesian
optimization, image completion, and epidemiology, TNP-KR with DKA outperforms
its Performer counterpart on nearly every benchmark, while TNP-KR with SA
achieves state-of-the-art results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Autoformalization using Type Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Auguste Poiroux, Gail Weiss, Viktor Kunčak, Antoine Bosselut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoformalization, the automatic translation of unconstrained natural
language into formal languages, has garnered significant attention due to its
potential applications in theorem proving, formal verification, and LLM output
checking. In this work, we analyze both current autoformalization methods and
the processes used to evaluate them, focusing specifically on the Lean 4
theorem proving language. We demonstrate that scaling type-check filtering with
self-consistency techniques on top of existing methods significantly improves
performance, achieving absolute accuracy gains of up to +18.4\% on ProofNet. To
support reproducibility and further research, we release our code, including
new symbolic equivalence for Lean formulas. We also release new benchmarks: a
new research-level mathematics dataset RLM25, a corrected ProofNet, and
ProofNetVerif with labeled correct and incorrect autoformalization pairs for
evaluating metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>New benchmarks released, see
  https://github.com/augustepoiroux/RLMEval ,
  https://huggingface.co/datasets/PAug/ProofNetSharp , and
  https://huggingface.co/datasets/PAug/ProofNetVerif . For code, see
  https://github.com/augustepoiroux/LeanInteract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sharp Analysis for KL-Regularized Contextual Bandits and RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heyang Zhao, Chenlu Ye, Quanquan Gu, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reverse-Kullback-Leibler (KL) regularization has emerged to be a predominant
technique used to enhance policy optimization in reinforcement learning (RL)
and reinforcement learning from human feedback (RLHF), which forces the learned
policy to stay close to a reference policy. While the effectiveness and
necessity of KL-regularization have been empirically demonstrated in various
practical scenarios, current theoretical analysis of KL-regularized RLHF still
obtains the same $\mathcal{O}(1 / \epsilon^2)$ sample complexity as problems
without KL-regularization. To understand the fundamental distinction between
policy learning objectives with KL-regularization and ones without
KL-regularization, we are the first to theoretically demonstrate the power of
KL-regularization by providing a sharp analysis for KL-regularized contextual
bandits and RLHF, revealing an $\mathcal{O}(1 / \epsilon)$ sample complexity
when $\epsilon$ is sufficiently small.
  We further explore the role of data coverage in contextual bandits and RLHF.
While the coverage assumption is commonly employed in offline RLHF to link the
samples from the reference policy to the optimal policy, often at the cost of a
multiplicative dependence on the coverage coefficient, its impact on the sample
complexity of online RLHF remains unclear. Previous theoretical analyses of
online RLHF typically require explicit exploration and additional structural
assumptions on the reward function class. In contrast, we show that with
sufficient coverage from the reference policy, a simple two-stage mixed
sampling strategy can achieve a sample complexity with only an additive
dependence on the coverage coefficient. Our results provide a comprehensive
understanding of the roles of KL-regularization and data coverage in RLHF,
shedding light on the design of more efficient RLHF algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locally Private Estimation with Public Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Ma, Ke Jia, Hanfang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We initiate the study of locally differentially private (LDP) learning with
public features. We define semi-feature LDP, where some features are publicly
available while the remaining ones, along with the label, require protection
under local differential privacy. Under semi-feature LDP, we demonstrate that
the mini-max convergence rate for non-parametric regression is significantly
reduced compared to that of classical LDP. Then we propose HistOfTree, an
estimator that fully leverages the information contained in both public and
private features. Theoretically, HistOfTree reaches the mini-max optimal
convergence rate. Empirically, HistOfTree achieves superior performance on both
synthetic and real data. We also explore scenarios where users have the
flexibility to select features for protection manually. In such cases, we
propose an estimator and a data-driven parameter tuning strategy, leading to
analogous theoretical and empirical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards bandit-based prompt-tuning for in-the-wild foundation agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06358v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06358v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Finn Rietz, Oleg Smirnov, Sara Karimi, Lele Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting has emerged as the dominant paradigm for adapting large,
pre-trained transformer-based models to downstream tasks. The Prompting
Decision Transformer (PDT) enables large-scale, multi-task offline
reinforcement learning pre-training by leveraging stochastic trajectory prompts
to identify the target task. However, these prompts are sampled uniformly from
expert demonstrations, overlooking a critical limitation: Not all prompts are
equally informative for differentiating between tasks. To address this, we
propose an inference time bandit-based prompt-tuning framework that explores
and optimizes trajectory prompt selection to enhance task performance. Our
experiments indicate not only clear performance gains due to bandit-based
prompt-tuning, but also better sample complexity, scalability, and prompt space
exploration compared to prompt-tuning baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Source Disentanglement in Neural Audio Codec <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Bie, Xubo Liu, Gaël Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural audio codecs have significantly advanced audio compression by
efficiently converting continuous audio signals into discrete tokens. These
codecs preserve high-quality sound and enable sophisticated sound generation
through generative models trained on these tokens. However, existing neural
codec models are typically trained on large, undifferentiated audio datasets,
neglecting the essential discrepancies between sound domains like speech,
music, and environmental sound effects. This oversight complicates data
modeling and poses additional challenges to the controllability of sound
generation. To tackle these issues, we introduce the Source-Disentangled Neural
Audio Codec (SD-Codec), a novel approach that combines audio coding and source
separation. By jointly learning audio resynthesis and separation, SD-Codec
explicitly assigns audio signals from different domains to distinct codebooks,
sets of discrete representations. Experimental results indicate that SD-Codec
not only maintains competitive resynthesis quality but also, supported by the
separation results, demonstrates successful disentanglement of different
sources in the latent space, thereby enhancing interpretability in audio codec
and providing potential finer control over the audio generation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2025, project page: https://xiaoyubie1994.github.io/sdcodec/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concentration of Non-Isotropic Random Tensors with Applications to
  Learning and Empirical Risk Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.04259v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.04259v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathieu Even, Laurent Massoulié
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimension is an inherent bottleneck to some modern learning tasks, where
optimization methods suffer from the size of the data. In this paper, we study
non-isotropic distributions of data and develop tools that aim at reducing
these dimensional costs by a dependency on an effective dimension rather than
the ambient one. Based on non-asymptotic estimates of the metric entropy of
ellipsoids -- that prove to generalize to infinite dimensions -- and on a
chaining argument, our uniform concentration bounds involve an effective
dimension instead of the global dimension, improving over existing results. We
show the importance of taking advantage of non-isotropic properties in learning
problems with the following applications: i) we improve state-of-the-art
results in statistical preconditioning for communication-efficient distributed
optimization, ii) we introduce a non-isotropic randomized smoothing for
non-smooth optimization. Both applications cover a class of functions that
encompasses empirical risk minization (ERM) for linear models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Handling missing values in clinical machine learning: Insights from an
  expert study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09591v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09591v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lena Stempfle, Arthur James, Julie Josse, Tobias Gauss, Fredrik D. Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inherently interpretable machine learning (IML) models offer valuable support
for clinical decision-making but face challenges when features contain missing
values. Traditional approaches, such as imputation or discarding incomplete
records, are often impractical in scenarios where data is missing at test time.
We surveyed 55 clinicians from 29 French trauma centers, collecting 20 complete
responses to study their interaction with three IML models in a real-world
clinical setting for predicting hemorrhagic shock with missing values. Our
findings reveal that while clinicians recognize the value of interpretability
and are familiar with common IML approaches, traditional imputation techniques
often conflict with their intuition. Instead of imputing unobserved values,
they rely on observed features combined with medical intuition and experience.
As a result, methods that natively handle missing values are preferred. These
findings underscore the need to integrate clinical reasoning into future IML
models to enhance human-computer interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, restructured writing from previous version and
  additional results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accuracy and Robustness of Weight-Balancing Methods for Training PINNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Barreau, Haoming Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-Informed Neural Networks (PINNs) have emerged as powerful tools for
integrating physics-based models with data by minimizing both data and physics
losses. However, this multi-objective optimization problem is notoriously
challenging, with some benchmark problems leading to unfeasible solutions. To
address these issues, various strategies have been proposed, including adaptive
weight adjustments in the loss function. In this work, we introduce clear
definitions of accuracy and robustness in the context of PINNs and propose a
novel training algorithm based on the Primal-Dual (PD) optimization framework.
Our approach enhances the robustness of PINNs while maintaining comparable
performance to existing weight-balancing methods. Numerical experiments
demonstrate that the PD method consistently achieves reliable solutions across
all investigated cases, even in the low-data regime, and can be easily
implemented, facilitating its practical adoption. The code is available at
https://github.com/haoming-SHEN/Accuracy-and-Robustness-of-Weight-Balancing-Methods-for-Training-PINNs.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re-evaluating Automatic LLM System Ranking for Alignment with <span class="highlight-title">Human</span>
  Preference <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqi Gao, Yixin Liu, Xinyu Hu, Xiaojun Wan, Jonathan Bragg, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating and ranking the capabilities of different LLMs is crucial for
understanding their performance and alignment with human preferences. Due to
the high cost and time-consuming nature of human evaluations, an automatic LLM
bencher (i.e., an automatic evaluation framework that aims to rank LLMs based
on their alignment with human preferences) is indispensable. An automatic LLM
bencher consists of four components: the input set (e.g., a user instruction),
the evaluation model (e.g., an LLM), the evaluation type (e.g., pairwise
comparison), and the aggregation method (e.g., the ELO rating system). However,
previous work has not thoroughly explored how to select these components or how
their different combinations influence the results. In this work, through
controlled experiments, we provide a series of recommendations on how to choose
each component to better automate the evaluation of LLMs. Furthermore, we
discovered that when evaluating LLMs with similar performance, the performance
of the automatic LLM bencher declines sharply, underscoring the limitations of
current benchers and calling for future work. Lastly, we found that the
evaluation models' performance at the instance level (e.g., the accuracy of
selecting the best output) does not always align with their effectiveness when
used as a component of a bencher, highlighting the importance of dedicated
system-level evaluation of benchers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Amortized Bayesian Inference with Self-Consistency Losses on
  Unlabeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aayush Mishra, Daniel Habermann, Marvin Schmitt, Stefan T. Radev, Paul-Christian Bürkner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural amortized Bayesian inference (ABI) can solve probabilistic inverse
problems orders of magnitude faster than classical methods. However, neural ABI
is not yet sufficiently robust for widespread and safe applicability. In
particular, when performing inference on observations outside of the scope of
the simulated data seen during training, for example, because of model
misspecification, the posterior approximations are likely to become highly
biased. Due to the bad pre-asymptotic behavior of current neural posterior
estimators in the out-of-simulation regime, the resulting estimation biases
cannot be fixed in acceptable time by just simulating more training data. In
this proof-of-concept paper, we propose a semi-supervised approach that enables
training not only on (labeled) simulated data generated from the model, but
also on unlabeled data originating from any source, including real-world data.
To achieve the latter, we exploit Bayesian self-consistency properties that can
be transformed into strictly proper losses without requiring knowledge of true
parameter values, that is, without requiring data labels. The results of our
initial experiments show remarkable improvements in the robustness of ABI on
out-of-simulation data. Even if the observed data is far away from both labeled
and unlabeled training data, inference remains highly accurate. If our findings
also generalize to other scenarios and model classes, we believe that our new
method represents a major breakthrough in neural ABI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PSformer: Parameter-efficient Transformer with Segment Attention for
  Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlong Wang, Jian Xu, Fei Ma, Shao-Lun Huang, Danny Dongning Sun, Xiao-Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting remains a critical challenge across various domains,
often complicated by high-dimensional data and long-term dependencies. This
paper presents a novel transformer architecture for time series forecasting,
incorporating two key innovations: parameter sharing (PS) and Spatial-Temporal
Segment Attention (SegAtt). We also define the time series segment as the
concatenation of sequence patches from the same positions across different
variables. The proposed model, PSformer, reduces the number of training
parameters through the parameter sharing mechanism, thereby improving model
efficiency and scalability. The introduction of SegAtt could enhance the
capability of capturing local spatio-temporal dependencies by computing
attention over the segments, and improve global representation by integrating
information across segments. The combination of parameter sharing and SegAtt
significantly improves the forecasting performance. Extensive experiments on
benchmark datasets demonstrate that PSformer outperforms popular baselines and
other transformer-based approaches in terms of accuracy and scalability,
establishing itself as an accurate and scalable tool for time series
forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Singular leaning coefficients and efficiency in learning theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miki Aoyagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singular learning models with non-positive Fisher information matrices
include neural networks, reduced-rank regression, Boltzmann machines, normal
mixture models, and others. These models have been widely used in the
development of learning machines. However, theoretical analysis is still in its
early stages. In this paper, we examine learning coefficients, which indicate
the general learning efficiency of deep linear learning models and three-layer
neural network models with ReLU units. Finally, we extend the results to
include the case of the Softmax function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changdae Oh, Yixuan Li, Kyungwoo Song, Sangdoo Yun, Dongyoon Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting a pre-trained foundation model on downstream tasks should ensure
robustness against distribution shifts without the need to retrain the whole
model. Although existing weight interpolation methods are simple yet effective,
we argue their static nature limits downstream performance while achieving
efficiency. In this work, we propose DaWin, a training-free dynamic weight
interpolation method that leverages the entropy of individual models over each
unlabeled test sample to assess model expertise, and compute per-sample
interpolation coefficients dynamically. Unlike previous works that typically
rely on additional training to learn such coefficients, our approach requires
no training. Then, we propose a mixture modeling approach that greatly reduces
inference overhead raised by dynamic interpolation. We validate DaWin on the
large-scale visual recognition benchmarks, spanning 14 tasks across robust
fine-tuning -- ImageNet and derived five distribution shift benchmarks -- and
multi-task learning with eight classification tasks. Results demonstrate that
DaWin achieves significant performance gain in considered settings, with
minimal computational overhead. We further discuss DaWin's analytic behavior to
explain its empirical success.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AURO: Reinforcement Learning for Adaptive User Retention Optimization in
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenghai Xue, Qingpeng Cai, Bin Yang, Lantao Hu, Peng Jiang, Kun Gai, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Reinforcement Learning (RL) has garnered increasing attention
for its ability of optimizing user retention in recommender systems. A primary
obstacle in this optimization process is the environment non-stationarity
stemming from the continual and complex evolution of user behavior patterns
over time, such as variations in interaction rates and retention propensities.
These changes pose significant challenges to existing RL algorithms for
recommendations, leading to issues with dynamics and reward distribution
shifts. This paper introduces a novel approach called \textbf{A}daptive
\textbf{U}ser \textbf{R}etention \textbf{O}ptimization (AURO) to address this
challenge. To navigate the recommendation policy in non-stationary
environments, AURO introduces an state abstraction module in the policy
network. The module is trained with a new value-based loss function, aligning
its output with the estimated performance of the current policy. As the policy
performance of RL is sensitive to environment drifts, the loss function enables
the state abstraction to be reflective of environment changes and notify the
recommendation policy to adapt accordingly. Additionally, the non-stationarity
of the environment introduces the problem of implicit cold start, where the
recommendation policy continuously interacts with users displaying novel
behavior patterns. AURO encourages exploration guarded by performance-based
rejection sampling to maintain a stable recommendation quality in the
cost-sensitive online environment. Extensive empirical analysis are conducted
in a user retention simulator, the MovieLens dataset, and a live short-video
recommendation platform, demonstrating AURO's superior performance against all
evaluated baseline algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Web Conference 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Assetization via Resources-decoupled Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianzhe Zhao, Feida Zhu, Lingyan He, Zixin Tang, Mingce Gao, Shiyu Yang, Guibing Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of the digital economy, data is increasingly recognized
as an essential resource for both work and life. However, due to privacy
concerns, data owners tend to maximize the value of data through the
circulation of information rather than direct data transfer. Federated learning
(FL) provides an effective approach to collaborative training models while
preserving privacy. However, as model parameters and training data grow, there
are not only real differences in data resources between different data owners,
but also mismatches between data and computing resources. These challenges lead
to inadequate collaboration among data owners, compute centers, and model
owners, reducing the global utility of the three parties and the effectiveness
of data assetization. In this work, we first propose a framework for
resource-decoupled FL involving three parties. Then, we design a Tripartite
Stackelberg Model and theoretically analyze the Stackelberg-Nash equilibrium
(SNE) for participants to optimize global utility. Next, we propose the
Quality-aware Dynamic Resources-decoupled FL algorithm (QD-RDFL), in which we
derive and solve the optimal strategies of all parties to achieve SNE using
backward induction. We also design a dynamic optimization mechanism to improve
the optimal strategy profile by evaluating the contribution of data quality
from data owners to the global model during real training. Finally, our
extensive experiments demonstrate that our method effectively encourages the
linkage of the three parties involved, maximizing the global utility and value
of data assets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point Cloud Synthesis Using Inner Product Transforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ernst Röell, Bastian Rieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-cloud synthesis, i.e. the generation of novel point clouds from an
input distribution, remains a challenging task, for which numerous complex
machine-learning models have been devised. We develop a novel method that
encodes geometrical-topological characteristics of point clouds using inner
products, leading to a highly-efficient point cloud representation with
provable expressivity properties. Integrated into deep learning models, our
encoding exhibits high quality in typical tasks like reconstruction,
generation, and interpolation, with inference times orders of magnitude faster
than existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZeroDiff: Solidified Visual-Semantic Correlation in Zero-Shot Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Ye, Shreyank N. Gowda, Xiaowei Huang, Haotian Xu, Yaochu Jin, Kaizhu Huang, Xiaobo Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot Learning (ZSL) aims to enable classifiers to identify unseen
classes. This is typically achieved by generating visual features for unseen
classes based on learned visual-semantic correlations from seen classes.
However, most current generative approaches heavily rely on having a sufficient
number of samples from seen classes. Our study reveals that a scarcity of seen
class samples results in a marked decrease in performance across many
generative ZSL techniques. We argue, quantify, and empirically demonstrate that
this decline is largely attributable to spurious visual-semantic correlations.
To address this issue, we introduce ZeroDiff, an innovative generative
framework for ZSL that incorporates diffusion mechanisms and contrastive
representations to enhance visual-semantic correlations. ZeroDiff comprises
three key components: (1) Diffusion augmentation, which naturally transforms
limited data into an expanded set of noised data to mitigate generative model
overfitting; (2) Supervised-contrastive (SC)-based representations that
dynamically characterize each limited sample to support visual feature
generation; and (3) Multiple feature discriminators employing a
Wasserstein-distance-based mutual learning approach, evaluating generated
features from various perspectives, including pre-defined semantics, SC-based
representations, and the diffusion process. Extensive experiments on three
popular ZSL benchmarks demonstrate that ZeroDiff not only achieves significant
improvements over existing ZSL methods but also maintains robust performance
even with scarce training data. Our codes are available at
https://github.com/FouriYe/ZeroDiff_ICLR25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spindle: Efficient Distributed Training of Multi-Task Large Models via
  Wavefront Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Wang, Shenhan Zhu, Fangcheng Fu, Xupeng Miao, Jie Zhang, Juan Zhu, Fan Hong, Yong Li, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent foundation models are capable of handling multiple tasks and multiple
data modalities with the unified base model structure and several specialized
model components. However, efficient training of such multi-task (MT)
multi-modal (MM) models poses significant system challenges due to the
sophisticated model architecture and the heterogeneous workloads of different
tasks and modalities.
  In this paper, we propose Spindle, a brand new training system tailored for
resource-efficient and high-performance training of MT MM models via wavefront
scheduling. The key idea of Spindle is to decompose the model execution into
waves and address the joint optimization problem sequentially, including both
heterogeneity-aware workload parallelization and dependency-driven execution
scheduling. We build our system and evaluate it on various MT MM models.
Experiments demonstrate the superior performance and efficiency of Spindle,
with speedup ratio up to 71% compared to state-of-the-art training systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Density Ratio Estimation with Conditional Probability Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Yu, Arto Klami, Aapo Hyvärinen, Anna Korba, Omar Chehab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Density ratio estimation in high dimensions can be reframed as integrating a
certain quantity, the time score, over probability paths which interpolate
between the two densities. In practice, the time score has to be estimated
based on samples from the two densities. However, existing methods for this
problem remain computationally expensive and can yield inaccurate estimates.
Inspired by recent advances in generative modeling, we introduce a novel
framework for time score estimation, based on a conditioning variable. Choosing
the conditioning variable judiciously enables a closed-form objective function.
We demonstrate that, compared to previous approaches, our approach results in
faster learning of the time score and competitive or better estimation
accuracies of the density ratio on challenging tasks. Furthermore, we establish
theoretical guarantees on the error of the estimated density ratio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A PAC-Bayesian Link Between Generalisation and Flat Minima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Haddouche, Paul Viallard, Umut Simsekli, Benjamin Guedj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern machine learning usually involves predictors in the overparameterised
setting (number of trained parameters greater than dataset size), and their
training yields not only good performance on training data, but also good
generalisation capacity. This phenomenon challenges many theoretical results,
and remains an open problem. To reach a better understanding, we provide novel
generalisation bounds involving gradient terms. To do so, we combine the
PAC-Bayes toolbox with Poincar\'e and Log-Sobolev inequalities, avoiding an
explicit dependency on the dimension of the predictor space. Our results
highlight the positive influence of flat minima (being minima with a
neighbourhood nearly minimising the learning problem as well) on generalisation
performance, involving directly the benefits of the optimisation phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at International Conference on Algorithmic Learning Theory
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamCatalyst: Fast and High-Quality 3D Editing via <span class="highlight-title">Control</span>ling
  Editability and Identity Preservation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwook Kim, Seonho Lee, Jaeyo Shin, Jiho Choi, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score distillation sampling (SDS) has emerged as an effective framework in
text-driven 3D editing tasks, leveraging diffusion models for 3D-consistent
editing. However, existing SDS-based 3D editing methods suffer from long
training times and produce low-quality results. We identify that the root cause
of this performance degradation is \textit{their conflict with the sampling
dynamics of diffusion models}. Addressing this conflict allows us to treat SDS
as a diffusion reverse process for 3D editing via sampling from data space. In
contrast, existing methods naively distill the score function using diffusion
models. From these insights, we propose DreamCatalyst, a novel framework that
considers these sampling dynamics in the SDS framework. Specifically, we devise
the optimization process of our DreamCatalyst to approximate the diffusion
reverse process in editing tasks, thereby aligning with diffusion sampling
dynamics. As a result, DreamCatalyst successfully reduces training time and
improves editing quality. Our method offers two modes: (1) a fast mode that
edits Neural Radiance Fields (NeRF) scenes approximately 23 times faster than
current state-of-the-art NeRF editing methods, and (2) a high-quality mode that
produces superior results about 8 times faster than these methods. Notably, our
high-quality mode outperforms current state-of-the-art NeRF editing methods in
terms of both speed and quality. DreamCatalyst also surpasses the
state-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing
itself as an effective and model-agnostic 3D editing solution. See more
extensive results on our project page: https://dream-catalyst.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Multiple Knowledge Graphs in a Single Pass 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaming Yang, Zhe Wang, Ziyu Guan, Wei Zhao, Weigang Lu, Xinyan Huang, Jiangtao Cui, Xiaofei He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) is to identify equivalent entities across different
knowledge graphs (KGs), which can help fuse these KGs into a more comprehensive
one. Previous EA methods mainly focus on aligning a pair of KGs, and to the
best of our knowledge, no existing EA method considers aligning multiple (more
than two) KGs. To fill this research gap, in this work, we study a novel
problem of aligning multiple KGs and propose an effective framework named
MultiEA to solve the problem. First, we embed the entities of all the candidate
KGs into a common feature space by a shared KG encoder. Then, we explore three
alignment strategies to minimize the distances among pre-aligned entities. In
particular, we propose an innovative inference enhancement technique to improve
the alignment performance by incorporating high-order similarities. Finally, to
verify the effectiveness of MultiEA, we construct two new real-world benchmark
datasets and conduct extensive experiments on them. The results show that our
MultiEA can effectively and efficiently align multiple KGs in a single pass. We
release the source codes of MultiEA at: https://github.com/kepsail/MultiEA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlexSP: Accelerating Large Language Model Training via Flexible Sequence
  Parallelism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01523v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01523v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Wang, Shiju Wang, Shenhan Zhu, Fangcheng Fu, Xinyi Liu, Xuefeng Xiao, Huixia Li, Jiashi Li, Faming Wu, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extending the context length (i.e., the maximum supported sequence length) of
LLMs is of paramount significance. To facilitate long context training of LLMs,
sequence parallelism has emerged as an essential technique, which scatters each
input sequence across multiple devices and necessitates communication to
process the sequence. In essence, existing sequence parallelism methods assume
homogeneous sequence lengths (i.e., all input sequences are equal in length)
and therefore leverages a single, static scattering strategy for all input
sequences. However, in reality, the sequence lengths in LLM training corpora
exhibit substantial variability, often following a long-tail distribution,
which leads to workload heterogeneity.
  In this paper, we show that employing a single, static strategy results in
inefficiency and resource under-utilization, highlighting the need for adaptive
approaches to handle the heterogeneous workloads across sequences. To address
this, we propose a heterogeneity-adaptive sequence parallelism method. For each
training step, our approach captures the variability in sequence lengths and
assigns the optimal combination of scattering strategies based on workload
characteristics. We model this problem as a linear programming optimization and
design an efficient and effective solver to find the optimal solution.
Furthermore, we implement our method in a high-performance system that supports
adaptive parallelization in distributed LLM training. Experimental results
demonstrate that our system outperforms state-of-the-art training frameworks by
up to 1.98x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Information Theoretic Text-to-Image Alignment <span class="chip">ICLR25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20759v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20759v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Giulio Franzese, Alessandro Finamore, Massimo Gallo, Pietro Michiardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models for Text-to-Image (T2I) conditional generation have recently
achieved tremendous success. Yet, aligning these models with user's intentions
still involves a laborious trial-and-error process, and this challenging
alignment problem has attracted considerable attention from the research
community. In this work, instead of relying on fine-grained linguistic analyses
of prompts, human annotation, or auxiliary vision-language models, we use
Mutual Information (MI) to guide model alignment. In brief, our method uses
self-supervised fine-tuning and relies on a point-wise (MI) estimation between
prompts and images to create a synthetic fine-tuning set for improving model
alignment. Our analysis indicates that our method is superior to the
state-of-the-art, yet it only requires the pre-trained denoising network of the
T2I model itself to estimate MI, and a simple fine-tuning strategy that
improves alignment while maintaining image quality. Code available at
https://github.com/Chao0511/mitune.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear at ICLR25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Denoising Task Difficulty-based Curriculum for Training Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10348v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10348v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Young Kim, Hyojun Go, Soonwoo Kwon, Hyun-Gyoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based generative models have emerged as powerful tools in the realm
of generative modeling. Despite extensive research on denoising across various
timesteps and noise levels, a conflict persists regarding the relative
difficulties of the denoising tasks. While various studies argue that lower
timesteps present more challenging tasks, others contend that higher timesteps
are more difficult. To address this conflict, our study undertakes a
comprehensive examination of task difficulties, focusing on convergence
behavior and changes in relative entropy between consecutive probability
distributions across timesteps. Our observational study reveals that denoising
at earlier timesteps poses challenges characterized by slower convergence and
higher relative entropy, indicating increased task difficulty at these lower
timesteps. Building on these observations, we introduce an easy-to-hard
learning scheme, drawing from curriculum learning, to enhance the training
process of diffusion models. By organizing timesteps or noise levels into
clusters and training models with ascending orders of difficulty, we facilitate
an order-aware training regime, progressing from easier to harder denoising
tasks, thereby deviating from the conventional approach of training diffusion
models simultaneously across all timesteps. Our approach leads to improved
performance and faster convergence by leveraging benefits of curriculum
learning, while maintaining orthogonality with existing improvements in
diffusion training techniques. We validate these advantages through
comprehensive experiments in image generation tasks, including unconditional,
class-conditional, and text-to-image generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tackling Dimensional Collapse toward Comprehensive Universal Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung-Chieh Fang, Po-Yi Lu, Hsuan-Tien Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal Domain Adaptation (UniDA) addresses unsupervised domain adaptation
where target classes may differ arbitrarily from source ones, except for a
shared subset. An important approach, partial domain matching (PDM), aligns
only shared classes but struggles in extreme cases where many source classes
are absent in the target domain, underperforming the most naive baseline that
trains on only source data. In this work, we identify that the failure of PDM
for extreme UniDA stems from dimensional collapse (DC) in target
representations. To address target DC, we propose to jointly leverage the
alignment and uniformity techniques in modern self-supervised learning (SSL) on
the unlabeled target data to preserve the intrinsic structure of the learned
representations. Our experimental results confirm that SSL consistently
advances PDM and delivers new state-of-the-art results across a broader
benchmark of UniDA scenarios with different portions of shared classes,
representing a crucial step toward truly comprehensive UniDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calibrating LLMs with Information-Theoretic Evidential Deep Learning <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06351v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06351v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yawei Li, David Rügamer, Bernd Bischl, Mina Rezaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuned large language models (LLMs) often exhibit overconfidence,
particularly when trained on small datasets, resulting in poor calibration and
inaccurate uncertainty estimates. Evidential Deep Learning (EDL), an
uncertainty-aware approach, enables uncertainty estimation in a single forward
pass, making it a promising method for calibrating fine-tuned LLMs. However,
despite its computational efficiency, EDL is prone to overfitting, as its
training objective can result in overly concentrated probability distributions.
To mitigate this, we propose regularizing EDL by incorporating an information
bottleneck (IB). Our approach IB-EDL suppresses spurious information in the
evidence generated by the model and encourages truly predictive information to
influence both the predictions and uncertainty estimates. Extensive experiments
across various fine-tuned LLMs and tasks demonstrate that IB-EDL outperforms
both existing EDL and non-EDL approaches. By improving the trustworthiness of
LLMs, IB-EDL facilitates their broader adoption in domains requiring high
levels of confidence calibration. Code is available at
https://github.com/sandylaker/ib-edl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages; 3 figures; accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Complexity of Learning Sparse Super<span class="highlight-title">pose</span>d Features with Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of deep networks is crucially attributed to their ability to
capture latent features within a representation space. In this work, we
investigate whether the underlying learned features of a model can be
efficiently retrieved through feedback from an agent, such as a large language
model (LLM), in the form of relative \textit{triplet comparisons}. These
features may represent various constructs, including dictionaries in LLMs or
components of a covariance matrix of Mahalanobis distances. We analyze the
feedback complexity associated with learning a feature matrix in sparse
settings. Our results establish tight bounds when the agent is permitted to
construct activations and demonstrate strong upper bounds in sparse scenarios
when the agent's feedback is limited to distributional information. We validate
our theoretical findings through experiments on two distinct applications:
feature recovery from Recursive Feature Machine-trained models and dictionary
extraction from sparse autoencoders trained on Large Language Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract
  Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04620v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04620v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pragnya Ramjee, Bhuvan Sachdeva, Satvik Golechha, Shreyas Kulkarni, Geeta Fulari, Kaushik Murali, Mohit Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The healthcare landscape is evolving, with patients seeking reliable
information about their health conditions and available treatment options.
Despite the abundance of information sources, the digital age overwhelms
individuals with excess, often inaccurate information. Patients primarily trust
medical professionals, highlighting the need for expert-endorsed health
information. However, increased patient loads on experts has led to reduced
communication time, impacting information sharing. To address this gap, we
developed CataractBot. CataractBot answers cataract surgery related questions
instantly using an LLM to query a curated knowledge base, and provides
expert-verified responses asynchronously. It has multimodal and multilingual
capabilities. In an in-the-wild deployment study with 49 patients and
attendants, 4 doctors, and 2 patient coordinators, CataractBot demonstrated
potential, providing anytime accessibility, saving time, accommodating diverse
literacy levels, alleviating power differences, and adding a privacy layer
between patients and doctors. Users reported that their trust in the system was
established through expert verification. Broadly, our results could inform
future work on designing expert-mediated LLM bots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRKAN: Parameter-Reduced Kolmogorov-Arnold Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07032v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07032v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang-Thang Ta, Duy-Quy Thai, Anh Tran, Grigori Sidorov, Alexander Gelbukh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kolmogorov-Arnold Networks (KANs) represent an innovation in neural network
architectures, offering a compelling alternative to Multi-Layer Perceptrons
(MLPs) in models such as Convolutional Neural Networks (CNNs), Recurrent Neural
Networks (RNNs), and Transformers. By advancing network design, KANs drive
groundbreaking research and enable transformative applications across various
scientific domains involving neural networks. However, existing KANs often
require significantly more parameters in their network layers than MLPs. To
address this limitation, this paper introduces PRKANs (Parameter-Reduced
Kolmogorov-Arnold Networks), which employ several methods to reduce the
parameter count in KAN layers, making them comparable to MLP layers.
Experimental results on the MNIST and Fashion-MNIST datasets demonstrate that
PRKANs outperform several existing KANs, and their variant with attention
mechanisms rivals the performance of MLPs, albeit with slightly longer training
times. Furthermore, the study highlights the advantages of Gaussian Radial
Basis Functions (GRBFs) and layer normalization in KAN designs. The repository
for this work is available at: https://github.com/hoangthangta/All-KAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotically Optimal Change Detection for Unnormalized Pre- and
  Post-Change Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Sanjeev Kulkarni, H. Vincent Poor, Taposh Banerjee, Vahid Tarokh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of detecting changes when only unnormalized
pre- and post-change distributions are accessible. This situation happens in
many scenarios in physics such as in ferromagnetism, crystallography,
magneto-hydrodynamics, and thermodynamics, where the energy models are
difficult to normalize.
  Our approach is based on the estimation of the Cumulative Sum (CUSUM)
statistics, which is known to produce optimal performance. We first present an
intuitively appealing approximation method. Unfortunately, this produces a
biased estimator of the CUSUM statistics and may cause performance degradation.
We then propose the Log-Partition Approximation Cumulative Sum (LPA-CUSUM)
algorithm based on thermodynamic integration (TI) in order to estimate the
log-ratio of normalizing constants of pre- and post-change distributions. It is
proved that this approach gives an unbiased estimate of the log-partition
function and the CUSUM statistics, and leads to an asymptotically optimal
performance. Moreover, we derive a relationship between the required sample
size for thermodynamic integration and the desired detection delay performance,
offering guidelines for practical parameter selection. Numerical studies are
provided demonstrating the efficacy of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks at a Fraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rucha Bhalchandra Joshi, Sagar Prakash Barad, Nidhi Tiwari, Subhankar Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have emerged as powerful tools for learning
representations of graph-structured data. In addition to real-valued GNNs,
quaternion GNNs also perform well on tasks on graph-structured data. With the
aim of reducing the energy footprint, we reduce the model size while
maintaining accuracy comparable to that of the original-sized GNNs. This paper
introduces Quaternion Message Passing Neural Networks (QMPNNs), a framework
that leverages quaternion space to compute node representations. Our approach
offers a generalizable method for incorporating quaternion representations into
GNN architectures at one-fourth of the original parameter count. Furthermore,
we present a novel perspective on Graph Lottery Tickets, redefining their
applicability within the context of GNNs and QMPNNs. We specifically aim to
find the initialization lottery from the subnetwork of the GNNs that can
achieve comparable performance to the original GNN upon training. Thereby
reducing the trainable model parameters even further. To validate the
effectiveness of our proposed QMPNN framework and LTH for both GNNs and QMPNNs,
we evaluate their performance on real-world datasets across three fundamental
graph-based tasks: node classification, link prediction, and graph
classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, accepted at PAKKD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A duality framework for analyzing random feature and two-layer neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05642v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05642v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrui Chen, Jihao Long, Lei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning functions within the
$\mathcal{F}_{p,\pi}$ and Barron spaces, which play crucial roles in
understanding random feature models (RFMs), two-layer neural networks, as well
as kernel methods. Leveraging tools from information-based complexity (IBC), we
establish a dual equivalence between approximation and estimation, and then
apply it to study the learning of the preceding function spaces. The duality
allows us to focus on the more tractable problem between approximation and
estimation. To showcase the efficacy of our duality framework, we delve into
two important but under-explored problems:
  1) Random feature learning beyond kernel regime: We derive sharp bounds for
learning $\mathcal{F}_{p,\pi}$ using RFMs. Notably, the learning is efficient
without the curse of dimensionality for $p>1$. This underscores the extended
applicability of RFMs beyond the traditional kernel regime, since
$\mathcal{F}_{p,\pi}$ with $p<2$ is strictly larger than the corresponding
reproducing kernel Hilbert space (RKHS) where $p=2$.
  2) The $L^\infty$ learning of RKHS: We establish sharp, spectrum-dependent
characterizations for the convergence of $L^\infty$ learning error in both
noiseless and noisy settings. Surprisingly, we show that popular kernel ridge
regression can achieve near-optimal performance in $L^\infty$ learning, despite
it primarily minimizing square loss.
  To establish the aforementioned duality, we introduce a type of IBC, termed
$I$-complexity, to measure the size of a function class. Notably,
$I$-complexity offers a tight characterization of learning in noiseless
settings, yields lower bounds comparable to Le Cam's in noisy settings, and is
versatile in deriving upper bounds. We believe that our duality framework holds
potential for broad application in learning analysis across more scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Annals of Statistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Ensemble Belief Propagation for Efficient Inference in
  High-Dimensional Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08193v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08193v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan MacKinlay, Russell Tsuchida, Dan Pagendam, Petra Kuhnert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient inference in high-dimensional models is a central challenge in
machine learning. We introduce the Gaussian Ensemble Belief Propagation (GEnBP)
algorithm, which combines the strengths of the Ensemble Kalman Filter (EnKF)
and Gaussian Belief Propagation (GaBP) to address this challenge. GEnBP updates
ensembles of prior samples into posterior samples by passing low-rank local
messages over the edges of a graphical model, enabling efficient handling of
high-dimensional states, parameters, and complex, noisy, black-box generation
processes. By utilizing local message passing within a graphical model
structure, GEnBP effectively manages complex dependency structures and remains
computationally efficient even when the ensemble size is much smaller than the
inference dimension -- a common scenario in spatiotemporal modeling, image
processing, and physical model inversion. We demonstrate that GEnBP can be
applied to various problem structures, including data assimilation, system
identification, and hierarchical models, and show through experiments that it
outperforms existing belief propagation methods in terms of accuracy and
computational efficiency.
  Supporting code is available at https://github.com/danmackinlay/GEnBP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under conference submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Plug-in Approach for Average-Reward and Discounted MDPs: Optimal
  Sample Complexity Analysis <span class="chip">ALT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Zurek, Yudong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the sample complexity of the plug-in approach for learning
$\varepsilon$-optimal policies in average-reward Markov decision processes
(MDPs) with a generative model. The plug-in approach constructs a model
estimate then computes an average-reward optimal policy in the estimated model.
Despite representing arguably the simplest algorithm for this problem, the
plug-in approach has never been theoretically analyzed. Unlike the more
well-studied discounted MDP reduction method, the plug-in approach requires no
prior problem information or parameter tuning. Our results fill this gap and
address the limitations of prior approaches, as we show that the plug-in
approach is optimal in several well-studied settings without using prior
knowledge. Specifically it achieves the optimal diameter- and mixing-based
sample complexities of $\widetilde{O}\left(SA \frac{D}{\varepsilon^2}\right)$
and $\widetilde{O}\left(SA \frac{\tau_{\mathrm{unif}}}{\varepsilon^2}\right)$,
respectively, without knowledge of the diameter $D$ or uniform mixing time
$\tau_{\mathrm{unif}}$. We also obtain span-based bounds for the plug-in
approach, and complement them with algorithm-specific lower bounds suggesting
that they are unimprovable. Our results require novel techniques for analyzing
long-horizon problems which may be broadly useful and which also improve
results for the discounted plug-in approach, removing effective-horizon-related
sample size restrictions and obtaining the first optimal complexity bounds for
the full range of sample sizes without reward perturbation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 36th International Conference on Algorithmic Learning
  Theory (ALT 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GroverGPT: A Large Language Model with 8 Billion Parameters for Quantum
  Searching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.00135v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.00135v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Wang, Pingzhi Li, Min Chen, Jinglei Cheng, Junyu Liu, Tianlong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing is an exciting non-Von Neumann paradigm, offering provable
speedups over classical computing for specific problems. However, the practical
limits of classical simulatability for quantum circuits remain unclear,
especially with current noisy quantum devices. In this work, we explore the
potential of leveraging Large Language Models (LLMs) to simulate the output of
a quantum Turing machine using Grover's quantum circuits, known to provide
quadratic speedups over classical counterparts. To this end, we developed
GroverGPT, a specialized model based on LLaMA's 8-billion-parameter
architecture, trained on over 15 trillion tokens. Unlike brute-force
state-vector simulations, which demand substantial computational resources,
GroverGPT employs pattern recognition to approximate quantum search algorithms
without explicitly representing quantum states. Analyzing 97K quantum search
instances, GroverGPT consistently outperformed OpenAI's GPT-4o (45\% accuracy),
achieving nearly 100\% accuracy on 6- and 10-qubit datasets when trained on
4-qubit or larger datasets. It also demonstrated strong generalization,
surpassing 95\% accuracy for systems with over 20 qubits when trained on 3- to
6-qubit data. Analysis indicates GroverGPT captures quantum features of
Grover's search rather than classical patterns, supported by novel prompting
strategies to enhance performance. Although accuracy declines with increasing
system size, these findings offer insights into the practical boundaries of
classical simulatability. This work suggests task-specific LLMs can surpass
general-purpose models like GPT-4o in quantum algorithm learning and serve as
powerful tools for advancing quantum research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages including appendices. v2, v3: Add more experiments include
  ablation tests. Fix the terminology about infidelity. Add more benchmarks
  including Llama-3.2-3B and DeepSeek-v2-Lite</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ xPatch: Dual-Stream Time Series Forecasting with Exponential
  Seasonal-Trend Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17323v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17323v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artyom Stitsyuk, Jaesik Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the application of transformer-based models in time-series
forecasting has received significant attention. While often demonstrating
promising results, the transformer architecture encounters challenges in fully
exploiting the temporal relations within time series data due to its attention
mechanism. In this work, we design eXponential Patch (xPatch for short), a
novel dual-stream architecture that utilizes exponential decomposition.
Inspired by the classical exponential smoothing approaches, xPatch introduces
the innovative seasonal-trend exponential decomposition module. Additionally,
we propose a dual-flow architecture that consists of an MLP-based linear stream
and a CNN-based non-linear stream. This model investigates the benefits of
employing patching and channel-independence techniques within a non-transformer
model. Finally, we develop a robust arctangent loss function and a sigmoid
learning rate adjustment scheme, which prevent overfitting and boost
forecasting performance. The code is available at the following repository:
https://github.com/stitsyuk/xPatch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformer-based Stagewise Decomposition for Large-Scale Multistage
  Stochastic Optimization <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanyeong Kim, Jongwoong Park, Hyunglip Bae, Woo Chang Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving large-scale multistage stochastic programming (MSP) problems poses a
significant challenge as commonly used stagewise decomposition algorithms,
including stochastic dual dynamic programming (SDDP), face growing time
complexity as the subproblem size and problem count increase. Traditional
approaches approximate the value functions as piecewise linear convex functions
by incrementally accumulating subgradient cutting planes from the primal and
dual solutions of stagewise subproblems. Recognizing these limitations, we
introduce TranSDDP, a novel Transformer-based stagewise decomposition
algorithm. This innovative approach leverages the structural advantages of the
Transformer model, implementing a sequential method for integrating subgradient
cutting planes to approximate the value function. Through our numerical
experiments, we affirm TranSDDP's effectiveness in addressing MSP problems. It
efficiently generates a piecewise linear approximation for the value function,
significantly reducing computation time while preserving solution quality, thus
marking a promising progression in the treatment of large-scale multistage
stochastic programming problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023 (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G2PDiffusion: Genotype-to-Phenotype Prediction with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdi Liu, Zhangyang Gao, Hong Chang, Stan Z. Li, Shiguang Shan, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering the genotype-phenotype relationship is crucial for genetic
engineering, which will facilitate advances in fields such as crop breeding,
conservation biology, and personalized medicine. Current research usually
focuses on single species and small datasets due to limitations in phenotypic
data collection, especially for traits that require visual assessments or
physical measurements. Deciphering complex and composite phenotypes, such as
morphology, from genetic data at scale remains an open question. To break
through traditional generic models that rely on simplified assumptions, this
paper introduces G2PDiffusion, the first-of-its-kind diffusion model designed
for genotype-to-phenotype generation across multiple species. Specifically, we
use images to represent morphological phenotypes across species and redefine
phenotype prediction as conditional image generation. To this end, this paper
introduces an environment-enhanced DNA sequence conditioner and trains a stable
diffusion model with a novel alignment method to improve genotype-to-phenotype
consistency. Extensive experiments demonstrate that our approach enhances
phenotype prediction accuracy across species, capturing subtle genetic
variations that contribute to observable traits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble quantile-based deep learning framework for streamflow and flood
  prediction in Australian catchments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohitash Chandra, Arpit Kapoor, Siddharth Khedkar, Jim Ng, R. Willem Vervoort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, climate extremes such as floods have created significant
environmental and economic hazards for Australia. Deep learning methods have
been promising for predicting extreme climate events; however, large flooding
events present a critical challenge due to factors such as model calibration
and missing data. We present an ensemble quantile-based deep learning framework
that addresses large-scale streamflow forecasts using quantile regression for
uncertainty projections in prediction. We evaluate selected univariate and
multivariate deep learning models and catchment strategies. Furthermore, we
implement a multistep time-series prediction model using the CAMELS dataset for
selected catchments across Australia. The ensemble model employs a set of
quantile deep learning models for streamflow determined by historical
streamflow data. We utilise the streamflow prediction and obtain flood
probability using flood frequency analysis and compare it with historical
flooding events for selected catchments. Our results demonstrate notable
efficacy and uncertainties in streamflow forecasts with varied catchment
properties. Our flood probability estimates show good accuracy in capturing the
historical floods from the selected catchments. This underscores the potential
for our deep learning framework to revolutionise flood forecasting across
diverse regions and be implemented as an early warning system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Clustered Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19272v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19272v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saber Malekmohammadi, Afaf Taik, Golnoosh Farnadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL), which is a decentralized machine learning (ML)
approach, often incorporates differential privacy (DP) to provide rigorous data
privacy guarantees. Previous works attempted to address high structured data
heterogeneity in vanilla FL settings through clustering clients (a.k.a
clustered FL), but these methods remain sensitive and prone to errors, further
exacerbated by the DP noise. This vulnerability makes the previous methods
inappropriate for differentially private FL (DPFL) settings with structured
data heterogeneity. To address this gap, we propose an algorithm for
differentially private clustered FL, which is robust to the DP noise in the
system and identifies the underlying clients' clusters correctly. To this end,
we propose to cluster clients based on both their model updates and training
loss values. Furthermore, for clustering clients' model updates at the end of
the first round, our proposed approach addresses the server's uncertainties by
employing large batch sizes as well as Gaussian Mixture Models (GMM) to reduce
the impact of DP and stochastic noise and avoid potential clustering errors.
This idea is efficient especially in privacy-sensitive scenarios with more DP
noise. We provide theoretical analysis to justify our approach and evaluate it
across diverse data distributions and privacy budgets. Our experimental results
show its effectiveness in addressing large structured data heterogeneity in
DPFL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STP: Self-play LLM Theorem Provers with Iterative Conjecturing and
  Proving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00212v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00212v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kefan Dong, Tengyu Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental challenge in formal theorem proving by LLMs is the lack of
high-quality training data. Although reinforcement learning or expert iteration
partially mitigates this issue by alternating between LLM generating proofs and
finetuning them on correctly generated ones, performance quickly plateaus due
to the scarcity of correct proofs (sparse rewards). To keep improving the
models with limited data, we draw inspiration from mathematicians, who
continuously develop new results, partly by proposing novel conjectures or
exercises (which are often variants of known results) and attempting to solve
them. We design the Self-play Theorem Prover (STP) that simultaneously takes on
two roles, conjecturer and prover, each providing training signals to the
other. The conjecturer is trained iteratively on previously generated
conjectures that are barely provable by the current prover, which incentivizes
it to generate increasingly challenging conjectures over time. The prover
attempts to prove the conjectures with standard expert iteration. We evaluate
STP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens
generated during the training in Lean, STP proves 26.3% of the statements in
the LeanWorkbook dataset, doubling the previous best result of 13.2% achieved
through expert iteration. The final model achieves state-of-the-art performance
among whole-proof generation methods on miniF2F-test (61.7%, pass@3200),
Proofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@3200).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are KANs Effective for Multivariate Time Series Forecasting? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Han, Xinfeng Zhang, Yiling Wu, Zhenduo Zhang, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series forecasting is a crucial task that predicts the
future states based on historical inputs. Related techniques have been
developing in parallel with the machine learning community, from early
statistical learning methods to current deep learning methods. Despite their
significant advancements, existing methods continue to struggle with the
challenge of inadequate interpretability. The rise of the Kolmogorov-Arnold
Network (KAN) provides a new perspective to solve this challenge, but current
work has not yet concluded whether KAN is effective in time series forecasting
tasks. In this paper, we aim to evaluate the effectiveness of KANs in
time-series forecasting from the perspectives of performance, integrability,
efficiency, and interpretability. To this end, we propose the Multi-layer
Mixture-of-KAN network (MMK), which achieves excellent performance while
retaining KAN's ability to be transformed into a combination of symbolic
functions. The core module of MMK is the mixture-of-KAN layer, which uses a
mixture-of-experts structure to assign variables to best-matched KAN experts.
Then, we explore some useful experimental strategies to deal with the issues in
the training stage. Finally, we compare MMK and various baselines on seven
datasets. Extensive experimental and visualization results demonstrate that
KANs are effective in multivariate time series forecasting. Code is available
at: https://github.com/2448845600/EasyTSF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeBridge: Non-Stationarity Matters for Long-term Time Series
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04442v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04442v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Liu, Beiliang Wu, Yifan Hu, Naiqi Li, Tao Dai, Jigang Bao, Shu-tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-stationarity poses significant challenges for multivariate time series
forecasting due to the inherent short-term fluctuations and long-term trends
that can lead to spurious regressions or obscure essential long-term
relationships. Most existing methods either eliminate or retain
non-stationarity without adequately addressing its distinct impacts on
short-term and long-term modeling. Eliminating non-stationarity is essential
for avoiding spurious regressions and capturing local dependencies in
short-term modeling, while preserving it is crucial for revealing long-term
cointegration across variates. In this paper, we propose TimeBridge, a novel
framework designed to bridge the gap between non-stationarity and dependency
modeling in long-term time series forecasting. By segmenting input series into
smaller patches, TimeBridge applies Integrated Attention to mitigate short-term
non-stationarity and capture stable dependencies within each variate, while
Cointegrated Attention preserves non-stationarity to model long-term
cointegration across variates. Extensive experiments show that TimeBridge
consistently achieves state-of-the-art performance in both short-term and
long-term forecasting. Additionally, TimeBridge demonstrates exceptional
performance in financial forecasting on the CSI 500 and S&P 500 indices,
further validating its robustness and effectiveness. Code is available at
https://github.com/Hank0626/TimeBridge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Diffusion for Large-Scale Parameter Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Wang, Dongwen Tang, Wangbo Zhao, Konstantin Schürholt, Zhangyang Wang, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter generation has long struggled to match the scale of today large
vision and language models, curbing its broader utility. In this paper, we
introduce Recurrent Diffusion for Large Scale Parameter Generation (RPG), a
novel framework that generates full neural network parameters up to hundreds of
millions on a single GPU. Our approach first partitions a networks parameters
into non-overlapping tokens, each corresponding to a distinct portion of the
model. A recurrent mechanism then learns the inter token relationships,
producing prototypes which serve as conditions for a diffusion process that
ultimately synthesizes the full parameters. Across a spectrum of architectures
and tasks including ResNets, ConvNeXts and ViTs on ImageNet 1K and COCO, and
even LoRA based LLMs RPG achieves performance on par with fully trained
networks while avoiding excessive memory overhead. Notably, it generalizes
beyond its training set to generate valid parameters for previously unseen
tasks, highlighting its flexibility in dynamic and open ended scenarios. By
overcoming the longstanding memory and scalability barriers, RPG serves as a
critical advance in AI generating AI, potentially enabling efficient weight
generation at scales previously deemed infeasible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Generating 200 million parameters in just minutes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Foresight to Forethought: VLM-In-the-Loop Policy Steering via
  Latent Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Wu, Ran Tian, Gokul Swamy, Andrea Bajcsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While generative robot policies have demonstrated significant potential in
learning complex, multimodal behaviors from demonstrations, they still exhibit
diverse failures at deployment-time. Policy steering offers an elegant solution
to reducing the chance of failure by using an external verifier to select from
low-level actions proposed by an imperfect generative policy. Here, one might
hope to use a Vision Language Model (VLM) as a verifier, leveraging its
open-world reasoning capabilities. However, off-the-shelf VLMs struggle to
understand the consequences of low-level robot actions as they are represented
fundamentally differently than the text and images the VLM was trained on. In
response, we propose FOREWARN, a novel framework to unlock the potential of
VLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is
to decouple the VLM's burden of predicting action outcomes (foresight) from
evaluation (forethought). For foresight, we leverage a latent world model to
imagine future latent states given diverse low-level action plans. For
forethought, we align the VLM with these predicted latent states to reason
about the consequences of actions in its native representation--natural
language--and effectively filter proposed plans. We validate our framework
across diverse robotic manipulation tasks, demonstrating its ability to bridge
representational gaps and provide robust, generalizable policy steering. Videos
can be found on the project website: https://yilin-wu98.github.io/forewarn/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VolleyBots: A Testbed for Multi-Drone Volleyball Game Combining <span class="highlight-title">Motion</span>
  <span class="highlight-title">Control</span> and Strategic Play 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01932v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01932v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelai Xu, Chao Yu, Ruize Zhang, Huining Yuan, Xiangmin Yi, Shilong Ji, Chuqi Wang, Wenhao Tang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent reinforcement learning (MARL) has made significant progress,
largely fueled by the development of specialized testbeds that enable
systematic evaluation of algorithms in controlled yet challenging scenarios.
However, existing testbeds often focus on purely virtual simulations or limited
robot morphologies such as robotic arms, quadrupeds, and humanoids, leaving
high-mobility platforms with real-world physical constraints like drones
underexplored. To bridge this gap, we present VolleyBots, a new MARL testbed
where multiple drones cooperate and compete in the sport of volleyball under
physical dynamics. VolleyBots features a turn-based interaction model under
volleyball rules, a hierarchical decision-making process that combines motion
control and strategic play, and a high-fidelity simulation for seamless
sim-to-real transfer. We provide a comprehensive suite of tasks ranging from
single-drone drills to multi-drone cooperative and competitive tasks,
accompanied by baseline evaluations of representative MARL and game-theoretic
algorithms. Results in simulation show that while existing algorithms handle
simple tasks effectively, they encounter difficulty in complex tasks that
require both low-level control and high-level strategy. We further demonstrate
zero-shot deployment of a simulation-learned policy to real-world drones,
highlighting VolleyBots' potential to propel MARL research involving agile
robotic platforms. The project page is at
https://sites.google.com/view/thu-volleybots/home.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of
  Multimodal Large Language Models <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have shown significant promise in
various applications, leading to broad interest from researchers and
practitioners alike. However, a comprehensive evaluation of their long-context
capabilities remains underexplored. To address these gaps, we introduce the
MultiModal Needle-in-a-haystack (MMNeedle) benchmark, specifically designed to
assess the long-context capabilities of MLLMs. Besides multi-image input, we
employ image stitching to further increase the input context length, and
develop a protocol to automatically generate labels for sub-image level
retrieval. Essentially, MMNeedle evaluates MLLMs by stress-testing their
capability to locate a target sub-image (needle) within a set of images
(haystack) based on textual instructions and descriptions of image contents.
This setup necessitates an advanced understanding of extensive visual contexts
and effective information retrieval within long-context image inputs. With this
benchmark, we evaluate state-of-the-art MLLMs, encompassing both API-based and
open-source models. The findings reveal that GPT-4o consistently surpasses
other models in long-context scenarios, but suffers from hallucination problems
in negative samples, i.e., when needles are not in the haystacks. Our
comprehensive long-context evaluation of MLLMs also sheds lights on the
considerable performance gap between API-based and open-source models. All the
code, data, and instructions required to reproduce the main results are
available at https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SymbolFit: Automatic Parametric Modeling with Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09851v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09851v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Fung Tsoi, Dylan Rankin, Cecile Caillol, Miles Cranmer, Sridhara Dasu, Javier Duarte, Philip Harris, Elliot Lipeles, Vladimir Loncar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SymbolFit, a framework that automates parametric modeling by
using symbolic regression to perform a machine-search for functions that fit
the data, while simultaneously providing uncertainty estimates in a single run.
Traditionally, constructing a parametric model to accurately describe binned
data has been a manual and iterative process, requiring an adequate functional
form to be determined before the fit can be performed. The main challenge
arises when the appropriate functional forms cannot be derived from first
principles, especially when there is no underlying true closed-form function
for the distribution. In this work, we address this problem by utilizing
symbolic regression, a machine learning technique that explores a vast space of
candidate functions without needing a predefined functional form, treating the
functional form itself as a trainable parameter. Our approach is demonstrated
in data analysis applications in high-energy physics experiments at the CERN
Large Hadron Collider (LHC). We demonstrate its effectiveness and efficiency
using five real proton-proton collision datasets from new physics searches at
the LHC, namely the background modeling in resonance searches for high-mass
dijet, trijet, paired-dijet, diphoton, and dimuon events. We also validate the
framework using several toy datasets with one and more variables.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>53 pages, 35 figures. Under review. The API can be used
  out-of-the-box and is available at https://github.com/hftsoi/symbolfit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Multi-Step Reasoning Abilities of Language Models through
  Direct Q-Function Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixuan Ji, Guanlin Liu, Ning Dai, Qingping Yang, Renjie Zheng, Zheng Wu, Chen Dun, Quanquan Gu, Lin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) plays a crucial role in aligning large language
models (LLMs) with human preferences and improving their ability to perform
complex tasks. However, current approaches either require significant
computational resources due to the use of multiple models and extensive online
sampling for training (e.g., PPO) or are framed as bandit problems (e.g., DPO,
DRO), which often struggle with multi-step reasoning tasks, such as math
problem solving and complex reasoning that involve long chains of thought. To
overcome these limitations, we introduce Direct Q-function Optimization (DQO),
which formulates the response generation process as a Markov Decision Process
(MDP) and utilizes the soft actor-critic (SAC) framework to optimize a
Q-function directly parameterized by the language model. The MDP formulation of
DQO offers structural advantages over bandit-based methods, enabling more
effective process supervision. Experimental results on two math problem-solving
datasets, GSM8K and MATH, demonstrate that DQO outperforms previous methods,
establishing it as a promising offline reinforcement learning approach for
aligning language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning for Scalable and Optimal Load Shedding Under Power
  System Contingency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Zhou, Hao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt and effective corrective actions in response to unexpected
contingencies are crucial for improving power system resilience and preventing
cascading blackouts. The optimal load shedding (OLS) accounting for network
limits has the potential to address the diverse system-wide impacts of
contingency scenarios as compared to traditional local schemes. However, due to
the fast cascading propagation of initial contingencies, real-time OLS
solutions are challenging to attain in large systems with high computation and
communication needs. In this paper, we propose a decentralized design that
leverages offline training of a neural network (NN) model for individual load
centers to autonomously construct the OLS solutions from locally available
measurements. Our learning-for-OLS approach can greatly reduce the computation
and communication needs during online emergency responses, thus preventing the
cascading propagation of contingencies for enhanced power grid resilience.
Numerical studies on both the IEEE 118-bus system and a synthetic Texas
2000-bus system have demonstrated the efficiency and effectiveness of our
scalable OLS learning design for timely power system emergency operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group & Reweight: A Novel Cost-Sensitive Approach to Mitigating Class
  Imbalance in Network Traffic Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19214v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19214v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wumei Du, Dong Liang, Yiqin Lv, Xingxing Liang, Guanlin Wu, Qi Wang, Zheng Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet services have led to the eruption of network traffic, and machine
learning on these Internet data has become an indispensable tool, especially
when the application is risk-sensitive. This paper focuses on network traffic
classification in the presence of severe class imbalance. Such a distributional
trait mostly drifts the optimal decision boundary and results in an
unsatisfactory solution. This raises safety concerns in the network traffic
field when previous class imbalance methods hardly deal with numerous minority
malicious classes. To alleviate these effects, we design a group & reweight
strategy for alleviating class imbalance. Inspired by the group
distributionally optimization framework, our approach heuristically clusters
classes into groups, iteratively updates the non-parametric weights for
separate classes, and optimizes the learning model by minimizing reweighted
losses. We theoretically interpret the optimization process from a Stackelberg
game and perform extensive experiments on typical benchmarks. Results show that
our approach can not only suppress the negative effect of class imbalance but
also improve the comprehensive performance in prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeDiT: General-pur<span class="highlight-title">pose</span> Diffusion Transformers for Time Series
  Foundation Model <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Defu Cao, Wen Ye, Yizhou Zhang, Yan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models, particularly Large Language Models (LLMs), have
revolutionized text and video processing, yet time series data presents
distinct challenges for such approaches due to domain-specific features such as
missing values, multi-resolution characteristics, etc. Furthermore, the
de-facto autoregressive transformers tend to learn deterministic temporal
dependencies within pre-trained data while overlooking inherent uncertainties
and lacking integration of physical constraints. In this paper, we introduce
TimeDiT, a diffusion transformer model that synergistically combines
transformer-based temporal dependency learning with diffusion-based
probabilistic sampling. TimeDiT employs a unified masking mechanism to
harmonize the training and inference process across diverse tasks while
introducing a theoretically grounded, finetuning-free model editing strategy
that enables flexible integration of external knowledge during sampling.
Acknowledging the challenges of unifying multiple downstream tasks under a
single model, our systematic evaluation demonstrates TimeDiT's effectiveness
both in fundamental tasks, i.e., forecasting and imputation, through
zero-shot/fine-tuning; and in domain tasks, i.e., multi-resolution forecasting,
anomaly detection, and data generation, establishing it as a
\textit{proto-foundation model} that bridges the gap between general-purpose
and domain-specific models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 Pages, 11 Figures, 22 Tables. First present at ICML 2024 Workshop
  on Foundation Models in the Wild</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAFR: Neuron Redistribution for Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruidi Chang, Chunyuan Deng, Hanjie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Superposition refers to encoding representations of multiple features within
a single neuron, which is common in deep neural networks. This property allows
neurons to combine and represent multiple features, enabling the model to
capture intricate information and handle complex tasks. Despite promising
performance, the model's interpretability has been diminished. This paper
presents a novel approach to enhance model interpretability by regularizing
feature superposition. We introduce SAFR, which simply applies regularizations
to the loss function to promote monosemantic representations for important
tokens while encouraging polysemanticity for correlated token pairs, where
important tokens and correlated token pairs are identified via VMASK and
attention weights respectively. We evaluate SAFR with a transformer model on
two classification tasks. Experiments demonstrate the effectiveness of SAFR in
improving model interpretability without compromising prediction performance.
Besides, SAFR provides explanations by visualizing the neuron allocation within
the intermediate layers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Symbolic Regression through Offline Reinforcement Learning:
  A Co-Design Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Tian, Wenqi Zhou, Michele Viscione, Hao Dong, David Kammer, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic Regression (SR) holds great potential for uncovering underlying
mathematical and physical relationships from observed data. However, the vast
combinatorial space of possible expressions poses significant challenges for
both online search methods and pre-trained transformer models. Additionally,
current state-of-the-art approaches typically do not consider the integration
of domain experts' prior knowledge and do not support iterative interactions
with the model during the equation discovery process. To address these
challenges, we propose the Symbolic Q-network (Sym-Q), an advanced interactive
framework for large-scale symbolic regression. Unlike previous large-scale
transformer-based SR approaches, Sym-Q leverages reinforcement learning without
relying on a transformer-based decoder. This formulation allows the agent to
learn through offline reinforcement learning using any type of tree encoder,
enabling more efficient training and inference. Furthermore, we propose a
co-design mechanism, where the reinforcement learning-based Sym-Q facilitates
effective interaction with domain experts at any stage of the equation
discovery process. Users can dynamically modify generated nodes of the
expression, collaborating with the agent to tailor the mathematical expression
to best fit the problem and align with the assumed physical laws, particularly
when there is prior partial knowledge of the expected behavior. Our experiments
demonstrate that the pre-trained Sym-Q surpasses existing SR algorithms on the
challenging SSDNC benchmark. Moreover, we experimentally show on real-world
cases that its performance can be further enhanced by the interactive co-design
mechanism, with Sym-Q achieving greater performance gains than other
state-of-the-art models. Our reproducible code is available at
https://github.com/EPFL-IMOS/Sym-Q.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work should not be a new submission but instead should be an
  update to my existing article, arXiv:2402.05306</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analytical Lyapunov Function Discovery: An RL-based Generative Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohan Zou, Jie Feng, Hao Zhao, Yuanyuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in learning-based methods, finding valid Lyapunov functions
for nonlinear dynamical systems remains challenging. Current neural network
approaches face two main issues: challenges in scalable verification and
limited interpretability. To address these, we propose an end-to-end framework
using transformers to construct analytical Lyapunov functions (local), which
simplifies formal verification, enhances interpretability, and provides
valuable insights for control engineers. Our framework consists of a
transformer-based trainer that generates candidate Lyapunov functions and a
falsifier that verifies candidate expressions and refines the model via
risk-seeking policy gradient. Unlike Alfarano et al. (2024), which utilizes
pre-training and seeks global Lyapunov functions for low-dimensional systems,
our model is trained from scratch via reinforcement learning (RL) and succeeds
in finding local Lyapunov functions for high-dimensional and non-polynomial
systems. Given the analytical nature of the candidates, we employ efficient
optimization methods for falsification during training and formal verification
tools for the final verification. We demonstrate the efficiency of our approach
on a range of nonlinear dynamical systems with up to ten dimensions and show
that it can discover Lyapunov functions not previously identified in the
control literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages (8+18), preprint for discussion. Haohan and Jie contribute
  equally</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RenderBox: Expressive Performance Rendering with Text <span class="highlight-title">Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Zhang, Akira Maezawa, Simon Dixon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressive music performance rendering involves interpreting symbolic scores
with variations in timing, dynamics, articulation, and instrument-specific
techniques, resulting in performances that capture musical can emotional
intent. We introduce RenderBox, a unified framework for text-and-score
controlled audio performance generation across multiple instruments, applying
coarse-level controls through natural language descriptions and granular-level
controls using music scores. Based on a diffusion transformer architecture and
cross-attention joint conditioning, we propose a curriculum-based paradigm that
trains from plain synthesis to expressive performance, gradually incorporating
controllable factors such as speed, mistakes, and style diversity.
  RenderBox achieves high performance compared to baseline models across key
metrics such as FAD and CLAP, and also tempo and pitch accuracy under different
prompting tasks. Subjective evaluation further demonstrates that RenderBox is
able to generate controllable expressive performances that sound natural and
musically engaging, aligning well with prompts and intent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoTextVQA: Towards Egocentric Scene-Text Aware <span class="highlight-title">Video</span> Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EgoTextVQA, a novel and rigorously constructed benchmark for
egocentric QA assistance involving scene text. EgoTextVQA contains 1.5K
ego-view videos and 7K scene-text aware questions that reflect real-user needs
in outdoor driving and indoor house-keeping activities. The questions are
designed to elicit identification and reasoning on scene text in an egocentric
and dynamic environment. With EgoTextVQA, we comprehensively evaluate 10
prominent multimodal large language models. Currently, all models struggle, and
the best results (Gemini 1.5 Pro) are around 33% accuracy, highlighting the
severe deficiency of these techniques in egocentric QA assistance. Our further
investigations suggest that precise temporal grounding and multi-frame
reasoning, along with high resolution and auxiliary scene-text inputs, are key
for better performance. With thorough analyses and heuristic suggestions, we
hope EgoTextVQA can serve as a solid testbed for research in egocentric
scene-text QA assistance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Lu, Yize Li, Yanzhi Wang, Wei Wang, Wei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image compression under ultra-low bitrates remains challenging for both
conventional learned image compression (LIC) and generative vector-quantized
(VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy
quantization, while generative VQ modeling gives poor fidelity due to the
mismatch between learned generative priors and specific inputs. In this work,
we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream
framework that utilizes both generative VQ-modeling and diffusion models, as
well as conventional LIC, to achieve both high fidelity and high perceptual
quality. Different from previous hybrid methods that directly use pre-trained
LIC models to generate low-quality fidelity-preserving information from heavily
quantized latent, we use diffusion models to extract high-quality complimentary
fidelity information from the ground-truth input, which can enhance the system
performance in several aspects: improving indices map prediction, enhancing the
fidelity-preserving output of the LIC stream, and refining conditioned image
reconstruction with VQ-latent correction. In addition, our diffusion model is
based on a dense representative vector (DRV), which is lightweight with very
simple sampling schedulers. Extensive experiments demonstrate that our
HDCompression outperforms the previous conventional LIC, generative
VQ-modeling, and hybrid frameworks in both quantitative metrics and qualitative
visualization, providing balanced robust compression performance at ultra-low
bitrates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Audio Codec Identification Using Overlapping LCS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farzane Jafari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio data are widely exchanged over telecommunications networks. Due to the
limitations of network resources, these data are typically compressed before
transmission. Various methods are available for compressing audio data. To
access such audio information, it is first necessary to identify the codec used
for compression. One of the most effective approaches for audio codec
identification involves analyzing the content of received packets. In these
methods, statistical features extracted from the packets are utilized to
determine the codec employed. This paper proposes a novel method for audio
codec classification based on features derived from the overlapped longest
common sub-string and sub-sequence (LCS). The simulation results, which
achieved an accuracy of 97% for 8 KB packets, demonstrate the superiority of
the proposed method over conventional approaches. This method divides each 8 KB
packet into fifteen 1 KB packets with a 50% overlap. The results indicate that
this division has no significant impact on the simulation outcomes, while
significantly speeding up the feature extraction, being eight times faster than
the traditional method for extracting LCS features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BioVL-QR: Egocentric Biochemical Vision-and-Language <span class="highlight-title">Dataset</span> Using Micro
  QR Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomohiro Nishimoto, Taichi Nishimura, Koki Yamamoto, Keisuke Shirai, Hirotaka Kameko, Yuto Haneji, Tomoya Yoshida, Keiya Kajimura, Taiyu Cui, Chihiro Nishiwaki, Eriko Daikoku, Natsuko Okuda, Fumihito Ono, Shinsuke Mori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces BioVL-QR, a biochemical vision-and-language dataset
comprising 23 egocentric experiment videos, corresponding protocols, and
vision-and-language alignments. A major challenge in understanding biochemical
videos is detecting equipment, reagents, and containers because of the
cluttered environment and indistinguishable objects. Previous studies assumed
manual object annotation, which is costly and time-consuming. To address the
issue, we focus on Micro QR Codes. However, detecting objects using only Micro
QR Codes is still difficult due to blur and occlusion caused by object
manipulation. To overcome this, we propose an object labeling method combining
a Micro QR Code detector with an off-the-shelf hand object detector. As an
application of the method and BioVL-QR, we tackled the task of localizing the
procedural steps in an instructional video. The experimental results show that
using Micro QR Codes and our method improves biochemical video understanding.
Data and code are available through https://nishi10mo.github.io/BioVL-QR/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-10T00:00:00Z">2025-02-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">186</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Learning for k-SAT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Galanis, Leslie Ann Goldberg, Xusheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consider a $k$-SAT formula $\Phi$ where every variable appears at most $d$
times, and let $\sigma$ be a satisfying assignment of $\Phi$ sampled
proportionally to $e^{\beta m(\sigma)}$ where $m(\sigma)$ is the number of
variables set to true and $\beta$ is a real parameter. Given $\Phi$ and
$\sigma$, can we learn the value of $\beta$ efficiently?
  This problem falls into a recent line of works about single-sample
("one-shot") learning of Markov random fields. The $k$-SAT setting we consider
here was recently studied by Galanis, Kandiros, and Kalavasis (SODA'24) where
they showed that single-sample learning is possible when roughly $d\leq
2^{k/6.45}$ and impossible when $d\geq (k+1) 2^{k-1}$. Crucially, for their
impossibility results they used the existence of unsatisfiable instances which,
aside from the gap in $d$, left open the question of whether the feasibility
threshold for one-shot learning is dictated by the satisfiability threshold of
$k$-SAT formulas of bounded degree.
  Our main contribution is to answer this question negatively. We show that
one-shot learning for $k$-SAT is infeasible well below the satisfiability
threshold; in fact, we obtain impossibility results for degrees $d$ as low as
$k^2$ when $\beta$ is sufficiently large, and bootstrap this to small values of
$\beta$ when $d$ scales exponentially with $k$, via a probabilistic
construction. On the positive side, we simplify the analysis of the learning
algorithm and obtain significantly stronger bounds on $d$ in terms of $\beta$.
In particular, for the uniform case $\beta\rightarrow 0$ that has been studied
extensively in the sampling literature, our analysis shows that learning is
possible under the condition $d\lesssim 2^{k/2}$. This is nearly optimal (up to
constant factors) in the sense that it is known that sampling a
uniformly-distributed satisfying assignment is NP-hard for $d\gtrsim 2^{k/2}$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unconstrained Body Recognition at Altitude and Range: Comparing Four
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blake A Myers, Matthew Q Hill, Veda Nandan Gandi, Thomas M Metz, Alice J O'Toole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents an investigation of four distinct approaches to long-term
person identification using body shape. Unlike short-term re-identification
systems that rely on temporary features (e.g., clothing), we focus on learning
persistent body shape characteristics that remain stable over time. We
introduce a body identification model based on a Vision Transformer (ViT) (Body
Identification from Diverse Datasets, BIDDS) and on a Swin-ViT model
(Swin-BIDDS). We also expand on previous approaches based on the Linguistic and
Non-linguistic Core ResNet Identity Models (LCRIM and NLCRIM), but with
improved training. All models are trained on a large and diverse dataset of
over 1.9 million images of approximately 5k identities across 9 databases.
Performance was evaluated on standard re-identification benchmark datasets
(MARS, MSMT17, Outdoor Gait, DeepChange) and on an unconstrained dataset that
includes images at a distance (from close-range to 1000m), at altitude (from an
unmanned aerial vehicle, UAV), and with clothing change. A comparative analysis
across these models provides insights into how different backbone architectures
and input image sizes impact long-term body identification performance across
real-world conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier-enhanced Neural Networks For Systems Biology Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enze Xu, Minghan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of systems biology, differential equations are commonly used to
model biological systems, but solving them for large-scale and complex systems
can be computationally expensive. Recently, the integration of machine learning
and mathematical modeling has offered new opportunities for scientific
discoveries in biology and health. The emerging physics-informed neural network
(PINN) has been proposed as a solution to this problem. However, PINN can be
computationally expensive and unreliable for complex biological systems. To
address these issues, we propose the Fourier-enhanced Neural Networks for
systems biology (SB-FNN). SB-FNN uses an embedded Fourier neural network with
an adaptive activation function and a cyclic penalty function to optimize the
prediction of biological dynamics, particularly for biological systems that
exhibit oscillatory patterns. Experimental results demonstrate that SB-FNN
achieves better performance and is more efficient than PINN for handling
complex biological models. Experimental results on cellular and population
models demonstrate that SB-FNN outperforms PINN in both accuracy and
efficiency, making it a promising alternative approach for handling complex
biological models. The proposed method achieved better performance on six
biological models and is expected to replace PINN as the most advanced method
in systems biology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFE: Self-Supervised Anomaly Detection Framework for Intrusion
  Detection <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvin Li, Zhengli Shang, Onat Gungor, Tajana Rosing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of IoT devices has significantly increased network
vulnerabilities, creating an urgent need for effective Intrusion Detection
Systems (IDS). Machine Learning-based IDS (ML-IDS) offer advanced detection
capabilities but rely on labeled attack data, which limits their ability to
identify unknown threats. Self-Supervised Learning (SSL) presents a promising
solution by using only normal data to detect patterns and anomalies. This paper
introduces SAFE, a novel framework that transforms tabular network intrusion
data into an image-like format, enabling Masked Autoencoders (MAEs) to learn
robust representations of network behavior. The features extracted by the MAEs
are then incorporated into a lightweight novelty detector, enhancing the
effectiveness of anomaly detection. Experimental results demonstrate that SAFE
outperforms the state-of-the-art anomaly detection method, Scale Learning-based
Deep Anomaly Detection method (SLAD), by up to 26.2% and surpasses the
state-of-the-art SSL-based network intrusion detection approach, Anomal-E, by
up to 23.5% in F1-score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the AAAI-25 Workshop on Artificial Intelligence for Cyber
  Security (AICS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Choroidal image analysis for OCT image sequences with applications in
  systemic health 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Burke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The choroid, a highly vascular layer behind the retina, is an extension of
the central nervous system and has parallels with the renal cortex, with blood
flow far exceeding that of the brain and kidney. Thus, there has been growing
interest of choroidal blood flow reflecting physiological status of systemic
disease. Optical coherence tomography (OCT) enables high-resolution imaging of
the choroid, but conventional analysis methods remain manual or semi-automatic,
limiting reproducibility, standardisation and clinical utility. In this thesis,
I develop several new methods to analyse the choroid in OCT image sequences,
with each successive method improving on its predecessors. I first develop two
semi-automatic approaches for choroid region (Gaussian Process Edge Tracing,
GPET) and vessel (Multi-scale Median Cut Quantisation, MMCQ) analysis, which
improve on manual approaches but remain user-dependent. To address this, I
introduce DeepGPET, a deep learning-based region segmentation method which
improves on execution time, reproducibility, and end-user accessibility, but
lacks choroid vessel analysis and automatic feature measurement. Improving on
this, I developed Choroidalyzer, a deep learning-based pipeline to segment the
choroidal space and vessels and generate fully automatic, clinically meaningful
and reproducible choroidal features. I provide rigorous evaluation of these
four approaches and consider their potential clinical value in three
applications into systemic health: OCTANE, assessing choroidal changes in renal
transplant recipients and donors; PREVENT, exploring choroidal associations
with Alzheimer's risk factors at mid-life; D-RISCii, assessing choroidal
variation and feasibility of OCT in critical care. In short, this thesis
contributes many open-source tools for standardised choroidal measurement and
highlights the choroid's potential as a biomarker in systemic health.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis toward a doctorate degree at the University of Edinburgh.
  PhD funded by the Medical Research Council (grant MR/N013166/1). Reviewed and
  examined by Dr. Roly Megaw (internal) and Prof. Pearse Keane (external) in
  December 2024 and ratified in the same month by the university. Official
  record found here: https://era.ed.ac.uk/handle/1842/42956</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Covariance Matrix Estimation in Sketched Newton Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Kuang, Mihai Anitescu, Sen Na
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the ubiquity of streaming data, online algorithms have been widely used
for parameter estimation, with second-order methods particularly standing out
for their efficiency and robustness. In this paper, we study an online sketched
Newton method that leverages a randomized sketching technique to perform an
approximate Newton step in each iteration, thereby eliminating the
computational bottleneck of second-order methods. While existing studies have
established the asymptotic normality of sketched Newton methods, a consistent
estimator of the limiting covariance matrix remains an open problem. We propose
a fully online covariance matrix estimator that is constructed entirely from
the Newton iterates and requires no matrix factorization. Compared to
covariance estimators for first-order online methods, our estimator for
second-order methods is batch-free. We establish the consistency and
convergence rate of our estimator, and coupled with asymptotic normality
results, we can then perform online statistical inference for the model
parameters based on sketched Newton methods. We also discuss the extension of
our estimator to constrained problems, and demonstrate its superior performance
on regression problems as well as benchmark problems in the CUTEst set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 2 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Likelihood-Free Estimation for Spatiotemporal Hawkes processes with
  missing data and application to predictive policing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pramit Das, Moulinath Banerjee, Yuekai Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing use of AI technology, many police departments use
forecasting software to predict probable crime hotspots and allocate patrolling
resources effectively for crime prevention. The clustered nature of crime data
makes self-exciting Hawkes processes a popular modeling choice. However, one
significant challenge in fitting such models is the inherent missingness in
crime data due to non-reporting, which can bias the estimated parameters of the
predictive model, leading to inaccurate downstream hotspot forecasts, often
resulting in over or under-policing in various communities, especially the
vulnerable ones. Our work introduces a Wasserstein Generative Adversarial
Networks (WGAN) driven likelihood-free approach to account for unreported
crimes in Spatiotemporal Hawkes models. We demonstrate through empirical
analysis how this methodology improves the accuracy of parametric estimation in
the presence of data missingness, leading to more reliable and efficient
policing strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Game of Coding With an Unknown Adversary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanzaleh Akbarinodehi, Parsa Moradi, Mohammad Ali Maddah-Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by emerging decentralized applications, the \emph{game of coding}
framework has been recently introduced to address scenarios where the
adversary's control over coded symbols surpasses the fundamental limits of
traditional coding theory. Still, the reward mechanism available in
decentralized systems, motivates the adversary to act rationally. While the
decoder, as the data collector (DC), has an acceptance and rejection mechanism,
followed by an estimation module, the adversary aims to maximize its utility,
as an increasing function of (1) the chance of acceptance (to increase the
reward), and (2) estimation error. On the other hand, the decoder also adjusts
its acceptance rule to maximize its own utility, as (1) an increasing function
of the chance of acceptance (to keep the system functional), (2) decreasing
function of the estimation error. Prior works within this framework rely on the
assumption that the game is complete, that is, both the DC and the adversary
are fully aware of each other's utility functions. However, in practice, the
decoder is often unaware of the utility of the adversary. To address this
limitation, we develop an algorithm enabling the DC to commit to a strategy
that achieves within the vicinity of the equilibrium, without knowledge of the
adversary's utility function. Our approach builds on an observation that at the
equilibrium, the relationship between the probability of acceptance and the
mean squared error (MSE) follows a predetermined curve independent of the
specific utility functions of the players. By exploiting this invariant
relationship, the DC can iteratively refine its strategy based on observable
parameters, converging to a near-optimal solution. We provide theoretical
guarantees on sample complexity and accuracy of the proposed scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Distribution Prediction: A Unified Approach to Multimodal
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Tian, Xiaotong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction with multimodal data-encompassing tabular, textual, and
visual inputs or outputs-is fundamental to advancing analytics in diverse
application domains. Traditional approaches often struggle to integrate
heterogeneous data types while maintaining high predictive accuracy. We
introduce Generative Distribution Prediction (GDP), a novel framework that
leverages multimodal synthetic data generation-such as conditional diffusion
models-to enhance predictive performance across structured and unstructured
modalities. GDP is model-agnostic, compatible with any high-fidelity generative
model, and supports transfer learning for domain adaptation. We establish a
rigorous theoretical foundation for GDP, providing statistical guarantees on
its predictive accuracy when using diffusion models as the generative backbone.
By estimating the data-generating distribution and adapting to various loss
functions for risk minimization, GDP enables accurate point predictions across
multimodal settings. We empirically validate GDP on four supervised learning
tasks-tabular data prediction, question answering, image captioning, and
adaptive quantile regression-demonstrating its versatility and effectiveness
across diverse domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Systematic Reasoning Abilities of Large Language Models
  through Graph Coloring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Heyman, Joel Zylberberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary large language models are powerful problem-solving tools, but
they exhibit weaknesses in their reasoning abilities which ongoing research
seeks to mitigate. We investigate graph coloring as a means of evaluating an
LLM's capacities for systematic step-by-step reasoning and possibility space
exploration, as well as effects of semantic problem framing. We test Claude 3.5
Sonnet, Llama 3.1 405B, Gemini 1.5 Pro, GPT-4o, o1-mini, and DeepSeek-R1 on a
dataset of $k$-coloring problems with $2 \leq k \leq 4$ and vertex count $4
\leq n \leq 8$, using partial algorithmic solvers to further categorize
problems by difficulty. In addition to substantial but varying framing effects,
we find that all models except o1-mini and R1 exhibit $>60\%$ error rates on
difficult problem types in all frames ($>15\%$ for o1-mini and $>10\%$ for R1),
and no model achieves perfect accuracy even in the simple domain of 2-coloring
4-vertex graphs. Our results highlight both the considerable recent progress in
LLM systematic reasoning and the limits of its reliability, especially in
relation to increasing computational costs. We expect that more complex graph
coloring problems, and procedural generation of arbitrary-complexity reasoning
problems more broadly, offer further untapped potential for LLM benchmarking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages (8 excluding references and appendices); 8 figures (3
  excluding appendices)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Clustering of Categorical Big Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bipana Thapaliya, Yu Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The K-Modes algorithm, developed for clustering categorical data, is of high
algorithmic simplicity but suffers from unreliable performances in clustering
quality and clustering efficiency, both heavily influenced by the choice of
initial cluster centers. In this paper, we investigate Bisecting K-Modes
(BK-Modes), a successive bisecting process to find clusters, in examining how
good the cluster centers out of the bisecting process will be when used as
initial centers for the K-Modes. The BK-Modes works by splitting a dataset into
multiple clusters iteratively with one cluster being chosen and bisected into
two clusters in each iteration. We use the sum of distances of data to their
cluster centers as the selection metric to choose a cluster to be bisected in
each iteration. This iterative process stops when K clusters are produced. The
centers of these K clusters are then used as the initial cluster centers for
the K-Modes. Experimental studies of the BK-Modes were carried out and were
compared against the K-Modes with multiple sets of initial cluster centers as
well as the best of the existing methods we found so far in our survey.
Experimental results indicated good performances of BK-Modes both in the
clustering quality and efficiency for large datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Thompson Sampling via Generation of Missing Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly W. Zhang, Tiffany Tianhui Cai, Hongseok Namkoong, Daniel Russo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a framework for Thompson sampling contextual bandit algorithms,
in which the algorithm's ability to quantify uncertainty and make decisions
depends on the quality of a generative model that is learned offline. Instead
of viewing uncertainty in the environment as arising from unobservable latent
parameters, our algorithm treats uncertainty as stemming from missing, but
potentially observable, future outcomes. If these future outcomes were all
observed, one could simply make decisions using an "oracle" policy fit on the
complete dataset. Inspired by this conceptualization, at each decision-time,
our algorithm uses a generative model to probabilistically impute missing
future outcomes, fits a policy using the imputed complete dataset, and uses
that policy to select the next action. We formally show that this algorithm is
a generative formulation of Thompson Sampling and prove a state-of-the-art
regret bound for it. Notably, our regret bound i) depends on the probabilistic
generative model only through the quality of its offline prediction loss, and
ii) applies to any method of fitting the "oracle" policy, which easily allows
one to adapt Thompson sampling to decision-making settings with fairness and/or
resource constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Continual Learning: Concepts, Challenges, and Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parisa Hamedi, Roozbeh Razavi-Far, Ehsan Hallaji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Continual Learning (FCL) has emerged as a robust solution for
collaborative model training in dynamic environments, where data samples are
continuously generated and distributed across multiple devices. This survey
provides a comprehensive review of FCL, focusing on key challenges such as
heterogeneity, model stability, communication overhead, and privacy
preservation. We explore various forms of heterogeneity and their impact on
model performance. Solutions to non-IID data, resource-constrained platforms,
and personalized learning are reviewed in an effort to show the complexities of
handling heterogeneous data distributions. Next, we review techniques for
ensuring model stability and avoiding catastrophic forgetting, which are
critical in non-stationary environments. Privacy-preserving techniques are
another aspect of FCL that have been reviewed in this work. This survey has
integrated insights from federated learning and continual learning to present
strategies for improving the efficacy and scalability of FCL systems, making it
applicable to a wide range of real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Deep Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amy Yu, Erik Lebedev, Lincoln Everett, Xiaoxin Chen, Terry Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical brief introduces Deep Agent, an advanced autonomous AI system
designed to manage complex multi-phase tasks through a novel hierarchical task
management architecture. The system's foundation is built on our Hierarchical
Task DAG (HTDAG) framework, which dynamically decomposes high-level objectives
into manageable sub-tasks while rigorously maintaining dependencies and
execution coherence. Deep Agent advances beyond traditional agent systems
through three key innovations: First, it implements a recursive two-stage
planner-executor architecture that enables continuous task refinement and
adaptation as circumstances change. Second, it features an Autonomous API &
Tool Creation (AATC) system that automatically generates reusable components
from UI interactions, substantially reducing operational costs for similar
tasks. Third, it incorporates Prompt Tweaking Engine and Autonomous Prompt
Feedback Learning components that optimize Large Language Model prompts for
specific scenarios, enhancing both inference accuracy and operational
stability. These components are integrated to form a service infrastructure
that manages user contexts, handles complex task dependencies, and orchestrates
end-to-end agentic workflow execution. Through this sophisticated architecture,
Deep Agent establishes a novel paradigm in self-governing AI systems,
demonstrating robust capability to independently handle intricate, multi-step
tasks while maintaining consistent efficiency and reliability through
continuous self-optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SnipGen: A Mining Repository Framework for Evaluating LLMs for Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Rodriguez-Cardenas, Alejandro Velasco, Denys Poshyvany
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LLMs), such as transformer-based neural networks trained on
billions of parameters, have become increasingly prevalent in software
engineering (SE). These models, trained on extensive datasets that include code
repositories, exhibit remarkable capabilities for SE tasks. However, evaluating
their effectiveness poses significant challenges, primarily due to the
potential overlap between the datasets used for training and those employed for
evaluation. To address this issue, we introduce SnipGen, a comprehensive
repository mining framework designed to leverage prompt engineering across
various downstream tasks for code generation. SnipGen aims to mitigate data
contamination by generating robust testbeds and crafting tailored data points
to assist researchers and practitioners in evaluating LLMs for code-related
tasks. In our exploratory study, SnipGen mined approximately 227K data points
from 338K recent code changes in GitHub commits, focusing on method-level
granularity. SnipGen features a collection of prompt templates that can be
combined to create a Chain-of-Thought-like sequence of prompts, enabling a
nuanced assessment of LLMs' code generation quality. By providing the mining
tool, the methodology, and the dataset, SnipGen empowers researchers and
practitioners to rigorously evaluate and interpret LLMs' performance in
software engineering contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting of Classification Models with <span class="highlight-title">Human</span>-in-the-Loop Computational
  Visual Knowledge Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Williams, Boris Kovalerchuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-risk artificial intelligence and machine learning classification tasks,
such as healthcare diagnosis, require accurate and interpretable prediction
models. However, classifier algorithms typically sacrifice individual
case-accuracy for overall model accuracy, limiting analysis of class overlap
areas regardless of task significance. The Adaptive Boosting meta-algorithm,
which won the 2003 G\"odel Prize, analytically assigns higher weights to
misclassified cases to reclassify. However, it relies on weaker base
classifiers that are iteratively strengthened, limiting improvements from base
classifiers. Combining visual and computational approaches enables selecting
stronger base classifiers before boosting. This paper proposes moving boosting
methodology from focusing on only misclassified cases to all cases in the class
overlap areas using Computational and Interactive Visual Learning (CIVL) with a
Human-in-the-Loop. It builds classifiers in lossless visualizations integrating
human domain expertise and visual insights. A Divide and Classify process
splits cases to simple and complex, classifying these individually through
computational analysis and data visualization with lossless visualization
spaces of Parallel Coordinates or other General Line Coordinates. After finding
pure and overlap class areas simple cases in pure areas are classified,
generating interpretable sub-models like decision rules in Propositional and
First-order Logics. Only multidimensional cases in the overlap areas are
losslessly visualized simplifying end-user cognitive tasks to identify
difficult case patterns, including engineering features to form new
classifiable patterns. Demonstration shows a perfectly accurate and losslessly
interpretable model of the Iris dataset, and simulated data shows generalized
benefits to accuracy and interpretability of models, increasing end-user
confidence in discovered models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Consistency Analysis of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Patwardhan, Vivek Vaidya, Ashish Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI (Gen AI) with large language models (LLMs) are being widely
adopted across the industry, academia and government. Cybersecurity is one of
the key sectors where LLMs can be and/or are already being used. There are a
number of problems that inhibit the adoption of trustworthy Gen AI and LLMs in
cybersecurity and such other critical areas. One of the key challenge to the
trustworthiness and reliability of LLMs is: how consistent an LLM is in its
responses?
  In this paper, we have analyzed and developed a formal definition of
consistency of responses of LLMs. We have formally defined what is consistency
of responses and then develop a framework for consistency evaluation. The paper
proposes two approaches to validate consistency: self-validation, and
validation across multiple LLMs. We have carried out extensive experiments for
several LLMs such as GPT4oMini, GPT3.5, Gemini, Cohere, and Llama3, on a
security benchmark consisting of several cybersecurity questions: informational
and situational. Our experiments corroborate the fact that even though these
LLMs are being considered and/or already being used for several cybersecurity
tasks today, they are often inconsistent in their responses, and thus are
untrustworthy and unreliable for cybersecurity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 12 figures, 3 tables, 3 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PrismAvatar: Real-time animated 3D neural head avatars on edge devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashant Raina, Felix Taubner, Mathieu Tuli, Eu Wern Teh, Kevin Ferreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PrismAvatar: a 3D head avatar model which is designed specifically
to enable real-time animation and rendering on resource-constrained edge
devices, while still enjoying the benefits of neural volumetric rendering at
training time. By integrating a rigged prism lattice with a 3D morphable head
model, we use a hybrid rendering model to simultaneously reconstruct a
mesh-based head and a deformable NeRF model for regions not represented by the
3DMM. We then distill the deformable NeRF into a rigged mesh and neural
textures, which can be animated and rendered efficiently within the constraints
of the traditional triangle rendering pipeline. In addition to running at 60
fps with low memory usage on mobile devices, we find that our trained models
have comparable quality to state-of-the-art 3D avatar models on desktop
devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Allophony in Self-Supervised Speech Models for Atypical
  Pronunciation Assessment <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwanghee Choi, Eunjung Yeo, Kalvin Chang, Shinji Watanabe, David Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Allophony refers to the variation in the phonetic realization of a phoneme
based on its phonetic environment. Modeling allophones is crucial for atypical
pronunciation assessment, which involves distinguishing atypical from typical
pronunciations. However, recent phoneme classifier-based approaches often
simplify this by treating various realizations as a single phoneme, bypassing
the complexity of modeling allophonic variation. Motivated by the acoustic
modeling capabilities of frozen self-supervised speech model (S3M) features, we
propose MixGoP, a novel approach that leverages Gaussian mixture models to
model phoneme distributions with multiple subclusters. Our experiments show
that MixGoP achieves state-of-the-art performance across four out of five
datasets, including dysarthric and non-native speech. Our analysis further
suggests that S3M features capture allophonic variation more effectively than
MFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP
with S3M features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025. Codebase available at
  https://github.com/juice500ml/acoustic-units-for-ood</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning for Everyone: Simplifying Healthcare Analytics with
  BigQuery ML 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Amir Salari, Bahareh Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) is transforming healthcare by enabling predictive
analytics, personalized treatments, and improved patient outcomes. However,
traditional ML workflows require specialized skills, infrastructure, and
resources, limiting accessibility for many healthcare professionals. This paper
explores how Google Cloud's BigQuery ML simplifies the development and
deployment of ML models using SQL, reducing technical barriers. Through a case
study on diabetes prediction using the Diabetes Health Indicators Dataset, we
evaluate three predictive models: Logistic Regression, Boosted Tree, and Deep
Neural Network (DNN). Our results demonstrate that the Boosted Tree model
achieves the highest performance, making it highly effective for diabetes
prediction. This study highlights BigQuery ML's role in democratizing machine
learning by providing a scalable, efficient, and accessible solution for
healthcare analytics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Focus: Artificial Intelligence, Healthcare analytics, cloud
  computing, BigQuery ML</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Neurodegenerative Diseases using Frame-Level Handwriting
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Laouedj, Yuzhe Wang, Jesus Villalba, Thomas Thebaud, Laureano Moro-Velazquez, Najim Dehak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we explored the use of spectrograms to represent handwriting
signals for assessing neurodegenerative diseases, including 42 healthy controls
(CTL), 35 subjects with Parkinson's Disease (PD), 21 with Alzheimer's Disease
(AD), and 15 with Parkinson's Disease Mimics (PDM). We applied CNN and
CNN-BLSTM models for binary classification using both multi-channel fixed-size
and frame-based spectrograms. Our results showed that handwriting tasks and
spectrogram channel combinations significantly impacted classification
performance. The highest F1-score (89.8%) was achieved for AD vs. CTL, while PD
vs. CTL reached 74.5%, and PD vs. PDM scored 77.97%. CNN consistently
outperformed CNN-BLSTM. Different sliding window lengths were tested for
constructing frame-based spectrograms. A 1-second window worked best for AD,
longer windows improved PD classification, and window length had little effect
on PD vs. PDM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIMS.au: A <span class="highlight-title">Dataset</span> for the Analysis of Modern Slavery Countermeasures in
  Corporate Statements <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adriana Eufrosiana Bora, Pierre-Luc St-Charles, Mirko Bronzi, Arsène Fansi Tchango, Bruno Rousseau, Kerrie Mengersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite over a decade of legislative efforts to address modern slavery in the
supply chains of large corporations, the effectiveness of government oversight
remains hampered by the challenge of scrutinizing thousands of statements
annually. While Large Language Models (LLMs) can be considered a well
established solution for the automatic analysis and summarization of documents,
recognizing concrete modern slavery countermeasures taken by companies and
differentiating those from vague claims remains a challenging task. To help
evaluate and fine-tune LLMs for the assessment of corporate statements, we
introduce a dataset composed of 5,731 modern slavery statements taken from the
Australian Modern Slavery Register and annotated at the sentence level. This
paper details the construction steps for the dataset that include the careful
design of annotation specifications, the selection and preprocessing of
statements, and the creation of high-quality annotation subsets for effective
model evaluations. To demonstrate our dataset's utility, we propose a machine
learning methodology for the detection of sentences relevant to mandatory
reporting requirements set by the Australian Modern Slavery Act. We then follow
this methodology to benchmark modern language models under zero-shot and
supervised learning settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready. ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Sinkhorn 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Kulcsar, Vyacheslav Kungurtsev, Georgios Korpas, Giulio Giaconi, William Shoosmith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we investigate the potential of solving the discrete Optimal
Transport (OT) problem with entropy regularization in a federated learning
setting. Recall that the celebrated Sinkhorn algorithm transforms the classical
OT linear program into strongly convex constrained optimization, facilitating
first order methods for otherwise intractably large problems. A common
contemporary setting that remains an open problem as far as the application of
Sinkhorn is the presence of data spread across clients with distributed
inter-communication, either due to clients whose privacy is a concern, or
simply by necessity of processing and memory hardware limitations. In this work
we investigate various natural procedures, which we refer to as Federated
Sinkhorn, that handle distributed environments where data is partitioned across
multiple clients. We formulate the problem as minimizing the transport cost
with an entropy regularization term, subject to marginal constraints, where
block components of the source and target distribution vectors are locally
known to clients corresponding to each block. We consider both synchronous and
asynchronous variants as well as all-to-all and server-client communication
topology protocols. Each procedure allows clients to compute local operations
on their data partition while periodically exchanging information with others.
We provide theoretical guarantees on convergence for the different variants
under different possible conditions. We empirically demonstrate the algorithms
performance on synthetic datasets and a real-world financial risk assessment
application. The investigation highlights the subtle tradeoffs associated with
computation and communication time in different settings and how they depend on
problem size and sparsity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference Alignment on Diffusion Model: A Comprehensive <span class="highlight-title">Survey</span> for
  Image Generation and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihao Wu, Xiaonan Si, Chi Xing, Jianhong Wang, Gaojie Jin, Guangliang Cheng, Lijun Zhang, Xiaowei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of preference alignment with diffusion models (DMs) has
emerged as a transformative approach to enhance image generation and editing
capabilities. Although integrating diffusion models with preference alignment
strategies poses significant challenges for novices at this intersection,
comprehensive and systematic reviews of this subject are still notably lacking.
To bridge this gap, this paper extensively surveys preference alignment with
diffusion models in image generation and editing. First, we systematically
review cutting-edge optimization techniques such as reinforcement learning with
human feedback (RLHF), direct preference optimization (DPO), and others,
highlighting their pivotal role in aligning preferences with DMs. Then, we
thoroughly explore the applications of aligning preferences with DMs in
autonomous driving, medical imaging, robotics, and more. Finally, we
comprehensively discuss the challenges of preference alignment with DMs. To our
knowledge, this is the first survey centered on preference alignment with DMs,
providing insights to drive future innovation in this dynamic area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence Intervals for Evaluation of Data Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yuan, Wenxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In data mining, when binary prediction rules are used to predict a binary
outcome, many performance measures are used in a vast array of literature for
the purposes of evaluation and comparison. Some examples include classification
accuracy, precision, recall, F measures, and Jaccard index. Typically, these
performance measures are only approximately estimated from a finite dataset,
which may lead to findings that are not statistically significant. In order to
properly quantify such statistical uncertainty, it is important to provide
confidence intervals associated with these estimated performance measures. We
consider statistical inference about general performance measures used in data
mining, with both individual and joint confidence intervals. These confidence
intervals are based on asymptotic normal approximations and can be computed
fast, without needs to do bootstrap resampling. We study the finite sample
coverage probabilities for these confidence intervals and also propose a
`blurring correction' on the variance to improve the finite sample performance.
This 'blurring correction' generalizes the plus-four method from binomial
proportion to general performance measures used in data mining. Our framework
allows multiple performance measures of multiple classification rules to be
inferred simultaneously for comparisons.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DROP: Poison Dilution via Knowledge Distillation for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Syros, Anshuman Suri, Farinaz Koushanfar, Cristina Nita-Rotaru, Alina Oprea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning is vulnerable to adversarial manipulation, where malicious
clients can inject poisoned updates to influence the global model's behavior.
While existing defense mechanisms have made notable progress, they fail to
protect against adversaries that aim to induce targeted backdoors under
different learning and attack configurations. To address this limitation, we
introduce DROP (Distillation-based Reduction Of Poisoning), a novel defense
mechanism that combines clustering and activity-tracking techniques with
extraction of benign behavior from clients via knowledge distillation to tackle
stealthy adversaries that manipulate low data poisoning rates and diverse
malicious client ratios within the federation. Through extensive
experimentation, our approach demonstrates superior robustness compared to
existing defenses across a wide range of learning configurations. Finally, we
evaluate existing defenses and our method under the challenging setting of
non-IID client data distribution and highlight the challenges of designing a
resilient FL defense in this setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Language Models are RNNs: Balancing Parallelization and
  Expressivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Schöne, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, Jannes Gladrow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-space models (SSMs) and transformers dominate the language modeling
landscape. However, they are constrained to a lower computational complexity
than classical recurrent neural networks (RNNs), limiting their expressivity.
In contrast, RNNs lack parallelization during training, raising fundamental
questions about the trade off between parallelization and expressivity. We
propose implicit SSMs, which iterate a transformation until convergence to a
fixed point. Theoretically, we show that implicit SSMs implement the non-linear
state-transitions of RNNs. Empirically, we find that only approximate
fixed-point convergence suffices, enabling the design of a scalable training
curriculum that largely retains parallelization, with full convergence required
only for a small subset of tokens. Our approach demonstrates superior
state-tracking capabilities on regular languages, surpassing transformers and
SSMs. We further scale implicit SSMs to natural language reasoning tasks and
pretraining of large-scale language models up to 1.3B parameters on 207B tokens
- representing, to our knowledge, the largest implicit model trained to date.
Notably, our implicit models outperform their explicit counterparts on standard
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Image to <span class="highlight-title">Video</span>: An Empirical Study of Diffusion Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Vélez, Luisa F. Polanía, Yi Yang, Chuhan Zhang, Rishab Kabra, Anurag Arnab, Mehdi S. M. Sajjadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have revolutionized generative modeling, enabling
unprecedented realism in image and video synthesis. This success has sparked
interest in leveraging their representations for visual understanding tasks.
While recent works have explored this potential for image generation, the
visual understanding capabilities of video diffusion models remain largely
uncharted. To address this gap, we systematically compare the same model
architecture trained for video versus image generation, analyzing the
performance of their latent representations on various downstream tasks
including image classification, action recognition, depth estimation, and
tracking. Results show that video diffusion models consistently outperform
their image counterparts, though we find a striking range in the extent of this
superiority. We further analyze features extracted from different layers and
with varying noise levels, as well as the effect of model size and training
budget on representation and generation quality. This work marks the first
direct comparison of video and image diffusion objectives for visual
understanding, offering insights into the role of temporal information in
representation learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Outsourced diffusion sampling: Efficient posterior inference in latent
  spaces of generative models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddarth Venkatraman, Mohsin Hasan, Minsu Kim, Luca Scimeca, Marcin Sendera, <span class="highlight-author">Yoshua Bengio</span>, Glen Berseth, Nikolay Malkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Any well-behaved generative model over a variable $\mathbf{x}$ can be
expressed as a deterministic transformation of an exogenous ('outsourced')
Gaussian noise variable $\mathbf{z}$: $\mathbf{x}=f_\theta(\mathbf{z})$. In
such a model (e.g., a VAE, GAN, or continuous-time flow-based model), sampling
of the target variable $\mathbf{x} \sim p_\theta(\mathbf{x})$ is
straightforward, but sampling from a posterior distribution of the form
$p(\mathbf{x}\mid\mathbf{y}) \propto
p_\theta(\mathbf{x})r(\mathbf{x},\mathbf{y})$, where $r$ is a constraint
function depending on an auxiliary variable $\mathbf{y}$, is generally
intractable. We propose to amortize the cost of sampling from such posterior
distributions with diffusion models that sample a distribution in the noise
space ($\mathbf{z}$). These diffusion samplers are trained by reinforcement
learning algorithms to enforce that the transformed samples
$f_\theta(\mathbf{z})$ are distributed according to the posterior in the data
space ($\mathbf{x}$). For many models and constraints of interest, the
posterior in the noise space is smoother than the posterior in the data space,
making it more amenable to such amortized inference. Our method enables
conditional sampling under unconditional GAN, (H)VAE, and flow-based priors,
comparing favorably both with current amortized and non-amortized inference
methods. We demonstrate the proposed outsourced diffusion sampling in several
experiments with large pretrained prior models: conditional image generation,
reinforcement learning with human feedback, and protein structure generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Epistemic Uncertainty in Conformal Scores: A Unified Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luben M. C. Cabezas, Vagner S. Santos, Thiago R. Ramos, Rafael Izbicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction methods create prediction bands with distribution-free
guarantees but do not explicitly capture epistemic uncertainty, which can lead
to overconfident predictions in data-sparse regions. Although recent conformal
scores have been developed to address this limitation, they are typically
designed for specific tasks, such as regression or quantile regression.
Moreover, they rely on particular modeling choices for epistemic uncertainty,
restricting their applicability. We introduce $\texttt{EPICSCORE}$, a
model-agnostic approach that enhances any conformal score by explicitly
integrating epistemic uncertainty. Leveraging Bayesian techniques such as
Gaussian Processes, Monte Carlo Dropout, or Bayesian Additive Regression Trees,
$\texttt{EPICSCORE}$ adaptively expands predictive intervals in regions with
limited data while maintaining compact intervals where data is abundant. As
with any conformal method, it preserves finite-sample marginal coverage.
Additionally, it also achieves asymptotic conditional coverage. Experiments
demonstrate its good performance compared to existing methods. Designed for
compatibility with any Bayesian model, but equipped with distribution-free
guarantees, $\texttt{EPICSCORE}$ provides a general-purpose framework for
uncertainty quantification in prediction problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Fleet Efficiency: Analyzing and Optimizing Large-Scale
  Google TPU Systems with ML Productivity Goodput 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arissa Wongpanich, Tayo Oguntebi, Jose Baiocchi Paredes, Yu Emma Wang, Phitchaya Mangpo Phothilimthana, Ritwika Mitra, Zongwei Zhou, Naveen Kumar, Vijay Janapa Reddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen the emergence of machine learning (ML) workloads
deployed in warehouse-scale computing (WSC) settings, also known as ML fleets.
As the computational demands placed on ML fleets have increased due to the rise
of large models and growing demand for ML applications, it has become
increasingly critical to measure and improve the efficiency of such systems.
However, there is not yet an established methodology to characterize ML fleet
performance and identify potential performance optimizations accordingly. This
paper presents a large-scale analysis of an ML fleet based on Google's TPUs,
introducing a framework to capture fleet-wide efficiency, systematically
evaluate performance characteristics, and identify optimization strategies for
the fleet. We begin by defining an ML fleet, outlining its components, and
analyzing an example Google ML fleet in production comprising thousands of
accelerators running diverse workloads. Our study reveals several critical
insights: first, ML fleets extend beyond the hardware layer, with model, data,
framework, compiler, and scheduling layers significantly impacting performance;
second, the heterogeneous nature of ML fleets poses challenges in
characterizing individual workload performance; and third, traditional
utilization-based metrics prove insufficient for ML fleet characterization. To
address these challenges, we present the "ML Productivity Goodput" (MPG) metric
to measure ML fleet efficiency. We show how to leverage this metric to
characterize the fleet across the ML system stack. We also present methods to
identify and optimize performance bottlenecks using MPG, providing strategies
for managing warehouse-scale ML systems in general. Lastly, we demonstrate
quantitative evaluations from applying these methods to a real ML fleet for
internal-facing Google TPU workloads, where we observed tangible improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Conic Proxy for Semidefinite Relaxation of AC Optimal Power Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guancheng Qiu, Mathieu Tanneau, Pascal Van Hentenryck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The nonlinear, non-convex AC Optimal Power Flow (AC-OPF) problem is
fundamental for power systems operations. The intrinsic complexity of AC-OPF
has fueled a growing interest in the development of optimization proxies for
the problem, i.e., machine learning models that predict high-quality,
close-to-optimal solutions. More recently, dual conic proxy architectures have
been proposed, which combine machine learning and convex relaxations of AC-OPF,
to provide valid certificates of optimality using learning-based methods.
Building on this methodology, this paper proposes, for the first time, a dual
conic proxy architecture for the semidefinite (SDP) relaxation of AC-OPF
problems. Although the SDP relaxation is stronger than the second-order cone
relaxation considered in previous work, its practical use has been hindered by
its computational cost. The proposed method combines a neural network with a
differentiable dual completion strategy that leverages the structure of the
dual SDP problem. This approach guarantees dual feasibility, and therefore
valid dual bounds, while providing orders of magnitude of speedups compared to
interior-point algorithms. The paper also leverages self-supervised learning,
which alleviates the need for time-consuming data generation and allows to
train the proposed models efficiently. Numerical experiments are presented on
several power grid benchmarks with up to 500 buses. The results demonstrate
that the proposed SDP-based proxies can outperform weaker conic relaxations,
while providing several orders of magnitude speedups compared to a
state-of-the-art interior-point SDP solver.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User-Preference Meets Pareto-Optimality: Multi-Objective Bayesian
  Optimization with Local Gradient Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Hang Sai Ip, Ankush Chakrabarty, Ali Mesbah, Diego Romeres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating user preferences into multi-objective Bayesian optimization
(MOBO) allows for personalization of the optimization procedure. Preferences
are often abstracted in the form of an unknown utility function, estimated
through pairwise comparisons of potential outcomes. However, utility-driven
MOBO methods can yield solutions that are dominated by nearby solutions, as
non-dominance is not enforced. Additionally, classical MOBO commonly relies on
estimating the entire Pareto-front to identify the Pareto-optimal solutions,
which can be expensive and ignore user preferences. Here, we present a new
method, termed preference-utility-balanced MOBO (PUB-MOBO), that allows users
to disambiguate between near-Pareto candidate solutions. PUB-MOBO combines
utility-based MOBO with local multi-gradient descent to refine user-preferred
solutions to be near-Pareto-optimal. To this end, we propose a novel
preference-dominated utility function that concurrently preserves
user-preferences and dominance amongst candidate solutions. A key advantage of
PUB-MOBO is that the local search is restricted to a (small) region of the
Pareto-front directed by user preferences, alleviating the need to estimate the
entire Pareto-front. PUB-MOBO is tested on three synthetic benchmark problems:
DTLZ1, DTLZ2 and DH1, as well as on three real-world problems: Vehicle Safety,
Conceptual Marine Design, and Car Side Impact. PUB-MOBO consistently
outperforms state-of-the-art competitors in terms of proximity to the
Pareto-front and utility regret across all the problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Diffusion for Certifiable Few-shot Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fady Rezk, Royson Lee, Henry Gouk, Timothy Hospedales, Minyoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern large-scale deep learning, a prevalent and effective workflow for
solving low-data problems is adapting powerful pre-trained foundation models
(FMs) to new tasks via parameter-efficient fine-tuning (PEFT). However, while
empirically effective, the resulting solutions lack generalisation guarantees
to certify their accuracy - which may be required for ethical or legal reasons
prior to deployment in high-importance applications. In this paper we develop a
novel transfer learning approach that is designed to facilitate non-vacuous
learning theoretic generalisation guarantees for downstream tasks, even in the
low-shot regime. Specifically, we first use upstream tasks to train a
distribution over PEFT parameters. We then learn the downstream task by a
sample-and-evaluate procedure -- sampling plausible PEFTs from the trained
diffusion model and selecting the one with the highest likelihood on the
downstream data. Crucially, this confines our model hypothesis to a finite set
of PEFT samples. In contrast to learning in the typical continuous hypothesis
spaces of neural network weights, this facilitates tighter risk certificates.
We instantiate our bound and show non-trivial generalization guarantees
compared to existing learning approaches which lead to vacuous bounds in the
low-shot regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task Offloading in Vehicular Edge Computing using Deep Reinforcement
  Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashab Uddin, Ahmed Hamdi Sakr, Ning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing demand for Intelligent Transportation Systems (ITS) has
introduced significant challenges in managing the complex,
computation-intensive tasks generated by modern vehicles while offloading tasks
to external computing infrastructures such as edge computing (EC), nearby
vehicular , and UAVs has become influential solution to these challenges.
However, traditional computational offloading strategies often struggle to
adapt to the dynamic and heterogeneous nature of vehicular environments. In
this study, we explored the potential of Reinforcement Learning (RL) and Deep
Reinforcement Learning (DRL) frameworks to optimize computational offloading
through adaptive, real-time decision-making, and we have thoroughly
investigated the Markov Decision Process (MDP) approaches on the existing
literature. The paper focuses on key aspects such as standardized learning
models, optimized reward structures, and collaborative multi-agent systems,
aiming to advance the understanding and application of DRL in vehicular
networks. Our findings offer insights into enhancing the efficiency,
scalability, and robustness of ITS, setting the stage for future innovations in
this rapidly evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 Pages, 3 Figures, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable automated ischaemic stroke lesion segmentation with vision
  transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Foulon, Robert Gray, James K. Ruffle, Jonathan Best, Tianbo Xu, Henry Watkins, Jane Rondina, Guilherme Pombo, Dominic Giles, Paul Wright, Marcela Ovando-Tellez, H. Rolf Jäger, Jorge Cardoso, Sebastien Ourselin, Geraint Rees, Parashkev Nachev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ischaemic stroke, a leading cause of death and disability, critically relies
on neuroimaging for characterising the anatomical pattern of injury.
Diffusion-weighted imaging (DWI) provides the highest expressivity in ischemic
stroke but poses substantial challenges for automated lesion segmentation:
susceptibility artefacts, morphological heterogeneity, age-related
comorbidities, time-dependent signal dynamics, instrumental variability, and
limited labelled data. Current U-Net-based models therefore underperform, a
problem accentuated by inadequate evaluation metrics that focus on mean
performance, neglecting anatomical, subpopulation, and acquisition-dependent
variability. Here, we present a high-performance DWI lesion segmentation tool
addressing these challenges through optimized vision transformer-based
architectures, integration of 3563 annotated lesions from multi-site data, and
algorithmic enhancements, achieving state-of-the-art results. We further
propose a novel evaluative framework assessing model fidelity, equity (across
demographics and lesion subtypes), anatomical precision, and robustness to
instrumental variability, promoting clinical and research utility. This work
advances stroke imaging by reconciling model expressivity with domain-specific
challenges and redefining performance benchmarks to prioritize equity and
generalizability, critical for personalized medicine and mechanistic research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 7 figures, 2 tables, 1 supplementary table, 2 supplementary
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matryoshka Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Nair, Puranjay Datta, Jeff Dean, Prateek Jain, Aditya Kusupati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantizing model weights is critical for reducing the communication and
inference costs of large models. However, quantizing models -- especially to
low precisions like int4 or int2 -- requires a trade-off in model quality;
int2, in particular, is known to severely degrade model quality. Consequently,
practitioners are often forced to maintain multiple models with different
quantization levels or serve a single model that best satisfies the
quality-latency trade-off. On the other hand, integer data types, such as int8,
inherently possess a nested (Matryoshka) structure where smaller bit-width
integers, like int4 or int2, are nested within the most significant bits. This
paper proposes Matryoshka Quantization (MatQuant), a novel multi-scale
quantization technique that addresses the challenge of needing multiple
quantized models. It allows training and maintaining just one model, which can
then be served at different precision levels. Furthermore, due to the
co-training and co-distillation regularization provided by MatQuant, the int2
precision models extracted by MatQuant can be up to $10\%$ more accurate than
standard int2 quantization (using techniques like QAT or OmniQuant). This
represents significant progress in model quantization, demonstrated by the fact
that, with the same recipe, an int2 FFN-quantized Gemma-2 9B model is more
accurate than an int8 FFN-quantized Gemma-2 2B model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepCrossAttention: Supercharging Transformer Residual Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mike Heddes, Adel Javanmard, Kyriakos Axiotis, Gang Fu, MohammadHossein Bateni, Vahab Mirrokni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer networks have achieved remarkable success across diverse domains,
leveraging a variety of architectural innovations, including residual
connections. However, traditional residual connections, which simply sum the
outputs of previous layers, can dilute crucial information. This work
introduces DeepCrossAttention (DCA), an approach that enhances residual
learning in transformers. DCA employs learnable, input-dependent weights to
dynamically combine layer outputs, enabling the model to selectively focus on
the most relevant information in any of the previous layers. Furthermore, DCA
incorporates depth-wise cross-attention, allowing for richer interactions
between layers at different depths. Our language modeling experiments show that
DCA achieves improved perplexity for a given training time. Moreover, DCA
obtains the same model quality up to 3x faster while adding a negligible number
of parameters. Theoretical analysis confirms that DCA provides an improved
trade-off between accuracy and model size when the ratio of collective layer
ranks to the ambient dimension falls below a critical threshold.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RelGNN: Composite Message Passing for Relational Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlang Chen, Charilaos Kanatsoulis, Jure Leskovec
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive tasks on relational databases are critical in real-world
applications spanning e-commerce, healthcare, and social media. To address
these tasks effectively, Relational Deep Learning (RDL) encodes relational data
as graphs, enabling Graph Neural Networks (GNNs) to exploit relational
structures for improved predictions. However, existing heterogeneous GNNs often
overlook the intrinsic structural properties of relational databases, leading
to modeling inefficiencies. Here we introduce RelGNN, a novel GNN framework
specifically designed to capture the unique characteristics of relational
databases. At the core of our approach is the introduction of atomic routes,
which are sequences of nodes forming high-order tripartite structures. Building
upon these atomic routes, RelGNN designs new composite message passing
mechanisms between heterogeneous nodes, allowing direct single-hop interactions
between them. This approach avoids redundant aggregations and mitigates
information entanglement, ultimately leading to more efficient and accurate
predictive modeling. RelGNN is evaluated on 30 diverse real-world tasks from
RelBench (Fey et al., 2024), and consistently achieves state-of-the-art
accuracy with up to 25% improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Limit of Outcome Reward for Learning Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, Weihan Cao, Jiangning Liu, Hongwei Liu, Junnan Liu, Songyang Zhang, Dahua Lin, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning abilities, especially those for solving complex math problems, are
crucial components of general intelligence. Recent advances by proprietary
companies, such as o-series models of OpenAI, have made remarkable progress on
reasoning tasks. However, the complete technical details remain unrevealed, and
the techniques that are believed certainly to be adopted are only reinforcement
learning (RL) and the long chain of thoughts. This paper proposes a new RL
framework, termed OREAL, to pursue the performance limit that can be achieved
through \textbf{O}utcome \textbf{RE}w\textbf{A}rd-based reinforcement
\textbf{L}earning for mathematical reasoning tasks, where only binary outcome
rewards are easily accessible. We theoretically prove that behavior cloning on
positive trajectories from best-of-N (BoN) sampling is sufficient to learn the
KL-regularized optimal policy in binary feedback environments. This formulation
further implies that the rewards of negative samples should be reshaped to
ensure the gradient consistency between positive and negative samples. To
alleviate the long-existing difficulties brought by sparse rewards in RL, which
are even exacerbated by the partial correctness of the long chain of thought
for reasoning tasks, we further apply a token-level reward model to sample
important tokens in reasoning trajectories for learning. With OREAL, for the
first time, a 7B model can obtain 94.0 pass@1 accuracy on MATH-500 through RL,
being on par with 32B models. OREAL-32B also surpasses previous 32B models
trained by distillation with 95.0 pass@1 accuracy on MATH-500. Our
investigation also indicates the importance of initial policy models and
training queries for RL. Code, models, and data will be released to benefit
future research\footnote{https://github.com/InternLM/OREAL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We released our code, data, and model on
  https://github.com/InternLM/OREAL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning an Optimal Assortment Policy under Observational Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Han, Han Zhong, Miao Lu, Jose Blanchet, Zhengyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the fundamental problem of offline assortment optimization under the
Multinomial Logit (MNL) model, where sellers must determine the optimal subset
of the products to offer based solely on historical customer choice data. While
most existing approaches to learning-based assortment optimization focus on the
online learning of the optimal assortment through repeated interactions with
customers, such exploration can be costly or even impractical in many
real-world settings. In this paper, we consider the offline learning paradigm
and investigate the minimal data requirements for efficient offline assortment
optimization. To this end, we introduce Pessimistic Rank-Breaking (PRB), an
algorithm that combines rank-breaking with pessimistic estimation. We prove
that PRB is nearly minimax optimal by establishing the tight suboptimality
upper bound and a nearly matching lower bound. This further shows that "optimal
item coverage" - where each item in the optimal assortment appears sufficiently
often in the historical data - is both sufficient and necessary for efficient
offline learning. This significantly relaxes the previous requirement of
observing the complete optimal assortment in the data. Our results provide
fundamental insights into the data requirements for offline assortment
optimization under the MNL model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Internet-Scale Training For Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The predominant approach for training web navigation agents gathers human
demonstrations for a set of popular websites and hand-written tasks, but it is
becoming clear that human data are an inefficient resource. We develop a
pipeline to facilitate Internet-scale training for agents without laborious
human annotations. In the first stage, an LLM generates tasks for 150k diverse
websites. In the next stage, LLM agents complete tasks and produce
trajectories. In the final stage, an LLM reviews the trajectories and judges
their success. Language models are competitive with human annotators, detecting
and filtering out harmful content with an accuracy of 97%, generating feasible
tasks with an 89% rate, and judging successful trajectories with an 82.6%
accuracy. Scaling the pipeline, agents based on Llama 3.1 70B solve 16.7% of
tasks for 150k sites. Training on the data generated by our pipeline is
competitive with training on human demonstrations. In data-limited settings
derived from Mind2Web and WebLINX, we improve Step Accuracy by up to +89.5% and
+122.1% respectively for agents trained on mixtures of data from our pipeline,
and human data. When training agents with all available human data from these
benchmarks, agents fail to generalize to diverse real sites, and adding our
data improves their generalization by +149.0% for WebLINX and +156.3% for
Mind2Web. Code will be available at: data-for-agents.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Performance of Explainable AI Models with Constrained Concept
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geyu Liang, Senne Michielssen, Salar Fattahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The trade-off between accuracy and interpretability has long been a challenge
in machine learning (ML). This tension is particularly significant for emerging
interpretable-by-design methods, which aim to redesign ML algorithms for
trustworthy interpretability but often sacrifice accuracy in the process. In
this paper, we address this gap by investigating the impact of deviations in
concept representations-an essential component of interpretable models-on
prediction performance and propose a novel framework to mitigate these effects.
The framework builds on the principle of optimizing concept embeddings under
constraints that preserve interpretability. Using a generative model as a
test-bed, we rigorously prove that our algorithm achieves zero loss while
progressively enhancing the interpretability of the resulting model.
Additionally, we evaluate the practical performance of our proposed framework
in generating explainable predictions for image classification tasks across
various benchmarks. Compared to existing explainable methods, our approach not
only improves prediction accuracy while preserving model interpretability
across various large-scale benchmarks but also achieves this with significantly
lower computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Emergence of Thinking in LLMs I: Searching for the Right
  Intuition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghao Ye, Khiem Duc Pham, Xinzhi Zhang, Sivakanth Gopi, Baolin Peng, Beibin Li, Janardhan Kulkarni, Huseyin A. Inan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent AI advancements, such as OpenAI's new models, are transforming LLMs
into LRMs (Large Reasoning Models) that perform reasoning during inference,
taking extra time and compute for higher-quality outputs. We aim to uncover the
algorithmic framework for training LRMs. Methods like self-consistency, PRM,
and AlphaZero suggest reasoning as guided search. We ask: what is the simplest,
most scalable way to enable search in LLMs?
  We propose a post-training framework called Reinforcement Learning via
Self-Play (RLSP). RLSP involves three steps: (1) supervised fine-tuning with
human or synthetic demonstrations of the reasoning process, (2) using an
exploration reward signal to encourage diverse and efficient reasoning
behaviors, and (3) RL training with an outcome verifier to ensure correctness
while preventing reward hacking. Our key innovation is to decouple exploration
and correctness signals during PPO training, carefully balancing them to
improve performance and efficiency.
  Empirical studies in the math domain show that RLSP improves reasoning. On
the Llama-3.1-8B-Instruct model, RLSP can boost performance by 23% in MATH-500
test set; On AIME 2024 math problems, Qwen2.5-32B-Instruct improved by 10% due
to RLSP. However, a more important finding of this work is that the models
trained using RLSP, even with the simplest exploration reward that encourages
the model to take more intermediate steps, showed several emergent behaviors
such as backtracking, exploration of ideas, and verification. These findings
demonstrate that RLSP framework might be enough to enable emergence of complex
reasoning abilities in LLMs when scaled. Lastly, we propose a theory as to why
RLSP search strategy is more suitable for LLMs inspired by a remarkable result
that says CoT provably increases computational power of LLMs, which grows as
the number of steps in CoT \cite{li2024chain,merrill2023expresssive}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Abstract shortened for arXiv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neighborhood-Order Learning Graph Attention Network for Fake News
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batool Lakzaei, Mostafa Haghir Chehreghani, Alireza Bagheri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fake news detection is a significant challenge in the digital age, which has
become increasingly important with the proliferation of social media and online
communication networks. Graph Neural Networks (GNN)-based methods have shown
high potential in analyzing graph-structured data for this problem. However, a
major limitation in conventional GNN architectures is their inability to
effectively utilize information from neighbors beyond the network's layer
depth, which can reduce the model's accuracy and effectiveness. In this paper,
we propose a novel model called Neighborhood-Order Learning Graph Attention
Network (NOL-GAT) for fake news detection. This model allows each node in each
layer to independently learn its optimal neighborhood order. By doing so, the
model can purposefully and efficiently extract critical information from
distant neighbors. The NOL-GAT architecture consists of two main components: a
Hop Network that determines the optimal neighborhood order and an Embedding
Network that updates node embeddings using these optimal neighborhoods. To
evaluate the model's performance, experiments are conducted on various fake
news datasets. Results demonstrate that NOL-GAT significantly outperforms
baseline models in metrics such as accuracy and F1-score, particularly in
scenarios with limited labeled data. Features such as mitigating the
over-squashing problem, improving information flow, and reducing computational
complexity further highlight the advantages of the proposed model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present that hierarchical LLM reasoning via scaling thought templates can
effectively optimize the reasoning search space and outperform the mathematical
reasoning capabilities of powerful LLMs like OpenAI o1-preview and DeepSeek V3.
We train our ReasonFlux-32B model with only 8 GPUs and introduces three
innovations: (i) a structured and generic thought template library, containing
around 500 high-level thought templates capable of generalizing to similar or
relevant reasoning problems; (ii) performing hierarchical reinforcement
learning on a sequence of thought templates instead of long CoTs, optimizing a
base LLM to plan out an optimal template trajectory for gradually handling
complex problems; (iii) a brand new inference scaling system that enables
hierarchical LLM reasoning by adaptively scaling thought templates at inference
time. With a template trajectory containing sequential thought templates, our
ReasonFlux-32B significantly advances math reasoning capabilities to
state-of-the-art levels. Notably, on the MATH benchmark, it achieves an
accuracy of 91.2% and surpasses o1-preview by 6.7%. On the USA Math Olympiad
(AIME) benchmark, ReasonFlux-32B solves an average of 56.7% of problems,
surpassing o1-preview and DeepSeek-V3 by 27% and 45%, respectively. Code:
https://github.com/Gen-Verse/ReasonFlux
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Gen-Verse/ReasonFlux</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Particle Tracking with Neuromorphic Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Coradin, Fabio Cufino, Muhammad Awais, Tommaso Dorigo, Enrico Lupi, Eleonora Porcu, Jinu Raj, Fredrik Sandin, Mia Tosi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the application of a neural network architecture for identifying
charged particle trajectories via unsupervised learning of delays and synaptic
weights using a spike-time-dependent plasticity rule. In the considered model,
the neurons receive time-encoded information on the position of particle hits
in a tracking detector for a particle collider, modeled according to the
geometry of the Compact Muon Solenoid Phase II detector. We show how a spiking
neural network is capable of successfully identifying in a completely
unsupervised way the signal left by charged particles in the presence of
conspicuous noise from accidental or combinatorial hits. These results open the
way to applications of neuromorphic computing to particle tracking, motivating
further studies into its potential for real-time, low-power particle tracking
in future high-energy physics experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 21 figures, submitted to MDPI Particles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Train for the Worst, Plan for the Best: Understanding Token Ordering in
  Masked Diffusions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, Sitan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, masked diffusion models (MDMs) have emerged as a promising
alternative approach for generative modeling over discrete domains. Compared to
autoregressive models (ARMs), MDMs trade off complexity at training time with
flexibility at inference time. At training time, they must learn to solve an
exponentially large number of infilling problems, but at inference time, they
can decode tokens in essentially arbitrary order. In this work, we closely
examine these two competing effects. On the training front, we theoretically
and empirically demonstrate that MDMs indeed train on computationally
intractable subproblems compared to their autoregressive counterparts. On the
inference front, we show that a suitable strategy for adaptively choosing the
token decoding order significantly enhances the capabilities of MDMs, allowing
them to sidestep hard subproblems. On logic puzzles like Sudoku, we show that
adaptive inference can boost solving accuracy in pretrained MDMs from $<7$% to
$\approx 90$%, even outperforming ARMs with $7\times$ as many parameters and
that were explicitly trained via teacher forcing to learn the right order of
decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are all models wrong? Fundamental limits in distribution-free empirical
  model falsification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel M. Müller, Yuetian Luo, Rina Foygel Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In statistics and machine learning, when we train a fitted model on available
data, we typically want to ensure that we are searching within a model class
that contains at least one accurate model -- that is, we would like to ensure
an upper bound on the model class risk (the lowest possible risk that can be
attained by any model in the class). However, it is also of interest to
establish lower bounds on the model class risk, for instance so that we can
determine whether our fitted model is at least approximately optimal within the
class, or, so that we can decide whether the model class is unsuitable for the
particular task at hand. Particularly in the setting of interpolation learning
where machine learning models are trained to reach zero error on the training
data, we might ask if, at the very least, a positive lower bound on the model
class risk is possible -- or are we unable to detect that "all models are
wrong"? In this work, we answer these questions in a distribution-free setting
by establishing a model-agnostic, fundamental hardness result for the problem
of constructing a lower bound on the best test error achievable over a model
class, and examine its implications on specific model classes such as
tree-based methods and linear regression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ History-Guided <span class="highlight-title">Video</span> Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du, Russ Tedrake, Vincent Sitzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classifier-free guidance (CFG) is a key technique for improving conditional
generation in diffusion models, enabling more accurate control while enhancing
sample quality. It is natural to extend this technique to video diffusion,
which generates video conditioned on a variable number of context frames,
collectively referred to as history. However, we find two key challenges to
guiding with variable-length history: architectures that only support
fixed-size conditioning, and the empirical observation that CFG-style history
dropout performs poorly. To address this, we propose the Diffusion Forcing
Transformer (DFoT), a video diffusion architecture and theoretically grounded
training objective that jointly enable conditioning on a flexible number of
history frames. We then introduce History Guidance, a family of guidance
methods uniquely enabled by DFoT. We show that its simplest form, vanilla
history guidance, already significantly improves video generation quality and
temporal consistency. A more advanced method, history guidance across time and
frequency further enhances motion dynamics, enables compositional
generalization to out-of-distribution history, and can stably roll out
extremely long videos. Website: https://boyuan.space/history-guidance
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://boyuan.space/history-guidance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When, Where and Why to Average Weights? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niccolò Ajroldi, Antonio Orvieto, Jonas Geiping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Averaging checkpoints along the training trajectory is a simple yet powerful
approach to improve the generalization performance of Machine Learning models
and reduce training time. Motivated by these potential gains, and in an effort
to fairly and thoroughly benchmark this technique, we present an extensive
evaluation of averaging techniques in modern Deep Learning, which we perform
using AlgoPerf \citep{dahl_benchmarking_2023}, a large-scale benchmark for
optimization algorithms. We investigate whether weight averaging can reduce
training time, improve generalization, and replace learning rate decay, as
suggested by recent literature. Our evaluation across seven architectures and
datasets reveals that averaging significantly accelerates training and yields
considerable efficiency gains, at the price of a minimal implementation and
memory cost, while mildly improving generalization across all considered
workloads. Finally, we explore the relationship between averaging and learning
rate annealing and show how to optimally combine the two to achieve the best
performances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Case for a unified surrogate modelling framework in the age of AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Semenova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surrogate models are widely used in natural sciences, engineering, and
machine learning to approximate complex systems and reduce computational costs.
However, the current landscape lacks standardisation across key stages of the
pipeline, including data collection, sampling design, model class selection,
evaluation metrics, and downstream task performance analysis. This
fragmentation limits reproducibility, reliability, and cross-domain
applicability. The issue has only been exacerbated by the AI revolution and a
new suite of surrogate model classes that it offers. In this position paper, we
argue for the urgent need for a unified framework to guide the development and
evaluation of surrogate models. We outline essential steps for constructing a
comprehensive pipeline and discuss alternative perspectives, such as the
benefits of domain-specific frameworks. By advocating for a standardised
approach, this paper seeks to improve the reliability of surrogate modelling,
foster cross-disciplinary knowledge transfer, and, as a result, accelerate
scientific progress.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What makes a good feedforward computational graph? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Vitvitskyi, João G. M. Araújo, Marc Lackenby, Petar Veličković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As implied by the plethora of literature on graph rewiring, the choice of
computational graph employed by a neural network can make a significant impact
on its downstream performance. Certain effects related to the computational
graph, such as under-reaching and over-squashing, may even render the model
incapable of learning certain functions. Most of these effects have only been
thoroughly studied in the domain of undirected graphs; however, recent years
have seen a significant rise in interest in feedforward computational graphs:
directed graphs without any back edges. In this paper, we study the desirable
properties of a feedforward computational graph, discovering two important
complementary measures: fidelity and mixing time, and evaluating a few popular
choices of graphs through the lens of these measures. Our study is backed by
both theoretical analyses of the metrics' asymptotic behaviour for various
graphs, as well as correlating these metrics to the performance of trained
neural network models using the corresponding graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress -- comments welcome. 16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Occam's model: Selecting simpler representations for better
  transferability estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prabhant Singh, Sibylle Hess, Joaquin Vanschoren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning models that have been pre-trained on large datasets has become a
cornerstone of modern machine learning workflows. With the widespread
availability of online model repositories, such as Hugging Face, it is now
easier than ever to fine-tune pre-trained models for specific tasks. This
raises a critical question: which pre-trained model is most suitable for a
given task? This problem is called transferability estimation. In this work, we
introduce two novel and effective metrics for estimating the transferability of
pre-trained models. Our approach is grounded in viewing transferability as a
measure of how easily a pre-trained model's representations can be trained to
separate target classes, providing a unique perspective on transferability
estimation. We rigorously evaluate the proposed metrics against
state-of-the-art alternatives across diverse problem settings, demonstrating
their robustness and practical utility. Additionally, we present theoretical
insights that explain our metrics' efficacy and adaptability to various
scenarios. We experimentally show that our metrics increase Kendall's Tau by up
to 32% compared to the state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incentivizing Desirable Effort Profiles in Strategic Classification: The
  Role of Causality and Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valia Efthymiou, Chara Podimata, Diptangshu Sen, Juba Ziani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study strategic classification in binary decision-making settings where
agents can modify their features in order to improve their classification
outcomes. Importantly, our work considers the causal structure across different
features, acknowledging that effort in a given feature may affect other
features. The main goal of our work is to understand \emph{when and how much
agent effort is invested towards desirable features}, and how this is
influenced by the deployed classifier, the causal structure of the agent's
features, their ability to modify them, and the information available to the
agent about the classifier and the feature causal graph.
  In the complete information case, when agents know the classifier and the
causal structure of the problem, we derive conditions ensuring that rational
agents focus on features favored by the principal. We show that designing
classifiers to induce desirable behavior is generally non-convex, though
tractable in special cases. We also extend our analysis to settings where
agents have incomplete information about the classifier or the causal graph.
While optimal effort selection is again a non-convex problem under general
uncertainty, we highlight special cases of partial uncertainty where this
selection problem becomes tractable. Our results indicate that uncertainty
drives agents to favor features with higher expected importance and lower
variance, potentially misaligning with principal preferences. Finally,
numerical experiments based on a cardiovascular disease risk study illustrate
how to incentivize desirable modifications under uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Multi-Normalization for Stateless and Scalable LLM Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meyer Scetbon, Chao Ma, Wenbo Gong, Edward Meeds
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large language models (LLMs) typically relies on adaptive optimizers
like Adam (Kingma & Ba, 2015) which store additional state information to
accelerate convergence but incur significant memory overhead. Recent efforts,
such as SWAN (Ma et al., 2024) address this by eliminating the need for
optimizer states while achieving performance comparable to Adam via a
multi-step preprocessing procedure applied to instantaneous gradients.
Motivated by the success of SWAN, we introduce a novel framework for designing
stateless optimizers that normalizes stochastic gradients according to multiple
norms. To achieve this, we propose a simple alternating scheme to enforce the
normalization of gradients w.r.t these norms. We show that our procedure can
produce, up to an arbitrary precision, a fixed-point of the problem, and that
SWAN is a particular instance of our approach with carefully chosen norms,
providing a deeper understanding of its design. However, SWAN's computationally
expensive whitening/orthogonalization step limit its practicality for large
LMs. Using our principled perspective, we develop of a more efficient,
scalable, and practical stateless optimizer. Our algorithm relaxes the
properties of SWAN, significantly reducing its computational cost while
retaining its memory efficiency, making it applicable to training large-scale
models. Experiments on pre-training LLaMA models with up to 1 billion
parameters demonstrate a 3X speedup over Adam with significantly reduced memory
requirements, outperforming other memory-efficient baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A note on the physical interpretation of neural PDE's 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sauro Succi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We highlight a formal and substantial analogy between Machine Learning (ML)
algorithms and discrete dynamical systems (DDS) in relaxation form. The analogy
offers a transparent interpretation of the weights in terms of physical
information-propagation processes and identifies the model function of the
forward ML step with the local attractor of the corresponding discrete
dynamics. Besides improving the explainability of current ML applications, this
analogy may also facilitate the development of a new class ML algorithms with a
reduced number of weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resurrecting saturated LLM benchmarks with adversarial encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Ivanov, Dmitrii Volkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work showed that small changes in benchmark questions can reduce LLMs'
reasoning and recall. We explore two such changes: pairing questions and adding
more answer options, on three benchmarks: WMDP-bio, GPQA, and MMLU variants. We
find that for more capable models, these predictably reduce performance,
essentially heightening the performance ceiling of a benchmark and unsaturating
it again. We suggest this approach can resurrect old benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, Ying Fan, Jungtaek Kim, Hyung Il Koo, Kannan Ramchandran, Dimitris Papailiopoulos, Kangwook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process Reward Models (PRMs) have proven effective at enhancing mathematical
reasoning for Large Language Models (LLMs) by leveraging increased
inference-time computation. However, they are predominantly trained on
mathematical data and their generalizability to non-mathematical domains has
not been rigorously studied. In response, this work first shows that current
PRMs have poor performance in other domains. To address this limitation, we
introduce VersaPRM, a multi-domain PRM trained on synthetic reasoning data
generated using our novel data generation and annotation method. VersaPRM
achieves consistent performance gains across diverse domains. For instance, in
the MMLU-Pro category of Law, VersaPRM via weighted majority voting, achieves a
7.9% performance gain over the majority voting baseline -- surpassing
Qwen2.5-Math-PRM's gain of 1.3%. We further contribute to the community by
open-sourcing all data, code and models for VersaPRM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Loss-Based Sample Reweighting for Improved Large Language Model
  Pretraining <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daouda Sow, Herbert Woisetschläger, Saikiran Bulusu, Shiqiang Wang, Hans-Arno Jacobsen, Yingbin Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining large language models (LLMs) on vast and heterogeneous datasets
is crucial for achieving state-of-the-art performance across diverse downstream
tasks. However, current training paradigms treat all samples equally,
overlooking the importance or relevance of individual samples throughout the
training process. Existing reweighting strategies, which primarily focus on
group-level data importance, fail to leverage fine-grained instance-level
information and do not adapt dynamically to individual sample importance as
training progresses. In this paper, we introduce novel algorithms for dynamic,
instance-level data reweighting aimed at improving both the efficiency and
effectiveness of LLM pretraining. Our methods adjust the weight of each
training sample based on its loss value in an online fashion, allowing the
model to dynamically focus on more informative or important samples at the
current training stage. In particular, our framework allows us to
systematically devise reweighting strategies deprioritizing redundant or
uninformative data, which we find tend to work best. Furthermore, we develop a
new theoretical framework for analyzing the impact of loss-based reweighting on
the convergence of gradient-based optimization, providing the first formal
characterization of how these strategies affect convergence bounds. We
empirically validate our approach across a spectrum of tasks, from pretraining
7B and 1.4B parameter LLMs to smaller-scale language models and linear
regression problems, demonstrating that our loss-based reweighting approach can
lead to faster convergence and significantly improved performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICLR 2025. Code base available:
  https://github.com/sowmaster/Sample-Level-Loss-Reweighting-ICLR-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlexDeMo: Decoupled Momentum Optimization for Fully and Hybrid Sharded
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mogens Henrik From, Jacob Nielsen, Lukas Galke, Peter Schneider-Kamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training large neural network models requires extensive computational
resources, often distributed across several nodes and accelerators. Recent
findings suggest that it may be sufficient to only exchange the fast moving
components of the gradients, while accumulating momentum locally (Decoupled
Momentum, or DeMo). However, when considering larger models that do not fit on
a single accelerate, the exchange of gradient information and the integration
of DeMo needs to be reconsidered. Here, we propose employing a hybrid strategy,
FlexDeMo, whereby nodes fully synchronize locally between different GPUs and
inter-node communication is improved through only using the fast-moving
components. This effectively combines previous hybrid sharding strategies with
the advantages of decoupled momentum. Our experimental results show that
FlexDeMo is on par with AdamW in terms of validation loss, demonstrating its
viability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient
  Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Sheshukova, Sergey Samsonov, Denis Belomestny, Eric Moulines, Qi-Man Shao, Zhuo-Song Zhang, Alexey Naumov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we establish non-asymptotic convergence rates in the central
limit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient
descent (SGD). Our analysis builds on the result of the Gaussian approximation
for nonlinear statistics of independent random variables of Shao and Zhang
(2022). Using this result, we prove the non-asymptotic validity of the
multiplier bootstrap for constructing the confidence sets for the optimal
solution of an optimization problem. In particular, our approach avoids the
need to approximate the limiting covariance of Polyak-Ruppert SGD iterates,
which allows us to derive approximation rates in convex distance of order up to
$1/\sqrt{n}$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RSAttAE: An Information-Aware Attention-based Autoencoder Recommender
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Dadashzadeh Taromi, Sina Heydari, Mohsen Hooshmand, Majid Ramezani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems play a crucial role in modern life, including information
retrieval, the pharmaceutical industry, retail, and entertainment. The
entertainment sector, in particular, attracts significant attention and
generates substantial profits. This work proposes a new method for predicting
unknown user-movie ratings to enhance customer satisfaction. To achieve this,
we utilize the MovieLens 100K dataset. Our approach introduces an
attention-based autoencoder to create meaningful representations and the
XGBoost method for rating predictions. The results demonstrate that our
proposal outperforms most of the existing state-of-the-art methods.
Availability: github.com/ComputationIASBS/RecommSys
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Attention Heads Compete or Cooperate during Counting? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pál Zsámboki, Ádám Fraknói, Máté Gedeon, András Kornai, Zsolt Zombori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an in-depth mechanistic interpretability analysis of training
small transformers on an elementary task, counting, which is a crucial
deductive step in many algorithms. In particular, we investigate the
collaboration/competition among the attention heads: we ask whether the
attention heads behave as a pseudo-ensemble, all solving the same subtask, or
they perform different subtasks, meaning that they can only solve the original
task in conjunction. Our work presents evidence that on the semantics of the
counting task, attention heads behave as a pseudo-ensemble, but their outputs
need to be aggregated in a non-uniform manner in order to create an encoding
that conforms to the syntax. Our source code will be available upon
publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairDropout: Using Example-Tied Dropout to Enhance Generalization of
  Minority Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geraldin Nanfack, Eugene Belilovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models frequently exploit spurious features in training data to
achieve low training error, often resulting in poor generalization when faced
with shifted testing distributions. To address this issue, various methods from
imbalanced learning, representation learning, and classifier recalibration have
been proposed to enhance the robustness of deep neural networks against
spurious correlations. In this paper, we observe that models trained with
empirical risk minimization tend to generalize well for examples from the
majority groups while memorizing instances from minority groups. Building on
recent findings that show memorization can be localized to a limited number of
neurons, we apply example-tied dropout as a method we term FairDropout, aimed
at redirecting this memorization to specific neurons that we subsequently drop
out during inference. We empirically evaluate FairDropout using the
subpopulation benchmark suite encompassing vision, language, and healthcare
tasks, demonstrating that it significantly reduces reliance on spurious
correlations, and outperforms state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent Advances, Applications and Open Challenges in Machine Learning
  for Health: Reflections from Research Roundtables at ML4H 2024 Symposium 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Adibi, Xu Cao, Zongliang Ji, Jivat Neet Kaur, Winston Chen, Elizabeth Healey, Brighton Nuwagira, Wenqian Ye, Geoffrey Woollard, Maxwell A Xu, Hejie Cui, Johnny Xi, Trenton Chang, Vasiliki Bikia, Nicole Zhang, Ayush Noori, Yuan Xia, Md. Belal Hossain, Hanna A. Frank, Alina Peluso, Yuan Pu, Shannon Zejiang Shen, John Wu, Adibvafa Fallahpour, Sazan Mahbub, Ross Duncan, Yuwei Zhang, Yurui Cao, Zuheng Xu, Michael Craig, Rahul G. Krishnan, Rahmatollah Beheshti, James M. Rehg, Mohammad Ehsanul Karim, Megan Coffee, Leo Anthony Celi, Jason Alan Fries, Mohsen Sadatsafavi, Dennis Shung, Shannon McWeeney, Jessica Dafflon, Sarah Jabbour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fourth Machine Learning for Health (ML4H) symposium was held in person on
December 15th and 16th, 2024, in the traditional, ancestral, and unceded
territories of the Musqueam, Squamish, and Tsleil-Waututh Nations in Vancouver,
British Columbia, Canada. The symposium included research roundtable sessions
to foster discussions between participants and senior researchers on timely and
relevant topics for the ML4H community. The organization of the research
roundtables at the conference involved 13 senior and 27 junior chairs across 13
tables. Each roundtable session included an invited senior chair (with
substantial experience in the field), junior chairs (responsible for
facilitating the discussion), and attendees from diverse backgrounds with an
interest in the session's topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Audio Helps for Cognitive State Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adil Soubki, John Murzaku, Peter Zeng, Owen Rambow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The NLP community has broadly focused on text-only approaches of cognitive
state tasks, but audio can provide vital missing cues through prosody. We posit
that text-to-speech models learn to track aspects of cognitive state in order
to produce naturalistic audio, and that the signal audio models implicitly
identify is orthogonal to the information that language models exploit. We
present Synthetic Audio Data fine-tuning (SAD), a framework where we show that
7 tasks related to cognitive state modeling benefit from multimodal training on
both text and zero-shot synthetic audio data from an off-the-shelf TTS system.
We show an improvement over the text-only modality when adding synthetic audio
data to text-only corpora. Furthermore, on tasks and corpora that do contain
gold audio, we show our SAD framework achieves competitive performance with
text and synthetic audio compared to text and gold audio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>John Murzaku and Adil Soubki contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neumann eigenmaps for landmark embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Sule, Wojciech Czaja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Neumann eigenmaps (NeuMaps), a novel approach for enhancing the
standard diffusion map embedding using landmarks, i.e distinguished samples
within the dataset. By interpreting these landmarks as a subgraph of the larger
data graph, NeuMaps are obtained via the eigendecomposition of a renormalized
Neumann Laplacian. We show that NeuMaps offer two key advantages: (1) they
provide a computationally efficient embedding that accurately recovers the
diffusion distance associated with the reflecting random walk on the subgraph,
and (2) they naturally incorporate the Nystr\"om extension within the diffusion
map framework through the discrete Neumann boundary condition. Through examples
in digit classification and molecular dynamics, we demonstrate that NeuMaps not
only improve upon existing landmark-based embedding methods but also enhance
the stability of diffusion map embeddings to the removal of highly significant
points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Trick, No Treat: Pursuits and Challenges Towards Simulation-free
  Training of Neural Samplers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun He, Yuanqi Du, Francisco Vargas, Dinghuai Zhang, Shreyas Padhy, RuiKang OuYang, Carla Gomes, José Miguel Hernández-Lobato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the sampling problem, where the aim is to draw samples from a
distribution whose density is known only up to a normalization constant. Recent
breakthroughs in generative modeling to approximate a high-dimensional data
distribution have sparked significant interest in developing neural
network-based methods for this challenging problem. However, neural samplers
typically incur heavy computational overhead due to simulating trajectories
during training. This motivates the pursuit of simulation-free training
procedures of neural samplers. In this work, we propose an elegant modification
to previous methods, which allows simulation-free training with the help of a
time-dependent normalizing flow. However, it ultimately suffers from severe
mode collapse. On closer inspection, we find that nearly all successful neural
samplers rely on Langevin preconditioning to avoid mode collapsing. We
systematically analyze several popular methods with various objective functions
and demonstrate that, in the absence of Langevin preconditioning, most of them
fail to adequately cover even a simple target. Finally, we draw attention to a
strong baseline by combining the state-of-the-art MCMC method, Parallel
Tempering (PT), with an additional generative model to shed light on future
explorations of neural samplers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Arbel, David Salinas, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent foundational models for tabular data, such as TabPFN, have
demonstrated remarkable effectiveness in adapting to new tasks through
in-context learning. However, these models overlook a crucial equivariance
property: the arbitrary ordering of target dimensions should not influence
model predictions. In this study, we identify this oversight as a source of
incompressible error, termed the equivariance gap, which introduces instability
in predictions. To mitigate these issues, we propose a novel model designed to
preserve equivariance across output dimensions. Our experimental results
indicate that our proposed model not only addresses these pitfalls effectively
but also achieves competitive benchmark performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHIRLA: Comprehensive High-resolution Identification and
  Re-identification for Large-scale Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bessie Dominguez-Dager, Felix Escalona, Francisco Gomez-Donoso, Miguel Cazorla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person re-identification (Re-ID) is a key challenge in computer vision,
requiring the matching of individuals across different cameras, locations, and
time periods. While most research focuses on short-term scenarios with minimal
appearance changes, real-world applications demand robust Re-ID systems capable
of handling long-term scenarios, where persons' appearances can change
significantly due to variations in clothing and physical characteristics. In
this paper, we present CHIRLA, Comprehensive High-resolution Identification and
Re-identification for Large-scale Analysis, a novel dataset specifically
designed for long-term person Re-ID. CHIRLA consists of recordings from
strategically placed cameras over a seven-month period, capturing significant
variations in both temporal and appearance attributes, including controlled
changes in participants' clothing and physical features. The dataset includes
22 individuals, four connected indoor environments, and seven cameras. We
collected more than five hours of video that we semi-automatically labeled to
generate around one million bounding boxes with identity annotations. By
introducing this comprehensive benchmark, we aim to facilitate the development
and evaluation of Re-ID algorithms that can reliably perform in challenging,
long-term real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantile Multi-Armed Bandits with 1-bit Feedback <span class="chip">ALT 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Lau, Jonathan Scarlett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study a variant of best-arm identification involving
elements of risk sensitivity and communication constraints. Specifically, the
goal of the learner is to identify the arm with the highest quantile reward,
while the communication from an agent (who observes rewards) and the learner
(who chooses actions) is restricted to only one bit of feedback per arm pull.
We propose an algorithm that utilizes noisy binary search as a subroutine,
allowing the learner to estimate quantile rewards through 1-bit feedback. We
derive an instance-dependent upper bound on the sample complexity of our
algorithm and provide an algorithm-independent lower bound for specific
instances, with the two matching to within logarithmic factors under mild
conditions, or even to within constant factors in certain low error probability
scaling regimes. The lower bound is applicable even in the absence of
communication constraints, and thus we conclude that restricting to 1-bit
feedback has a minimal impact on the scaling of the sample complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ALT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAILS: Risk-Aware Iterated Local Search for Joint SLA Decomposition and
  Service Provider Management in Multi-Domain Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cyril Shih-Huan Hsu, Chrysa Papagianni, Paola Grosso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of the fifth generation (5G) technology has transformed mobile
networks into multi-service environments, necessitating efficient network
slicing to meet diverse Service Level Agreements (SLAs). SLA decomposition
across multiple network domains, each potentially managed by different service
providers, poses a significant challenge due to limited visibility into
real-time underlying domain conditions. This paper introduces Risk-Aware
Iterated Local Search (RAILS), a novel risk model-driven meta-heuristic
framework designed to jointly address SLA decomposition and service provider
selection in multi-domain networks. By integrating online risk modeling with
iterated local search principles, RAILS effectively navigates the complex
optimization landscape, utilizing historical feedback from domain controllers.
We formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP)
problem and prove its NP-hardness. Extensive simulations demonstrate that RAILS
achieves near-optimal performance, offering an efficient, real-time solution
for adaptive SLA management in modern multi-domain networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been submitted to IEEE HPSR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Deep Audio Representations for Hearables <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Gröger, Pascal Baumann, Ludovic Amruthalingam, Laurent Simon, Ruksana Giurda, Simone Lionetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively steering hearable devices requires understanding the acoustic
environment around the user. In the computational analysis of sound scenes,
foundation models have emerged as the state of the art to produce
high-performance, robust, multi-purpose audio representations. We introduce and
release Deep Evaluation of Audio Representations (DEAR), the first dataset and
benchmark to evaluate the efficacy of foundation models in capturing essential
acoustic properties for hearables. The dataset includes 1,158 audio tracks,
each 30 seconds long, created by spatially mixing proprietary monologues with
commercial, high-quality recordings of everyday acoustic scenes. Our benchmark
encompasses eight tasks that assess the general context, speech sources, and
technical acoustic properties of the audio scenes. Through our evaluation of
four general-purpose audio representation models, we demonstrate that the BEATs
model significantly surpasses its counterparts. This superiority underscores
the advantage of models trained on diverse audio collections, confirming their
applicability to a wide array of auditory tasks, including encoding the
environment properties necessary for hearable steering. The DEAR dataset and
associated code are available at https://dear-dataset.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iLOCO: Distribution-Free Inference for Feature Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camille Little, Lili Zheng, Genevera Allen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature importance measures are widely studied and are essential for
understanding model behavior, guiding feature selection, and enhancing
interpretability. However, many machine learning fitted models involve complex,
higher-order interactions between features. Existing feature importance metrics
fail to capture these higher-order effects while existing interaction metrics
often suffer from limited applicability or excessive computation; no methods
exist to conduct statistical inference for feature interactions. To bridge this
gap, we first propose a new model-agnostic metric, interaction
Leave-One-Covariate-Out iLOCO, for measuring the importance of higher-order
feature interactions. Next, we leverage recent advances in LOCO inference to
develop distribution-free and assumption-light confidence intervals for our
iLOCO metric. To address computational challenges, we also introduce an
ensemble learning method for calculating the iLOCO metric and confidence
intervals that we show is both computationally and statistically efficient. We
validate our iLOCO metric and our confidence intervals on both synthetic and
real data sets, showing that our approach outperforms existing methods and
provides the first inferential approach to detecting feature interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Samples to Question Trained Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Mehmet Kıral, Nurşen Aydın, Ş. İlker Birbil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing need for investigating how machine learning models
operate. With this work, we aim to understand trained machine learning models
by questioning their data preferences. We propose a mathematical framework that
allows us to probe trained models and identify their preferred samples in
various scenarios including prediction-risky, parameter-sensitive, or
model-contrastive samples. To showcase our framework, we pose these queries to
a range of models trained on a range of classification and regression tasks,
and receive answers in the form of generated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimation of Food Intake Quantity Using Inertial Signals from
  Smartwatches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Levi, Konstantinos Kyritsis, Vasileios Papapanagiotou, Georgios Tsakiridis, Anastasios Delopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate monitoring of eating behavior is crucial for managing obesity and
eating disorders such as bulimia nervosa. At the same time, existing methods
rely on multiple and/or specialized sensors, greatly harming adherence and
ultimately, the quality and continuity of data. This paper introduces a novel
approach for estimating the weight of a bite, from a commercial smartwatch. Our
publicly-available dataset contains smartwatch inertial data from ten
participants, with manually annotated start and end times of each bite along
with their corresponding weights from a smart scale, under semi-controlled
conditions. The proposed method combines extracted behavioral features such as
the time required to load the utensil with food, with statistical features of
inertial signals, that serve as input to a Support Vector Regression model to
estimate bite weights. Under a leave-one-subject-out cross-validation scheme,
our approach achieves a mean absolute error (MAE) of 3.99 grams per bite. To
contextualize this performance, we introduce the improvement metric, that
measures the relative MAE difference compared to a baseline model. Our method
demonstrates a 17.41% improvement, while the adapted state-of-the art method
shows a -28.89% performance against that same baseline. The results presented
in this work establish the feasibility of extracting meaningful bite weight
estimates from commercial smartwatch inertial sensors alone, laying the
groundwork for future accessible, non-invasive dietary monitoring systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript submitted for review to 47th Annual International
  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Koopman-Equivariant Gaussian Processes <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petar Bevanda, Max Beier, Armin Lederer, Alexandre Capone, Stefan Sosnowski, Sandra Hirche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Credible forecasting and representation learning of dynamical systems are of
ever-increasing importance for reliable decision-making. To that end, we
propose a family of Gaussian processes (GP) for dynamical systems with linear
time-invariant responses, which are nonlinear only in initial conditions. This
linearity allows us to tractably quantify forecasting and representational
uncertainty, simultaneously alleviating the challenge of computing the
distribution of trajectories from a GP-based dynamical system and enabling a
new probabilistic treatment of learning Koopman operator representations. Using
a trajectory-based equivariance -- which we refer to as \textit{Koopman
equivariance} -- we obtain a GP model with enhanced generalization
capabilities. To allow for large-scale regression, we equip our framework with
variational inference based on suitable inducing points. Experiments
demonstrate on-par and often better forecasting performance compared to
kernel-based methods for learning dynamical systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 28th International Conference on Artificial
  Intelligence and Statistics (AISTATS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoETuner: Optimized Mixture of Expert Serving with Balanced Expert
  Placement and Token Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokjin Go, Divya Mahajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) model architecture has emerged as a promising
solution for scaling transformer models efficiently, offering sparse activation
that reduces computational costs while increasing model capacity. However, as
MoE models scale, they need to be distributed across GPU devices, thus face
critical performance bottlenecks due to their large memory footprint. Expert
parallelism distributes experts across GPUs, however, faces key challenges
including an unbalanced token routing and expert activation, resulting in
communication tail latency and processing inefficiencies. While existing
solutions address some of these issues, they fail to resolve the dual
challenges of load imbalance and communication skew. The imbalance in token
processing load across experts causes uneven processing times on different
GPUs, while communication skew between GPUs leads to unbalanced inter-GPU data
transfers. These factors degrade the performance of MoE models by increasing
tail latency and reducing overall throughput. To address these limitations, we
propose an Integer Linear Programming (ILP) formulation to optimize expert
placement by jointly considering token load, communication, and computation
costs. We exploit the property that there is a token routing dependency across
layers, where tokens routed to a specific expert in one layer are likely to be
routed to a limited set of experts in the subsequent layer. Our solution,
MoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU
token routing costs and balances token processing across devices, thereby
reducing tail latency and end-to-end execution time. Experimental results
demonstrate 9.3% and 17.5% of end-to-end speedups for single-node and
multi-node inference respectively, showcasing the potential of our ILP-based
optimization for offering expert parallel solutions for next-generation MoEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Annotation Augmentation Boosts Translation between Molecules
  and Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Zhong, Simon Sataa-Yu Larsen, Haoyu Guo, Tao Tang, Kuangyu Zhou, Davide Mottin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in AI for biological research focus on integrating
molecular data with natural language to accelerate drug discovery. However, the
scarcity of high-quality annotations limits progress in this area. This paper
introduces LA$^3$, a Language-based Automatic Annotation Augmentation framework
that leverages large language models to augment existing datasets, thereby
improving AI training. We demonstrate the effectiveness of LA$^3$ by creating
an enhanced dataset, LaChEBI-20, where we systematically rewrite the
annotations of molecules from an established dataset. These rewritten
annotations preserve essential molecular information while providing more
varied sentence structures and vocabulary. Using LaChEBI-20, we train LaMolT5
based on a benchmark architecture to learn the mapping between molecular
representations and augmented annotations.
  Experimental results on text-based *de novo* molecule generation and molecule
captioning demonstrate that LaMolT5 outperforms state-of-the-art models.
Notably, incorporating LA$^3$ leads to improvements of up to 301% over the
benchmark architecture. Furthermore, we validate the effectiveness of LA$^3$
notable applications in *image*, *text* and *graph* tasks, affirming its
versatility and utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Classification and Anatomical Localization of Tissues in SPECT
  Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Abdul Hafeez Khan, Samuel Morries Boddepalli, Siddhartha Bhattacharyya, Debasis Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate classification and anatomical localization are essential for
effective medical diagnostics and research, which may be efficiently performed
using deep learning techniques. However, availability of limited labeled data
poses a significant challenge. To address this, we adapted Prototypical
Networks and the Propagation-Reconstruction Network (PRNet) for few-shot
classification and localization, respectively, in Single Photon Emission
Computed Tomography (SPECT) images. For the proof of concept we used a
2D-sliced image cropped around heart. The Prototypical Network, with a
pre-trained ResNet-18 backbone, classified ventricles, myocardium, and liver
tissues with 96.67% training and 93.33% validation accuracy. PRNet, adapted for
2D imaging with an encoder-decoder architecture and skip connections, achieved
a training loss of 1.395, accurately reconstructing patches and capturing
spatial relationships. These results highlight the potential of Prototypical
Networks for tissue classification with limited labeled data and PRNet for
anatomical landmark localization, paving the way for improved performance in
deep learning frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Estimation of Pediatric Heart Rate Variability from BOLD-fMRI: A
  Machine Learning Approach Using Dynamic Connectivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdoljalil Addeh, Karen Ardila, Rebecca J Williams, G. Bruce Pike, M. Ethan MacDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many pediatric fMRI studies, cardiac signals are often missing or of poor
quality. A tool to extract Heart Rate Variation (HRV) waveforms directly from
fMRI data, without the need for peripheral recording devices, would be highly
beneficial. We developed a machine learning framework to accurately reconstruct
HRV for pediatric applications. A hybrid model combining one-dimensional
Convolutional Neural Networks (1D-CNN) and Gated Recurrent Units (GRU) analyzed
BOLD signals from 628 ROIs, integrating past and future data. The model
achieved an 8% improvement in HRV accuracy, as evidenced by enhanced
performance metrics. This approach eliminates the need for peripheral
photoplethysmography devices, reduces costs, and simplifies procedures in
pediatric fMRI. Additionally, it improves the robustness of pediatric fMRI
studies, which are more sensitive to physiological and developmental variations
than those in adults.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, ISMSMR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Select before Act: Spatially Decoupled Action Repetition for Continuous
  <span class="highlight-title">Control</span> <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Buqing Nie, Yangqing Fu, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has achieved remarkable success in various
continuous control tasks, such as robot manipulation and locomotion. Different
to mainstream RL which makes decisions at individual steps, recent studies have
incorporated action repetition into RL, achieving enhanced action persistence
with improved sample efficiency and superior performance. However, existing
methods treat all action dimensions as a whole during repetition, ignoring
variations among them. This constraint leads to inflexibility in decisions,
which reduces policy agility with inferior effectiveness. In this work, we
propose a novel repetition framework called SDAR, which implements Spatially
Decoupled Action Repetition through performing closed-loop act-or-repeat
selection for each action dimension individually. SDAR achieves more flexible
repetition strategies, leading to an improved balance between action
persistence and diversity. Compared to existing repetition frameworks, SDAR is
more sample efficient with higher policy performance and reduced action
fluctuation. Experiments are conducted on various continuous control scenarios,
demonstrating the effectiveness of spatially decoupled repetition design
proposed in this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amortized In-Context Bayesian Posterior Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Mittal, Niels Leif Bracher, Guillaume Lajoie, Priyank Jaini, Marcus Brubaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian inference provides a natural way of incorporating prior beliefs and
assigning a probability measure to the space of hypotheses. Current solutions
rely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and
Variational Inference (VI), which need to be re-run whenever new observations
are available. Amortization, through conditional estimation, is a viable
strategy to alleviate such difficulties and has been the guiding principle
behind simulation-based inference, neural processes and in-context methods
using pre-trained models. In this work, we conduct a thorough comparative
analysis of amortized in-context Bayesian posterior estimation methods from the
lens of different optimization objectives and architectural choices. Such
methods train an amortized estimator to perform posterior parameter inference
by conditioning on a set of data examples passed as context to a sequence model
such as a transformer. In contrast to language models, we leverage permutation
invariant architectures as the true posterior is invariant to the ordering of
context examples. Our empirical study includes generalization to
out-of-distribution tasks, cases where the assumed underlying model is
misspecified, and transfer from simulated to real problems. Subsequently, it
highlights the superiority of the reverse KL estimator for predictive problems,
especially when combined with the transformer architecture and normalizing
flows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Release Moment Estimation with Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita P. Kalinin, Jalaj Upadhyay, Christoph H. Lampert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Joint Moment Estimation (JME), a method for continually and
privately estimating both the first and second moments of data with reduced
noise compared to naive approaches. JME uses the matrix mechanism and a joint
sensitivity analysis to allow the second moment estimation with no additional
privacy cost, thereby improving accuracy while maintaining privacy. We
demonstrate JME's effectiveness in two applications: estimating the running
mean and covariance matrix for Gaussian density estimation, and model training
with DP-Adam on CIFAR-10.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffeomorphic Temporal Alignment Nets for Time-series Joint Alignment
  and Averaging <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ron Shapira Weber, Oren Freifeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In time-series analysis, nonlinear temporal misalignment remains a pivotal
challenge that forestalls even simple averaging. Since its introduction, the
Diffeomorphic Temporal Alignment Net (DTAN), which we first introduced (Weber
et al., 2019) and further developed in (Weber & Freifeld, 2023), has proven
itself as an effective solution for this problem (these conference papers are
earlier partial versions of the current manuscript). DTAN predicts and applies
diffeomorphic transformations in an input-dependent manner, thus facilitating
the joint alignment (JA) and averaging of time-series ensembles in an
unsupervised or a weakly-supervised manner. The inherent challenges of the
weakly/unsupervised setting, particularly the risk of trivial solutions through
excessive signal distortion, are mitigated using either one of two distinct
strategies: 1) a regularization term for warps; 2) using the Inverse
Consistency Averaging Error (ICAE). The latter is a novel, regularization-free
approach which also facilitates the JA of variable-length signals. We also
further extend our framework to incorporate multi-task learning (MT-DTAN),
enabling simultaneous time-series alignment and classification. Additionally,
we conduct a comprehensive evaluation of different backbone architectures,
demonstrating their efficacy in time-series alignment tasks. Finally, we
showcase the utility of our approach in enabling Principal Component Analysis
(PCA) for misaligned time-series data. Extensive experiments across 128 UCR
datasets validate the superiority of our approach over contemporary averaging
methods, including both traditional and learning-based approaches, marking a
significant advancement in the field of time-series analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript covers and extends the papers: Diffeomorphic Temporal
  Alignment Nets (DTAN; NeruIPS 2019) and Regularization-free Diffeomorphic
  Temporal Alignment Nets (ICML 2023). Additional contributions: Multi-tasking
  DTAN, PCA-DTAN and more</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hephaestus: Improving Fundamental Agent Capabilities of Large Language
  Models through Continual Pre-Training <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Zhuang, Jingfeng Yang, Haoming Jiang, Xin Liu, Kewei Cheng, Sanket Lokegaonkar, Yifan Gao, Qing Ping, Tianyi Liu, Binxuan Huang, Zheng Li, Zhengyang Wang, Pei Chen, Ruijie Wang, Rongzhi Zhang, Nasser Zalmout, Priyanka Nigam, Bing Yin, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous
agents typically rely on complex prompting or extensive fine-tuning, which
often fails to introduce new capabilities while preserving strong
generalizability. We introduce Hephaestus-Forge, the first large-scale
pre-training corpus designed to enhance the fundamental capabilities of LLM
agents in API function calling, intrinsic reasoning and planning, and adapting
to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data
encompassing 76,537 APIs, including both tool documentation to introduce
knowledge of API functions and function calling trajectories to strengthen
intrinsic reasoning. To explore effective training protocols, we investigate
scaling laws to identify the optimal recipe in data mixing ratios. By continual
pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale
open-source LLMs and rivals commercial LLMs on three agent benchmarks,
demonstrating the effectiveness of our pre-training corpus in enhancing
fundamental agentic capabilities and generalization of LLMs to new tasks or
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ evclust: Python library for evidential clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armel Soubeiga, Violaine Antoine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A recent developing trend in clustering is the advancement of algorithms that
not only identify clusters within data, but also express and capture the
uncertainty of cluster membership. Evidential clustering addresses this by
using the Dempster-Shafer theory of belief functions, a framework designed to
manage and represent uncertainty. This approach results in a credal partition,
a structured set of mass functions that quantify the uncertain assignment of
each object to potential groups. The Python framework evclust, presented in
this paper, offers a suite of efficient evidence clustering algorithms as well
as tools for visualizing, evaluating and analyzing credal partitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures, Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning based Triggering Function for Early
  Classifiers of Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aurélien Renault, Alexis Bondu, Antoine Cornuéjols, Vincent Lemaire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early Classification of Time Series (ECTS) has been recognized as an
important problem in many areas where decisions have to be taken as soon as
possible, before the full data availability, while time pressure increases.
Numerous ECTS approaches have been proposed, based on different triggering
functions, each taking into account various pieces of information related to
the incoming time series and/or the output of a classifier. Although their
performances have been empirically compared in the literature, no studies have
been carried out on the optimality of these triggering functions that involve
``man-tailored'' decision rules. Based on the same information, could there be
better triggering functions? This paper presents one way to investigate this
question by showing first how to translate ECTS problems into Reinforcement
Learning (RL) ones, where the very same information is used in the state space.
A thorough comparison of the performance obtained by ``handmade'' approaches
and their ``RL-based'' counterparts has been carried out. A second question
investigated in this paper is whether a different combination of information,
defining the state space in RL systems, can achieve even better performance.
Experiments show that the system we describe, called \textsc{Alert},
significantly outperforms its state-of-the-art competitors on a large number of
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Minimal Search Space for Conditional Causal Bandits <span class="chip">ICML2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco N. F. Q. Simoes, Itai Feigenbaum, Mehdi Dastani, Thijs van Ommen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal knowledge can be used to support decision-making problems. This has
been recognized in the causal bandits literature, where a causal (multi-armed)
bandit is characterized by a causal graphical model and a target variable. The
arms are then interventions on the causal model, and rewards are samples of the
target variable. Causal bandits were originally studied with a focus on hard
interventions. We focus instead on cases where the arms are conditional
interventions, which more accurately model many real-world decision-making
problems by allowing the value of the intervened variable to be chosen based on
the observed values of other variables. This paper presents a graphical
characterization of the minimal set of nodes guaranteed to contain the optimal
conditional intervention, which maximizes the expected reward. We then propose
an efficient algorithm with a time complexity of $O(|V| + |E|)$ to identify
this minimal set of nodes. We prove that the graphical characterization and the
proposed algorithm are correct. Finally, we empirically demonstrate that our
algorithm significantly prunes the search space and substantially accelerates
convergence rates when integrated into standard multi-armed bandit algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICML2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Red Teaming: Breaking Policies Without Breaking Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudha Majumdar, Mohit Sharma, Dmitry Kalashnikov, Sumeet Singh, Pierre Sermanet, Vikas Sindhwani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visuomotor policies trained via imitation learning are capable of performing
challenging manipulation tasks, but are often extremely brittle to lighting,
visual distractors, and object locations. These vulnerabilities can depend
unpredictably on the specifics of training, and are challenging to expose
without time-consuming and expensive hardware evaluations. We propose the
problem of predictive red teaming: discovering vulnerabilities of a policy with
respect to environmental factors, and predicting the corresponding performance
degradation without hardware evaluations in off-nominal scenarios. In order to
achieve this, we develop RoboART: an automated red teaming (ART) pipeline that
(1) modifies nominal observations using generative image editing to vary
different environmental factors, and (2) predicts performance under each
variation using a policy-specific anomaly detector executed on edited
observations. Experiments across 500+ hardware trials in twelve off-nominal
conditions for visuomotor diffusion policies demonstrate that RoboART predicts
performance degradation with high accuracy (less than 0.19 average difference
between predicted and real success rates). We also demonstrate how predictive
red teaming enables targeted data collection: fine-tuning with data collected
under conditions predicted to be adverse boosts baseline performance by 2-7x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Impact of the Utility in Semivalue-based Data Valuation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélissa Tamine, Benjamin Heymann, Patrick Loiseau, Maxime Vono
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semivalue-based data valuation in machine learning (ML) quantifies the
contribution of individual data points to a downstream ML task by leveraging
principles from cooperative game theory and the notion of utility. While this
framework has been used in practice for assessing data quality, our experiments
reveal inconsistent valuation outcomes across different utilities, albeit all
related to ML performance. Beyond raising concerns about the reliability of
data valuation, this inconsistency is challenging to interpret, as it stems
from the complex interaction of the utility with data points and semivalue
weights, which has barely been studied in prior work. In this paper, we take a
first step toward clarifying the utility impact on semivalue-based data
valuation. Specifically, we provide geometric interpretations of this impact
for a broad family of classification utilities, which includes the accuracy and
the arithmetic mean. We introduce the notion of spatial signatures: given a
semivalue, data points can be embedded into a two-dimensional space, and
utility functions map to the dual of this space. This geometric perspective
separates the influence of the dataset and semivalue from that of the utility,
providing a theoretical explanation for the experimentally observed sensitivity
of valuation outcomes to the utility choice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Membership Inference Risks in Quantized Models: A Theoretical and
  Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Aubinais, Philippe Formont, Pablo Piantanida, Elisabeth Gassiat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantizing machine learning models has demonstrated its effectiveness in
lowering memory and inference costs while maintaining performance levels
comparable to the original models. In this work, we investigate the impact of
quantization procedures on the privacy of data-driven models, specifically
focusing on their vulnerability to membership inference attacks. We derive an
asymptotic theoretical analysis of Membership Inference Security (MIS),
characterizing the privacy implications of quantized algorithm weights against
the most powerful (and possibly unknown) attacks. Building on these theoretical
insights, we propose a novel methodology to empirically assess and rank the
privacy levels of various quantization procedures. Using synthetic datasets, we
demonstrate the effectiveness of our approach in assessing the MIS of different
quantizers. Furthermore, we explore the trade-off between privacy and
performance using real-world data and models in the context of molecular
modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging GPT-4o Efficiency for Detecting Rework Anomaly in Business
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Derakhshan, Paolo Ceravolo, Fatemeh Mohammadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the effectiveness of GPT-4o-2024-08-06, one of the
Large Language Models (LLM) from OpenAI, in detecting business process
anomalies, with a focus on rework anomalies. In our study, we developed a
GPT-4o-based tool capable of transforming event logs into a structured format
and identifying reworked activities within business event logs. The analysis
was performed on a synthetic dataset designed to contain rework anomalies but
free of loops. To evaluate the anomaly detection capabilities of GPT
4o-2024-08-06, we used three prompting techniques: zero-shot, one-shot, and
few-shot. These techniques were tested on different anomaly distributions,
namely normal, uniform, and exponential, to identify the most effective
approach for each case. The results demonstrate the strong performance of
GPT-4o-2024-08-06. On our dataset, the model achieved 96.14% accuracy with
one-shot prompting for the normal distribution, 97.94% accuracy with few-shot
prompting for the uniform distribution, and 74.21% accuracy with few-shot
prompting for the exponential distribution. These results highlight the model's
potential as a reliable tool for detecting rework anomalies in event logs and
how anomaly distribution and prompting strategy influence the model's
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 images, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Scatter Matrix Estimation for Elliptical Distributions in
  Polynomial Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Novikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of computationally efficient robust estimation of
scatter matrices of elliptical distributions under the strong contamination
model. We design polynomial time algorithms that achieve dimension-independent
error in Frobenius norm.
  Our first result is a sequence of efficient algorithms that approaches nearly
optimal error. Specifically, under a mild assumption on the eigenvalues of the
scatter matrix $\Sigma$, for every $t \in \mathbb{N}$, we design an estimator
that, given $n = d^{O(t)}$ samples, in time $n^{O(t)}$ finds $\hat{\Sigma}$
such that $ \Vert{\Sigma^{-1/2}\, ({\hat{\Sigma} - \Sigma})\,
\Sigma^{-1/2}}\Vert_{\text{F}} \le O(t \cdot \varepsilon^{1-\frac{1}{t}})$,
where $\varepsilon$ is the fraction of corruption. We do not require any
assumptions on the moments of the distribution, while all previously known
computationally efficient algorithms for robust covariance/scatter estimation
with dimension-independent error rely on strong assumptions on the moments,
such as sub-Gaussianity or (certifiable) hypercontractivity.
  Furthermore, under a stronger assumption on the eigenvalues of $\Sigma$
(that, in particular, is satisfied by all matrices with constant condition
number),
  we provide a fast (sub-quadratic in the input size) algorithm that, given
nearly optimal number of samples $n = \tilde{O}(d^2/\varepsilon)$, in time
$\tilde{O}({nd^2 poly(1/\varepsilon)})$ finds $\hat{\Sigma}$ such that
$\Vert\hat{\Sigma} - \Sigma\Vert_{\text{F}} \le O(\Vert{\Sigma}\Vert \cdot
\sqrt{\varepsilon})$.
  Our approach is based on robust covariance estimation of the spatial sign
(the projection onto the sphere of radius $\sqrt{d}$) of elliptical
distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is API Access to LLMs Useful for Generating Private Synthetic Tabular
  Data? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marika Swanberg, Ryan McKenna, Edo Roth, Albert Cheu, Peter Kairouz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially private (DP) synthetic data is a versatile tool for enabling
the analysis of private data. Recent advancements in large language models
(LLMs) have inspired a number of algorithm techniques for improving DP
synthetic data generation. One family of approaches uses DP finetuning on the
foundation model weights; however, the model weights for state-of-the-art
models may not be public. In this work we propose two DP synthetic tabular data
algorithms that only require API access to the foundation model. We adapt the
Private Evolution algorithm (Lin et al., 2023; Xie et al., 2024) -- which was
designed for image and text data -- to the tabular data domain. In our
extension of Private Evolution, we define a query workload-based distance
measure, which may be of independent interest. We propose a family of
algorithms that use one-shot API access to LLMs, rather than adaptive queries
to the LLM. Our findings reveal that API-access to powerful LLMs does not
always improve the quality of DP synthetic data compared to established
baselines that operate without such access. We provide insights into the
underlying reasons and propose improvements to LLMs that could make them more
effective for this application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation and Regularization for Learning Group Equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oskar Nordenfors, Axel Flinth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many machine learning tasks, known symmetries can be used as an inductive
bias to improve model performance. In this paper, we consider learning group
equivariance through training with data augmentation. We summarize results from
a previous paper of our own, and extend the results to show that equivariance
of the trained model can be achieved through training on augmented data in
tandem with regularization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Krum Federated Chain (KFC): Using blockchain to defend against
  adversarial attacks in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario García-Márquez, Nuria Rodríguez-Barroso, M. Victoria Luzón, Francisco Herrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning presents a nascent approach to machine learning, enabling
collaborative model training across decentralized devices while safeguarding
data privacy. However, its distributed nature renders it susceptible to
adversarial attacks. Integrating blockchain technology with Federated Learning
offers a promising avenue to enhance security and integrity. In this paper, we
tackle the potential of blockchain in defending Federated Learning against
adversarial attacks. First, we test Proof of Federated Learning, a well known
consensus mechanism designed ad-hoc to federated contexts, as a defense
mechanism demonstrating its efficacy against Byzantine and backdoor attacks
when at least one miner remains uncompromised. Second, we propose Krum
Federated Chain, a novel defense strategy combining Krum and Proof of Federated
Learning, valid to defend against any configuration of Byzantine or backdoor
attacks, even when all miners are compromised. Our experiments conducted on
image classification datasets validate the effectiveness of our proposed
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Neural Networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dimension-free Regret for Learning Asymmetric Linear Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annie Marsden, Elad Hazan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previously, methods for learning marginally stable linear dynamical systems
either required the transition matrix to be symmetric or incurred regret bounds
that scale polynomially with the system's hidden dimension. In this work, we
introduce a novel method that overcomes this trade-off, achieving
dimension-free regret despite the presence of asymmetric matrices and marginal
stability. Our method combines spectral filtering with linear predictors and
employs Chebyshev polynomials in the complex plane to construct a novel
spectral filtering basis. This construction guarantees sublinear regret in an
online learning framework, without relying on any statistical or generative
assumptions. Specifically, we prove that as long as the transition matrix has
eigenvalues with complex component bounded by $1/\mathrm{poly} \log T$, then
our method achieves regret $\tilde{O}(T^{9/10})$ when compared to the best
linear dynamical predictor in hindsight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequence Transferability and Task Order Selection in Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thinh Nguyen, Cuong N. Nguyen, Quang Pham, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Cuong V. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In continual learning, understanding the properties of task sequences and
their relationships to model performance is important for developing advanced
algorithms with better accuracy. However, efforts in this direction remain
underdeveloped despite encouraging progress in methodology development. In this
work, we investigate the impacts of sequence transferability on continual
learning and propose two novel measures that capture the total transferability
of a task sequence, either in the forward or backward direction. Based on the
empirical properties of these measures, we then develop a new method for the
task order selection problem in continual learning. Our method can be shown to
offer a better performance than the conventional strategy of random task
selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-efficient Learning of Concepts with Theoretical Guarantees: from
  Data to Concepts without Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hidde Fokkema, Tim van Erven, Sara Magliacane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is a vital part of many real-world systems, but several
concerns remain about the lack of interpretability, explainability and
robustness of black-box AI systems. Concept-based models (CBM) address some of
these challenges by learning interpretable concepts from high-dimensional data,
e.g. images, which are used to predict labels. An important issue in CBMs is
concept leakage, i.e., spurious information in the learned concepts, which
effectively leads to learning "wrong" concepts. Current mitigating strategies
are heuristic, have strong assumptions, e.g., they assume that the concepts are
statistically independent of each other, or require substantial human
interaction in terms of both interventions and labels provided by annotators.
In this paper, we describe a framework that provides theoretical guarantees on
the correctness of the learned concepts and on the number of required labels,
without requiring any interventions. Our framework leverages causal
representation learning (CRL) to learn high-level causal variables from
low-level data, and learns to align these variables with interpretable
concepts. We propose a linear and a non-parametric estimator for this mapping,
providing a finite-sample high probability result in the linear case and an
asymptotic consistency result for the non-parametric estimator. We implement
our framework with state-of-the-art CRL methods, and show its efficacy in
learning the correct concepts in synthetic and image benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 16 figures, 9 Tables, Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ignore the KL Penalty! Boosting Exploration on Critical Tokens to
  Enhance RL Fine-Tuning <span class="chip">NAACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Vassoyan, Nathanaël Beau, Roman Plaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to achieve long-term goals is a key challenge in the current
development of large language models (LLMs). To address this, pre-trained LLMs
can be fine-tuned with reinforcement learning (RL) to explore solutions that
optimize a given goal. However, exploration with LLMs is difficult, as a
balance has to be struck between discovering new solutions and staying close
enough to the pre-trained model, so as not to degrade basic capabilities. This
is typically controlled with a Kullback-Leibler (KL) penalty. In this paper, we
investigate the exploration dynamics of a small language model on a simple
arithmetic task. We show how varying degrees of pre-training influence
exploration and demonstrate the importance of "critical tokens" which have a
dramatic impact on the final outcome. Consequently, we introduce a simple
modification to the KL penalty that favors exploration on critical tokens,
increasing the efficiency of the RL fine-tuning stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 5 tables. Accepted for publication in the
  Findings of the North American Chapter of the Association for Computational
  Linguistics (NAACL) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Properties of Wasserstein Gradient Flows for the Sliced-Wasserstein
  Distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christophe Vauthier, Quentin Mérigot, Anna Korba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the properties of the Sliced Wasserstein
Distance (SW) when employed as an objective functional. The SW metric has
gained significant interest in the optimal transport and machine learning
literature, due to its ability to capture intricate geometric properties of
probability distributions while remaining computationally tractable, making it
a valuable tool for various applications, including generative modeling and
domain adaptation. Our study aims to provide a rigorous analysis of the
critical points arising from the optimization of the SW objective. By computing
explicit perturbations, we establish that stable critical points of SW cannot
concentrate on segments. This stability analysis is crucial for understanding
the behaviour of optimization algorithms for models trained using the SW
objective. Furthermore, we investigate the properties of the SW objective,
shedding light on the existence and convergence behavior of critical points. We
illustrate our theoretical results through numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32p</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soobin Um, Beomsu Kim, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minority samples are underrepresented instances located in low-density
regions of a data manifold, and are valuable in many generative AI
applications, such as data augmentation, creative content generation, etc.
Unfortunately, existing diffusion-based minority generators often rely on
computationally expensive guidance dedicated for minority generation. To
address this, here we present a simple yet powerful guidance-free approach
called Boost-and-Skip for generating minority samples using diffusion models.
The key advantage of our framework requires only two minimal changes to
standard generative processes: (i) variance-boosted initialization and (ii)
timestep skipping. We highlight that these seemingly-trivial modifications are
supported by solid theoretical and empirical evidence, thereby effectively
promoting emergence of underrepresented minority features. Our comprehensive
experiments demonstrate that Boost-and-Skip greatly enhances the capability of
generating minority samples, even rivaling guidance-based state-of-the-art
approaches while requiring significantly fewer computations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Based Offline Reinforcement Learning with Reliability-Guaranteed
  Sequence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based offline reinforcement learning (MORL) aims to learn a policy by
exploiting a dynamics model derived from an existing dataset. Applying
conservative quantification to the dynamics model, most existing works on MORL
generate trajectories that approximate the real data distribution to facilitate
policy learning by using current information (e.g., the state and action at
time step $t$). However, these works neglect the impact of historical
information on environmental dynamics, leading to the generation of unreliable
trajectories that may not align with the real data distribution. In this paper,
we propose a new MORL algorithm \textbf{R}eliability-guaranteed
\textbf{T}ransformer (RT), which can eliminate unreliable trajectories by
calculating the cumulative reliability of the generated trajectory (i.e., using
a weighted variational distance away from the real data). Moreover, by sampling
candidate actions with high rewards, RT can efficiently generate high-return
trajectories from the existing offline data. We theoretically prove the
performance guarantees of RT in policy learning, and empirically demonstrate
its effectiveness against state-of-the-art model-based methods on several
benchmark tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WyckoffDiff - A Generative Diffusion Model for Crystal Symmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Ekström Kelvinius, Oskar B. Andersson, Abhijith S. Parackal, Dong Qian, Rickard Armiento, Fredrik Lindsten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crystalline materials often exhibit a high level of symmetry. However, most
generative models do not account for symmetry, but rather model each atom
without any constraints on its position or element. We propose a generative
model, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based
descriptions of crystals. This is enabled by considering a crystal structure
representation that encodes all symmetry, and we design a novel neural network
architecture which enables using this representation inside a discrete
generative model framework. In addition to respecting symmetry by construction,
the discrete nature of our model enables fast generation. We additionally
present a new metric, Fr\'echet Wrenformer Distance, which captures the
symmetry aspects of the materials generated, and we benchmark WyckoffDiff
against recently proposed generative models for crystal generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logarithmic Regret of Exploration in Average Reward Markov Decision
  Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Boone, Bruno Gaujal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In average reward Markov decision processes, state-of-the-art algorithms for
regret minimization follow a well-established framework: They are model-based,
optimistic and episodic. First, they maintain a confidence region from which
optimistic policies are computed using a well-known subroutine called Extended
Value Iteration (EVI). Second, these policies are used over time windows called
episodes, each ended by the Doubling Trick (DT) rule or a variant thereof. In
this work, without modifying EVI, we show that there is a significant advantage
in replacing (DT) by another simple rule, that we call the Vanishing
Multiplicative (VM) rule. When managing episodes with (VM), the algorithm's
regret is, both in theory and in practice, as good if not better than with
(DT), while the one-shot behavior is greatly improved. More specifically, the
management of bad episodes (when sub-optimal policies are being used) is much
better under (VM) than (DT) by making the regret of exploration logarithmic
rather than linear. These results are made possible by a new in-depth
understanding of the contrasting behaviors of confidence regions during good
and bad episodes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-dimensional Functions are Efficiently Learnable under Randomly
  Biased Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisabetta Cornacchia, Dan Mikulincer, Elchanan Mossel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of learning single index and multi index models has gained
significant interest as a fundamental task in high-dimensional statistics. Many
recent works have analysed gradient-based methods, particularly in the setting
of isotropic data distributions, often in the context of neural network
training. Such studies have uncovered precise characterisations of algorithmic
sample complexity in terms of certain analytic properties of the target
function, such as the leap, information, and generative exponents. These
properties establish a quantitative separation between low and high complexity
learning tasks. In this work, we show that high complexity cases are rare.
Specifically, we prove that introducing a small random perturbation to the data
distribution--via a random shift in the first moment--renders any Gaussian
single index model as easy to learn as a linear function. We further extend
this result to a class of multi index models, namely sparse Boolean functions,
also known as Juntas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing software for non-discrimination: an updated and extended audit
  in the Italian car insurance domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Rondina, Antonio Vetrò, Riccardo Coppola, Oumaima Regragrui, Alessandro Fabris, Gianmaria Silvello, Gian Antonio Susto, Juan Carlos De Martin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context. As software systems become more integrated into society's
infrastructure, the responsibility of software professionals to ensure
compliance with various non-functional requirements increases. These
requirements include security, safety, privacy, and, increasingly,
non-discrimination.
  Motivation. Fairness in pricing algorithms grants equitable access to basic
services without discriminating on the basis of protected attributes.
  Method. We replicate a previous empirical study that used black box testing
to audit pricing algorithms used by Italian car insurance companies, accessible
through a popular online system. With respect to the previous study, we
enlarged the number of tests and the number of demographic variables under
analysis.
  Results. Our work confirms and extends previous findings, highlighting the
problematic permanence of discrimination across time: demographic variables
significantly impact pricing to this day, with birthplace remaining the main
discriminatory factor against individuals not born in Italian cities. We also
found that driver profiles can determine the number of quotes available to the
user, denying equal opportunities to all.
  Conclusion. The study underscores the importance of testing for
non-discrimination in software systems that affect people's everyday lives.
Performing algorithmic audits over time makes it possible to evaluate the
evolution of such algorithms. It also demonstrates the role that empirical
software engineering can play in making software systems more accountable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FEMBA: Efficient and Scalable EEG Analysis with a Bidirectional Mamba
  Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Tegon, Thorir Mar Ingolfsson, Xiaying Wang, Luca Benini, Yawei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and efficient electroencephalography (EEG) analysis is essential for
detecting seizures and artifacts in long-term monitoring, with applications
spanning hospital diagnostics to wearable health devices. Robust EEG analytics
have the potential to greatly improve patient care. However, traditional deep
learning models, especially Transformer-based architectures, are hindered by
their quadratic time and memory complexity, making them less suitable for
resource-constrained environments. To address these challenges, we present
FEMBA (Foundational EEG Mamba + Bidirectional Architecture), a novel
self-supervised framework that establishes new efficiency benchmarks for EEG
analysis through bidirectional state-space modeling. Unlike Transformer-based
models, which incur quadratic time and memory complexity, FEMBA scales linearly
with sequence length, enabling more scalable and efficient processing of
extended EEG recordings. Trained on over 21,000 hours of unlabeled EEG and
fine-tuned on three downstream tasks, FEMBA achieves competitive performance in
comparison with transformer models, with significantly lower computational
cost. Specifically, it reaches 81.82% balanced accuracy (0.8921 AUROC) on TUAB
and 0.949 AUROC on TUAR, while a tiny 7.8M-parameter variant demonstrates
viability for resource-constrained devices. These results pave the way for
scalable, general-purpose EEG analytics in both clinical and highlight FEMBA as
a promising candidate for wearable applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, 5 tables, pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Large-scale <span class="highlight-title">Dataset</span> Compression: Shifting Focus From Labels
  to Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingao Xiao, Songhua Liu, Yang He, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation and dataset pruning are two prominent techniques for
compressing datasets to improve computational and storage efficiency. Despite
their overlapping objectives, these approaches are rarely compared directly.
Even within each field, the evaluation protocols are inconsistent across
various methods, which complicates fair comparisons and hinders
reproducibility. Considering these limitations, we introduce in this paper a
benchmark that equitably evaluates methodologies across both distillation and
pruning literatures. Notably, our benchmark reveals that in the mainstream
dataset distillation setting for large-scale datasets, which heavily rely on
soft labels from pre-trained models, even randomly selected subsets can achieve
surprisingly competitive performance. This finding suggests that an
overemphasis on soft labels may be diverting attention from the intrinsic value
of the image data, while also imposing additional burdens in terms of
generation, storage, and application. To address these issues, we propose a new
framework for dataset compression, termed Prune, Combine, and Augment (PCA),
which focuses on leveraging image data exclusively, relies solely on hard
labels for evaluation, and achieves state-of-the-art performance in this setup.
By shifting the emphasis back to the images, our benchmark and PCA framework
pave the way for more balanced and accessible techniques in dataset compression
research. Our code is available at:
https://github.com/ArmandXiao/Rethinking-Dataset-Compression
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work In Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyper Compressed Fine-Tuning of Large Foundation Models with Quantum
  Inspired Adapters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Snehal Raj, Brian Coyle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained large foundation models for specific tasks has become
increasingly challenging due to the computational and storage demands
associated with full parameter updates. Parameter-Efficient Fine-Tuning (PEFT)
methods address this issue by updating only a small subset of model parameters
using adapter modules. In this work, we propose \emph{Quantum-Inspired
Adapters}, a PEFT approach inspired by Hamming-weight preserving quantum
circuits from quantum machine learning literature. These models can be both
expressive and parameter-efficient by operating in a combinatorially large
space while simultaneously preserving orthogonality in weight parameters. We
test our proposed adapters by adapting large language models and large vision
transformers on benchmark datasets. Our method can achieve 99.2\% of the
performance of existing fine-tuning methods such LoRA with a 44x parameter
compression on language understanding datasets like GLUE and VTAB. Compared to
existing orthogonal fine-tuning methods such as OFT or BOFT, we achieve 98\%
relative performance with 25x fewer parameters. This demonstrates competitive
performance paired with a significant reduction in trainable parameters.
Through ablation studies, we determine that combining multiple Hamming-weight
orders with orthogonality and matrix compounding are essential for performant
fine-tuning. Our findings suggest that Quantum-Inspired Adapters offer a
promising direction for efficient adaptation of language and vision models in
resource-constrained environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CS-SHAP: Extending SHAP to Cyclic-Spectral Domain for Better
  Interpretability of Intelligent Fault Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Chen, Xingjian Dong, Kui Hu, Kangkang Chen, Zhike Peng, Guang Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks (NNs), with their powerful nonlinear mapping and end-to-end
capabilities, are widely applied in mechanical intelligent fault diagnosis
(IFD). However, as typical black-box models, they pose challenges in
understanding their decision basis and logic, limiting their deployment in
high-reliability scenarios. Hence, various methods have been proposed to
enhance the interpretability of IFD. Among these, post-hoc approaches can
provide explanations without changing model architecture, preserving its
flexibility and scalability. However, existing post-hoc methods often suffer
from limitations in explanation forms. They either require preprocessing that
disrupts the end-to-end nature or overlook fault mechanisms, leading to
suboptimal explanations. To address these issues, we derived the
cyclic-spectral (CS) transform and proposed the CS-SHAP by extending Shapley
additive explanations (SHAP) to the CS domain. CS-SHAP can evaluate
contributions from both carrier and modulation frequencies, aligning more
closely with fault mechanisms and delivering clearer and more accurate
explanations. Three datasets are utilized to validate the superior
interpretability of CS-SHAP, ensuring its correctness, reproducibility, and
practical performance. With open-source code and outstanding interpretability,
CS-SHAP has the potential to be widely adopted and become the post-hoc
interpretability benchmark in IFD, even in other classification tasks. The code
is available on https://github.com/ChenQian0618/CS-SHAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Summarized Data: Gaussian Process Regression with Sample
  Quasi-Likelihood <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17455v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17455v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Shikuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian process regression is a powerful Bayesian nonlinear regression
method. Recent research has enabled the capture of many types of observations
using non-Gaussian likelihoods. To deal with various tasks in spatial modeling,
we benefit from this development. Difficulties still arise when we can only
access summarized data consisting of representative features, summary
statistics, and data point counts. Such situations frequently occur primarily
due to concerns about confidentiality and management costs associated with
spatial data. This study tackles learning and inference using only summarized
data within the framework of Gaussian process regression. To address this
challenge, we analyze the approximation errors in the marginal likelihood and
posterior distribution that arise from utilizing representative features. We
also introduce the concept of sample quasi-likelihood, which facilitates
learning and inference using only summarized data. Non-Gaussian likelihoods
satisfying certain assumptions can be captured by specifying a variance
function that characterizes a sample quasi-likelihood function. Theoretical and
experimental results demonstrate that the approximation performance is
influenced by the granularity of summarized data relative to the length scale
of covariance functions. Experiments on a real-world dataset highlight the
practicality of our method for spatial modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures, 5 tables, AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Introspective Planning: Aligning Robots' Uncertainty with Inherent Task
  Ambiguity <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06529v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06529v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiqu Liang, Zixu Zhang, Jaime Fernández Fisac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit advanced reasoning skills, enabling
robots to comprehend natural language instructions and strategically plan
high-level actions through proper grounding. However, LLM hallucination may
result in robots confidently executing plans that are misaligned with user
goals or even unsafe in critical scenarios. Additionally, inherent ambiguity in
natural language instructions can introduce uncertainty into the LLM's
reasoning and planning processes.We propose introspective planning, a
systematic approach that align LLM's uncertainty with the inherent ambiguity of
the task. Our approach constructs a knowledge base containing introspective
reasoning examples as post-hoc rationalizations of human-selected safe and
compliant plans, which are retrieved during deployment. Evaluations on three
tasks, including a newly introduced safe mobile manipulation benchmark,
demonstrate that introspection substantially improves both compliance and
safety over state-of-the-art LLM-based planning methods. Furthermore, we
empirically show that introspective planning, in combination with conformal
prediction, achieves tighter confidence bounds, maintaining statistical success
guarantees while minimizing unnecessary user clarification requests. The
webpage and code are accessible at https://introplan.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InvestESG: A multi-agent reinforcement learning benchmark for studying
  climate investment as a social dilemma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09856v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09856v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Hou, Jiayi Yuan, Joel Z. Leibo, Natasha Jaques
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark
designed to study the impact of Environmental, Social, and Governance (ESG)
disclosure mandates on corporate climate investments. The benchmark models an
intertemporal social dilemma where companies balance short-term profit losses
from climate mitigation efforts and long-term benefits from reducing climate
risk, while ESG-conscious investors attempt to influence corporate behavior
through their investment decisions. Companies allocate capital across
mitigation, greenwashing, and resilience, with varying strategies influencing
climate outcomes and investor preferences. We are releasing open-source
versions of InvestESG in both PyTorch and JAX, which enable scalable and
hardware-accelerated simulations for investigating competing incentives in
mitigate climate change. Our experiments show that without ESG-conscious
investors with sufficient capital, corporate mitigation efforts remain limited
under the disclosure mandate. However, when a critical mass of investors
prioritizes ESG, corporate cooperation increases, which in turn reduces climate
risks and enhances long-term financial stability. Additionally, providing more
information about global climate risks encourages companies to invest more in
mitigation, even without investor involvement. Our findings align with
empirical research using real-world data, highlighting MARL's potential to
inform policy by providing insights into large-scale socio-economic challenges
through efficient testing of alternative policy and market designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinith M. Suriyakumar, Rohan Alur, Ayush Sekhari, Manish Raghavan, Ashia C. Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models rely on massive, web-scale datasets. Training
them from scratch is computationally expensive, and as a result, developers
often prefer to make incremental updates to existing models. These updates
often compose fine-tuning steps (to learn new concepts or improve model
performance) with "unlearning" steps (to "forget" existing concepts, such as
copyrighted works or explicit content). In this work, we demonstrate a critical
and previously unknown vulnerability that arises in this paradigm: even under
benign, non-adversarial conditions, fine-tuning a text-to-image diffusion model
on seemingly unrelated images can cause it to "relearn" concepts that were
previously "unlearned." We comprehensively investigate the causes and scope of
this phenomenon, which we term concept resurgence, by performing a series of
experiments which compose "concept unlearning" with subsequent fine-tuning of
Stable Diffusion v1.4 and Stable Diffusion v2.1. Our findings underscore the
fragility of composing incremental model updates, and raise serious new
concerns about current approaches to ensuring the safety and alignment of
text-to-image diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Risk Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyat Mahajan, Mohammad Pezeshki, Charles Arnal, Ioannis Mitliagkas, Kartik Ahuja, Pascal Vincent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional generalization is a crucial step towards developing
data-efficient intelligent machines that generalize in human-like ways. In this
work, we tackle a challenging form of distribution shift, termed compositional
shift, where some attribute combinations are completely absent at training but
present in the test distribution. This shift tests the model's ability to
generalize compositionally to novel attribute combinations in discriminative
tasks. We model the data with flexible additive energy distributions, where
each energy term represents an attribute, and derive a simple alternative to
empirical risk minimization termed compositional risk minimization (CRM). We
first train an additive energy classifier to predict the multiple attributes
and then adjust this classifier to tackle compositional shifts. We provide an
extensive theoretical analysis of CRM, where we show that our proposal
extrapolates to special affine hulls of seen attribute combinations. Empirical
evaluations on benchmark datasets confirms the improved robustness of CRM
compared to other methods from the literature designed to tackle various forms
of subpopulation shifts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Assimilation with Machine Learning Surrogate Models: A Case Study
  with FourCastNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melissa Adrian, Daniel Sanz-Alonso, Rebecca Willett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern data-driven surrogate models for weather forecasting provide accurate
short-term predictions but inaccurate and nonphysical long-term forecasts. This
paper investigates online weather prediction using machine learning surrogates
supplemented with partial and noisy observations. We empirically demonstrate
and theoretically justify that, despite the long-time instability of the
surrogates and the sparsity of the observations, filtering estimates can remain
accurate in the long-time horizon. As a case study, we integrate FourCastNet, a
weather surrogate model, within a variational data assimilation framework using
partial, noisy ERA5 data. Our results show that filtering estimates remain
accurate over a year-long assimilation window and provide effective initial
conditions for forecasting tasks, including extreme event prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Universal Neural Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00133v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00133v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Geuter, Gregor Kornhardt, Ingimar Tomasson, Vaios Laschos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal Transport (OT) problems are a cornerstone of many applications, but
solving them is computationally expensive. To address this problem, we propose
UNOT (Universal Neural Optimal Transport), a novel framework capable of
accurately predicting (entropic) OT distances and plans between discrete
measures of variable resolution for a given cost function. UNOT builds on
Fourier Neural Operators, a universal class of neural networks that map between
function spaces and that are discretization-invariant, which enables our
network to process measures of varying sizes. The network is trained
adversarially using a second, generating network and a self-supervised
bootstrapping loss. We theoretically justify the use of FNOs, prove that our
generator is universal, and that minimizing the bootstrapping loss provably
minimizes the ground truth loss. Through extensive experiments, we show that
our network not only accurately predicts optimal transport distances and plans
across a wide range of datasets, but also captures the geometry of the
Wasserstein space correctly. Furthermore, we show that our network can be used
as a state-of-the-art initialization for the Sinkhorn algorithm, significantly
outperforming existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reranking Laws for Language Generation: A Communication-Theoretic
  Perspective <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        António Farinhas, Haau-Sing Li, André F. T. Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure large language models (LLMs) are used safely, one must reduce their
propensity to hallucinate or to generate unacceptable answers. A simple and
often used strategy is to first let the LLM generate multiple hypotheses and
then employ a reranker to choose the best one. In this paper, we draw a
parallel between this strategy and the use of redundancy to decrease the error
rate in noisy communication channels. We conceptualize the generator as a
sender transmitting multiple descriptions of a message through parallel noisy
channels. The receiver decodes the message by ranking the (potentially
corrupted) descriptions and selecting the one found to be most reliable. We
provide conditions under which this protocol is asymptotically error-free
(i.e., yields an acceptable answer almost surely) even in scenarios where the
reranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the
channel distributions are statistically dependent. We use our framework to
obtain reranking laws which we validate empirically on two real-world tasks
using LLMs: text-to-code generation with DeepSeek-Coder 7B and machine
translation of medical data with TowerInstruct 13B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Focus On This, Not That! Steering LLMs With Adaptive Feature
  Specification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22944v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22944v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom A. Lamb, Adam Davies, Alasdair Paren, Philip H. S. Torr, Francesco Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of Instruction Tuning (IT) in training large language
models (LLMs) to perform arbitrary user-specified tasks, these models often
still leverage spurious or biased features learned from their training data,
leading to undesired behaviours when deploying them in new contexts. In this
work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to
condition their responses by focusing on specific features whilst ignoring
others, leading to different behaviours based on what features are specified.
Across several experimental settings, we show that focus-tuned models can be
adaptively steered by focusing on different features at inference-time: for
instance, robustness can be improved by focusing on task-causal features and
ignoring spurious features, and social bias can be mitigated by ignoring
demographic categories. Furthermore, FIT can steer behaviour in new contexts,
generalising under distribution shift and to new unseen features at inference
time, and thereby facilitating more robust, fair, and controllable LLM
applications in real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlearning-based Neural Interpretations <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Lam Choi, Alexandre Duplessis, Serge Belongie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient-based interpretations often require an anchor point of comparison to
avoid saturation in computing feature importance. We show that current
baselines defined using static functions--constant mapping, averaging or
blurring--inject harmful colour, texture or frequency assumptions that deviate
from model behaviour. This leads to accumulation of irregular gradients,
resulting in attribution maps that are biased, fragile and manipulable.
Departing from the static approach, we propose UNI to compute an (un)learnable,
debiased and adaptive baseline by perturbing the input towards an unlearning
direction of steepest ascent. Our method discovers reliable baselines and
succeeds in erasing salient features, which in turn locally smooths the
high-curvature decision boundaries. Our analyses point to unlearning as a
promising avenue for generating faithful, efficient and robust interpretations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting artificial neural networks to detect genome-wide
  association signals for complex traits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burak Yelmen, Maris Alver, Merve Nur Güler, Estonian Biobank Research Team, Flora Jay, Lili Milani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Investigating the genetic architecture of complex diseases is challenging due
to the multifactorial and interactive landscape of genomic and environmental
influences. Although genome-wide association studies (GWAS) have identified
thousands of variants for multiple complex traits, conventional statistical
approaches can be limited by simplified assumptions such as linearity and lack
of epistasis in models. In this work, we trained artificial neural networks to
predict complex traits using both simulated and real genotype-phenotype
datasets. We extracted feature importance scores via different post hoc
interpretability methods to identify potentially associated loci (PAL) for the
target phenotype and devised an approach for obtaining p-values for the
detected PAL. Simulations with various parameters demonstrated that associated
loci can be detected with good precision using strict selection criteria. By
applying our approach to the schizophrenia cohort in the Estonian Biobank, we
detected multiple loci associated with this highly polygenic and heritable
disorder. There was significant concordance between PAL and loci previously
associated with schizophrenia and bipolar disorder, with enrichment analyses of
genes within the identified PAL predominantly highlighting terms related to
brain morphology and function. With advancements in model optimization and
uncertainty quantification, artificial neural networks have the potential to
enhance the identification of genomic loci associated with complex diseases,
offering a more comprehensive approach for GWAS and serving as initial
screening tools for subsequent functional studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 main figures, 1 main table. Extensive changes from the
  previous version including new methodology for obtaining statistical
  significance and extended discussion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-End Learning Framework for Solving Non-Markovian Optimal <span class="highlight-title">Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaole Zhang, Peiyu Zhang, Xiongye Xiao, Shixuan Li, Vasileios Tzoumas, Vijay Gupta, Paul Bogdan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integer-order calculus often falls short in capturing the long-range
dependencies and memory effects found in many real-world processes. Fractional
calculus addresses these gaps via fractional-order integrals and derivatives,
but fractional-order dynamical systems pose substantial challenges in system
identification and optimal control due to the lack of standard control
methodologies. In this paper, we theoretically derive the optimal control via
\textit{linear quadratic regulator} (LQR) for \textit{fractional-order linear
time-invariant }(FOLTI) systems and develop an end-to-end deep learning
framework based on this theoretical foundation. Our approach establishes a
rigorous mathematical model, derives analytical solutions, and incorporates
deep learning to achieve data-driven optimal control of FOLTI systems. Our key
contributions include: (i) proposing an innovative system identification method
control strategy for FOLTI systems, (ii) developing the first end-to-end
data-driven learning framework, \textbf{F}ractional-\textbf{O}rder
\textbf{L}earning for \textbf{O}ptimal \textbf{C}ontrol (FOLOC), that learns
control policies from observed trajectories, and (iii) deriving a theoretical
analysis of sample complexity to quantify the number of samples required for
accurate optimal control in complex real-world problems. Experimental results
indicate that our method accurately approximates fractional-order system
behaviors without relying on Gaussian noise assumptions, pointing to promising
avenues for advanced optimal control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regional Weather Variable Predictions by Machine Learning with
  Near-Surface Observational and Atmospheric Numerical Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihe Zhang, Bryce Turney, Purushottam Sigdel, Xu Yuan, Eric Rappin, Adrian Lago, Sytske Kimball, Li Chen, Paul Darby, Lu Peng, Sercan Aygun, Yazhou Tu, M. Hassan Najafi, Nian-Feng Tzeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and timely regional weather prediction is vital for sectors
dependent on weather-related decisions. Traditional prediction methods, based
on atmospheric equations, often struggle with coarse temporal resolutions and
inaccuracies. This paper presents a novel machine learning (ML) model, called
MiMa (short for Micro-Macro), that integrates both near-surface observational
data from Kentucky Mesonet stations (collected every five minutes, known as
Micro data) and hourly atmospheric numerical outputs (termed as Macro data) for
fine-resolution weather forecasting. The MiMa model employs an encoder-decoder
transformer structure, with two encoders for processing multivariate data from
both datasets and a decoder for forecasting weather variables over short time
horizons. Each instance of the MiMa model, called a modelet, predicts the
values of a specific weather parameter at an individual Mesonet station. The
approach is extended with Re-MiMa modelets, which are designed to predict
weather variables at ungauged locations by training on multivariate data from a
few representative stations in a region, tagged with their elevations. Re-MiMa
(short for Regional-MiMa) can provide highly accurate predictions across an
entire region, even in areas without observational stations. Experimental
results show that MiMa significantly outperforms current models, with Re-MiMa
offering precise short-term forecasts for ungauged locations, marking a
significant advancement in weather forecasting accuracy and applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Unknowable Limits to Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19223v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19223v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiani Yan, Charles Rahal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a rigorous decomposition of predictive error, highlighting that
not all 'irreducible' error is genuinely immutable. Many domains stand to
benefit from iterative enhancements in measurement, construct validity, and
modeling. Our approach demonstrates how apparently 'unpredictable' outcomes can
become more tractable with improved data (across both target and features) and
refined algorithms. By distinguishing aleatoric from epistemic error, we
delineate how accuracy may asymptotically improve--though inherent
stochasticity may remain--and offer a robust framework for advancing
computational research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SequentialAttention++ for Block Sparsification: Differentiable Pruning
  Meets Combinatorial Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taisuke Yasuda, Kyriakos Axiotis, Gang Fu, MohammadHossein Bateni, Vahab Mirrokni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network pruning is a key technique towards engineering large yet
scalable, interpretable, and generalizable models. Prior work on the subject
has developed largely along two orthogonal directions: (1) differentiable
pruning for efficiently and accurately scoring the importance of parameters,
and (2) combinatorial optimization for efficiently searching over the space of
sparse models. We unite the two approaches, both theoretically and empirically,
to produce a coherent framework for structured neural network pruning in which
differentiable pruning guides combinatorial optimization algorithms to select
the most important sparse set of parameters. Theoretically, we show how many
existing differentiable pruning techniques can be understood as nonconvex
regularization for group sparse optimization, and prove that for a wide class
of nonconvex regularizers, the global optimum is unique, group-sparse, and
provably yields an approximate solution to a sparse convex optimization
problem. The resulting algorithm that we propose, SequentialAttention++,
advances the state of the art in large-scale neural network block-wise pruning
tasks on the ImageNet and Criteo datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Communication-Efficient Federated Optimization over Semi-Decentralized
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18787v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18787v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Wang, Yuejie Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large-scale federated and decentralized learning, communication efficiency
is one of the most challenging bottlenecks. While gossip communication -- where
agents can exchange information with their connected neighbors -- is more
cost-effective than communicating with the remote server, it often requires a
greater number of communication rounds, especially for large and sparse
networks. To tackle the trade-off, we examine the communication efficiency
under a semi-decentralized communication protocol, in which agents can perform
both agent-to-agent and agent-to-server communication in a probabilistic
manner. We design a tailored communication-efficient algorithm over
semi-decentralized networks, referred to as PISCO, which inherits the
robustness to data heterogeneity thanks to gradient tracking and allows
multiple local updates for saving communication. We establish the convergence
rate of PISCO for nonconvex problems and show that PISCO enjoys a linear
speedup in terms of the number of agents and local updates. Our numerical
results highlight the superior communication efficiency of PISCO and its
resilience to data heterogeneity and various network topologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Conversational and Collaborative Signals for Conversational
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Bin Rabiah, Nafis Sadeq, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommendation systems (CRS) leverage contextual information
from conversations to generate recommendations but often struggle due to a lack
of collaborative filtering (CF) signals, which capture user-item interaction
patterns essential for accurate recommendations. We introduce Reddit-ML32M, a
dataset that links Reddit conversations with interactions on MovieLens 32M, to
enrich item representations by leveraging collaborative knowledge and
addressing interaction sparsity in conversational datasets. We propose an
LLM-based framework that uses Reddit-ML32M to align LLM-generated
recommendations with CF embeddings, refining rankings for better performance.
We evaluate our framework against three sets of baselines: CF-based
recommenders using only interactions from CRS tasks, traditional CRS models,
and LLM-based methods relying on conversational context without item
representations. Our approach achieves consistent improvements, including a
12.32% increase in Hit Rate and a 9.9% improvement in NDCG, outperforming the
best-performing baseline that relies on conversational context but lacks
collaborative item representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Model Evaluation using SMART Filtering of Benchmark <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vipul Gupta, Candace Ross, David Pantoja, Rebecca J. Passonneau, Megan Ung, Adina Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most challenging problems facing NLP today is evaluation. Some of
the most pressing issues pertain to benchmark saturation, data contamination,
and diversity in the quality of test examples. To address these concerns, we
propose Selection Methodology for Accurate, Reduced, and Targeted (SMART)
filtering, a novel approach to select a high-quality subset of examples from
existing benchmark datasets by systematically removing less informative and
less challenging examples. Our approach applies three filtering criteria,
removing (i) easy examples, (ii) data-contaminated examples, and (iii) examples
that are similar to each other based on distance in an embedding space. We
demonstrate the effectiveness of SMART on three multiple choice QA datasets,
where our methodology increases efficiency by reducing dataset size by 48\% on
average, while increasing Pearson correlation with rankings from ChatBot Arena,
a more open-ended human evaluation setting. Our method enables us to be more
efficient, whether using SMART to make new benchmarks more challenging or to
revitalize older datasets, while still preserving the relative model rankings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GIST: Greedy Independent Set Thresholding for Diverse Data Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Fahrbach, Srikumar Ramalingam, Morteza Zadimoghaddam, Sara Ahmadian, Gui Citovsky, Giulia DeSalvo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel subset selection problem called min-distance
diversification with monotone submodular utility ($\textsf{MDMS}$), which has a
wide variety of applications in machine learning, e.g., data sampling and
feature selection. Given a set of points in a metric space, the goal of
$\textsf{MDMS}$ is to maximize an objective function combining a monotone
submodular utility term and a min-distance diversity term between any pair of
selected points, subject to a cardinality constraint. We propose the
$\texttt{GIST}$ algorithm, which achieves a $\frac{1}{2}$-approximation
guarantee for $\textsf{MDMS}$ by approximating a series of maximum independent
set problems with a bicriteria greedy algorithm. We also prove that it is
NP-hard to approximate to within a factor of $0.5584$. Finally, we demonstrate
that $\texttt{GIST}$ outperforms existing benchmarks for on a real-world image
classification task that studies single-shot subset selection for ImageNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fernández Fisac
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Reinforcement Learning from Human Feedback (RLHF) has shown promise in
aligning generative AI, we present empirical evidence that it can also cause
severe, systematic misalignment. We hypothesize that this stems from evaluator
feedback depending on downstream outcome predictions (foresight) that can be
influenced by the AI's output, inducing Goodhart's law dynamics. Conversely,
our theoretical analysis shows that conditioning evaluator feedback on
downstream observations (hindsight) inhibits this effect by decoupling the
alignment signal from potentially compromised predictions-crucially, the result
holds even if the observed outcomes are sampled from the AI's own world model.
Building on this insight, we introduce Reinforcement Learning from Hindsight
Simulation (RLHS), which presents plausible simulated outcomes to evaluators
before eliciting feedback. We demonstrate RLHS on online (PPO) and offline
(DPO) large language model fine-tuning, obtaining superior alignment over RLHF
in controlled consultancy-type experiments and user studies. We evaluate
post-hoc on the TruthfulQA benchmark and find that, even after single-task
fine-tuning, both RLHF misalignment and RLHS alignment carry over to
substantially different settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speculative Diffusion Decoding: Accelerating Language Generation through
  Diffusion <span class="chip">NAACL
  2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05636v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05636v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob K Christopher, Brian R Bartoldson, Tal Ben-Nun, Michael Cardei, Bhavya Kailkhura, Ferdinando Fioretto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding has emerged as a widely adopted method to accelerate
large language model inference without sacrificing the quality of the model
outputs. While this technique has facilitated notable speed improvements by
enabling parallel sequence verification, its efficiency remains inherently
limited by the reliance on incremental token generation in existing draft
models. To overcome this limitation, this paper proposes an adaptation of
speculative decoding which uses discrete diffusion models to generate draft
sequences. This allows parallelization of both the drafting and verification
steps, providing significant speedups to the inference process. Our proposed
approach, $\textit{Speculative Diffusion Decoding (SpecDiff)}$, is validated on
standard language generation benchmarks and empirically demonstrated to provide
up to 7.2x speedups over standard generation processes and up to 1.75x speedups
over existing speculative decoding approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 2025 Annual Conference of the Nations of the
  Americas Chapter of the Association for Computational Linguistics (NAACL
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FairDP: Certified Fairness with Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16474v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16474v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khang Tran, Ferdinando Fioretto, Issa Khalil, My T. Thai, Linh Thi Xuan Phan NhatHai Phan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FairDP, a novel training mechanism designed to provide
group fairness certification for the trained model's decisions, along with a
differential privacy (DP) guarantee to protect training data. The key idea of
FairDP is to train models for distinct individual groups independently, add
noise to each group's gradient for data privacy protection, and progressively
integrate knowledge from group models to formulate a comprehensive model that
balances privacy, utility, and fairness in downstream tasks. By doing so,
FairDP ensures equal contribution from each group while gaining control over
the amount of DP-preserving noise added to each group's contribution. To
provide fairness certification, FairDP leverages the DP-preserving noise to
statistically quantify and bound fairness metrics. An extensive theoretical and
empirical analysis using benchmark datasets validates the efficacy of FairDP
and improved trade-offs between model utility, privacy, and fairness compared
with existing methods. Our empirical results indicate that FairDP can improve
fairness metrics by more than 65% on average while attaining marginal utility
drop (less than 4% on average) under a rigorous DP-preservation across
benchmark datasets compared with existing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 3rd IEEE Conference on Secure and Trustworthy Machine
  Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shapley-PC: Constraint-based Causal Structure Learning with a Shapley
  Inspired Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11582v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11582v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabrizio Russo, Francesca Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal Structure Learning (CSL), also referred to as causal discovery,
amounts to extracting causal relations among variables in data. CSL enables the
estimation of causal effects from observational data alone, avoiding the need
to perform real life experiments. Constraint-based CSL leverages conditional
independence tests to perform causal discovery. We propose Shapley-PC, a novel
method to improve constraint-based CSL algorithms by using Shapley values over
the possible conditioning sets, to decide which variables are responsible for
the observed conditional (in)dependences. We prove soundness, completeness and
asymptotic consistency of Shapley-PC and run a simulation study showing that
our proposed algorithm is superior to existing versions of PC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for CLeaR 2025 - 47 pages (with appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting and Editing Vision-Language Representations to Mitigate
  Hallucinations <span class="chip">ICLR '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Jiang, Anish Kachinthaya, Suzie Petryk, Yossi Gandelsman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the internal representations of vision-language models (VLMs)
to address hallucinations, a persistent challenge despite advances in model
size and training. We project VLMs' internal image representations to their
language vocabulary and observe more confident output probabilities on real
objects than hallucinated objects. We additionally use these output
probabilities to spatially localize real objects. Building on this approach, we
introduce a knowledge erasure algorithm that removes hallucinations by linearly
orthogonalizing image features with respect to hallucinated object features. We
show that targeted edits to a model's latent representations can reduce
hallucinations by up to 25.7% on the COCO2014 dataset while preserving
performance. Our findings demonstrate how a deeper understanding of VLMs'
latent representations can enhance reliability and enable novel capabilities,
such as zero-shot segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR '25. Project page:
  http://anishk23733.github.io/vl-interp/. V2 added more experiments in
  appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MediSyn: A Generalist Text-Guided Latent Diffusion Model For Diverse
  Medical Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09806v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09806v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Cho, Mrudang Mathur, Cyril Zakka, Dhamanpreet Kaur, Matthew Leipzig, Alex Dalal, Aravind Krishnan, Eubee Koo, Karen Wai, Cindy S. Zhao, Rohan Shad, Robyn Fong, Ross Wightman, Akshay Chaudhari, William Hiesinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning algorithms require extensive data to achieve robust
performance. However, data availability is often restricted in the medical
domain due to patient privacy concerns. Synthetic data presents a possible
solution to these challenges. Recently, image generative models have found
increasing use for medical applications but are often designed for singular
medical specialties and imaging modalities, thus limiting their broader
utility. To address this, we introduce MediSyn: a text-guided, latent diffusion
model capable of generating synthetic images from 6 medical specialties and 10
image types. The synthetic images are validated by expert clinicians for
alignment with their corresponding text prompts. Furthermore, a direct
comparison of the synthetic images against the real images confirms that our
model synthesizes novel images and, crucially, may preserve patient privacy.
Finally, classifiers trained on a mixture of synthetic and real data achieve
similar performance to those trained on twice the amount of real data. Our
findings highlight the immense potential for generalist image generative models
to accelerate algorithmic research and development in medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Unifying Interpretability and <span class="highlight-title">Control</span>: Evaluation via
  Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usha Bhalla, Suraj Srinivas, Asma Ghandeharioun, Himabindu Lakkaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing complexity and capability of large language models, a need
to understand model reasoning has emerged, often motivated by an underlying
goal of controlling and aligning models. While numerous interpretability and
steering methods have been proposed as solutions, they are typically designed
either for understanding or for control, seldom addressing both. Additionally,
the lack of standardized applications, motivations, and evaluation metrics
makes it difficult to assess methods' practical utility and efficacy. To
address the aforementioned issues, we argue that intervention is a fundamental
goal of interpretability and introduce success criteria to evaluate how well
methods can control model behavior through interventions. To evaluate existing
methods for this ability, we unify and extend four popular interpretability
methods-sparse autoencoders, logit lens, tuned lens, and probing-into an
abstract encoder-decoder framework, enabling interventions on interpretable
features that can be mapped back to latent representations to control model
outputs. We introduce two new evaluation metrics: intervention success rate and
coherence-intervention tradeoff, designed to measure the accuracy of
explanations and their utility in controlling model behavior. Our findings
reveal that (1) while current methods allow for intervention, their
effectiveness is inconsistent across features and models, (2) lens-based
methods outperform SAEs and probes in achieving simple, concrete interventions,
and (3) mechanistic interventions often compromise model coherence,
underperforming simpler alternatives, such as prompting, and highlighting a
critical shortcoming of current interpretability approaches in applications
requiring control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vertical Federated Learning with Missing Features During Training and
  Inference <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Valdeira, Shiqiang Wang, Yuejie Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertical federated learning trains models from feature-partitioned datasets
across multiple clients, who collaborate without sharing their local data.
Standard approaches assume that all feature partitions are available during
both training and inference. Yet, in practice, this assumption rarely holds, as
for many samples only a subset of the clients observe their partition. However,
not utilizing incomplete samples during training harms generalization, and not
supporting them during inference limits the utility of the model. Moreover, if
any client leaves the federation after training, its partition becomes
unavailable, rendering the learned model unusable. Missing feature blocks are
therefore a key challenge limiting the applicability of vertical federated
learning in real-world scenarios. To address this, we propose LASER-VFL, a
vertical federated learning method for efficient training and inference of
split neural network-based models that is capable of handling arbitrary sets of
partitions. Our approach is simple yet effective, relying on the sharing of
model parameters and on task-sampling to train a family of predictors. We show
that LASER-VFL achieves a $\mathcal{O}({1}/{\sqrt{T}})$ convergence rate for
nonconvex objectives and, under the Polyak-{\L}ojasiewicz inequality, it
achieves linear convergence to a neighborhood of the optimum. Numerical
experiments show improved performance of LASER-VFL over the baselines.
Remarkably, this is the case even in the absence of missing features. For
example, for CIFAR-100, we see an improvement in accuracy of $18.2\%$ when each
of four feature blocks is observed with a probability of 0.5 and of $7.4\%$
when all features are observed. The code for this work is available at
https://github.com/Valdeira/LASER-VFL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Time-Varying Multi-Region Communications via Scalable Markovian
  Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00397v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00397v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihan Li, Yule Wang, Chengrui Li, Anqi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and constructing brain communications that capture dynamic
communications across multiple regions is fundamental to modern system
neuroscience, yet current methods struggle to find time-varying region-level
communications or scale to large neural datasets with long recording durations.
We present a novel framework using Markovian Gaussian Processes to learn brain
communications with time-varying temporal delays from multi-region neural
recordings, named Adaptive Delay Model (ADM). Our method combines Gaussian
Processes with State Space Models and employs parallel scan inference
algorithms, enabling efficient scaling to large datasets while identifying
concurrent communication patterns that evolve over time. This time-varying
approach captures how brain region interactions shift dynamically during
cognitive processes. Validated on synthetic and multi-region neural recordings
datasets, our approach discovers both the directionality and temporal dynamics
of neural communication. This work advances our understanding of distributed
neural computation and provides a scalable tool for analyzing dynamic brain
networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DP-SGD with weight clipping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18001v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18001v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Barczewski, Jan Ramon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, due to the popularity of deep neural networks and other methods
whose training typically relies on the optimization of an objective function,
and due to concerns for data privacy, there is a lot of interest in
differentially private gradient descent methods. To achieve differential
privacy guarantees with a minimum amount of noise, it is important to be able
to bound precisely the sensitivity of the information which the participants
will observe. In this study, we present a novel approach that mitigates the
bias arising from traditional gradient clipping. By leveraging a public upper
bound of the Lipschitz value of the current model and its current location
within the search domain, we can achieve refined noise level adjustments. We
present a new algorithm with improved differential privacy guarantees and a
systematic empirical evaluation, showing that our new approach outperforms
existing approaches also in practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planetarium: A Rigorous Benchmark for Translating Text to Structured
  Planning Languages <span class="chip">NAACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael L. Littman, Stephen H. Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have explored using language models for planning problems. One
approach examines translating natural language descriptions of planning tasks
into structured planning languages, such as the planning domain definition
language (PDDL). Existing evaluation methods struggle to ensure semantic
correctness and rely on simple or unrealistic datasets. To bridge this gap, we
introduce \textit{Planetarium}, a benchmark designed to evaluate language
models' ability to generate PDDL code from natural language descriptions of
planning tasks. \textit{Planetarium} features a novel PDDL equivalence
algorithm that flexibly evaluates the correctness of generated PDDL, along with
a dataset of 145,918 text-to-PDDL pairs across 73 unique state combinations
with varying levels of difficulty. Finally, we evaluate several API-access and
open-weight language models that reveal this task's complexity. For example,
96.1\% of the PDDL problem descriptions generated by GPT-4o are syntactically
parseable, 94.4\% are solvable, but only 24.8\% are semantically correct,
highlighting the need for a more rigorous benchmark for this problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL Main Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Witnesses Defend: A Witness Graph Topological Layer for Adversarial
  Graph Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14161v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14161v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naheed Anjum Arafat, Debabrota Basu, Yulia Gel, Yuzhou Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capitalizing on the intuitive premise that shape characteristics are more
robust to perturbations, we bridge adversarial graph learning with the emerging
tools from computational topology, namely, persistent homology representations
of graphs. We introduce the concept of witness complex to adversarial analysis
on graphs, which allows us to focus only on the salient shape characteristics
of graphs, yielded by the subset of the most essential nodes (i.e., landmarks),
with minimal loss of topological information on the whole graph. The remaining
nodes are then used as witnesses, governing which higher-order graph
substructures are incorporated into the learning process. Armed with the
witness mechanism, we design Witness Graph Topological Layer (WGTL), which
systematically integrates both local and global topological graph feature
representations, the impact of which is, in turn, automatically controlled by
the robust regularized topological loss. Given the attacker's budget, we derive
the important stability guarantees of both local and global topology encodings
and the associated robust topological loss. We illustrate the versatility and
efficiency of WGTL by its integration with five GNNs and three existing
non-topological defense mechanisms. Our extensive experiments across six
datasets demonstrate that WGTL boosts the robustness of GNNs across a range of
perturbations and against a range of adversarial attacks. Our datasets and
source codes are available at https://github.com/toggled/WGTL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E<span class="highlight-title">motion</span> estimation from <span class="highlight-title">video</span> footage with LSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samer Attrah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion estimation in general is a field that has been studied for a long
time, and several approaches exist using machine learning. in this paper, we
present an LSTM model, that processes the blend-shapes produced by the library
MediaPipe, for a face detected in a live stream of a camera, to estimate the
main emotion from the facial expressions, this model is trained on the FER2013
dataset and delivers a result of 71% accuracy and 62% f1-score which meets the
accuracy benchmark of the FER2013 dataset, with significantly reduced
computation costs.
https://github.com/Samir-atra/Emotion_estimation_from_video_footage_with_LSTM_ML_algorithm
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 34 references, 4 tables, 3 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinkQ: An LLM-Assisted Visual Interface for Knowledge Graph
  Question-Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06621v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06621v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harry Li, Gabriel Appleby, Ashley Suh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LinkQ, a system that leverages a large language model (LLM) to
facilitate knowledge graph (KG) query construction through natural language
question-answering. Traditional approaches often require detailed knowledge of
a graph querying language, limiting the ability for users -- even experts -- to
acquire valuable insights from KGs. LinkQ simplifies this process by
implementing a multistep protocol in which the LLM interprets a user's
question, then systematically converts it into a well-formed query. LinkQ helps
users iteratively refine any open-ended questions into precise ones, supporting
both targeted and exploratory analysis. Further, LinkQ guards against the LLM
hallucinating outputs by ensuring users' questions are only ever answered from
ground truth KG data. We demonstrate the efficacy of LinkQ through a
qualitative study with five KG practitioners. Our results indicate that
practitioners find LinkQ effective for KG question-answering, and desire future
LLM-assisted exploratory data analysis systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Open-source code: https://github.com/mit-ll/linkq</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Label-free segmentation from cardiac ultrasound using self-supervised
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04979v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04979v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danielle L. Ferreira, Zaynaf Salaymang, Rima Arnaout
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation and measurement of cardiac chambers is critical in cardiac
ultrasound but is laborious and poorly reproducible. Neural networks can
assist, but supervised approaches require the same laborious manual
annotations. We built a pipeline for self-supervised (no manual labels)
segmentation combining computer vision, clinical domain knowledge, and deep
learning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393
echocardiograms (4,476,266 images; mean 61 years, 51% female), using the
resulting segmentations to calculate biometrics. We also tested against
external images from an additional 10,030 patients with available manual
tracings of the left ventricle. r2 between clinically measured and
pipeline-predicted measurements were similar to reported inter-clinician
variation and comparable to supervised learning across several different
measurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber
size and function was 0.85 (range 0.71-0.97) compared to clinical measurements.
A subset of test echocardiograms (n=553) had corresponding cardiac MRIs, where
MRI is the gold standard. Correlation between pipeline and MRI measurements was
similar to that between clinical echocardiogram and MRI. Finally, the pipeline
accurately segments the left ventricle with an average Dice score of 0.89 (95%
CI [0.89]) in the external, manually labeled dataset. Our results demonstrate a
manual-label free, clinically valid, and highly scalable method for
segmentation from ultrasound, a noisy but globally important imaging modality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 3 Tables, 7 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Private Federated Learning In Real World Application -- A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Ji, Bortik Bandyopadhyay, Congzheng Song, Natarajan Krishnaswami, Prabal Vashisht, Rigel Smiroldo, Isabel Litton, Sayantan Mahinder, Mona Chitnis, Andrew W Hill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an implementation of machine learning model training
using private federated learning (PFL) on edge devices. We introduce a novel
framework that uses PFL to address the challenge of training a model using
users' private data. The framework ensures that user data remain on individual
devices, with only essential model updates transmitted to a central server for
aggregation with privacy guarantees. We detail the architecture of our app
selection model, which incorporates a neural network with attention mechanisms
and ambiguity handling through uncertainty management. Experiments conducted
through off-line simulations and on device training demonstrate the feasibility
of our approach in real-world scenarios. Our results show the potential of PFL
to improve the accuracy of an app selection model by adapting to changes in
user behavior over time, while adhering to privacy standards. The insights
gained from this study are important for industries looking to implement PFL,
offering a robust strategy for training a predictive model directly on edge
devices while ensuring user data privacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tamper-Resistant Safeguards for Open-Weight LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00761v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00761v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks, Mantas Mazeika
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advances in the capabilities of large language models (LLMs) have
raised widespread concerns regarding their potential for malicious use.
Open-weight LLMs present unique challenges, as existing safeguards lack
robustness to tampering attacks that modify model weights. For example, recent
works have demonstrated that refusal and unlearning safeguards can be trivially
removed with a few steps of fine-tuning. These vulnerabilities necessitate new
approaches for enabling the safe release of open-weight LLMs. We develop a
method, called TAR, for building tamper-resistant safeguards into open-weight
LLMs such that adversaries cannot remove the safeguards even after hundreds of
steps of fine-tuning. In extensive evaluations and red teaming analyses, we
find that our method greatly improves tamper-resistance while preserving benign
capabilities. Our results demonstrate that progress on tamper-resistance is
possible, opening up a promising new avenue to improve the safety and security
of open-weight LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://www.tamper-resistant-safeguards.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GHOST: Gaussian Hypothesis Open-Set Technique <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Rabinowitz, Steve Cruz, Manuel Günther, Terrance E. Boult
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluations of large-scale recognition methods typically focus on overall
performance. While this approach is common, it often fails to provide insights
into performance across individual classes, which can lead to fairness issues
and misrepresentation. Addressing these gaps is crucial for accurately
assessing how well methods handle novel or unseen classes and ensuring a fair
evaluation. To address fairness in Open-Set Recognition (OSR), we demonstrate
that per-class performance can vary dramatically. We introduce Gaussian
Hypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithm
that models deep features using class-wise multivariate Gaussian distributions
with diagonal covariance matrices. We apply Z-score normalization to logits to
mitigate the impact of feature magnitudes that deviate from the model's
expectations, thereby reducing the likelihood of the network assigning a high
score to an unknown sample. We evaluate GHOST across multiple ImageNet-1K
pre-trained deep networks and test it with four different unknown datasets.
Using standard metrics such as AUOSCR, AUROC and FPR95, we achieve
statistically significant improvements, advancing the state-of-the-art in
large-scale OSR. Source code is provided online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI Conference on Artificial Intelligence 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Audio Editing Features as User-Centric Privacy Defenses
  Against Large Language Model(LLM) Based E<span class="highlight-title">motion</span> Inference Attacks <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohd. Farhan Israk Soumik, W. K. M. Mithsara, Abdur R. Shahid, Ahmed Imteaj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of speech-enabled technologies, including virtual
assistants, video conferencing platforms, and wearable devices, has raised
significant privacy concerns, particularly regarding the inference of sensitive
emotional information from audio data. Existing privacy-preserving methods
often compromise usability and security, limiting their adoption in practical
scenarios. This paper introduces a novel, user-centric approach that leverages
familiar audio editing techniques, specifically pitch and tempo manipulation,
to protect emotional privacy without sacrificing usability. By analyzing
popular audio editing applications on Android and iOS platforms, we identified
these features as both widely available and usable. We rigorously evaluated
their effectiveness against a threat model, considering adversarial attacks
from diverse sources, including Deep Neural Networks (DNNs), Large Language
Models (LLMs), and and reversibility testing. Our experiments, conducted on
three distinct datasets, demonstrate that pitch and tempo manipulation
effectively obfuscates emotional data. Additionally, we explore the design
principles for lightweight, on-device implementation to ensure broad
applicability across various devices and platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation(Poster) at PPAI-25: The 6th AAAI Workshop
  on Privacy-Preserving Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Reconstruction for Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have become fundamental in semi-supervised
learning for graph representation, leveraging their ability to capture complex
node relationships. A recent trend in GNN research focuses on \textbf{adaptive
k-hop structure learning}, moving beyond fixed-hop aggregation to more flexible
and dynamic neighborhood selection. While GAMLP \cite{Zhang_2022} employs
separate MLP layers for each k-hop domain and ImprovingTE
\cite{Yao2023ImprovingTE} enhances this by injecting contextualized
substructure information, these methods still rely heavily on predefined
sampling strategies, which may limit their ability to generalize and maintain
stable accuracy. To address these limitations, we propose an \textbf{adaptive
reconstruction framework} that dynamically refines k-hop structure learning.
Inspired by "coreset selection" \cite{guo2022deepcore}, our approach adaptively
\textbf{reconstructs} node neighborhoods to optimize message passing, ensuring
more \textbf{effective and context-aware information flow} across the graph. To
further enhance structural robustness, we introduce two key modules: the
\textbf{Distance Recomputator} and the \textbf{Topology Reconstructor}
(\textcolor{blue}{DRTR}). The Distance Recomputator \textbf{reassesses and
recalibrates} node distances based on adaptive graph properties, leading to
\textbf{improved node embeddings} that better reflect latent relationships.
Meanwhile, the Topology Reconstructor \textbf{dynamically refines local graph
structures}, enabling the model to \textbf{adapt to evolving graph topologies}
and mitigate the impact of noise and mislabeled data. Empirical evaluations
demonstrate that our \textbf{adaptive reconstruction framework} achieves
\textbf{significant improvements} over existing k-hop-based models, providing
more \textbf{stable and accurate} performance in various graph learning
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoverUp: Coverage-Guided LLM-Based Test Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16218v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16218v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Altmayer Pizzorno, Emery D. Berger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Testing is an essential part of software development. Test generation tools
attempt to automate the otherwise labor-intensive task of test creation, but
generating high-coverage tests remains challenging. This paper proposes
CoverUp, a novel approach to driving the generation of high-coverage Python
regression tests. CoverUp combines coverage analysis, code context, and
feedback in prompts that iteratively guide the LLM to generate tests that
improve line and branch coverage. We evaluate our prototype CoverUp
implementation across a benchmark of challenging code derived from open-source
Python projects and show that CoverUp substantially improves on the state of
the art. Compared to CodaMosa, a hybrid search/LLM-based test generator,
CoverUp achieves a per-module median line+branch coverage of 80% (vs. 47%).
Compared to MuTAP, a mutation- and LLM-based test generator, CoverUp achieves
an overall line+branch coverage of 90% (vs. 77%). We also demonstrate that
CoverUp's performance stems not only from the LLM used but from the combined
effectiveness of its components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEAD: Large Foundation Model for EEG-Based Alzheimer's Disease Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihe Wang, Nan Huang, Nadia Mammone, Marco Cecchi, Xiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalogram (EEG) provides a non-invasive, highly accessible, and
cost-effective solution for Alzheimer's Disease (AD) detection. However,
existing methods, whether based on manual feature extraction or deep learning,
face two major challenges: the lack of large-scale datasets for robust feature
learning and evaluation, and poor detection performance due to inter-subject
variations. To address these challenges, we curate an EEG-AD corpus containing
813 subjects, which forms the world's largest EEG-AD dataset to the best of our
knowledge. Using this unique dataset, we propose LEAD, the first large
foundation model for EEG-based AD detection. Our method encompasses an entire
pipeline, from data selection and preprocessing to self-supervised contrastive
pretraining, fine-tuning, and key setups such as subject-independent evaluation
and majority voting for subject-level detection. We pre-train the model on 11
EEG datasets and unified fine-tune it on 5 AD datasets. Our self-supervised
pre-training design includes sample-level and subject-level contrasting to
extract useful general EEG features. Fine-tuning is performed on 5
channel-aligned datasets together. The backbone encoder incorporates temporal
and channel embeddings to capture features across both temporal and spatial
dimensions. Our method demonstrates outstanding AD detection performance,
achieving up to a 9.86% increase in F1 score at the sample-level and up to a
9.31% at the subject-level compared to state-of-the-art methods. The results of
our model strongly confirm the effectiveness of contrastive pre-training and
channel-aligned unified fine-tuning for addressing inter-subject variation. The
source code is at https://github.com/DL4mHealth/LEAD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformalized Strategy-Proof Auctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12016v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12016v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Maor Lotan, Inbal Talgam-Cohen, Yaniv Romano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auctions are key for maximizing sellers' revenue and ensuring truthful
bidding among buyers. Recently, an approach known as differentiable economics
based on machine learning (ML) has shown promise in learning powerful auction
mechanisms for multiple items and participants. However, this approach has no
guarantee of strategy-proofness at test time. Strategy-proofness is crucial as
it ensures that buyers are incentivized to bid their true valuations, leading
to optimal and fair auction outcomes without the risk of manipulation. In this
work, we propose a formulation of statistical strategy-proofness auction
mechanism, ensuring that the probability of regret exceeding a predefined
threshold is strictly controlled. Building upon conformal prediction
techniques, we develop an auction acceptance rule that leverages regret
predictions to guarantee that the data-driven auction mechanism meets the
statistical strategy-proofness requirement with high probability. Our approach
represents a practical middle-ground between two extremes: forcing zero-regret
at the cost of significant revenue loss, and naively using ML to construct
auctions with the hope of attaining low regret at test time. Numerical
experiments demonstrate the necessity of the proposed method, the validity of
our theoretical result, and its applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Family of Distributions of Random Subsets for <span class="highlight-title">Control</span>ling Positive and
  Negative Dependence <span class="chip">AISTATS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takahiro Kawashima, Hideitsu Hino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positive and negative dependence are fundamental concepts that characterize
the attractive and repulsive behavior of random subsets. Although some
probabilistic models are known to exhibit positive or negative dependence, it
is challenging to seamlessly bridge them with a practicable probabilistic
model. In this study, we introduce a new family of distributions, named the
discrete kernel point process (DKPP), which includes determinantal point
processes and parts of Boltzmann machines. We also develop some computational
methods for probabilistic operations and inference with DKPPs, such as
calculating marginal and conditional probabilities and learning the parameters.
Our numerical experiments demonstrate the controllability of positive and
negative dependence and the effectiveness of the computational methods for
DKPPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AISTATS2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiaSynth: Synthetic Dialogue Generation Framework for Low Resource
  Dialogue Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sathya Krishnan Suresh, Wu Mengjun, Tushar Pranav, Eng Siong Chng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of domain-specific dialogue datasets limits the development of
dialogue systems across applications. Existing research is constrained by
general or niche datasets that lack sufficient scale for training dialogue
systems. To address this gap, we introduce DiaSynth - a synthetic dialogue
generation framework capable of generating high-quality, contextually rich
dialogues across a wide range of domains. Unlike existing frameworks, DiaSynth
uses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to
generate dynamic, domain-specific dialogues with simulated personas and diverse
conversational features. We perform our experiments by generating synthetic
data using different LLMs and few-shot examples from DialogSum and SAMSum. The
pretrained language models fine-tuned on the synthetic data outperform the base
models by 16.47% on dialogue summarization, while the comparison between models
fine-tuned on in-domain data and synthetic data shows that the synthetic data
is able to capture 90.48% of the performance distribution of the in-domain data
on dialogue summarization. The quality of the data generated also increases as
we increase the size of LLM from 3B to 8B. These results validate DiaSynth's
potential as a robust alternative to traditional data collection methods. We
open source the code and data generated for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Mitigating the Bias Inheritance in LLM-based Data
  Augmentation on Downstream Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miaomiao Li, Hao Chen, Yang Wang, Tingyuan Zhu, Weijia Zhang, Kaijie Zhu, Kam-Fai Wong, Jindong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating synthetic datasets via large language models (LLMs) themselves has
emerged as a promising approach to improve LLM performance. However, LLMs
inherently reflect biases present in their training data, leading to a critical
challenge: when these models generate synthetic data for training, they may
propagate and amplify their inherent biases that can significantly impact model
fairness and robustness on downstream tasks--a phenomenon we term bias
inheritance. This work presents the first systematic investigation in
understanding, analyzing, and mitigating bias inheritance. We study this
problem by fine-tuning LLMs with a combined dataset consisting of original and
LLM-augmented data, where bias ratio represents the proportion of augmented
data. Through systematic experiments across 10 classification and generation
tasks, we analyze how 6 different types of biases manifest at varying bias
ratios. Our results reveal that bias inheritance has nuanced effects on
downstream tasks, influencing both classification tasks and generation tasks
differently. Then, our analysis identifies three key misalignment factors:
misalignment of values, group data, and data distributions. Based on these
insights, we propose three mitigation strategies: token-based, mask-based, and
loss-based approaches. Experiments demonstrate that these strategies also work
differently on various tasks and bias, indicating the substantial challenges to
fully mitigate bias inheritance. We hope this work can provide valuable
insights to the research of LLM data augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report; 31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied Red Teaming for Auditing Robotic Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sathwik Karnik, Zhang-Wei Hong, Nishant Abhangi, Yen-Chen Lin, Tsun-Hsuan Wang, Christophe Dupuy, Rahul Gupta, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-conditioned robot models have the potential to enable robots to
perform a wide range of tasks based on natural language instructions. However,
assessing their safety and effectiveness remains challenging because it is
difficult to test all the different ways a single task can be phrased. Current
benchmarks have two key limitations: they rely on a limited set of
human-generated instructions, missing many challenging cases, and focus only on
task performance without assessing safety, such as avoiding damage. To address
these gaps, we introduce Embodied Red Teaming (ERT), a new evaluation method
that generates diverse and challenging instructions to test these models. ERT
uses automated red teaming techniques with Vision Language Models (VLMs) to
create contextually grounded, difficult instructions. Experimental results show
that state-of-the-art language-conditioned robot models fail or behave unsafely
on ERT-generated instructions, underscoring the shortcomings of current
benchmarks in evaluating real-world performance and safety. Code and videos are
available at: https://s-karnik.github.io/embodied-red-team-project-page.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better Fair than Sorry: Adversarial Missing Data Imputation for Fair
  GNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01591v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01591v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debolina Halder Lina, Arlei Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have achieved state-of-the-art results in many
relevant tasks where decisions might disproportionately impact specific
communities. However, existing work on fair GNNs often assumes that either
protected attributes are fully observed or that the missing protected attribute
imputation is fair. In practice, biases in the imputation will propagate to the
model outcomes, leading them to overestimate the fairness of their predictions.
We address this challenge by proposing Better Fair than Sorry (BFtS), a fair
missing data imputation model for protected attributes. The key design
principle behind BFtS is that imputations should approximate the worst-case
scenario for fairness -- i.e. when optimizing fairness is the hardest. We
implement this idea using a 3-player adversarial scheme where two adversaries
collaborate against a GNN-based classifier, and the classifier minimizes the
maximum bias. Experiments using synthetic and real datasets show that BFtS
often achieves a better fairness x accuracy trade-off than existing
alternatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do generative <span class="highlight-title">video</span> models learn physical principles from watching
  <span class="highlight-title">video</span>s? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, Robert Geirhos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI video generation is undergoing a revolution, with quality and realism
advancing rapidly. These advances have led to a passionate scientific debate:
Do video models learn "world models" that discover laws of physics -- or,
alternatively, are they merely sophisticated pixel predictors that achieve
visual realism without understanding the physical principles of reality? We
address this question by developing Physics-IQ, a comprehensive benchmark
dataset that can only be solved by acquiring a deep understanding of various
physical principles, like fluid dynamics, optics, solid mechanics, magnetism
and thermodynamics. We find that across a range of current models (Sora,
Runway, Pika, Lumiere, Stable Video Diffusion, and VideoPoet), physical
understanding is severely limited, and unrelated to visual realism. At the same
time, some test cases can already be successfully solved. This indicates that
acquiring certain physical principles from observation alone may be possible,
but significant challenges remain. While we expect rapid advances ahead, our
work demonstrates that visual realism does not imply physical understanding.
Our project page is at https://physics-iq.github.io; code at
https://github.com/google-deepmind/physics-IQ-benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying perturbation targets through causal differential networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menghua Wu, Umesh Padia, Sean H. Murphy, Regina Barzilay, Tommi Jaakkola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying variables responsible for changes to a biological system enables
applications in drug target discovery and cell engineering. Given a pair of
observational and interventional datasets, the goal is to isolate the subset of
observed variables that were the targets of the intervention. Directly applying
causal discovery algorithms is challenging: the data may contain thousands of
variables with as few as tens of samples per intervention, and biological
systems do not adhere to classical causality assumptions. We propose a
causality-inspired approach to address this practical setting. First, we infer
noisy causal graphs from the observational and interventional data. Then, we
learn to map the differences between these graphs, along with additional
statistical features, to sets of variables that were intervened upon. Both
modules are jointly trained in a supervised framework, on simulated and real
data that reflect the nature of biological interventions. This approach
consistently outperforms baselines for perturbation modeling on seven
single-cell transcriptomics datasets. We also demonstrate significant
improvements over current causal discovery methods for predicting soft and hard
intervention targets across a variety of synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning from Simulated Interactions via Multitask Prospective
  Rehearsal for Bionic Limb Behavior Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharmita Dey, Benjamin Paassen, Sarath Ravindran Nair, Sabri Boughorbel, Arndt F. Schilling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lower limb amputations and neuromuscular impairments severely restrict
mobility, necessitating advancements beyond conventional prosthetics. While
motorized bionic limbs show promise, their effectiveness depends on replicating
the dynamic coordination of human movement across diverse environments. In this
paper, we introduce a model for human behavior in the context of bionic
prosthesis control. Our approach leverages human locomotion demonstrations to
learn the synergistic coupling of the lower limbs, enabling the prediction of
the kinematic behavior of a missing limb during tasks such as walking, climbing
inclines, and stairs. We propose a multitasking, continually adaptive model
that anticipates and refines movements over time. At the core of our method is
a technique called multitask prospective rehearsal, that anticipates and
synthesizes future movements based on the previous prediction and employs a
corrective mechanism for subsequent predictions. Our evolving architecture
merges lightweight, task-specific modules on a shared backbone, ensuring both
specificity and scalability. We validate our model through experiments on
real-world human gait datasets, including transtibial amputees, across a wide
range of locomotion tasks. Results demonstrate that our approach consistently
outperforms baseline models, particularly in scenarios with distributional
shifts, adversarial perturbations, and noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Transactions on Machine Learning Research (TMLR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex
  composite losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05237v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05237v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Kong, Mónica Ribero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially-private stochastic gradient descent (DP-SGD) is a family of
iterative machine learning training algorithms that privatize gradients to
generate a sequence of differentially-private (DP) model parameters. It is also
the standard tool used to train DP models in practice, even though most users
are only interested in protecting the privacy of the final model. Tight DP
accounting for the last iterate would minimize the amount of noise required
while maintaining the same privacy guarantee and potentially increasing model
utility. However, last-iterate accounting is challenging, and existing works
require strong assumptions not satisfied by most implementations. These include
assuming (i) the global sensitivity constant is known - to avoid gradient
clipping; (ii) the loss function is Lipschitz or convex; and (iii) input
batches are sampled randomly.
  In this work, we forego any unrealistic assumptions and provide privacy
bounds for the most commonly used variant of DP-SGD, in which data is traversed
cyclically, gradients are clipped, and only the last model is released. More
specifically, we establish new Renyi differential privacy (RDP) upper bounds
for the last iterate under realistic assumptions of small stepsize and
Lipschitz smoothness of the loss function. Our general bounds also recover the
special-case convex bounds when the weak-convexity parameter of the objective
function approaches zero and no clipping is performed. The approach itself
leverages optimal transport techniques for last iterate bounds, which is a
nontrivial task when the data is traversed cyclically and the loss function is
nonconvex.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact full-RSB SAT/UNSAT transition in infinitely wide two-layer neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06717v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06717v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon L. Annesi, Enrico M. Malatesta, Francesco Zamponi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the problem of storing random pattern-label associations using two
classes of continuous non-convex weights models, namely the perceptron with
negative margin and an infinite-width two-layer neural network with
non-overlapping receptive fields and generic activation function. Using a
full-RSB ansatz we compute the exact value of the SAT/UNSAT transition.
Furthermore, in the case of the negative perceptron we show that the overlap
distribution of typical states displays an overlap gap (a disconnected support)
in certain regions of the phase diagram defined by the value of the margin and
the density of patterns to be stored. This implies that some recent theorems
that ensure convergence of Approximate Message Passing (AMP) based algorithms
to capacity are not applicable. Finally, we show that Gradient Descent is not
able to reach the maximal capacity, irrespectively of the presence of an
overlap gap for typical states. This finding, similarly to what occurs in
binary weight models, suggests that gradient-based algorithms are biased
towards highly atypical states, whose inaccessibility determines the
algorithmic threshold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ μP$^2$: Effective Sharpness Aware Minimization Requires Layerwise
  Perturbation Scaling <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Haas, Jin Xu, Volkan Cevher, Leena Chennuru Vankadara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharpness Aware Minimization (SAM) enhances performance across various neural
architectures and datasets. As models are continually scaled up to improve
performance, a rigorous understanding of SAM's scaling behaviour is paramount.
To this end, we study the infinite-width limit of neural networks trained with
SAM, using the Tensor Programs framework. Our findings reveal that the dynamics
of standard SAM effectively reduce to applying SAM solely in the last layer in
wide neural networks, even with optimal hyperparameters. In contrast, we
identify a stable parameterization with layerwise perturbation scaling, which
we call $\textit{Maximal Update and Perturbation Parameterization}$
($\mu$P$^2$), that ensures all layers are both feature learning and effectively
perturbed in the limit. Through experiments with MLPs, ResNets and Vision
Transformers, we empirically demonstrate that $\mu$P$^2$ achieves
hyperparameter transfer of the joint optimum of learning rate and perturbation
radius across model scales. Moreover, we provide an intuitive condition to
derive $\mu$P$^2$ for other perturbation rules like Adaptive SAM and SAM-ON,
also ensuring balanced perturbation effects across all layers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final NeurIPS 2024 camera-ready version. Differences to v1: Cleaner
  Figure 1, added Appendix H.3.2 showing that even MLPs can transfer optimal
  HPs in some versions of SP on CIFAR-10, small improvements in writing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Apriori_Goal algorithm for constructing association rules for a database
  with a given classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Billig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An efficient Apriori_Goal algorithm is proposed for constructing association
rules in a relational database with predefined classification. The target
parameter of the database specifies a finite number of goals $Goal_k$, for each
of which the algorithm constructs association rules of the form $X \Rightarrow
Goal_k$. The quality of the generated rules is characterized by five criteria:
two represent rule frequency, two reflect rule reliability, and the fifth is a
weighted sum of these four criteria.
  The algorithm initially generates rules with single premises, where the
correlation criterion between the premise $X$ and the conclusion $Goal_k$
exceeds a specified threshold. Then, rules with extended premises are built
based on the anti-monotonicity of rule frequency criteria and the monotonicity
of rule reliability criteria. Newly constructed rules tend to decrease in
frequency while increasing in reliability. The article proves several
statements that justify the rule construction process.
  The algorithm enables the construction of both high-frequency and rare rules
with low occurrence frequency but high reliability. It also allows for the
generation of negative rules with negative correlation between the premise and
conclusion, which can be valuable in practical applications for filtering out
undesired goals.
  The efficiency of the algorithm is based on two factors: the method of
encoding the database and its partitioning into subsets linked to the target
parameter. Time complexity estimates for rule construction are provided using a
medical database as an example.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preserving Privacy in Large Language Models: A <span class="highlight-title">Survey</span> on Current Threats
  and Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Miranda, Elena Sofia Ruzzetti, Andrea Santilli, Fabio Massimo Zanzotto, Sébastien Bratières, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) represent a significant advancement in
artificial intelligence, finding applications across various domains. However,
their reliance on massive internet-sourced datasets for training brings notable
privacy issues, which are exacerbated in critical domains (e.g., healthcare).
Moreover, certain application-specific scenarios may require fine-tuning these
models on private data. This survey critically examines the privacy threats
associated with LLMs, emphasizing the potential for these models to memorize
and inadvertently reveal sensitive information. We explore current threats by
reviewing privacy attacks on LLMs and propose comprehensive solutions for
integrating privacy mechanisms throughout the entire learning pipeline. These
solutions range from anonymizing training datasets to implementing differential
privacy during training or inference and machine unlearning after training. Our
comprehensive review of existing literature highlights ongoing challenges,
available tools, and future directions for preserving privacy in LLMs. This
work aims to guide the development of more secure and trustworthy AI systems by
providing a thorough understanding of privacy preservation methods and their
effectiveness in mitigating risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR)
  https://openreview.net/forum?id=Ss9MTTN7OL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Human</span>s Co-exist, So Must Embodied Artificial Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Kuehn, Joseph La Delfa, Miguel Vasco, Danica Kragic, Iolanda Leite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern embodied artificial agents excel in static, predefined tasks but fall
short in dynamic and long-term interactions with humans. On the other hand,
humans can adapt and evolve continuously, exploiting the situated knowledge
embedded in their environment and other agents, thus contributing to meaningful
interactions. We introduce the concept of co-existence for embodied artificial
agents and argues that it is a prerequisite for meaningful, long-term
interaction with humans. We take inspiration from biology and design theory to
understand how human and non-human organisms foster entities that co-exist
within their specific niches. Finally, we propose key research directions for
the machine learning community to foster co-existing embodied agents, focusing
on the principles, hardware and learning methods responsible for shaping them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inferring High-Order Couplings with Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aurélien Decelle, Alfonso de Jesús Navas Gómez, Beatriz Seoane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maximum entropy methods, based on the inverse Ising/Potts problem from
statistical mechanics, are essential for modeling interactions between pairs of
variables in data-driven problems across disciplines such as bioinformatics,
ecology, and neuroscience. Despite their considerable success, these methods
typically fail to capture higher-order interactions that are often essential
for understanding complex systems. Conversely, modern machine learning methods
capture these complex interactions, but the computational cost of interpretable
frameworks makes them impractical for real-world applications. Restricted
Boltzmann Machines (RBMs) provide a computationally efficient way to capture
statistical correlations using hidden nodes in a bipartite neural network. In
this study, we introduce a new method that maps RBMs to generalized Potts
models, allowing for the extraction of interactions up to any specified order.
This method utilizes large-$N$ approximations, enabled by the RBM's simple
structure, to extract effective many-body couplings with minimal computational
effort. Furthermore, we propose a robust framework for extracting higher-order
interactions in more complex probabilistic models and a simple gauge-fixing
method within the effective many-body Potts model. Our validation on synthetic
datasets confirms the method's ability to recover two- and three-body
interactions accurately. When applied to protein sequence data, the framework
competently reconstructs protein contact maps and provides performance
comparable to the best inverse Potts models. These findings confirm that RBMs
are an effective and streamlined tool for exploring higher-order interactions
within complex systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 Pages and 5 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking LLMs' Safeguard with Universal Magic Words for Text
  Embedding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The security issue of large language models (LLMs) has gained significant
attention recently, with various defense mechanisms developed to prevent
harmful outputs, among which safeguards based on text embedding models serve as
a fundamental defense. Through testing, we discover that the distribution of
text embedding model outputs is significantly biased with a large mean.
Inspired by this observation, we propose novel efficient methods to search for
universal magic words that can attack text embedding models. The universal
magic words as suffixes can move the embedding of any text towards the bias
direction, therefore manipulate the similarity of any text pair and mislead
safeguards. By appending magic words to user prompts and requiring LLMs to end
answers with magic words, attackers can jailbreak the safeguard. To eradicate
this security risk, we also propose defense mechanisms against such attacks,
which can correct the biased distribution of text embeddings in a train-free
manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence Diagram of Nonparametric Ranking for Uncertainty Assessment
  in Large Language Models Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zebin Wang, Yi Han, Ethan X. Fang, Lan Wang, Junwei Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the inference for the ranking of large language models (LLMs).
Alignment arises as a significant challenge to mitigate hallucinations in the
use of LLMs. Ranking LLMs has proven to be an effective tool to improve
alignment based on the best-of-$N$ policy. In this paper, we propose a new
inferential framework for hypothesis testing among the ranking for language
models. Our framework is based on a nonparametric contextual ranking framework
designed to assess large language models' domain-specific expertise, leveraging
nonparametric scoring methods to account for their sensitivity to the prompts.
To characterize the combinatorial complexity of the ranking, we introduce a
novel concept of confidence diagram, which leverages a Hasse diagram to
represent the entire confidence set of rankings by a single directed graph. We
show the validity of the proposed confidence diagram by advancing the Gaussian
multiplier bootstrap theory to accommodate the supremum of independent
empirical processes that are not necessarily identically distributed. Extensive
numerical experiments conducted on both synthetic and real data demonstrate
that our approach offers valuable insight into the evaluation for the
performance of different LLMs across various medical domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MT2ST: Adaptive Multi-Task to Single-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18038v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18038v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu, Yanxuan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient machine learning (ML) has become increasingly important as models
grow larger and data volumes expand. In this work, we address the trade-off
between generalization in multi-task learning (MTL) and precision in
single-task learning (STL) by introducing the Multi-Task to Single-Task (MT2ST)
framework. MT2ST is designed to enhance training efficiency and accuracy in
word embedding tasks, showcasing its value as a practical application of
efficient ML.
  Our framework employs two strategies: *Diminish*, which gradually reduces the
influence of auxiliary tasks, and *Switch*, which transitions training from MTL
to STL at a specific point. Empirical results show that MT2ST reduces training
time by 67\% compared to STL and by 13\% compared to traditional MTL, while
maintaining high accuracy. These findings highlight MT2ST as an efficient ML
solution tailored for optimizing word embedding training. Code is available at
https://github.com/NoakLiu/MT2ST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calibrated Unsupervised Anomaly Detection in Multivariate Time-series
  using Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saba Sanami, Amir G. Aghdam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates unsupervised anomaly detection in multivariate
time-series data using reinforcement learning (RL) in the latent space of an
autoencoder. A significant challenge is the limited availability of anomalous
data, often leading to misclassifying anomalies as normal events, thus raising
false negatives. RL can help overcome this limitation by promoting exploration
and balancing exploitation during training, effectively preventing overfitting.
Wavelet analysis is also utilized to enhance anomaly detection, enabling
time-series data decomposition into both time and frequency domains. This
approach captures anomalies at multiple resolutions, with wavelet coefficients
extracted to detect both sudden and subtle shifts in the data, thereby refining
the anomaly detection process. We calibrate the decision boundary by generating
synthetic anomalies and embedding a supervised framework within the model. This
supervised element aids the unsupervised learning process by fine-tuning the
decision boundary and increasing the model's capacity to distinguish between
normal and anomalous patterns effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication and presentation at the
  2025 IEEE International systems Conference (SysCon)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Panza: Design and Analysis of a Fully-Local Personalized Text Writing
  Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10994v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10994v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armand Nicolicioiu, Eugenia Iofinova, Andrej Jovanovic, Eldar Kurtic, Mahdi Nikdan, Andrei Panferov, Ilia Markov, Nir Shavit, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of powerful open-source large language models (LLMs) opens
exciting use-cases, such as using personal data to fine-tune these models to
imitate a user's unique writing style. Two key requirements for such assistants
are personalization - in the sense that the assistant should recognizably
reflect the user's own writing style - and privacy - users may justifiably be
wary of uploading extremely personal data, such as their email archive, to a
third-party service. In this paper, we present a new design and evaluation for
such an automated assistant, for the specific use case of email generation,
which we call Panza. Panza's personalization features are based on a
combination of fine-tuning using a variant of the Reverse Instructions
technique together with Retrieval-Augmented Generation (RAG). We demonstrate
that this combination allows us to fine-tune an LLM to reflect a user's writing
style using limited data, while executing on extremely limited resources, e.g.
on a free Google Colab instance. Our key methodological contribution is the
first detailed study of evaluation metrics for this personalized writing task,
and of how different choices of system components--the use of RAG and of
different fine-tuning approaches-impact the system's performance. Additionally,
we demonstrate that very little data - under 100 email samples - are sufficient
to create models that convincingly imitate humans. This finding showcases a
previously-unknown attack vector in language models - that access to a small
number of writing samples can allow a bad actor to cheaply create generative
models that imitate a target's writing style. We are releasing the full Panza
code as well as three new email datasets licensed for research use at
https://github.com/IST-DASLab/PanzaMail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Panza is available at https://github.com/IST-DASLab/PanzaMail</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LCQ: Low-Rank Codebook based Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen-Pu Cai, Ming-Yang Li, Wu-Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models~(LLMs) have recently demonstrated promising performance
in many tasks. However, the high storage and computational cost of LLMs has
become a challenge for deploying LLMs. Weight quantization has been widely used
for model compression, which can reduce both storage and computational cost.
Most existing weight quantization methods for LLMs use a rank-one codebook for
quantization, which results in substantial accuracy loss when the compression
ratio is high. In this paper, we propose a novel weight quantization method,
called low-rank codebook based quantization~(LCQ), for LLMs. LCQ adopts a
low-rank codebook, the rank of which can be larger than one, for quantization.
Experiments show that LCQ can achieve better accuracy than existing methods
with a negligibly extra storage cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closed-form Solutions: A New Perspective on Solving Differential
  Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Wei, Yanjie Li, Lina Yu, Weijun Li, Min Wu, Linjun Sun, Jufeng Han, Yan Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of analytical solutions for differential equations has
historically been limited by the need for extensive prior knowledge and
mathematical prowess; however, machine learning methods like genetic algorithms
have recently been applied to this end, albeit with issues of significant time
consumption and complexity. This paper presents a novel machine learning-based
solver, SSDE (Symbolic Solver for Differential Equations), which employs
reinforcement learning to derive symbolic closed-form solutions for various
differential equations. Our evaluations on a range of ordinary and partial
differential equations demonstrate that SSDE provides superior performance in
achieving analytical solutions compared to other machine learning approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Denoising Lévy Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18609v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18609v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Shariatian, Umut Simsekli, Alain Durmus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring noise distributions beyond Gaussian in diffusion models remains an
open challenge. While Gaussian-based models succeed within a unified SDE
framework, recent studies suggest that heavy-tailed noise distributions, like
$\alpha$-stable distributions, may better handle mode collapse and effectively
manage datasets exhibiting class imbalance, heavy tails, or prominent outliers.
Recently, Yoon et al.\ (NeurIPS 2023), presented the L\'evy-It\^o model (LIM),
directly extending the SDE-based framework to a class of heavy-tailed SDEs,
where the injected noise followed an $\alpha$-stable distribution, a rich class
of heavy-tailed distributions. However, the LIM framework relies on highly
involved mathematical techniques with limited flexibility, potentially
hindering broader adoption and further development. In this study, instead of
starting from the SDE formulation, we extend the denoising diffusion
probabilistic model (DDPM) by replacing the Gaussian noise with $\alpha$-stable
noise. By using only elementary proof techniques, the proposed approach,
Denoising L\'evy Probabilistic Models (DLPM), boils down to vanilla DDPM with
minor modifications. As opposed to the Gaussian case, DLPM and LIM yield
different training algorithms and different backward processes, leading to
distinct sampling algorithms. These fundamental differences translate favorably
for DLPM as compared to LIM: our experiments show improvements in coverage of
data distribution tails, better robustness to unbalanced datasets, and improved
computation times requiring smaller number of backward steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SageAttention2: Efficient Attention with Thorough Outlier Smoothing and
  Per-thread INT4 Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10958v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10958v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although quantization for linear layers has been widely used, its application
to accelerate the attention process remains limited. To further enhance the
efficiency of attention computation compared to SageAttention while maintaining
precision, we propose SageAttention2, which utilizes significantly faster 4-bit
matrix multiplication (Matmul) alongside additional precision-enhancing
techniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a
hardware-friendly thread-level granularity and quantize matrices $(\widetilde
P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the
accuracy of INT4 $QK^\top$. Third, we propose a two-level accumulation strategy
for $\widetilde PV$ to enhance the accuracy of FP8 $\widetilde PV$. The
operations per second (OPS) of SageAttention2 surpass FlashAttention2 and
xformers by about 3x and 4.5x on RTX4090, respectively. Moreover,
SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs,
while delivering much higher accuracy. Comprehensive experiments confirm that
our approach incurs negligible end-to-end metrics loss across diverse models,
including those for language, image, and video generation. The code is
available at https://github.com/thu-ml/SageAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Luca Marchetti, Gabriele Cesa, Pratik Kumar, Arash Behboodi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lattice reduction is a combinatorial optimization problem aimed at finding
the most orthogonal basis in a given lattice. The Lenstra-Lenstra-Lov\'asz
(LLL) algorithm is the best algorithm in the literature for solving this
problem. In light of recent research on algorithm discovery, in this work, we
would like to answer this question: is it possible to parametrize the algorithm
space for lattice reduction problem with neural networks and find an algorithm
without supervised data? Our strategy is to use equivariant and invariant
parametrizations and train in a self-supervised way. We design a deep neural
model outputting factorized unimodular matrices and train it in a
self-supervised manner by penalizing non-orthogonal lattice bases. We
incorporate the symmetries of lattice reduction into the model by making it
invariant to isometries and scaling of the ambient space and equivariant with
respect to the hyperocrahedral group permuting and flipping the lattice basis
elements. We show that this approach yields an algorithm with comparable
complexity and performance to the LLL algorithm on a set of benchmarks.
Additionally, motivated by certain applications for wireless communication, we
extend our method to a convolutional architecture which performs joint
reduction of spatially-correlated lattices arranged in a grid, thereby
amortizing its cost over multiple lattices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Data Augmentation for Few-Shot Time Series Forecasting: A
  Reinforcement Learning Approach Guided by a Model Zoo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Yuan, Yutong Wang, Yihong Chen, Yunbo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting, particularly in few-shot learning scenarios, is
challenging due to the limited availability of high-quality training data. To
address this, we present a pilot study on using reinforcement learning (RL) for
time series data augmentation. Our method, ReAugment, tackles three critical
questions: which parts of the training set should be augmented, how the
augmentation should be performed, and what advantages RL brings to the process.
Specifically, our approach maintains a forecasting model zoo, and by measuring
prediction diversity across the models, we identify samples with higher
probabilities for overfitting and use them as the anchor points for
augmentation. Leveraging RL, our method adaptively transforms the overfit-prone
samples into new data that not only enhances training set diversity but also
directs the augmented data to target regions where the forecasting models are
prone to overfitting. We validate the effectiveness of ReAugment across a wide
range of base models, showing its advantages in both standard time series
forecasting and few-shot learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Privacy Attacks on Trained Shallow Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Smorodinsky, Gal Vardi, Itay Safran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study what provable privacy attacks can be shown on trained, 2-layer ReLU
neural networks. We explore two types of attacks; data reconstruction attacks,
and membership inference attacks. We prove that theoretical results on the
implicit bias of 2-layer neural networks can be used to provably reconstruct a
set of which at least a constant fraction are training points in a univariate
setting, and can also be used to identify with high probability whether a given
point was used in the training set in a high dimensional setting. To the best
of our knowledge, our work is the first to show provable vulnerabilities in
this implicit-bias-driven setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAC-Chernoff Bounds: Understanding Generalization in the Interpolation
  Regime 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10947v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10947v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrés R. Masegosa, Luis A. Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a distribution-dependent PAC-Chernoff bound that
exhibits perfect tightness for interpolators, even within over-parameterized
model classes. This bound, which relies on basic principles of Large Deviation
Theory, defines a natural measure of the smoothness of a model, characterized
by simple real-valued functions. Building upon this bound and the new concept
of smoothness, we present an unified theoretical framework revealing why
certain interpolators show an exceptional generalization, while others falter.
We theoretically show how a wide spectrum of modern learning methodologies,
encompassing techniques such as $\ell_2$-norm, distance-from-initialization and
input-gradient regularization, in combination with data augmentation, invariant
architectures, and over-parameterization, collectively guide the optimizer
toward smoother interpolators, which, according to our theoretical framework,
are the ones exhibiting superior generalization performance. This study shows
that distribution-dependent bounds serve as a powerful tool to understand the
complex dynamics behind the generalization capabilities of over-parameterized
interpolators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 12 figures, published at JAIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Yue Liao, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EnerVerse, a generative robotics foundation model that
constructs and interprets embodied spaces. EnerVerse employs an autoregressive
video diffusion framework to predict future embodied spaces from instructions,
enhanced by a sparse context memory for long-term reasoning. To model the 3D
robotics world, we propose Free Anchor Views (FAVs), a multi-view video
representation offering flexible, task-adaptive perspectives to address
challenges like motion ambiguity and environmental constraints. Additionally,
we present EnerVerse-D, a data engine pipeline combining the generative model
with 4D Gaussian Splatting, forming a self-reinforcing data loop to reduce the
sim-to-real gap. Leveraging these innovations, EnerVerse translates 4D world
representations into physical actions via a policy head (EnerVerse-A), enabling
robots to execute task instructions. EnerVerse-A achieves state-of-the-art
performance in both simulation and real-world settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://sites.google.com/view/enerverse</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incentivizing Honesty among Competitors in Collaborative Learning and
  Optimization <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16272v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16272v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian E. Dorner, Nikola Konstantinov, Georgi Pashaliev, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative learning techniques have the potential to enable training
machine learning models that are superior to models trained on a single
entity's data. However, in many cases, potential participants in such
collaborative schemes are competitors on a downstream task, such as firms that
each aim to attract customers by providing the best recommendations. This can
incentivize dishonest updates that damage other participants' models,
potentially undermining the benefits of collaboration. In this work, we
formulate a game that models such interactions and study two learning tasks
within this framework: single-round mean estimation and multi-round SGD on
strongly-convex objectives. For a natural class of player actions, we show that
rational clients are incentivized to strongly manipulate their updates,
preventing learning. We then propose mechanisms that incentivize honest
communication and ensure learning quality comparable to full cooperation.
Lastly, we empirically demonstrate the effectiveness of our incentive scheme on
a standard non-convex federated learning benchmark. Our work shows that
explicitly modeling the incentives and actions of dishonest clients, rather
than assuming them malicious, can enable strong robustness guarantees for
collaborative learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated experimental results after fixing a mistake in the code.
  Previous version published in NeurIPS 2023; 37 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning in Zero-Sum Markov Games: Relaxing Strong Reachability and
  Mixing Time Assumptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08008v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08008v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reda Ouhamma, Maryam Kamgarpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address payoff-based decentralized learning in infinite-horizon zero-sum
Markov games. In this setting, each player makes decisions based solely on
received rewards, without observing the opponent's strategy or actions nor
sharing information. Prior works established finite-time convergence to an
approximate Nash equilibrium under strong reachability and mixing time
assumptions. We propose a convergent algorithm that significantly relaxes these
assumptions, requiring only the existence of a single policy (not necessarily
known) with bounded reachability and mixing time. Our key technical novelty is
introducing Tsallis entropy regularization to smooth the best-response policy
updates. By suitably tuning this regularization, we ensure sufficient
exploration, thus bypassing previous stringent assumptions on the MDP. By
establishing novel properties of the value and policy updates induced by the
Tsallis entropy regularizer, we prove finite-time convergence to an approximate
Nash equilibrium.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cardiverse: Harnessing LLMs for Novel Card Game Prototyping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danrui Li, Sen Zhang, Sam S. Sohn, Kaidong Hu, Muhammad Usman, Mubbasir Kapadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prototyping of computer games, particularly card games, requires
extensive human effort in creative ideation and gameplay evaluation. Recent
advances in Large Language Models (LLMs) offer opportunities to automate and
streamline these processes. However, it remains challenging for LLMs to design
novel game mechanics beyond existing databases, generate consistent gameplay
environments, and develop scalable gameplay AI for large-scale evaluations.
This paper addresses these challenges by introducing a comprehensive automated
card game prototyping framework. The approach highlights a graph-based indexing
method for generating novel game designs, an LLM-driven system for consistent
game code generation validated by gameplay records, and a gameplay AI
constructing method that uses an ensemble of LLM-generated action-value
functions optimized through self-play. These contributions aim to accelerate
card game prototyping, reduce human labor, and lower barriers to entry for game
developers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Musical Representations for Music Performance Question
  Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Diao, Chunhui Zhang, Tingxuan Wu, Ming Cheng, Zhongyu Ouyang, Weiyi Wu, Jiang Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music performances are representative scenarios for audio-visual modeling.
Unlike common scenarios with sparse audio, music performances continuously
involve dense audio signals throughout. While existing multimodal learning
methods on the audio-video QA demonstrate impressive capabilities in general
scenarios, they are incapable of dealing with fundamental problems within the
music performances: they underexplore the interaction between the multimodal
signals in performance and fail to consider the distinctive characteristics of
instruments and music. Therefore, existing methods tend to answer questions
regarding musical performances inaccurately. To bridge the above research gaps,
(i) given the intricate multimodal interconnectivity inherent to music data,
our primary backbone is designed to incorporate multimodal interactions within
the context of music; (ii) to enable the model to learn music characteristics,
we annotate and release rhythmic and music sources in the current music
datasets; (iii) for time-aware audio-visual modeling, we align the model's
music predictions with the temporal dimension. Our experiments show
state-of-the-art effects on the Music AVQA datasets. Our code is available at
https://github.com/xid32/Amuse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Code to Canvas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernhard O. Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The web-based dynamic geometry software CindyJS is a versatile tool to create
interactive applications for mathematics and other topics. In this workshop, we
will look at a code package that makes the creation of animations in CindyJS
easier and more streamlined. Animations, which can then be embedded into
presentations or be used in (lecture) videos. The focus lies on the creation of
the animations themselves and some of the technical and artistic fundamentals
to do so.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A workshop paper for the Bridges 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent Advances in Discrete Speech Tokens: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Guo, Zhihan Li, Hankun Wang, Bohan Li, Chongtian Shao, Hanglei Zhang, Chenpeng Du, Xie Chen, Shujie Liu, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of speech generation technologies in the era of large
language models (LLMs) has established discrete speech tokens as a foundational
paradigm for speech representation. These tokens, characterized by their
discrete, compact, and concise nature, are not only advantageous for efficient
transmission and storage, but also inherently compatible with the language
modeling framework, enabling seamless integration of speech into text-dominated
LLM architectures. Current research categorizes discrete speech tokens into two
principal classes: acoustic tokens and semantic tokens, each of which has
evolved into a rich research domain characterized by unique design philosophies
and methodological approaches. This survey systematically synthesizes the
existing taxonomy and recent innovations in discrete speech tokenization,
conducts a critical examination of the strengths and limitations of each
paradigm, and presents systematic experimental comparisons across token types.
Furthermore, we identify persistent challenges in the field and propose
potential research directions, aiming to offer actionable insights to inspire
future advancements in the development and application of discrete speech
tokens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures, 3 tables. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive
  Streaming <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Shi, Géraldine Morin, Simone Gasparini, Wei Tsang Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Extended Reality (XR) requires efficient streaming of 3D online
worlds, challenging current 3DGS representations to adapt to
bandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS
that supports adaptive streaming and progressive rendering. Our method
constructs a layered structure for cumulative representation, incorporates
dynamic opacity optimization to maintain visual fidelity, and utilizes
occupancy maps to efficiently manage Gaussian splats. This proposed model
offers a progressive representation supporting a continuous rendering quality
adapted for bandwidth-aware streaming. Extensive experiments validate the
effectiveness of our approach in balancing visual fidelity with the compactness
of the model, with up to 50.71% improvement in SSIM, 286.53% improvement in
LPIPS with 23% of the original model size, and shows its potential for
bandwidth-adapted 3D streaming and rendering applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV 2025; Project Page: https://yuang-ian.github.io/lapisgs/ ; Code:
  https://github.com/nus-vv-streams/lapis-gs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Identity-Aware Cross-Modal Retrieval: a <span class="highlight-title">Dataset</span> and a Baseline <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Messina, Lucia Vadicamo, Leo Maltese, Claudio Gennaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in deep learning have significantly enhanced
content-based retrieval methods, notably through models like CLIP that map
images and texts into a shared embedding space. However, these methods often
struggle with domain-specific entities and long-tail concepts absent from their
training data, particularly in identifying specific individuals. In this paper,
we explore the task of identity-aware cross-modal retrieval, which aims to
retrieve images of persons in specific contexts based on natural language
queries. This task is critical in various scenarios, such as for searching and
browsing personalized video collections or large audio-visual archives
maintained by national broadcasters. We introduce a novel dataset, COCO Person
FaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched
with deepfake-generated faces from VGGFace2. This dataset addresses the lack of
large-scale datasets needed for training and evaluating models for this task.
Our experiments assess the performance of different CLIP variations repurposed
for this task, including our architecture, Identity-aware CLIP (Id-CLIP), which
achieves competitive retrieval performance through targeted fine-tuning. Our
contributions lay the groundwork for more robust cross-modal retrieval systems
capable of recognizing long-tail identities and contextual nuances. Data and
code are available at https://github.com/mesnico/IdCLIP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as full paper at ECIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Confident Masking Attention Network for Audio-Visual
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Jinchao Zhu, Feng Dong, Shuyue Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio and visual signals typically occur simultaneously, and humans possess
an innate ability to correlate and synchronize information from these two
modalities. Recently, a challenging problem known as Audio-Visual Segmentation
(AVS) has emerged, intending to produce segmentation maps for sounding objects
within a scene. However, the methods proposed so far have not sufficiently
integrated audio and visual information, and the computational costs have been
extremely high. Additionally, the outputs of different stages have not been
fully utilized. To facilitate this research, we introduce a novel Progressive
Confident Masking Attention Network (PMCANet). It leverages attention
mechanisms to uncover the intrinsic correlations between audio signals and
visual frames. Furthermore, we design an efficient and effective
cross-attention module to enhance semantic perception by selecting query
tokens. This selection is determined through confidence-driven units based on
the network's multi-stage predictive outputs. Experiments demonstrate that our
network outperforms other AVS methods while requiring less computational
resources. The code is available at: https://github.com/PrettyPlate/PCMANet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, submitted to Elsevier Knowledge-Based System</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-09T00:00:00Z">2025-02-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Working Memory: Query-Guided Segment Refinement for Enhanced
  Multimodal Understanding <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Diao, Chunhui Zhang, Weiyi Wu, Zhongyu Ouyang, Peijun Qing, Ming Cheng, Soroush Vosoughi, Jiang Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal foundation models (MFMs) have demonstrated significant success in
tasks such as visual captioning, question answering, and image-text retrieval.
However, these models face inherent limitations due to their finite internal
capacity, which restricts their ability to process extended temporal sequences,
a crucial requirement for comprehensive video and audio analysis. To overcome
these challenges, we introduce a specialized cognitive module, temporal working
memory (TWM), which aims to enhance the temporal modeling capabilities of MFMs.
It selectively retains task-relevant information across temporal dimensions,
ensuring that critical details are preserved throughout the processing of video
and audio content. The TWM uses a query-guided attention approach to focus on
the most informative multimodal segments within temporal sequences. By
retaining only the most relevant content, TWM optimizes the use of the model's
limited capacity, enhancing its temporal modeling ability. This plug-and-play
module can be easily integrated into existing MFMs. With our TWM, nine
state-of-the-art models exhibit significant performance improvements across
tasks such as video captioning, question answering, and video-text retrieval.
By enhancing temporal modeling, TWM extends the capability of MFMs to handle
complex, time-sensitive data effectively. Our code is available at
https://github.com/xid32/NAACL_2025_TWM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speaker Embedding Informed Audiovisual Active Speaker Detection for
  Egocentric Recordings <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Clarke, Yoshihiko Gotoh, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audiovisual active speaker detection (ASD) addresses the task of determining
the speech activity of a candidate speaker given acoustic and visual data.
Typically, systems model the temporal correspondence of audiovisual cues, such
as the synchronisation between speech and lip movement. Recent work has
explored extending this paradigm by additionally leveraging speaker embeddings
extracted from candidate speaker reference speech. This paper proposes the
speaker comparison auxiliary network (SCAN) which uses speaker-specific
information from both reference speech and the candidate audio signal to
disambiguate challenging scenes when the visual signal is unresolvable.
Furthermore, an improved method for enrolling face-speaker libraries is
developed, which implements a self-supervised approach to video-based face
recognition. Fitting with the recent proliferation of wearable devices, this
work focuses on improving speaker-embedding-informed ASD in the context of
egocentric recordings, which can be characterised by acoustic noise and highly
dynamic scenes. SCAN is implemented with two well-established baselines, namely
TalkNet and Light-ASD; yielding a relative improvement in mAP of 14.5% and
10.3% on the Ego4D benchmark, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025. 5 pages, 4 figures. To appear in Proceedings
  of IEEE International Conference on Acoustics, Speech and Signal Processing
  (ICASSP), April 6-11, 2025, Hyderabad, India</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-scale <span class="highlight-title">Dataset</span> with Behavior, Attributes, and Content of Mobile
  Short-<span class="highlight-title">video</span> Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Shang, Chen Gao, Nian Li, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short-video platforms show an increasing impact on people's daily lives
nowadays, with billions of active users spending plenty of time each day. The
interactions between users and online platforms give rise to many scientific
problems across computational social science and artificial intelligence.
However, despite the rapid development of short-video platforms, currently
there are serious shortcomings in existing relevant datasets on three aspects:
inadequate user-video feedback, limited user attributes and lack of video
content. To address these problems, we provide a large-scale dataset with rich
user behavior, attributes and video content from a real mobile short-video
platform. This dataset covers 10,000 voluntary users and 153,561 videos, and we
conduct four-fold technical validations of the dataset. First, we verify the
richness of the behavior and attribute data. Second, we confirm the
representing ability of the content features. Third, we provide benchmarking
results on recommendation algorithms with our dataset. Finally, we explore the
filter bubble phenomenon on the platform using the dataset. We believe the
dataset could support the broad research community, including but not limited
to user modeling, social science, human behavior understanding, etc. The
dataset and code is available at
https://github.com/tsinghua-fib-lab/ShortVideo_dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Hybrid Intelligent Approach for Multimodal Detection of Suspected
  Disinformation on TikTok 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jared D. T. Guerrero-Sosa, Andres Montoro-Montarroso, Francisco P. Romero, Jesus Serrano-Guerrero, Jose A. Olivas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of the rapid dissemination of multimedia content, identifying
disinformation on social media platforms such as TikTok represents a
significant challenge. This study introduces a hybrid framework that combines
the computational power of deep learning with the interpretability of fuzzy
logic to detect suspected disinformation in TikTok videos. The methodology is
comprised of two core components: a multimodal feature analyser that extracts
and evaluates data from text, audio, and video; and a multimodal disinformation
detector based on fuzzy logic. These systems operate in conjunction to evaluate
the suspicion of spreading disinformation, drawing on human behavioural cues
such as body language, speech patterns, and text coherence. Two experiments
were conducted: one focusing on context-specific disinformation and the other
on the scalability of the model across broader topics. For each video
evaluated, high-quality, comprehensive, well-structured reports are generated,
providing a detailed view of the disinformation behaviours.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-Retrieval: A Multi-Style Retrieval Framework for STEM's Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanhao Jia, Xinyi Wu, Hao Li, Qinglin Zhang, Yuxiao Hu, Shuai Zhao, Wenqi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In AI-facilitated teaching, leveraging various query styles to interpret
abstract text descriptions is crucial for ensuring high-quality teaching.
However, current retrieval models primarily focus on natural text-image
retrieval, making them insufficiently tailored to educational scenarios due to
the ambiguities in the retrieval process. In this paper, we propose a diverse
expression retrieval task tailored to educational scenarios, supporting
retrieval based on multiple query styles and expressions. We introduce the STEM
Education Retrieval Dataset (SER), which contains over 24,000 query pairs of
different styles, and the Uni-Retrieval, an efficient and style-diversified
retrieval vision-language model based on prompt tuning. Uni-Retrieval extracts
query style features as prototypes and builds a continuously updated Prompt
Bank containing prompt tokens for diverse queries. This bank can updated during
test time to represent domain-specific knowledge for different subject
retrieval scenarios. Our framework demonstrates scalability and robustness by
dynamically retrieving prompt tokens based on prototype similarity, effectively
facilitating learning for unknown queries. Experimental results indicate that
Uni-Retrieval outperforms existing retrieval models in most retrieval tasks.
This advancement provides a scalable and precise solution for diverse
educational needs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-08T00:00:00Z">2025-02-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-Aware Adaptive <span class="highlight-title">Video</span> Streaming Using Latent Diffusion Models
  for Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijiang Yan, Jianhua Pei, Hongda Wu, Hina Tabassum, Ping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework for real-time adaptive-bitrate video
streaming by integrating latent diffusion models (LDMs) within the FFmpeg
techniques. This solution addresses the challenges of high bandwidth usage,
storage inefficiencies, and quality of experience (QoE) degradation associated
with traditional constant bitrate streaming (CBS) and adaptive bitrate
streaming (ABS). The proposed approach leverages LDMs to compress I-frames into
a latent space, offering significant storage and semantic transmission savings
without sacrificing high visual quality. While it keeps B-frames and P-frames
as adjustment metadata to ensure efficient video reconstruction at the user
side, the proposed framework is complemented with the most state-of-the-art
denoising and video frame interpolation (VFI) techniques. These techniques
mitigate semantic ambiguity and restore temporal coherence between frames, even
in noisy wireless communication environments. Experimental results demonstrate
the proposed method achieves high-quality video streaming with optimized
bandwidth usage, outperforming state-of-the-art solutions in terms of QoE and
resource efficiency. This work opens new possibilities for scalable real-time
video streaming in 5G and future post-5G networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submission for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniForm: A Unified Diffusion Transformer for Audio-<span class="highlight-title">Video</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhao, Linfeng Feng, Dongxu Ge, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a natural multimodal content, audible video delivers an immersive sensory
experience. Consequently, audio-video generation systems have substantial
potential. However, existing diffusion-based studies mainly employ relatively
independent modules for generating each modality, which lack exploration of
shared-weight generative modules. This approach may under-use the intrinsic
correlations between audio and visual modalities, potentially resulting in
sub-optimal generation quality. To address this, we propose UniForm, a unified
diffusion transformer designed to enhance cross-modal consistency. By
concatenating auditory and visual information, UniForm learns to generate audio
and video simultaneously within a unified latent space, facilitating the
creation of high-quality and well-aligned audio-visual pairs. Extensive
experiments demonstrate the superior performance of our method in joint
audio-video generation, audio-guided video generation, and video-guided audio
generation tasks. Our demos are available at https://uniform-t2av.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our demos are available at https://uniform-t2av.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAMSIC: Content-aware Masked Image Modeling Transformer for Stereo Image
  Compression <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08505v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08505v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Shenyuan Gao, Zhening Liu, Jiawei Shao, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing learning-based stereo image codec adopt sophisticated transformation
with simple entropy models derived from single image codecs to encode latent
representations. However, those entropy models struggle to effectively capture
the spatial-disparity characteristics inherent in stereo images, which leads to
suboptimal rate-distortion results. In this paper, we propose a stereo image
compression framework, named CAMSIC. CAMSIC independently transforms each image
to latent representation and employs a powerful decoder-free Transformer
entropy model to capture both spatial and disparity dependencies, by
introducing a novel content-aware masked image modeling (MIM) technique. Our
content-aware MIM facilitates efficient bidirectional interaction between prior
information and estimated tokens, which naturally obviates the need for an
extra Transformer decoder. Experiments show that our stereo image codec
achieves state-of-the-art rate-distortion performance on two stereo image
datasets Cityscapes and InStereo2K with fast encoding and decoding speed. Code
is available at https://github.com/Xinjie-Q/CAMSIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-07T00:00:00Z">2025-02-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Swap Joint Diffusion for Long-Form Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Dai, Chenxi Wang, Chang Li, Chen Wang, Jun Du, Kewei Li, Ruoyu Wang, Jiefeng Ma, Lei Sun, Jianqing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work on long-form audio generation using global-view diffusion or
iterative generation demands significant training or inference costs. While
recent advancements in multi-view joint diffusion for panoramic generation
provide an efficient option, they struggle with spectrum generation with severe
overlap distortions and high cross-view consistency costs. We initially explore
this phenomenon through the connectivity inheritance of latent maps and uncover
that averaging operations excessively smooth the high-frequency components of
the latent map. To address these issues, we propose Swap Forward (SaFa), a
frame-level latent swap framework that synchronizes multiple diffusions to
produce a globally coherent long audio with more spectrum details in a
forward-only manner. At its core, the bidirectional Self-Loop Latent Swap is
applied between adjacent views, leveraging stepwise diffusion trajectory to
adaptively enhance high-frequency components without disrupting low-frequency
components. Furthermore, to ensure cross-view consistency, the unidirectional
Reference-Guided Latent Swap is applied between the reference and the
non-overlap regions of each subview during the early stages, providing
centralized trajectory guidance. Quantitative and qualitative experiments
demonstrate that SaFa significantly outperforms existing joint diffusion
methods and even training-based long audio generation models. Moreover, we find
that it also adapts well to panoramic generation, achieving comparable
state-of-the-art performance with greater efficiency and model
generalizability. Project page is available at https://swapforward.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Multimodal Empathetic Response Generation: A Rich
  Text-Speech-Vision Avatar-based Benchmark <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Zixiang Meng, Meng Luo, Hong Han, Lizi Liao, Erik Cambria, Hao Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathetic Response Generation (ERG) is one of the key tasks of the affective
computing area, which aims to produce emotionally nuanced and compassionate
responses to user's queries. However, existing ERG research is predominantly
confined to the singleton text modality, limiting its effectiveness since human
emotions are inherently conveyed through multiple modalities. To combat this,
we introduce an avatar-based Multimodal ERG (MERG) task, entailing rich text,
speech, and facial vision information. We first present a large-scale
high-quality benchmark dataset, \textbf{AvaMERG}, which extends traditional
text ERG by incorporating authentic human speech audio and dynamic talking-face
avatar videos, encompassing a diverse range of avatar profiles and broadly
covering various topics of real-world scenarios. Further, we deliberately
tailor a system, named \textbf{Empatheia}, for MERG. Built upon a Multimodal
Large Language Model (MLLM) with multimodal encoder, speech and avatar
generators, Empatheia performs end-to-end MERG, with Chain-of-Empathetic
reasoning mechanism integrated for enhanced empathy understanding and
reasoning. Finally, we devise a list of empathetic-enhanced tuning strategies,
strengthening the capabilities of emotional accuracy and content,
avatar-profile consistency across modalities. Experimental results on AvaMERG
data demonstrate that Empatheia consistently shows superior performance than
baseline methods on both textual ERG and MERG. Overall, this work is expected
to pioneer the MERG research by introducing a novel benchmark and an end-to-end
model, laying a solid foundation for future advancements in multimodal
empathetic response generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TheWebConf (WWW) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDStream: Slashing Long-Tail Delay in Interactive <span class="highlight-title">Video</span> Streaming via
  Pseudo-Dual Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuedou Xiao, Yingying Zuo, Mingxuan Yan, Kezhong Liu, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end (E2E) delay is critical for interactive video streaming (IVS)
experiences, but remains unsatisfactory for its long-tail distribution caused
by periodic large keyframes. Conventional optimization strategies, such as
jitter buffer, bitrate adaptation, and customized encoding, either sacrifice
clarity, average delay, or compatibility. To address this issue, we propose
PDStream, a novel pseudo-dual streaming algorithm, aimed at minimizing E2E
delay while maintaining video clarity. The core idea is to split the two
functions, delay-sensitive playback and delay-tolerant reference, on keyframes
through dual streaming. Specifically, the playback function is held by a second
parallel stream, which comprises much smaller non-keyframes and is allocated
more immediate bandwidth for real-time performance. The reference function is
ensured by the first stream with keyframe preservation, allocated more
subsequent bandwidth to smooth out bursty traffic. Additionally, ``pseudo''
minimizes computational and transmission overheads by restricting dual streams
to brief activation only when keyframes appear, supported by corresponding
dual-stream bitrate allocation and adaptation to ensure delay and clarity. We
implement PDStream on a WebRTC-based IVS testbed with real-world network
traces. Results show that PDStream significantly outperforms prior algorithms,
reducing average E2E delay by 17.5\% and slashing its 97th percentile by
33.3\%, while keeping clarity under varying bandwidth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE INFOCOM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-tailed Medical Diagnosis with Relation-aware Representation
  Learning and Iterative Classifier Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Pan, Yupei Zhang, Qiushi Yang, Tan Li, Zhen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently computer-aided diagnosis has demonstrated promising performance,
effectively alleviating the workload of clinicians. However, the inherent
sample imbalance among different diseases leads algorithms biased to the
majority categories, leading to poor performance for rare categories. Existing
works formulated this challenge as a long-tailed problem and attempted to
tackle it by decoupling the feature representation and classification. Yet, due
to the imbalanced distribution and limited samples from tail classes, these
works are prone to biased representation learning and insufficient classifier
calibration. To tackle these problems, we propose a new Long-tailed Medical
Diagnosis (LMD) framework for balanced medical image classification on
long-tailed datasets. In the initial stage, we develop a Relation-aware
Representation Learning (RRL) scheme to boost the representation ability by
encouraging the encoder to capture intrinsic semantic features through
different data augmentations. In the subsequent stage, we propose an Iterative
Classifier Calibration (ICC) scheme to calibrate the classifier iteratively.
This is achieved by generating a large number of balanced virtual features and
fine-tuning the encoder using an Expectation-Maximization manner. The proposed
ICC compensates for minority categories to facilitate unbiased classifier
optimization while maintaining the diagnostic knowledge in majority classes.
Comprehensive experiments on three public long-tailed medical datasets
demonstrate that our LMD framework significantly surpasses state-of-the-art
approaches. The source code can be accessed at
https://github.com/peterlipan/LMD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted in Computers in Biology and Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An End-to-End Pipeline Perspective on <span class="highlight-title">Video</span> Streaming in Best-Effort
  Networks: A <span class="highlight-title">Survey</span> and Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05192v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05192v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Peroni, Sergey Gorinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remaining a dominant force in Internet traffic, video streaming captivates
end users, service providers, and researchers. This paper takes a pragmatic
approach to reviewing recent advances in the field by focusing on the prevalent
streaming paradigm that involves delivering long-form two-dimensional videos
over the best-effort Internet with client-side adaptive bitrate (ABR)
algorithms and assistance from content delivery networks (CDNs). To enhance
accessibility, we supplement the survey with tutorial material. Unlike existing
surveys that offer fragmented views, our work provides a holistic perspective
on the entire end-to-end streaming pipeline, from video capture by a
camera-equipped device to playback by the end user. Our novel perspective
covers the ingestion, processing, and distribution stages of the pipeline and
addresses key challenges such as video compression, upload, transcoding, ABR
algorithms, CDN support, and quality of experience. We review over 200 papers
and classify streaming designs by their problem-solving methodology, whether
based on intuition (simple heuristics), theory (formal optimization), or
machine learning (generalizable data patterns). The survey further refines
these methodology-based categories and characterizes each design by additional
traits such as compatible codecs and use of super resolution. We connect the
reviewed research to real-world applications by discussing the practices of
commercial streaming platforms. Finally, the survey highlights prominent
current trends and outlines future directions in video streaming.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-06T00:00:00Z">2025-02-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based
  Speech Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Ye, Xinfa Zhu, Chi-Min Chan, Xinsheng Wang, Xu Tan, Jiahe Lei, Yi Peng, Haohe Liu, Yizhu Jin, Zheqi DAI, Hongzhan Lin, Jianyi Chen, Xingjian Du, Liumeng Xue, Yunlin Chen, Zhifei Li, Lei Xie, Qiuqiang Kong, Yike Guo, Wei Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-based large language models (LLMs), particularly in
the GPT series and the o1 model, have demonstrated the effectiveness of scaling
both training-time and inference-time compute. However, current
state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring
separate models (e.g., diffusion models after LLM), complicating the decision
of whether to scale a particular model during training or testing. This work
makes the following contributions: First, we explore the scaling of train-time
and inference-time compute for speech synthesis. Second, we propose a simple
framework Llasa for speech synthesis that employs a single-layer vector
quantizer (VQ) codec and a single Transformer architecture to fully align with
standard LLMs such as Llama. Our experiments reveal that scaling train-time
compute for Llasa consistently improves the naturalness of synthesized speech
and enables the generation of more complex and accurate prosody patterns.
Furthermore, from the perspective of scaling inference-time compute, we employ
speech understanding models as verifiers during the search, finding that
scaling inference-time compute shifts the sampling modes toward the preferences
of specific verifiers, thereby improving emotional expressiveness, timbre
consistency, and content accuracy. In addition, we released the checkpoint and
training code for our TTS model (1B, 3B, 8B) and codec model publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CDIO: Cross-Domain Inference Optimization with Resource Preference
  Prediction for Edge-Cloud Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheming Yang, Wen Ji, Qi Guo, Dieli Hu, Chang Zhao, Xiaowei Li, Xuanlei Zhao, Yi Zhao, Chaoyu Gong, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, massive video tasks are processed by edge-cloud collaboration.
However, the diversity of task requirements and the dynamics of resources pose
great challenges to efficient inference, resulting in many wasted resources. In
this paper, we present CDIO, a cross-domain inference optimization framework
designed for edge-cloud collaboration. For diverse input tasks, CDIO can
predict resource preference types by analyzing spatial complexity and
processing requirements of the task. Subsequently, a cross-domain collaborative
optimization algorithm is employed to guide resource allocation in the
edge-cloud system. By ensuring that each task is matched with the ideal
servers, the edge-cloud system can achieve higher efficiency inference. The
evaluation results on public datasets demonstrate that CDIO can effectively
meet the accuracy and delay requirements for task processing. Compared to
state-of-the-art edge-cloud solutions, CDIO achieves a computing and bandwidth
consumption reduction of 20%-40%. And it can reduce energy consumption by more
than 40%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed
  Modalities and Heterogeneous Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.04400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.04400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keke Gai, Mohan Wang, Jing Yu, Dongjue Wang, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Federated Learning (MFL) enables multiple clients to
collaboratively train models on multimodal data while ensuring clients'
privacy. However, modality and task heterogeneity hinder clients from learning
a unified representation, weakening local model generalization, especially in
MFL with mixed modalities where only some clients have multimodal data. In this
work, we propose an Adaptive prototype-based Multimodal Federated Learning
(AproMFL) framework for mixed modalities and heterogeneous tasks to address the
aforementioned issues. Our AproMFL transfers knowledge through
adaptively-constructed prototypes without a prior public dataset. Clients
adaptively select prototype construction methods in line with tasks; server
converts client prototypes into unified multimodal prototypes and aggregates
them to form global prototypes, avoid clients keeping unified labels. We divide
the model into various modules and only aggregate mapping modules to reduce
communication and computation overhead. To address aggregation issues in
heterogeneity, we develop a client relationship graph-based scheme to
dynamically adjust aggregation weights. Extensive experiments on representative
datasets evidence effectiveness of AproMFL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MD-BERT: Action Recognition in Dark <span class="highlight-title">Video</span>s via Dynamic Multi-Stream
  Fusion and Temporal Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharana Dharshikgan Suresh Dass, Hrishav Bakul Barua, Ganesh Krishnasamy, Raveendran Paramesran, Raphael C. -W. Phan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action recognition in dark, low-light (under-exposed) or noisy videos is a
challenging task due to visibility degradation, which can hinder critical
spatiotemporal details. This paper proposes MD-BERT, a novel multi-stream
approach that integrates complementary pre-processing techniques such as gamma
correction and histogram equalization alongside raw dark frames to address
these challenges. We introduce the Dynamic Feature Fusion (DFF) module,
extending existing attentional fusion methods to a three-stream setting,
thereby capturing fine-grained and global contextual information across
different brightness and contrast enhancements. The fused spatiotemporal
features are then processed by a BERT-based temporal model, which leverages its
bidirectional self-attention to effectively capture long-range dependencies and
contextual relationships across frames. Extensive experiments on the ARID V1.0
and ARID V1.5 dark video datasets show that MD-BERT outperforms existing
methods, establishing a new state-of-the-art performance. Ablation studies
further highlight the individual contributions of each input stream and the
effectiveness of the proposed DFF and BERT modules. The official website of
this work is available at: https://github.com/HrishavBakulBarua/DarkBERT
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Accuracy and Generalization for Efficient Visual Tracking <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ram Zaveri, Shivang Patel, Yu Gu, Gianfranco Doretto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient visual trackers overfit to their training distributions and lack
generalization abilities, resulting in them performing well on their respective
in-distribution (ID) test sets and not as well on out-of-distribution (OOD)
sequences, imposing limitations to their deployment in-the-wild under
constrained resources. We introduce SiamABC, a highly efficient Siamese tracker
that significantly improves tracking performance, even on OOD sequences.
SiamABC takes advantage of new architectural designs in the way it bridges the
dynamic variability of the target, and of new losses for training. Also, it
directly addresses OOD tracking generalization by including a fast
backward-free dynamic test-time adaptation method that continuously adapts the
model according to the dynamic visual changes of the target. Our extensive
experiments suggest that SiamABC shows remarkable performance gains in OOD sets
while maintaining accurate performance on the ID benchmarks. SiamABC
outperforms MixFormerV2-S by 7.6\% on the OOD AVisT benchmark while being 3x
faster (100 FPS) on a CPU. Our code and models are available at
https://wvuvl.github.io/SiamABC/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-05T00:00:00Z">2025-02-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing World Dynamics in a Nutshell 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhong Shen, Xuanyu Yi, Mingbao Lin, Hanwang Zhang, Shuicheng Yan, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of efficiently representing casually captured
monocular videos in a spatially- and temporally-coherent manner. While existing
approaches predominantly rely on 2D/2.5D techniques treating videos as
collections of spatiotemporal pixels, they struggle with complex motions,
occlusions, and geometric consistency due to absence of temporal coherence and
explicit 3D structure. Drawing inspiration from monocular video as a projection
of the dynamic 3D world, we explore representing videos in their intrinsic 3D
form through continuous flows of Gaussian primitives in space-time. In this
paper, we propose NutWorld, a novel framework that efficiently transforms
monocular videos into dynamic 3D Gaussian representations in a single forward
pass. At its core, NutWorld introduces a structured spatial-temporal aligned
Gaussian (STAG) representation, enabling optimization-free scene modeling with
effective depth and flow regularization. Through comprehensive experiments, we
demonstrate that NutWorld achieves high-fidelity video reconstruction quality
while enabling various downstream applications in real-time. Demos and code
will be available at https://github.com/Nut-World/NutWorld.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Vision Language Model Fine-tuning for Text-based Person
  Anomaly Search <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi He, Shengeng Tang, Ao Liu, Lechao Cheng, Jingjing Wu, Yanyan Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the HFUT-LMC team's solution to the WWW 2025 challenge on
Text-based Person Anomaly Search (TPAS). The primary objective of this
challenge is to accurately identify pedestrians exhibiting either normal or
abnormal behavior within a large library of pedestrian images. Unlike
traditional video analysis tasks, TPAS significantly emphasizes understanding
and interpreting the subtle relationships between text descriptions and visual
data. The complexity of this task lies in the model's need to not only match
individuals to text descriptions in massive image datasets but also accurately
differentiate between search results when faced with similar descriptions. To
overcome these challenges, we introduce the Similarity Coverage Analysis (SCA)
strategy to address the recognition difficulty caused by similar text
descriptions. This strategy effectively enhances the model's capacity to manage
subtle differences, thus improving both the accuracy and reliability of the
search. Our proposed solution demonstrated excellent performance in this
challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2025 WWW Workshop on MORE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Implicit Multimodal Knowledge into Large Language Models for
  Zero-Resource Dialogue Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Zhang, Hui Ma, Jian Ding, Jian Wang, Bo Xu, Hongfei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating multimodal knowledge into large language models (LLMs) represents
a significant advancement in dialogue generation capabilities. However, the
effective incorporation of such knowledge in zero-resource scenarios remains a
substantial challenge due to the scarcity of diverse, high-quality dialogue
datasets. To address this, we propose the Visual Implicit Knowledge
Distillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs
for enriched dialogue generation in zero-resource contexts by leveraging
implicit multimodal knowledge. VIKDF comprises two main stages: knowledge
distillation, using an Implicit Query Transformer to extract and encode visual
implicit knowledge from image-text pairs into knowledge vectors; and knowledge
integration, employing a novel Bidirectional Variational Information Fusion
technique to seamlessly integrate these distilled vectors into LLMs. This
enables the LLMs to generate dialogues that are not only coherent and engaging
but also exhibit a deep understanding of the context through implicit
multimodal cues, effectively overcoming the limitations of zero-resource
scenarios. Our extensive experimentation across two dialogue datasets shows
that VIKDF outperforms existing state-of-the-art models in generating
high-quality dialogues. The code is available at
https://github.com/zhangbo-nlp/VIKDF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Information Fusion. The code is available at
  https://github.com/zhangbo-nlp/VIKDF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs <span class="chip">NAACL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10020v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10020v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Luo, Ke Lin, Chao Gu, Jiahui Hou, Lijie Wen, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of large language models (LLMs) in generating content
raises concerns about text copyright. Watermarking methods, particularly
logit-based approaches, embed imperceptible identifiers into text to address
these challenges. However, the widespread usage of watermarking across diverse
LLMs has led to an inevitable issue known as watermark collision during common
tasks, such as paraphrasing or translation. In this paper, we introduce
watermark collision as a novel and general philosophy for watermark attacks,
aimed at enhancing attack performance on top of any other attacking methods. We
also provide a comprehensive demonstration that watermark collision poses a
threat to all logit-based watermark algorithms, impacting not only specific
attack scenarios but also downstream applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long Paper, 9 pages, accepted at NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-04T00:00:00Z">2025-02-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMER: Crafting Interactive Extended Reality Worlds with JSON Data
  Generated by Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangong Chen, Xiaoyi Wu, Tian Lan, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Large Language Models (LLMs) like GPT-4 with Extended
Reality (XR) technologies offers the potential to build truly immersive XR
environments that interact with human users through natural language, e.g.,
generating and animating 3D scenes from audio inputs. However, the complexity
of XR environments makes it difficult to accurately extract relevant contextual
data and scene/object parameters from an overwhelming volume of XR artifacts.
It leads to not only increased costs with pay-per-use models, but also elevated
levels of generation errors. Moreover, existing approaches focusing on coding
script generation are often prone to generation errors, resulting in flawed or
invalid scripts, application crashes, and ultimately a degraded user
experience. To overcome these challenges, we introduce LLMER, a novel framework
that creates interactive XR worlds using JSON data generated by LLMs. Unlike
prior approaches focusing on coding script generation, LLMER translates natural
language inputs into JSON data, significantly reducing the likelihood of
application crashes and processing latency. It employs a multi-stage strategy
to supply only the essential contextual information adapted to the user's
request and features multiple modules designed for various XR tasks. Our
preliminary user study reveals the effectiveness of the proposed system, with
over 80% reduction in consumed tokens and around 60% reduction in task
completion time compared to state-of-the-art approaches. The analysis of users'
feedback also illuminates a series of directions for further optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the latent space of diffusion models directly through singular
  value decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Wang, Boyan Gao, Yanran Li, Zhao Wang, Xiaosong Yang, David A. Clifton, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the groundbreaking success of diffusion models in generating
high-fidelity images, their latent space remains relatively under-explored,
even though it holds significant promise for enabling versatile and
interpretable image editing capabilities. The complicated denoising trajectory
and high dimensionality of the latent space make it extremely challenging to
interpret. Existing methods mainly explore the feature space of U-Net in
Diffusion Models (DMs) instead of the latent space itself. In contrast, we
directly investigate the latent space via Singular Value Decomposition (SVD)
and discover three useful properties that can be used to control generation
results without the requirements of data collection and maintain identity
fidelity generated images. Based on these properties, we propose a novel image
editing framework that is capable of learning arbitrary attributes from one
pair of latent codes destined by text prompts in Stable Diffusion Models. To
validate our approach, extensive experiments are conducted to demonstrate its
effectiveness and flexibility in image editing. We will release our codes soon
to foster further research and applications in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EditIQ: Automated Cinematic Editing of Static Wide-Angle <span class="highlight-title">Video</span>s via
  Dialogue Interpretation and Saliency Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Girmaji, Bhav Beri, Ramanathan Subramanian, Vineet Gandhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present EditIQ, a completely automated framework for cinematically editing
scenes captured via a stationary, large field-of-view and high-resolution
camera. From the static camera feed, EditIQ initially generates multiple
virtual feeds, emulating a team of cameramen. These virtual camera shots termed
rushes are subsequently assembled using an automated editing algorithm, whose
objective is to present the viewer with the most vivid scene content. To
understand key scene elements and guide the editing process, we employ a
two-pronged approach: (1) a large language model (LLM)-based dialogue
understanding module to analyze conversational flow, coupled with (2) visual
saliency prediction to identify meaningful scene elements and camera shots
therefrom. We then formulate cinematic video editing as an energy minimization
problem over shot selection, where cinematic constraints determine shot
choices, transitions, and continuity. EditIQ synthesizes an aesthetically and
visually compelling representation of the original narrative while maintaining
cinematic coherence and a smooth viewing experience. Efficacy of EditIQ against
competing baselines is demonstrated via a psychophysical study involving twenty
participants on the BBC Old School dataset plus eleven theatre performance
videos. Video samples from EditIQ can be found at
https://editiq-ave.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 30th International Conference on Intelligent User
  Interfaces (IUI 25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Images that Sound: Composing Images and Sounds on a Single Canvas <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Daniel Geng, Andrew Owens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spectrograms are 2D representations of sound that look very different from
the images found in our visual world. And natural images, when played as
spectrograms, make unnatural sounds. In this paper, we show that it is possible
to synthesize spectrograms that simultaneously look like natural images and
sound like natural audio. We call these visual spectrograms images that sound.
Our approach is simple and zero-shot, and it leverages pre-trained
text-to-image and text-to-spectrogram diffusion models that operate in a shared
latent space. During the reverse process, we denoise noisy latents with both
the audio and image diffusion models in parallel, resulting in a sample that is
likely under both models. Through quantitative evaluations and perceptual
studies, we find that our method successfully generates spectrograms that align
with a desired audio prompt while also taking the visual appearance of a
desired image prompt. Please see our project page for video results:
https://ificl.github.io/images-that-sound/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. Project site:
  https://ificl.github.io/images-that-sound/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIDI-GPT: A <span class="highlight-title">Control</span>lable Generative Model for Computer-Assisted
  Multitrack Music Composition <span class="chip">AAAI 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Pasquier, Jeff Ens, Nathan Fradet, Paul Triana, Davide Rizzotti, Jean-Baptiste Rolland, Maryam Safi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present and release MIDI-GPT, a generative system based on the Transformer
architecture that is designed for computer-assisted music composition
workflows. MIDI-GPT supports the infilling of musical material at the track and
bar level, and can condition generation on attributes including: instrument
type, musical style, note density, polyphony level, and note duration. In order
to integrate these features, we employ an alternative representation for
musical material, creating a time-ordered sequence of musical events for each
track and concatenating several tracks into a single sequence, rather than
using a single time-ordered sequence where the musical events corresponding to
different tracks are interleaved. We also propose a variation of our
representation allowing for expressiveness. We present experimental results
that demonstrate that MIDI-GPT is able to consistently avoid duplicating the
musical material it was trained on, generate music that is stylistically
similar to the training dataset, and that attribute controls allow enforcing
various constraints on the generated material. We also outline several
real-world applications of MIDI-GPT, including collaborations with industry
partners that explore the integration and evaluation of MIDI-GPT into
commercial products, as well as several artistic works produced using it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EALD-MLLM: E<span class="highlight-title">motion</span> Analysis in Long-sequential and De-identity <span class="highlight-title">video</span>s
  with Multi-modal Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deng Li, Xin Liu, Bohao Xing, Baiqiang Xia, Yuan Zong, Bihan Wen, Heikki Kälviäinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion AI is the ability of computers to understand human emotional states.
Existing works have achieved promising progress, but two limitations remain to
be solved: 1) Previous studies have been more focused on short sequential video
emotion analysis while overlooking long sequential video. However, the emotions
in short sequential videos only reflect instantaneous emotions, which may be
deliberately guided or hidden. In contrast, long sequential videos can reveal
authentic emotions; 2) Previous studies commonly utilize various signals such
as facial, speech, and even sensitive biological signals (e.g.,
electrocardiogram). However, due to the increasing demand for privacy,
developing Emotion AI without relying on sensitive signals is becoming
important. To address the aforementioned limitations, in this paper, we
construct a dataset for Emotion Analysis in Long-sequential and De-identity
videos called EALD by collecting and processing the sequences of athletes'
post-match interviews. In addition to providing annotations of the overall
emotional state of each video, we also provide the Non-Facial Body Language
(NFBL) annotations for each player. NFBL is an inner-driven emotional
expression and can serve as an identity-free clue to understanding the
emotional state. Moreover, we provide a simple but effective baseline for
further research. More precisely, we evaluate the Multimodal Large Language
Models (MLLMs) with de-identification signals (e.g., visual, speech, and NFBLs)
to perform emotion analysis. Our experimental results demonstrate that: 1)
MLLMs can achieve comparable, even better performance than the supervised
single-modal models, even in a zero-shot scenario; 2) NFBL is an important cue
in long sequential emotion analysis. EALD will be available on the open-source
platform.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-03T00:00:00Z">2025-02-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Inverse Attention Network with Intrinsic Discriminant Feature
  Exploitation for Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlin Zhang, En Yu, Yi Shao, Shuai Li, Sujuan Hou, Jiande Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal fake news detection has garnered significant attention due to its
profound implications for social security. While existing approaches have
contributed to understanding cross-modal consistency, they often fail to
leverage modal-specific representations and explicit discrepant features. To
address these limitations, we propose a Multimodal Inverse Attention Network
(MIAN), a novel framework that explores intrinsic discriminative features based
on news content to advance fake news detection. Specifically, MIAN introduces a
hierarchical learning module that captures diverse intra-modal relationships
through local-to-global and local-to-local interactions, thereby generating
enhanced unimodal representations to improve the identification of fake news at
the intra-modal level. Additionally, a cross-modal interaction module employs a
co-attention mechanism to establish and model dependencies between the refined
unimodal representations, facilitating seamless semantic integration across
modalities. To explicitly extract inconsistency features, we propose an inverse
attention mechanism that effectively highlights the conflicting patterns and
semantic deviations introduced by fake news in both intra- and inter-modality.
Extensive experiments on benchmark datasets demonstrate that MIAN significantly
outperforms state-of-the-art methods, underscoring its pivotal contribution to
advancing social security through enhanced multimodal fake news detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BC-GAN: A Generative Adversarial Network for Synthesizing a Batch of
  Collocated Clothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Zhou, Haijun Zhang, Jianghong Ma, Jianyang Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collocated clothing synthesis using generative networks has become an
emerging topic in the field of fashion intelligence, as it has significant
potential economic value to increase revenue in the fashion industry. In
previous studies, several works have attempted to synthesize
visually-collocated clothing based on a given clothing item using generative
adversarial networks (GANs) with promising results. These works, however, can
only accomplish the synthesis of one collocated clothing item each time.
Nevertheless, users may require different clothing items to meet their multiple
choices due to their personal tastes and different dressing scenarios. To
address this limitation, we introduce a novel batch clothing generation
framework, named BC-GAN, which is able to synthesize multiple
visually-collocated clothing images simultaneously. In particular, to further
improve the fashion compatibility of synthetic results, BC-GAN proposes a new
fashion compatibility discriminator in a contrastive learning perspective by
fully exploiting the collocation relationship among all clothing items. Our
model was examined in a large-scale dataset with compatible outfits constructed
by ourselves. Extensive experiment results confirmed the effectiveness of our
proposed BC-GAN in comparison to state-of-the-art methods in terms of
diversity, visual authenticity, and fashion compatibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by IEEE TCSVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FCBoost-Net: A Generative Network for Synthesizing Multiple Collocated
  Outfits via Fashion Compatibility Boosting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Zhou, Haijun Zhang, Jianghong Ma, Jicong Fan, Zhao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outfit generation is a challenging task in the field of fashion technology,
in which the aim is to create a collocated set of fashion items that complement
a given set of items. Previous studies in this area have been limited to
generating a unique set of fashion items based on a given set of items, without
providing additional options to users. This lack of a diverse range of choices
necessitates the development of a more versatile framework. However, when the
task of generating collocated and diversified outfits is approached with
multimodal image-to-image translation methods, it poses a challenging problem
in terms of non-aligned image translation, which is hard to address with
existing methods. In this research, we present FCBoost-Net, a new framework for
outfit generation that leverages the power of pre-trained generative models to
produce multiple collocated and diversified outfits. Initially, FCBoost-Net
randomly synthesizes multiple sets of fashion items, and the compatibility of
the synthesized sets is then improved in several rounds using a novel fashion
compatibility booster. This approach was inspired by boosting algorithms and
allows the performance to be gradually improved in multiple steps. Empirical
evidence indicates that the proposed strategy can improve the fashion
compatibility of randomly synthesized fashion items as well as maintain their
diversity. Extensive experiments confirm the effectiveness of our proposed
framework with respect to visual authenticity, diversity, and fashion
compatibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at ACM Multimedia 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure & Personalized Music-to-<span class="highlight-title">Video</span> Generation via CHARCHA <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehul Agarwal, Gauri Agarwal, Santiago Benoit, Andrew Lippman, Jean Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music is a deeply personal experience and our aim is to enhance this with a
fully-automated pipeline for personalized music video generation. Our work
allows listeners to not just be consumers but co-creators in the music video
generation process by creating personalized, consistent and context-driven
visuals based on lyrics, rhythm and emotion in the music. The pipeline combines
multimodal translation and generation techniques and utilizes low-rank
adaptation on listeners' images to create immersive music videos that reflect
both the music and the individual. To ensure the ethical use of users'
identity, we also introduce CHARCHA (patent pending), a facial identity
verification protocol that protects people against unauthorized use of their
face while at the same time collecting authorized images from users for
personalizing their videos. This paper thus provides a secure and innovative
framework for creating deeply personalized music videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Creative AI Track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-02T00:00:00Z">2025-02-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Fine-Grained Guidance for Diffusion-Based Symbolic Music
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyu Zhu, Haoyu Liu, Ziyu Wang, Zhimin Jiang, Zeyu Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing generative models to create or conditionally create symbolic music
presents unique challenges due to the combination of limited data availability
and the need for high precision in note pitch. To address these challenges, we
introduce an efficient Fine-Grained Guidance (FGG) approach within diffusion
models. FGG guides the diffusion models to generate music that aligns more
closely with the control and intent of expert composers, which is critical to
improve the accuracy, listenability, and quality of generated music. This
approach empowers diffusion models to excel in advanced applications such as
improvisation, and interactive music creation. We derive theoretical
characterizations for both the challenges in symbolic music generation and the
effects of the FGG approach. We provide numerical experiments and subjective
evaluation to demonstrate the effectiveness of our approach. We have published
a demo page to showcase performances, as one of the first in the symbolic music
literature's demo pages that enables real-time interactive generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Image Generation with Large Multimodal Models <span class="chip">WWW'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyan Xu, Wenjie Wang, Yang Zhang, Biao Tang, Peng Yan, Fuli Feng, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized content filtering, such as recommender systems, has become a
critical infrastructure to alleviate information overload. However, these
systems merely filter existing content and are constrained by its limited
diversity, making it difficult to meet users' varied content needs. To address
this limitation, personalized content generation has emerged as a promising
direction with broad applications. Nevertheless, most existing research focuses
on personalized text generation, with relatively little attention given to
personalized image generation. The limited work in personalized image
generation faces challenges in accurately capturing users' visual preferences
and needs from noisy user-interacted images and complex multimodal
instructions. Worse still, there is a lack of supervised data for training
personalized image generation models.
  To overcome the challenges, we propose a Personalized Image Generation
Framework named Pigeon, which adopts exceptional large multimodal models with
three dedicated modules to capture users' visual preferences and needs from
noisy user history and multimodal instructions. To alleviate the data scarcity,
we introduce a two-stage preference alignment scheme, comprising masked
preference reconstruction and pairwise preference alignment, to align Pigeon
with the personalized image generation task. We apply Pigeon to personalized
sticker and movie poster generation, where extensive quantitative results and
human evaluation highlight its superiority over various generative baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in WWW'25</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-01T00:00:00Z">2025-02-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When End-to-End is Overkill: Rethinking Cascaded Speech-to-Text
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Min, Chenxu Hu, Yi Ren, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though end-to-end speech-to-text translation has been a great success, we
argue that the cascaded speech-to-text translation model still has its place,
which is usually criticized for the error propagation between automatic speech
recognition (ASR) and machine translation (MT) models. In this paper, we
explore the benefits of incorporating multiple candidates from ASR and
self-supervised speech features into MT. Our analysis reveals that the primary
cause of cascading errors stems from the increased divergence between similar
samples in the speech domain when mapped to the text domain. By including
multiple candidates and self-supervised speech features, our approach allows
the machine translation model to choose the right words and ensure precise
translation using various speech samples. This strategy minimizes error spread
and takes advantage of large ASR and MT datasets, along with pre-trained ASR/MT
models, while addressing associated issues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unit-based System and <span class="highlight-title">Dataset</span> for Expressive Direct Speech-to-Speech
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Min, Chenxu Hu, Yi Ren, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research in speech-to-speech translation (S2ST) primarily
concentrates on translation accuracy and speech naturalness, often overlooking
key elements like paralinguistic information, which is essential for conveying
emotions and attitudes in communication. To address this, our research
introduces a novel, carefully curated multilingual dataset from various movie
audio tracks. Each dataset pair is precisely matched for paralinguistic
information and duration. We enhance this by integrating multiple prosody
transfer techniques, aiming for translations that are accurate,
natural-sounding, and rich in paralinguistic details. Our experimental results
confirm that our model retains more paralinguistic information from the source
speech while maintaining high standards of translation accuracy and
naturalness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Audio-Visual Segmentation Models Truly Segment Sounding Objects? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Li, Wenjie Zhao, Ziru Huang, Yunhui Guo, Yapeng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike traditional visual segmentation, audio-visual segmentation (AVS)
requires the model not only to identify and segment objects but also to
determine whether they are sound sources. Recent AVS approaches, leveraging
transformer architectures and powerful foundation models like SAM, have
achieved impressive performance on standard benchmarks. Yet, an important
question remains: Do these models genuinely integrate audio-visual cues to
segment sounding objects? In this paper, we systematically investigate this
issue in the context of robust AVS. Our study reveals a fundamental bias in
current methods: they tend to generate segmentation masks based predominantly
on visual salience, irrespective of the audio context. This bias results in
unreliable predictions when sounds are absent or irrelevant. To address this
challenge, we introduce AVSBench-Robust, a comprehensive benchmark
incorporating diverse negative audio scenarios including silence, ambient
noise, and off-screen sounds. We also propose a simple yet effective approach
combining balanced training with negative samples and classifier-guided
similarity learning. Our extensive experiments show that state-of-theart AVS
methods consistently fail under negative audio conditions, demonstrating the
prevalence of visual bias. In contrast, our approach achieves remarkable
improvements in both standard metrics and robustness measures, maintaining
near-perfect false positive rates while preserving highquality segmentation
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Data Deluge to Data Curation: A Filtering-WoRA Paradigm for
  Efficient Text-based Person Search <span class="chip">WWW
  '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10292v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10292v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Sun, Hao Fei, Zhedong Zheng, Gangyi Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In text-based person search endeavors, data generation has emerged as a
prevailing practice, addressing concerns over privacy preservation and the
arduous task of manual annotation. Although the number of synthesized data can
be infinite in theory, the scientific conundrum persists that how much
generated data optimally fuels subsequent model training. We observe that only
a subset of the data in these constructed datasets plays a decisive role.
Therefore, we introduce a new Filtering-WoRA paradigm, which contains a
filtering algorithm to identify this crucial data subset and WoRA (Weighted
Low-Rank Adaptation) learning strategy for light fine-tuning. The filtering
algorithm is based on the cross-modality relevance to remove the lots of coarse
matching synthesis pairs. As the number of data decreases, we do not need to
fine-tune the entire model. Therefore, we propose a WoRA learning strategy to
efficiently update a minimal portion of model parameters. WoRA streamlines the
learning process, enabling heightened efficiency in extracting knowledge from
fewer, yet potent, data instances. Extensive experimentation validates the
efficacy of pretraining, where our model achieves advanced and efficient
retrieval performance on challenging real-world benchmarks. Notably, on the
CUHK-PEDES dataset, we have achieved a competitive mAP of 67.02% while reducing
model training time by 19.82%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, Proceedings of the ACM Web Conference 2025 (WWW
  '25)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-31T00:00:00Z">2025-01-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verifying Cross-modal Entity Consistency in News using Vision-language
  Models <span class="chip">ECIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Tahmasebi, David Ernst, Eric Müller-Budack, Ralph Ewerth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The web has become a crucial source of information, but it is also used to
spread disinformation, often conveyed through multiple modalities like images
and text. The identification of inconsistent cross-modal information, in
particular entities such as persons, locations, and events, is critical to
detect disinformation. Previous works either identify out-of-context
disinformation by assessing the consistency of images to the whole document,
neglecting relations of individual entities, or focus on generic entities that
are not relevant to news. So far, only few approaches have addressed the task
of validating entity consistency between images and text in news. However, the
potential of large vision-language models (LVLMs) has not been explored yet. In
this paper, we propose an LVLM-based framework for verifying Cross-modal Entity
Consistency~(LVLM4CEC), to assess whether persons, locations and events in news
articles are consistent across both modalities. We suggest effective prompting
strategies for LVLMs for entity verification that leverage reference images
crawled from web. Moreover, we extend three existing datasets for the task of
entity verification in news providing manual ground-truth data. Our results
show the potential of LVLMs for automating cross-modal entity verification,
showing improved accuracy in identifying persons and events when using evidence
images. Moreover, our method outperforms a baseline for location and event
verification in documents. The datasets and source code are available on GitHub
at https://github.com/TIBHannover/LVLM4CEC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in: European Conference on Information
  Retrieval (ECIR) 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-02-15T05:29:07.853421069Z">
            2025-02-15 05:29:07 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
